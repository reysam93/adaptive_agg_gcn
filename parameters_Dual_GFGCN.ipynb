{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fc7111261b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import utils\n",
    "from gsp_utils.baselines_archs import GCNN_2L\n",
    "from gsp_utils.baselines_models import NodeClassModel, GF_NodeClassModel\n",
    "from gsp_utils.data import normalize_gso\n",
    "from src.arch import GFGCN, GFGCNLayer, GFGCN_noh_Layer, GFGCN_Spows, Dual_GFGCN\n",
    "\n",
    "SEED = 15\n",
    "# PATH = 'results/diff_filters/'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "def summary_table(acc, index_name):\n",
    "    mean_accs = acc.mean(axis=1)\n",
    "    med_accs = np.median(acc, axis=1)\n",
    "    std_accs = acc.std(axis=1)\n",
    "    return DataFrame(np.vstack((mean_accs, med_accs, std_accs)).T, columns=['mean accs', 'med', 'std'], index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/.venv/lib/python3.10/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: WisconsinDataset\n",
      "Number of nodes: 251\n",
      "Number of features: 1703\n",
      "Shape of signals: torch.Size([251, 1703])\n",
      "Number of classes: 5\n",
      "Norm of A: 22.69361114501953\n",
      "Max value of A: 1.0\n",
      "Proportion of validation data: 0.32\n",
      "Proportion of test data: 0.20\n",
      "Node homophily: 0.13\n",
      "Edge homophily: 0.20\n"
     ]
    }
   ],
   "source": [
    "# Dataset must be from DGL\n",
    "dataset_name = 'WisconsinDataset'\n",
    "\n",
    "A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device,\n",
    "                                                     verb=True)\n",
    "N = A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without normalizing the GSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PARAMETERS (Alpha: None) -  0.89\n",
    "## Training params\n",
    "N_RUNS = 10\n",
    "N_EPOCHS = 200\n",
    "EPOCHS_h = 50  # 100\n",
    "EPOCHS_W = 25  # 10\n",
    "LR = .005\n",
    "WD = .001\n",
    "DROPOUT = .25\n",
    "ALPHA = .8  # .7 also works\n",
    "\n",
    "# BEST PARAMETERS\n",
    "## Architecture params\n",
    "N_LAYERS = 2\n",
    "K = 3\n",
    "HID_DIM = 32\n",
    "BIAS = True\n",
    "\n",
    "## Model params\n",
    "h0 = 1\n",
    "NORM = False\n",
    "\n",
    "IN_DIM = feat.shape[1]\n",
    "OUT_DIM = n_class\n",
    "\n",
    "ACT = nn.ReLU() \n",
    "LAST_ACT = nn.Softmax(dim=1)\n",
    "LOSS_FN = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# # BEST PARAMETERS (Alpha: None) -  0.88\n",
    "# ## Training params\n",
    "# N_RUNS = 10\n",
    "# N_EPOCHS = 200\n",
    "# EPOCHS_h = 50  # 25\n",
    "# EPOCHS_W = 25  # 25\n",
    "# LR = .005\n",
    "# WD = .001\n",
    "# DROPOUT = .25\n",
    "# ALPHA = None  # .7\n",
    "\n",
    "# # BEST PARAMETERS\n",
    "# ## Architecture params\n",
    "# N_LAYERS = 2\n",
    "# K = 3\n",
    "# HID_DIM = 32  # 64\n",
    "\n",
    "# ## Model params\n",
    "# h0 = 1  # 1\n",
    "# NORM = False\n",
    "\n",
    "# IN_DIM = feat.shape[1]\n",
    "# OUT_DIM = n_class\n",
    "\n",
    "# ACT = nn.ReLU() \n",
    "# LAST_ACT = nn.Softmax(dim=1)\n",
    "# LOSS_FN = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting eval/test acc/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m S \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor(A)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m GF_NodeClassModel(arch, S, K, masks, loss_fn, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m---> 33\u001b[0m loss, acc \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs_h\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs_W\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs_W\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m idx_max_acc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(acc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest val acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00macc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m][idx_max_acc]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  -  Best test acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmax(acc[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Investigacion/robust_minmax_gnn/gsp_utils/baselines_models.py:176\u001b[0m, in \u001b[0;36mGF_NodeClassModel.train\u001b[0;34m(self, X, labels, n_epochs, lr, wd, eval_freq, optim, epochs_h, epochs_W, clamp, patience, verb)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgnn_step(X, labels, opt_W, epochs_W)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Step h\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgnn_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs_h\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clamp:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39march\u001b[38;5;241m.\u001b[39mclamp_h()\n",
      "File \u001b[0;32m~/Investigacion/robust_minmax_gnn/gsp_utils/baselines_models.py:154\u001b[0m, in \u001b[0;36mGF_NodeClassModel.gnn_step\u001b[0;34m(self, X, labels, optim, iters)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iters):\n\u001b[1;32m    153\u001b[0m     optim\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 154\u001b[0m     labels_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43march\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(labels_hat[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_mask], labels[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_mask])\n\u001b[1;32m    156\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Investigacion/robust_minmax_gnn/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Investigacion/robust_minmax_gnn/src/arch.py:194\u001b[0m, in \u001b[0;36mDual_GFGCN.forward\u001b[0;34m(self, S, X)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, S, X):\n\u001b[0;32m--> 194\u001b[0m     y1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43march\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     y2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39march_t(S\u001b[38;5;241m.\u001b[39mT, X)\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha\u001b[38;5;241m*\u001b[39my1 \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha)\u001b[38;5;241m*\u001b[39my2\n",
      "File \u001b[0;32m~/Investigacion/robust_minmax_gnn/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Investigacion/robust_minmax_gnn/src/arch.py:160\u001b[0m, in \u001b[0;36mGFGCN.forward\u001b[0;34m(self, S, X)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, S, X):\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layers \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 160\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm:\n\u001b[1;32m    162\u001b[0m             X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn_layers[i](X)\n",
      "File \u001b[0;32m~/Investigacion/robust_minmax_gnn/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = N_EPOCHS\n",
    "epochs_h = EPOCHS_h\n",
    "epochs_W = EPOCHS_W\n",
    "lr =  LR\n",
    "wd = WD\n",
    "drop = DROPOUT\n",
    "L = N_LAYERS\n",
    "K_aux = K\n",
    "hid_dim = HID_DIM\n",
    "h0_aux = 1\n",
    "norm = False\n",
    "act = ACT\n",
    "lact = LAST_ACT\n",
    "loss_fn = LOSS_FN\n",
    "patience = 200\n",
    "bias = True\n",
    "alpha = None #ALPHA\n",
    "\n",
    "iters_aux = 1\n",
    "\n",
    "err1 = np.zeros(iters_aux)\n",
    "err2 = np.zeros(iters_aux)\n",
    "for i in range(iters_aux):\n",
    "\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "\n",
    "    # Create model\n",
    "    arch = Dual_GFGCN(IN_DIM, hid_dim, OUT_DIM, L, K_aux, alpha=alpha,\n",
    "                       act=act, last_act=lact, dropout=drop, init_h0=h0_aux, bias=bias)\n",
    "    S = torch.Tensor(A).to(device)\n",
    "    model = GF_NodeClassModel(arch, S, K, masks, loss_fn, device=device)\n",
    "    loss, acc = model.train(feat, labels, epochs, lr, wd, epochs_h=epochs_h, epochs_W=epochs_W,\n",
    "                            patience=patience)\n",
    "\n",
    "    idx_max_acc = np.argmax(acc[\"val\"])\n",
    "    print(f'Best val acc: {acc[\"val\"][idx_max_acc]:.3f}  -  Best test acc: {np.max(acc[\"test\"]):.3f}')\n",
    "    print(f'Test acc at best val: {acc[\"test\"][idx_max_acc]:.3f}')\n",
    "\n",
    "    acc_val = model.test(feat, S, labels, masks['val'])\n",
    "    acc_test = model.test(feat, S, labels, masks['test'])\n",
    "    print(f'Val acc: {acc_val:.3f}  -  Test acc: {acc_test:.3f}')\n",
    "\n",
    "    err1[i] = acc[\"test\"][idx_max_acc]\n",
    "    err2[i] = acc_test\n",
    "\n",
    "    print(arch.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1.], device='cuda:1', requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arch.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc at best val acc: 0.863 +- 0.000\n",
      "Acc at test: 0.863 +- 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb541548dc0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'Acc at best val acc: {err1.mean():.3f} +- {err1.std():.3f}')\n",
    "print(f'Acc at test: {err2.mean():.3f} +- {err1.std():.3f}')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=[6,4])\n",
    "plt.plot(acc['train'], 'b-', label='Acc train')\n",
    "plt.plot(acc['val'], 'g-', label='Acc val')\n",
    "plt.plot(acc['test'], 'r-', label='Acc test')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=[6,4])\n",
    "plt.plot(loss['train'], 'b-', label='Loss train')\n",
    "plt.plot(loss['val'], 'g-', label='Loss val')\n",
    "plt.plot(loss['test'], 'r-', label='Loss test')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training paramerters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 200-0.001-0.01-0.25: 0.745 (0.804)\n",
      "-1: 200-0.005-0.0005-0.25: 0.824 (0.863)\n",
      "-1: 200-0.005-0.001-0.25: 0.863 (0.882)\n",
      "-1: 200-0.005-0.005-0.25: 0.804 (0.863)\n",
      "-1: 200-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-1: 200-0.01-0.005-0.25: 0.824 (0.863)\n",
      "-1: 200-0.01-0.01-0.25: 0.882 (0.882)\n",
      "-1: 500-0.005-0.001-0.25: 0.843 (0.902)\n",
      "-1: 750-0.005-0.001-0.25: 0.824 (0.882)\n",
      "-1: 50-0.01-0.001-0.25: 0.804 (0.843)\n",
      "-1: 500-0.01-0.001-0.25: 0.863 (0.882)\n",
      "-1: 750-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-1: 200-0.005-0.001-0.5: 0.843 (0.863)\n",
      "-1: 200-0.005-0.001-0.75: 0.745 (0.843)\n",
      "-1: 200-0.01-0.005-0.5: 0.804 (0.882)\n",
      "-1: 200-0.01-0.005-0.75: 0.804 (0.863)\n",
      "-1: 500-0.005-0.001-0.5: 0.804 (0.882)\n",
      "-1: 750-0.005-0.001-0.5: 0.784 (0.863)\n",
      "-1: 500-0.005-0.0005-0.5: 0.843 (0.882)\n",
      "-1: 500-0.01-0.005-0.5: 0.863 (0.882)\n",
      "-2: 200-0.001-0.01-0.25: 0.882 (0.922)\n",
      "-2: 200-0.005-0.0005-0.25: 0.863 (0.922)\n",
      "-2: 200-0.005-0.001-0.25: 0.863 (0.902)\n",
      "-2: 200-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-2: 200-0.01-0.001-0.25: 0.882 (0.922)\n",
      "-2: 200-0.01-0.005-0.25: 0.863 (0.922)\n",
      "-2: 200-0.01-0.01-0.25: 0.843 (0.902)\n",
      "-2: 500-0.005-0.001-0.25: 0.843 (0.922)\n",
      "-2: 750-0.005-0.001-0.25: 0.882 (0.922)\n",
      "-2: 50-0.01-0.001-0.25: 0.863 (0.902)\n",
      "-2: 500-0.01-0.001-0.25: 0.843 (0.922)\n",
      "-2: 750-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-2: 200-0.005-0.001-0.5: 0.863 (0.882)\n",
      "-2: 200-0.005-0.001-0.75: 0.882 (0.902)\n",
      "-2: 200-0.01-0.005-0.5: 0.863 (0.941)\n",
      "-2: 200-0.01-0.005-0.75: 0.863 (0.902)\n",
      "-2: 500-0.005-0.001-0.5: 0.824 (0.922)\n",
      "-2: 750-0.005-0.001-0.5: 0.824 (0.882)\n",
      "-2: 500-0.005-0.0005-0.5: 0.824 (0.922)\n",
      "-2: 500-0.01-0.005-0.5: 0.863 (0.922)\n",
      "-3: 200-0.001-0.01-0.25: 0.863 (0.902)\n",
      "-3: 200-0.005-0.0005-0.25: 0.863 (0.922)\n",
      "-3: 200-0.005-0.001-0.25: 0.824 (0.922)\n",
      "-3: 200-0.005-0.005-0.25: 0.902 (0.922)\n",
      "-3: 200-0.01-0.001-0.25: 0.863 (0.922)\n",
      "-3: 200-0.01-0.005-0.25: 0.863 (0.922)\n",
      "-3: 200-0.01-0.01-0.25: 0.843 (0.902)\n",
      "-3: 500-0.005-0.001-0.25: 0.902 (0.922)\n",
      "-3: 750-0.005-0.001-0.25: 0.784 (0.922)\n",
      "-3: 50-0.01-0.001-0.25: 0.863 (0.882)\n",
      "-3: 500-0.01-0.001-0.25: 0.902 (0.922)\n",
      "-3: 750-0.01-0.001-0.25: 0.882 (0.961)\n",
      "-3: 200-0.005-0.001-0.5: 0.863 (0.922)\n",
      "-3: 200-0.005-0.001-0.75: 0.863 (0.863)\n",
      "-3: 200-0.01-0.005-0.5: 0.863 (0.922)\n",
      "-3: 200-0.01-0.005-0.75: 0.784 (0.843)\n",
      "-3: 500-0.005-0.001-0.5: 0.882 (0.922)\n",
      "-3: 750-0.005-0.001-0.5: 0.882 (0.922)\n",
      "-3: 500-0.005-0.0005-0.5: 0.863 (0.922)\n",
      "-3: 500-0.01-0.005-0.5: 0.882 (0.922)\n",
      "-4: 200-0.001-0.01-0.25: 0.922 (0.922)\n",
      "-4: 200-0.005-0.0005-0.25: 0.941 (0.941)\n",
      "-4: 200-0.005-0.001-0.25: 0.922 (0.961)\n",
      "-4: 200-0.005-0.005-0.25: 0.941 (0.961)\n",
      "-4: 200-0.01-0.001-0.25: 0.882 (0.941)\n",
      "-4: 200-0.01-0.005-0.25: 0.902 (0.961)\n",
      "-4: 200-0.01-0.01-0.25: 0.922 (0.961)\n",
      "-4: 500-0.005-0.001-0.25: 0.902 (0.980)\n",
      "-4: 750-0.005-0.001-0.25: 0.941 (0.961)\n",
      "-4: 50-0.01-0.001-0.25: 0.824 (0.922)\n",
      "-4: 500-0.01-0.001-0.25: 0.902 (0.961)\n",
      "-4: 750-0.01-0.001-0.25: 0.843 (0.922)\n",
      "-4: 200-0.005-0.001-0.5: 0.902 (0.941)\n",
      "-4: 200-0.005-0.001-0.75: 0.804 (0.882)\n",
      "-4: 200-0.01-0.005-0.5: 0.941 (0.961)\n",
      "-4: 200-0.01-0.005-0.75: 0.882 (0.922)\n",
      "-4: 500-0.005-0.001-0.5: 0.882 (0.922)\n",
      "-4: 750-0.005-0.001-0.5: 0.882 (0.922)\n",
      "-4: 500-0.005-0.0005-0.5: 0.882 (0.922)\n",
      "-4: 500-0.01-0.005-0.5: 0.922 (0.941)\n",
      "-5: 200-0.001-0.01-0.25: 0.843 (0.863)\n",
      "-5: 200-0.005-0.0005-0.25: 0.902 (0.941)\n",
      "-5: 200-0.005-0.001-0.25: 0.863 (0.902)\n",
      "-5: 200-0.005-0.005-0.25: 0.824 (0.863)\n",
      "-5: 200-0.01-0.001-0.25: 0.824 (0.882)\n",
      "-5: 200-0.01-0.005-0.25: 0.902 (0.922)\n",
      "-5: 200-0.01-0.01-0.25: 0.902 (0.941)\n",
      "-5: 500-0.005-0.001-0.25: 0.863 (0.863)\n",
      "-5: 750-0.005-0.001-0.25: 0.882 (0.941)\n",
      "-5: 50-0.01-0.001-0.25: 0.824 (0.882)\n",
      "-5: 500-0.01-0.001-0.25: 0.824 (0.882)\n",
      "-5: 750-0.01-0.001-0.25: 0.941 (0.941)\n",
      "-5: 200-0.005-0.001-0.5: 0.902 (0.941)\n",
      "-5: 200-0.005-0.001-0.75: 0.784 (0.922)\n",
      "-5: 200-0.01-0.005-0.5: 0.863 (0.941)\n",
      "-5: 200-0.01-0.005-0.75: 0.765 (0.843)\n",
      "-5: 500-0.005-0.001-0.5: 0.882 (0.922)\n",
      "-5: 750-0.005-0.001-0.5: 0.804 (0.902)\n",
      "-5: 500-0.005-0.0005-0.5: 0.863 (0.922)\n",
      "-5: 500-0.01-0.005-0.5: 0.882 (0.941)\n",
      "-6: 200-0.001-0.01-0.25: 0.843 (0.863)\n",
      "-6: 200-0.005-0.0005-0.25: 0.843 (0.902)\n",
      "-6: 200-0.005-0.001-0.25: 0.902 (0.902)\n",
      "-6: 200-0.005-0.005-0.25: 0.784 (0.863)\n",
      "-6: 200-0.01-0.001-0.25: 0.843 (0.882)\n",
      "-6: 200-0.01-0.005-0.25: 0.824 (0.902)\n",
      "-6: 200-0.01-0.01-0.25: 0.863 (0.902)\n",
      "-6: 500-0.005-0.001-0.25: 0.882 (0.922)\n",
      "-6: 750-0.005-0.001-0.25: 0.843 (0.902)\n",
      "-6: 50-0.01-0.001-0.25: 0.784 (0.863)\n",
      "-6: 500-0.01-0.001-0.25: 0.863 (0.863)\n",
      "-6: 750-0.01-0.001-0.25: 0.882 (0.902)\n",
      "-6: 200-0.005-0.001-0.5: 0.863 (0.863)\n",
      "-6: 200-0.005-0.001-0.75: 0.784 (0.824)\n",
      "-6: 200-0.01-0.005-0.5: 0.882 (0.882)\n",
      "-6: 200-0.01-0.005-0.75: 0.804 (0.843)\n",
      "-6: 500-0.005-0.001-0.5: 0.824 (0.863)\n",
      "-6: 750-0.005-0.001-0.5: 0.804 (0.882)\n",
      "-6: 500-0.005-0.0005-0.5: 0.824 (0.882)\n",
      "-6: 500-0.01-0.005-0.5: 0.824 (0.863)\n",
      "-7: 200-0.001-0.01-0.25: 0.824 (0.882)\n",
      "-7: 200-0.005-0.0005-0.25: 0.784 (0.902)\n",
      "-7: 200-0.005-0.001-0.25: 0.824 (0.902)\n",
      "-7: 200-0.005-0.005-0.25: 0.843 (0.902)\n",
      "-7: 200-0.01-0.001-0.25: 0.863 (0.882)\n",
      "-7: 200-0.01-0.005-0.25: 0.863 (0.902)\n",
      "-7: 200-0.01-0.01-0.25: 0.843 (0.902)\n",
      "-7: 500-0.005-0.001-0.25: 0.863 (0.902)\n",
      "-7: 750-0.005-0.001-0.25: 0.882 (0.922)\n",
      "-7: 50-0.01-0.001-0.25: 0.824 (0.882)\n",
      "-7: 500-0.01-0.001-0.25: 0.863 (0.922)\n",
      "-7: 750-0.01-0.001-0.25: 0.863 (0.902)\n",
      "-7: 200-0.005-0.001-0.5: 0.843 (0.922)\n",
      "-7: 200-0.005-0.001-0.75: 0.804 (0.824)\n",
      "-7: 200-0.01-0.005-0.5: 0.902 (0.922)\n",
      "-7: 200-0.01-0.005-0.75: 0.843 (0.843)\n",
      "-7: 500-0.005-0.001-0.5: 0.843 (0.902)\n",
      "-7: 750-0.005-0.001-0.5: 0.902 (0.922)\n",
      "-7: 500-0.005-0.0005-0.5: 0.843 (0.902)\n",
      "-7: 500-0.01-0.005-0.5: 0.863 (0.922)\n",
      "-8: 200-0.001-0.01-0.25: 0.745 (0.824)\n",
      "-8: 200-0.005-0.0005-0.25: 0.863 (0.922)\n",
      "-8: 200-0.005-0.001-0.25: 0.843 (0.902)\n",
      "-8: 200-0.005-0.005-0.25: 0.824 (0.882)\n",
      "-8: 200-0.01-0.001-0.25: 0.882 (0.922)\n",
      "-8: 200-0.01-0.005-0.25: 0.784 (0.863)\n",
      "-8: 200-0.01-0.01-0.25: 0.843 (0.882)\n",
      "-8: 500-0.005-0.001-0.25: 0.843 (0.922)\n",
      "-8: 750-0.005-0.001-0.25: 0.784 (0.902)\n",
      "-8: 50-0.01-0.001-0.25: 0.784 (0.804)\n",
      "-8: 500-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-8: 750-0.01-0.001-0.25: 0.824 (0.902)\n",
      "-8: 200-0.005-0.001-0.5: 0.863 (0.882)\n",
      "-8: 200-0.005-0.001-0.75: 0.725 (0.824)\n",
      "-8: 200-0.01-0.005-0.5: 0.725 (0.804)\n",
      "-8: 200-0.01-0.005-0.75: 0.765 (0.784)\n",
      "-8: 500-0.005-0.001-0.5: 0.863 (0.902)\n",
      "-8: 750-0.005-0.001-0.5: 0.863 (0.922)\n",
      "-8: 500-0.005-0.0005-0.5: 0.882 (0.922)\n",
      "-8: 500-0.01-0.005-0.5: 0.745 (0.804)\n",
      "-9: 200-0.001-0.01-0.25: 0.784 (0.784)\n",
      "-9: 200-0.005-0.0005-0.25: 0.784 (0.863)\n",
      "-9: 200-0.005-0.001-0.25: 0.824 (0.863)\n",
      "-9: 200-0.005-0.005-0.25: 0.804 (0.843)\n",
      "-9: 200-0.01-0.001-0.25: 0.863 (0.922)\n",
      "-9: 200-0.01-0.005-0.25: 0.843 (0.902)\n",
      "-9: 200-0.01-0.01-0.25: 0.804 (0.863)\n",
      "-9: 500-0.005-0.001-0.25: 0.824 (0.863)\n",
      "-9: 750-0.005-0.001-0.25: 0.804 (0.863)\n",
      "-9: 50-0.01-0.001-0.25: 0.843 (0.863)\n",
      "-9: 500-0.01-0.001-0.25: 0.804 (0.863)\n",
      "-9: 750-0.01-0.001-0.25: 0.863 (0.882)\n",
      "-9: 200-0.005-0.001-0.5: 0.863 (0.882)\n",
      "-9: 200-0.005-0.001-0.75: 0.784 (0.863)\n",
      "-9: 200-0.01-0.005-0.5: 0.784 (0.863)\n",
      "-9: 200-0.01-0.005-0.75: 0.824 (0.863)\n",
      "-9: 500-0.005-0.001-0.5: 0.843 (0.882)\n",
      "-9: 750-0.005-0.001-0.5: 0.804 (0.863)\n",
      "-9: 500-0.005-0.0005-0.5: 0.804 (0.882)\n",
      "-9: 500-0.01-0.005-0.5: 0.804 (0.863)\n",
      "-10: 200-0.001-0.01-0.25: 0.824 (0.843)\n",
      "-10: 200-0.005-0.0005-0.25: 0.863 (0.882)\n",
      "-10: 200-0.005-0.001-0.25: 0.863 (0.882)\n",
      "-10: 200-0.005-0.005-0.25: 0.882 (0.902)\n",
      "-10: 200-0.01-0.001-0.25: 0.804 (0.863)\n",
      "-10: 200-0.01-0.005-0.25: 0.843 (0.902)\n",
      "-10: 200-0.01-0.01-0.25: 0.843 (0.863)\n",
      "-10: 500-0.005-0.001-0.25: 0.804 (0.882)\n",
      "-10: 750-0.005-0.001-0.25: 0.902 (0.941)\n",
      "-10: 50-0.01-0.001-0.25: 0.843 (0.843)\n",
      "-10: 500-0.01-0.001-0.25: 0.824 (0.882)\n",
      "-10: 750-0.01-0.001-0.25: 0.843 (0.863)\n",
      "-10: 200-0.005-0.001-0.5: 0.824 (0.882)\n",
      "-10: 200-0.005-0.001-0.75: 0.765 (0.804)\n",
      "-10: 200-0.01-0.005-0.5: 0.863 (0.863)\n",
      "-10: 200-0.01-0.005-0.75: 0.784 (0.804)\n",
      "-10: 500-0.005-0.001-0.5: 0.863 (0.882)\n",
      "-10: 750-0.005-0.001-0.5: 0.863 (0.882)\n",
      "-10: 500-0.005-0.0005-0.5: 0.922 (0.922)\n",
      "-10: 500-0.01-0.005-0.5: 0.804 (0.863)\n",
      "----- 168.42 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [\n",
    "        # {'epochs': 200, 'lr': .001, 'wd': 1e-4, 'drop': .25},\n",
    "        # {'epochs': 200, 'lr': .001, 'wd': 5e-4, 'drop': .25},\n",
    "        # {'epochs': 200, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "        # {'epochs': 200, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-2, 'drop': .25},\n",
    "\n",
    "        # {'epochs': 200, 'lr': .005, 'wd': 1e-4, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-4, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .25}, # .866\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "\n",
    "        # {'epochs': 200, 'lr': .01, 'wd': 1e-4, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-2, 'drop': .25},\n",
    "\n",
    "        # {'epochs': 10, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "        # {'epochs': 50, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 750, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        # {'epochs': 10, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 50, 'lr': .01, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .01, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 750, 'lr': .01, 'wd': 1e-3, 'drop': .25},\n",
    "\n",
    "        # {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .5}, \n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .75},\n",
    "        # {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .75},\n",
    "\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .5},  # .87\n",
    "        {'epochs': 750, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 5e-4, 'drop': .5},\n",
    "        {'epochs': 500, 'lr': .01, 'wd': 5e-3, 'drop': .5},\n",
    "        ]\n",
    "\n",
    "best_accs1 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs1 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = Dual_GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, alpha=ALPHA,\n",
    "                          last_act=LAST_ACT, dropout=exp['drop'], init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S, K, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, exp['epochs'], exp['lr'], exp['wd'],\n",
    "                             epochs_h=EPOCHS_h, epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs1[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs1[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}: {best_val_accs1[j,i]:.3f} ({best_accs1[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}' for exp in EXPS]\n",
    "table_over1 = summary_table(best_accs1, index_name)\n",
    "table1 = summary_table(best_val_accs1, index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.01-0.25</th>\n",
       "      <td>0.827451</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.053913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0005-0.25</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.045775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.25</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.031373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.005-0.25</th>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.047059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.25</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.025110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.25</th>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.034187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.01-0.25</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.032575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0.25</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.030440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750-0.005-0.001-0.25</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.050564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50-0.01-0.001-0.25</th>\n",
       "      <td>0.825490</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.026956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.01-0.001-0.25</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.030691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750-0.01-0.001-0.25</th>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.031617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.5</th>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.023200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.75</th>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.045775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.5</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.058856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.75</th>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.038423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0.5</th>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.026597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750-0.005-0.001-0.5</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.039654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.0005-0.5</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.033044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.01-0.005-0.5</th>\n",
       "      <td>0.845098</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.048388</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean accs       med       std\n",
       "200-0.001-0.01-0.25     0.827451  0.833333  0.053913\n",
       "200-0.005-0.0005-0.25   0.852941  0.862745  0.045775\n",
       "200-0.005-0.001-0.25    0.858824  0.862745  0.031373\n",
       "200-0.005-0.005-0.25    0.847059  0.833333  0.047059\n",
       "200-0.01-0.001-0.25     0.854902  0.862745  0.025110\n",
       "200-0.01-0.005-0.25     0.850980  0.852941  0.034187\n",
       "200-0.01-0.01-0.25      0.858824  0.843137  0.032575\n",
       "500-0.005-0.001-0.25    0.856863  0.852941  0.030440\n",
       "750-0.005-0.001-0.25    0.852941  0.862745  0.050564\n",
       "50-0.01-0.001-0.25      0.825490  0.823529  0.026956\n",
       "500-0.01-0.001-0.25     0.852941  0.852941  0.030691\n",
       "750-0.01-0.001-0.25     0.862745  0.852941  0.031617\n",
       "200-0.005-0.001-0.5     0.862745  0.862745  0.023200\n",
       "200-0.005-0.001-0.75    0.794118  0.784314  0.045775\n",
       "200-0.01-0.005-0.5      0.849020  0.862745  0.058856\n",
       "200-0.01-0.005-0.75     0.811765  0.803922  0.038423\n",
       "500-0.005-0.001-0.5     0.850980  0.852941  0.026597\n",
       "750-0.005-0.001-0.5     0.841176  0.843137  0.039654\n",
       "500-0.005-0.0005-0.5    0.854902  0.852941  0.033044\n",
       "500-0.01-0.005-0.5      0.845098  0.862745  0.048388"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 1-200-10-5-True: 0.863 (0.863)\n",
      "-1: 1-200-25-5-True: 0.824 (0.843)\n",
      "-1: 1-200-50-5-True: 0.863 (0.882)\n",
      "-1: 1-200-25-10-True: 0.824 (0.863)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/srey/Investigacion/robust_minmax_gnn/parameters_Dual_GFGCN.ipynb Cell 15\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srey/Investigacion/robust_minmax_gnn/parameters_Dual_GFGCN.ipynb#X20sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srey/Investigacion/robust_minmax_gnn/parameters_Dual_GFGCN.ipynb#X20sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     model \u001b[39m=\u001b[39m GF_NodeClassModel(arch, S,  K, masks, LOSS_FN, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/srey/Investigacion/robust_minmax_gnn/parameters_Dual_GFGCN.ipynb#X20sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m     _, acc \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain(feat, labels, exp[\u001b[39m'\u001b[39;49m\u001b[39mepochs\u001b[39;49m\u001b[39m'\u001b[39;49m], LR, WD, epochs_h\u001b[39m=\u001b[39;49mexp[\u001b[39m'\u001b[39;49m\u001b[39mepochs_h\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srey/Investigacion/robust_minmax_gnn/parameters_Dual_GFGCN.ipynb#X20sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m                          epochs_W\u001b[39m=\u001b[39;49mexp[\u001b[39m'\u001b[39;49m\u001b[39mepochs_W\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srey/Investigacion/robust_minmax_gnn/parameters_Dual_GFGCN.ipynb#X20sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m best_accs2[j,i] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmax(acc[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/srey/Investigacion/robust_minmax_gnn/parameters_Dual_GFGCN.ipynb#X20sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m best_val_accs2[j,i] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtest(feat, S, labels, masks[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/Investigacion/robust_minmax_gnn/gsp_utils/baselines_models.py:176\u001b[0m, in \u001b[0;36mGF_NodeClassModel.train\u001b[0;34m(self, X, labels, n_epochs, lr, wd, eval_freq, optim, epochs_h, epochs_W, clamp, patience, verb)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgnn_step(X, labels, opt_W, epochs_W)\n\u001b[1;32m    175\u001b[0m \u001b[39m# Step h\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgnn_step(X, labels, opt_h, epochs_h)\n\u001b[1;32m    178\u001b[0m \u001b[39mif\u001b[39;00m clamp:\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39march\u001b[39m.\u001b[39mclamp_h()\n",
      "File \u001b[0;32m~/Investigacion/robust_minmax_gnn/gsp_utils/baselines_models.py:156\u001b[0m, in \u001b[0;36mGF_NodeClassModel.gnn_step\u001b[0;34m(self, X, labels, optim, iters)\u001b[0m\n\u001b[1;32m    154\u001b[0m labels_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39march(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mS, X)\n\u001b[1;32m    155\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn(labels_hat[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_mask], labels[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_mask])\n\u001b[0;32m--> 156\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    157\u001b[0m optim\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EXPS = [\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        # {'h0': 1, 'epochs': 1000, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "                \n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 2000, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 1, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 1, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 50, 'epochs_W': 1, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 10, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 25, 'alt': True},\n",
    "\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 5, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 5, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 50, 'epochs_W': 5, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 50, 'epochs_W': 25, 'alt': True},  # .88!\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 75, 'epochs_W': 25, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 5, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 5, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 5, 'epochs_W': 50, 'alt': True},   # .88!\n",
    "        \n",
    "        # {'h0': .1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "\n",
    "        {'h0': 1, 'epochs': 50, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 50, 'epochs_h': 50, 'epochs_W': 25, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 100, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 100, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 100, 'epochs_h': 50, 'epochs_W': 25, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 100, 'epochs_h': 50, 'epochs_W': 50, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},  # .87\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 50, 'epochs_W': 50, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 500, 'epochs_h': 50, 'epochs_W': 25, 'alt': True},\n",
    "    ]\n",
    "\n",
    "\n",
    "best_accs2 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs2 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = Dual_GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, alpha=ALPHA,\n",
    "                          last_act=LAST_ACT, dropout=DROPOUT, init_h0=exp['h0'])\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        if not exp['alt']:\n",
    "            model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "            _, acc = model.train(feat, labels, exp['epochs'], LR, WD)\n",
    "        else:\n",
    "            model = GF_NodeClassModel(arch, S,  K, masks, LOSS_FN, device=device)\n",
    "            _, acc = model.train(feat, labels, exp['epochs'], LR, WD, epochs_h=exp['epochs_h'],\n",
    "                                 epochs_W=exp['epochs_W'])\n",
    "\n",
    "        best_accs2[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs2[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"h0\"]}-{exp[\"epochs\"]}-{exp[\"epochs_h\"]}-{exp[\"epochs_W\"]}-{exp[\"alt\"]}: {best_val_accs2[j,i]:.3f} ({best_accs2[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"h0\"]}-{exp[\"epochs_h\"]}-{exp[\"epochs_W\"]}-{exp[\"alt\"]}' for exp in EXPS]\n",
    "table_over2 = summary_table(best_accs2, index_name)\n",
    "table2 = summary_table(best_val_accs2, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-1-10-5-True</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.053519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-25-5-True</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.036420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-50-5-True</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.045098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-25-10-True</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.046566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-50-25-True</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.045098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-75-25-True</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.042054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-5-25-True</th>\n",
       "      <td>0.798039</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.107772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-5-50-True</th>\n",
       "      <td>0.831373</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.132755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50-1-25-25-True</th>\n",
       "      <td>0.825490</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.056456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50-1-50-25-True</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.048980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100-1-25-25-True</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.043888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100-1-50-25-True</th>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.034856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-10-True</th>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.032575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-25-25-True</th>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.040375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-1-50-25-True</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.040423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean accs       med       std\n",
       "200-1-10-5-True    0.833333  0.852941  0.053519\n",
       "200-1-25-5-True    0.852941  0.862745  0.036420\n",
       "200-1-50-5-True    0.841176  0.852941  0.045098\n",
       "200-1-25-10-True   0.835294  0.852941  0.046566\n",
       "200-1-50-25-True   0.860784  0.843137  0.045098\n",
       "200-1-75-25-True   0.843137  0.843137  0.042054\n",
       "200-1-5-25-True    0.798039  0.823529  0.107772\n",
       "200-1-5-50-True    0.831373  0.862745  0.132755\n",
       "50-1-25-25-True    0.825490  0.813725  0.056456\n",
       "50-1-50-25-True    0.835294  0.852941  0.048980\n",
       "100-1-25-25-True   0.837255  0.843137  0.043888\n",
       "100-1-50-25-True   0.847059  0.862745  0.034856\n",
       "200-1-10-10-True   0.847059  0.843137  0.032575\n",
       "200-1-25-25-True   0.850980  0.862745  0.040375\n",
       "500-1-50-25-True   0.852941  0.872549  0.040423"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 0.25: 0.824 (0.882)\n",
      "-1: 0.5: 0.843 (0.863)\n",
      "-1: 0.6: 0.784 (0.902)\n",
      "-1: 0.7: 0.804 (0.863)\n",
      "-1: 0.8: 0.882 (0.882)\n",
      "-1: 1: 0.882 (0.882)\n",
      "-1: None: 0.843 (0.882)\n",
      "-2: 0.25: 0.824 (0.922)\n",
      "-2: 0.5: 0.922 (0.922)\n",
      "-2: 0.6: 0.824 (0.941)\n",
      "-2: 0.7: 0.882 (0.902)\n",
      "-2: 0.8: 0.922 (0.941)\n",
      "-2: 1: 0.804 (0.941)\n",
      "-2: None: 0.824 (0.922)\n",
      "-3: 0.25: 0.843 (0.961)\n",
      "-3: 0.5: 0.941 (0.941)\n",
      "-3: 0.6: 0.863 (0.941)\n",
      "-3: 0.7: 0.882 (0.941)\n",
      "-3: 0.8: 0.882 (0.922)\n",
      "-3: 1: 0.843 (0.922)\n",
      "-3: None: 0.843 (0.922)\n",
      "-4: 0.25: 0.863 (0.941)\n",
      "-4: 0.5: 0.922 (0.961)\n",
      "-4: 0.6: 0.941 (0.961)\n",
      "-4: 0.7: 0.941 (0.961)\n",
      "-4: 0.8: 0.922 (0.961)\n",
      "-4: 1: 0.863 (0.941)\n",
      "-4: None: 0.863 (0.941)\n",
      "-5: 0.25: 0.902 (0.941)\n",
      "-5: 0.5: 0.882 (0.922)\n",
      "-5: 0.6: 0.902 (0.941)\n",
      "-5: 0.7: 0.824 (0.922)\n",
      "-5: 0.8: 0.902 (0.922)\n",
      "-5: 1: 0.941 (0.941)\n",
      "-5: None: 0.882 (0.941)\n",
      "-6: 0.25: 0.824 (0.882)\n",
      "-6: 0.5: 0.882 (0.902)\n",
      "-6: 0.6: 0.863 (0.922)\n",
      "-6: 0.7: 0.902 (0.922)\n",
      "-6: 0.8: 0.863 (0.922)\n",
      "-6: 1: 0.863 (0.902)\n",
      "-6: None: 0.824 (0.902)\n",
      "-7: 0.25: 0.824 (0.882)\n",
      "-7: 0.5: 0.882 (0.902)\n",
      "-7: 0.6: 0.902 (0.902)\n",
      "-7: 0.7: 0.863 (0.922)\n",
      "-7: 0.8: 0.902 (0.902)\n",
      "-7: 1: 0.843 (0.922)\n",
      "-7: None: 0.882 (0.902)\n",
      "-8: 0.25: 0.804 (0.922)\n",
      "-8: 0.5: 0.804 (0.902)\n",
      "-8: 0.6: 0.843 (0.902)\n",
      "-8: 0.7: 0.843 (0.922)\n",
      "-8: 0.8: 0.863 (0.902)\n",
      "-8: 1: 0.824 (0.902)\n",
      "-8: None: 0.843 (0.902)\n",
      "-9: 0.25: 0.804 (0.863)\n",
      "-9: 0.5: 0.863 (0.902)\n",
      "-9: 0.6: 0.804 (0.902)\n",
      "-9: 0.7: 0.882 (0.902)\n",
      "-9: 0.8: 0.843 (0.902)\n",
      "-9: 1: 0.863 (0.902)\n",
      "-9: None: 0.824 (0.882)\n",
      "-10: 0.25: 0.843 (0.863)\n",
      "-10: 0.5: 0.882 (0.922)\n",
      "-10: 0.6: 0.902 (0.922)\n",
      "-10: 0.7: 0.902 (0.941)\n",
      "-10: 0.8: 0.922 (0.922)\n",
      "-10: 1: 0.922 (0.941)\n",
      "-10: None: 0.824 (0.863)\n",
      "----- 44.65 mins -----\n"
     ]
    }
   ],
   "source": [
    "# h0, norm (norm, not norm, norm H), sep vs joint training, optimizers iters (W vs h)\n",
    "EXPS = [\n",
    "        # {'alpha': 0},\n",
    "        {'alpha': .25},\n",
    "        # {'alpha': .4},\n",
    "        {'alpha': .5},\n",
    "        {'alpha': .6},\n",
    "        {'alpha': .7},  # .87\n",
    "        {'alpha': .8},  # .87\n",
    "        {'alpha': 1},\n",
    "        {'alpha': None},\n",
    "    ]\n",
    "\n",
    "\n",
    "best_accs2b = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs2b = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = Dual_GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, alpha=exp['alpha'],\n",
    "                          last_act=LAST_ACT, dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  K, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs2b[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs2b[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"alpha\"]}: {best_val_accs2b[j,i]:.3f} ({best_accs2b[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"alpha\"]}' for exp in EXPS]\n",
    "table_over2b = summary_table(best_accs2b, index_name)\n",
    "table2b = summary_table(best_val_accs2b, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.028006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.038223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.6</th>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.047222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.7</th>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.038473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>0.890196</td>\n",
       "      <td>0.892157</td>\n",
       "      <td>0.026597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.864706</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.039654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>None</th>\n",
       "      <td>0.845098</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.022270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean accs       med       std\n",
       "0.25   0.835294  0.823529  0.028006\n",
       "0.5    0.882353  0.882353  0.038223\n",
       "0.6    0.862745  0.862745  0.047222\n",
       "0.7    0.872549  0.882353  0.038473\n",
       "0.8    0.890196  0.892157  0.026597\n",
       "1      0.864706  0.862745  0.039654\n",
       "None   0.845098  0.843137  0.022270"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design - Bias True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 2-2-16: 0.824 (0.902)\n",
      "-1: 2-2-32: 0.863 (0.882)\n",
      "-1: 2-2-64: 0.824 (0.882)\n",
      "-1: 2-3-16: 0.765 (0.843)\n",
      "-1: 2-3-32: 0.863 (0.882)\n",
      "-1: 2-3-64: 0.843 (0.882)\n",
      "-1: 2-3-128: 0.824 (0.843)\n",
      "-1: 3-2-16: 0.569 (0.647)\n",
      "-1: 3-2-32: 0.863 (0.882)\n",
      "-1: 3-2-64: 0.824 (0.863)\n",
      "-2: 2-2-16: 0.902 (0.902)\n",
      "-2: 2-2-32: 0.824 (0.922)\n",
      "-2: 2-2-64: 0.863 (0.922)\n",
      "-2: 2-3-16: 0.804 (0.902)\n",
      "-2: 2-3-32: 0.902 (0.922)\n",
      "-2: 2-3-64: 0.843 (0.922)\n",
      "-2: 2-3-128: 0.863 (0.922)\n",
      "-2: 3-2-16: 0.863 (0.922)\n",
      "-2: 3-2-32: 0.882 (0.902)\n",
      "-2: 3-2-64: 0.863 (0.922)\n",
      "-3: 2-2-16: 0.706 (0.784)\n",
      "-3: 2-2-32: 0.863 (0.941)\n",
      "-3: 2-2-64: 0.843 (0.941)\n",
      "-3: 2-3-16: 0.804 (0.843)\n",
      "-3: 2-3-32: 0.843 (0.941)\n",
      "-3: 2-3-64: 0.863 (0.941)\n",
      "-3: 2-3-128: 0.843 (0.922)\n",
      "-3: 3-2-16: 0.824 (0.843)\n",
      "-3: 3-2-32: 0.902 (0.922)\n",
      "-3: 3-2-64: 0.882 (0.941)\n",
      "-4: 2-2-16: 0.784 (0.804)\n",
      "-4: 2-2-32: 0.882 (0.941)\n",
      "-4: 2-2-64: 0.882 (0.922)\n",
      "-4: 2-3-16: 0.863 (0.902)\n",
      "-4: 2-3-32: 0.902 (0.941)\n",
      "-4: 2-3-64: 0.902 (0.941)\n",
      "-4: 2-3-128: 0.863 (0.922)\n",
      "-4: 3-2-16: 0.510 (0.588)\n",
      "-4: 3-2-32: 0.922 (0.961)\n",
      "-4: 3-2-64: 0.863 (0.941)\n",
      "-5: 2-2-16: 0.824 (0.863)\n",
      "-5: 2-2-32: 0.863 (0.922)\n",
      "-5: 2-2-64: 0.863 (0.902)\n",
      "-5: 2-3-16: 0.843 (0.902)\n",
      "-5: 2-3-32: 0.882 (0.922)\n",
      "-5: 2-3-64: 0.882 (0.941)\n",
      "-5: 2-3-128: 0.804 (0.922)\n",
      "-5: 3-2-16: 0.804 (0.902)\n",
      "-5: 3-2-32: 0.882 (0.922)\n",
      "-5: 3-2-64: 0.922 (0.961)\n",
      "-6: 2-2-16: 0.804 (0.863)\n",
      "-6: 2-2-32: 0.824 (0.902)\n",
      "-6: 2-2-64: 0.843 (0.882)\n",
      "-6: 2-3-16: 0.804 (0.824)\n",
      "-6: 2-3-32: 0.843 (0.863)\n",
      "-6: 2-3-64: 0.863 (0.882)\n",
      "-6: 2-3-128: 0.804 (0.882)\n",
      "-6: 3-2-16: 0.804 (0.804)\n",
      "-6: 3-2-32: 0.882 (0.902)\n",
      "-6: 3-2-64: 0.843 (0.882)\n",
      "-7: 2-2-16: 0.824 (0.922)\n",
      "-7: 2-2-32: 0.863 (0.922)\n",
      "-7: 2-2-64: 0.882 (0.922)\n",
      "-7: 2-3-16: 0.843 (0.941)\n",
      "-7: 2-3-32: 0.804 (0.902)\n",
      "-7: 2-3-64: 0.863 (0.882)\n",
      "-7: 2-3-128: 0.843 (0.902)\n",
      "-7: 3-2-16: 0.588 (0.608)\n",
      "-7: 3-2-32: 0.843 (0.902)\n",
      "-7: 3-2-64: 0.804 (0.922)\n",
      "-8: 2-2-16: 0.745 (0.863)\n",
      "-8: 2-2-32: 0.882 (0.922)\n",
      "-8: 2-2-64: 0.843 (0.922)\n",
      "-8: 2-3-16: 0.784 (0.863)\n",
      "-8: 2-3-32: 0.882 (0.922)\n",
      "-8: 2-3-64: 0.843 (0.902)\n",
      "-8: 2-3-128: 0.843 (0.863)\n",
      "-8: 3-2-16: 0.706 (0.765)\n",
      "-8: 3-2-32: 0.843 (0.902)\n",
      "-8: 3-2-64: 0.863 (0.902)\n",
      "-9: 2-2-16: 0.804 (0.843)\n",
      "-9: 2-2-32: 0.824 (0.902)\n",
      "-9: 2-2-64: 0.824 (0.882)\n",
      "-9: 2-3-16: 0.804 (0.863)\n",
      "-9: 2-3-32: 0.863 (0.902)\n",
      "-9: 2-3-64: 0.804 (0.863)\n",
      "-9: 2-3-128: 0.863 (0.922)\n",
      "-9: 3-2-16: 0.765 (0.843)\n",
      "-9: 3-2-32: 0.824 (0.882)\n",
      "-9: 3-2-64: 0.843 (0.882)\n",
      "-10: 2-2-16: 0.882 (0.882)\n",
      "-10: 2-2-32: 0.882 (0.922)\n",
      "-10: 2-2-64: 0.863 (0.902)\n",
      "-10: 2-3-16: 0.745 (0.863)\n",
      "-10: 2-3-32: 0.804 (0.882)\n",
      "-10: 2-3-64: 0.922 (0.922)\n",
      "-10: 2-3-128: 0.922 (0.922)\n",
      "-10: 3-2-16: 0.902 (0.902)\n",
      "-10: 3-2-32: 0.863 (0.902)\n",
      "-10: 3-2-64: 0.804 (0.863)\n",
      "----- 56.83 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'L': 2, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 64},\n",
    "\n",
    "        # {'L': 2, 'K': 3, 'hid_dim': 8},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 64},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 128},\n",
    "\n",
    "        # {'L': 2, 'K': 4, 'hid_dim': 16},\n",
    "\n",
    "        # {'L': 3, 'K': 2, 'hid_dim': 8},\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 64},\n",
    "\n",
    "        # {'L': 4, 'K': 2, 'hid_dim': 16},\n",
    "        # {'L': 3, 'K': 3, 'hid_dim': 16},\n",
    "        ]\n",
    "\n",
    "best_accs3 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs3 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = Dual_GFGCN(IN_DIM, exp['hid_dim'], OUT_DIM, exp['L'], exp['K'], act=ACT, alpha=ALPHA,\n",
    "                          last_act=LAST_ACT, dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  exp['K'], masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs3[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs3[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}: {best_val_accs3[j,i]:.3f} ({best_accs3[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}' for exp in EXPS]\n",
    "table_over3 = summary_table(best_accs3, index_name)\n",
    "table3 = summary_table(best_val_accs3, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.809804</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.054797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.023283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-64</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.020092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.805882</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.034467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.033735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-64</th>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.031617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-128</th>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.032575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.127737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-32</th>\n",
       "      <td>0.870588</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.028006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-64</th>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.034187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean accs       med       std\n",
       "2-2-16    0.809804  0.813725  0.054797\n",
       "2-2-32    0.856863  0.862745  0.023283\n",
       "2-2-64    0.852941  0.852941  0.020092\n",
       "2-3-16    0.805882  0.803922  0.034467\n",
       "2-3-32    0.858824  0.862745  0.033735\n",
       "2-3-64    0.862745  0.862745  0.031617\n",
       "2-3-128   0.847059  0.843137  0.032575\n",
       "3-2-16    0.733333  0.784314  0.127737\n",
       "3-2-32    0.870588  0.872549  0.028006\n",
       "3-2-64    0.850980  0.852941  0.034187"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSO, bias, and batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1: orig-True-False: 0.843 (0.863)\n",
      "- 1: orig-True-True: 0.569 (0.843)\n",
      "- 1: orig-False-False: 0.824 (0.882)\n",
      "- 2: orig-True-False: 0.863 (0.882)\n",
      "- 2: orig-True-True: 0.745 (0.784)\n",
      "- 2: orig-False-False: 0.843 (0.882)\n",
      "- 3: orig-True-False: 0.863 (0.902)\n",
      "- 3: orig-True-True: 0.804 (0.863)\n",
      "- 3: orig-False-False: 0.882 (0.922)\n",
      "- 4: orig-True-False: 0.882 (0.961)\n",
      "- 4: orig-True-True: 0.863 (0.941)\n",
      "- 4: orig-False-False: 0.941 (0.961)\n",
      "- 5: orig-True-False: 0.902 (0.922)\n",
      "- 5: orig-True-True: 0.824 (0.863)\n",
      "- 5: orig-False-False: 0.843 (0.922)\n",
      "- 6: orig-True-False: 0.843 (0.882)\n",
      "- 6: orig-True-True: 0.804 (0.843)\n",
      "- 6: orig-False-False: 0.882 (0.882)\n",
      "- 7: orig-True-False: 0.882 (0.882)\n",
      "- 7: orig-True-True: 0.824 (0.882)\n",
      "- 7: orig-False-False: 0.843 (0.863)\n",
      "- 8: orig-True-False: 0.784 (0.882)\n",
      "- 8: orig-True-True: 0.824 (0.882)\n",
      "- 8: orig-False-False: 0.843 (0.902)\n",
      "- 9: orig-True-False: 0.784 (0.863)\n",
      "- 9: orig-True-True: 0.882 (0.882)\n",
      "- 9: orig-False-False: 0.843 (0.863)\n",
      "- 10: orig-True-False: 0.863 (0.902)\n",
      "- 10: orig-True-True: 0.863 (0.902)\n",
      "- 10: orig-False-False: 0.922 (0.922)\n",
      "----- 7.41 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [{'GSO': 'orig', 'bias': True, 'bn': False},\n",
    "        {'GSO': 'orig', 'bias': True, 'bn': True},\n",
    "        {'GSO': 'orig', 'bias': False, 'bn': True},\n",
    "        {'GSO': 'orig', 'bias': False, 'bn': False},\n",
    "        ]\n",
    "\n",
    "best_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = Dual_GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, alpha=ALPHA, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0, bias=exp['bias'], batch_norm=exp['bn'])\n",
    "        \n",
    "        if exp['GSO'] == 'sym':\n",
    "            A_aux = (A + A.T)/2\n",
    "            A_aux = np.where(A_aux > 0, 1, 0)\n",
    "        elif exp['GSO'] == 'trans':\n",
    "            A_aux = A.T\n",
    "        else:\n",
    "            A_aux = A\n",
    "\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A_aux, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A_aux).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  K, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs3b[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs3b[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'- {i+1}: {exp[\"GSO\"]}-{exp[\"bias\"]}-{exp[\"bn\"]}: {best_val_accs3b[j,i]:.3f} ({best_accs3b[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"GSO\"]}-{exp[\"bias\"]}-{exp[\"bn\"]}' for exp in EXPS]\n",
    "table_over3b = summary_table(best_accs3b, index_name)\n",
    "table3b = summary_table(best_val_accs3b, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>orig-True-False</th>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.037409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig-True-True</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.085379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig-False-False</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.036996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean accs       med       std\n",
       "orig-True-False    0.850980  0.862745  0.037409\n",
       "orig-True-True     0.800000  0.823529  0.085379\n",
       "orig-False-False   0.866667  0.843137  0.036996"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearities and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.843)\n",
      "-1: ReLU()-Softmax(dim=1)-NLLLoss(): 0.804 (0.863)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.804)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.863)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.863)\n",
      "-1: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.863)\n",
      "-1: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.843)\n",
      "-2: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-2: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.882)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.863)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.882 (0.882)\n",
      "-2: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-2: Identity()-Softmax(dim=1)-NLLLoss(): 0.863 (0.882)\n",
      "-3: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-3: ReLU()-Softmax(dim=1)-NLLLoss(): 0.824 (0.902)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.863)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.941)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-3: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.902)\n",
      "-3: Identity()-Softmax(dim=1)-NLLLoss(): 0.804 (0.804)\n",
      "-4: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.922 (0.941)\n",
      "-4: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.922)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.961)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.882)\n",
      "-4: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-4: Identity()-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-5: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-5: ReLU()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.882)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.941)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.824 (0.882)\n",
      "-5: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.863)\n",
      "-5: Identity()-Softmax(dim=1)-NLLLoss(): 0.784 (0.824)\n",
      "-6: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-6: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.902)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.804 (0.902)\n",
      "-6: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.843)\n",
      "-6: Identity()-Softmax(dim=1)-NLLLoss(): 0.725 (0.863)\n",
      "-7: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-7: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.882)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.725 (0.843)\n",
      "-7: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.882)\n",
      "-7: Identity()-Softmax(dim=1)-NLLLoss(): 0.902 (0.902)\n",
      "-8: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-8: ReLU()-Softmax(dim=1)-NLLLoss(): 0.765 (0.824)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.824)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.863)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-8: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-8: Identity()-Softmax(dim=1)-NLLLoss(): 0.804 (0.863)\n",
      "-9: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.882)\n",
      "-9: ReLU()-Softmax(dim=1)-NLLLoss(): 0.784 (0.843)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.863)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.804 (0.863)\n",
      "-9: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-9: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.882)\n",
      "-10: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-10: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.902)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.824 (0.882)\n",
      "-10: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-10: Identity()-Softmax(dim=1)-NLLLoss(): 0.765 (0.824)\n",
      "----- 19.22 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        # {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        # {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        # {'act': nn.LeakyReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        # {'act': nn.Identity(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        ]\n",
    "\n",
    "best_accs4 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs4 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = Dual_GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, alpha=ALPHA, act=exp['act'],\n",
    "                          last_act=exp['lact'], dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  K, masks, exp['loss'], device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs4[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs4[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}: {best_val_accs4[j,i]:.3f} ({best_accs4[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}' for exp in EXPS]\n",
    "table_over4 = summary_table(best_accs4, index_name)\n",
    "table4 = summary_table(best_val_accs4, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.026380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.036420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.039460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.030941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.827451</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.042779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.839216</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.031373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.819608</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.050221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean accs       med  \\\n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()             0.868627  0.872549   \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                      0.833333  0.843137   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()     0.833333  0.823529   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...   0.860784  0.862745   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...   0.827451  0.833333   \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()         0.839216  0.833333   \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                  0.819608  0.823529   \n",
       "\n",
       "                                                         std  \n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()            0.026380  \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                     0.036420  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()    0.039460  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...  0.030941  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...  0.042779  \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()        0.031373  \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                 0.050221  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the GSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PARAMETERS  - .87\n",
    "## Training params\n",
    "N_RUNS = 10\n",
    "N_EPOCHS = 200\n",
    "EPOCHS_h = 25  # 10\n",
    "EPOCHS_W = 25  # 10\n",
    "LR = .005  # .01\n",
    "WD = .005\n",
    "DROPOUT = .25\n",
    "ALPHA = .8  # .7\n",
    "\n",
    "# BEST PARAMETERS\n",
    "## Architecture params\n",
    "N_LAYERS = 3\n",
    "K = 2\n",
    "HID_DIM = 32\n",
    "\n",
    "## Model params\n",
    "h0 = .1  # 1\n",
    "NORM = True\n",
    "\n",
    "IN_DIM = feat.shape[1]\n",
    "OUT_DIM = n_class\n",
    "\n",
    "ACT = nn.ReLU() \n",
    "LAST_ACT = nn.Softmax(dim=1)\n",
    "LOSS_FN = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# # BEST PARAMETERS  - \n",
    "# ## Training params\n",
    "# N_RUNS = 10\n",
    "# N_EPOCHS = 200\n",
    "# EPOCHS_h = 10  # 25\n",
    "# EPOCHS_W = 10  # 25\n",
    "# LR = .005\n",
    "# WD = .001\n",
    "# DROPOUT = .25\n",
    "# ALPHA = None  # .7\n",
    "\n",
    "# # BEST PARAMETERS\n",
    "# ## Architecture params\n",
    "# N_LAYERS = 2\n",
    "# K = 3\n",
    "# HID_DIM = 32  # 64\n",
    "\n",
    "# ## Model params\n",
    "# h0 = 1  # 1\n",
    "# NORM = True\n",
    "\n",
    "# IN_DIM = feat.shape[1]\n",
    "# OUT_DIM = n_class\n",
    "\n",
    "# ACT = nn.ReLU() \n",
    "# LAST_ACT = nn.Softmax(dim=1)\n",
    "# LOSS_FN = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training paramerters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 200-0.005-0.0001-0.25: 0.745 (0.784)\n",
      "-1: 200-0.005-0.0005-0.25: 0.804 (0.863)\n",
      "-1: 200-0.01-0.001-0.25: 0.804 (0.902)\n",
      "-1: 200-0.005-0.001-0.25: 0.804 (0.863)\n",
      "-1: 200-0.005-0.001-0.5: 0.706 (0.863)\n",
      "-1: 200-0.001-0.001-0.25: 0.667 (0.706)\n",
      "-1: 200-0.01-0.005-0.25: 0.843 (0.882)\n",
      "-1: 200-0.005-0.005-0.25: 0.824 (0.863)\n",
      "-1: 200-0.001-0.005-0.25: 0.784 (0.824)\n",
      "-1: 200-0.005-0.01-0.25: 0.804 (0.863)\n",
      "-1: 500-0.01-0.001-0.25: 0.863 (0.863)\n",
      "-1: 500-0.005-0.001-0.25: 0.843 (0.863)\n",
      "-1: 500-0.001-0.001-0.25: 0.784 (0.843)\n",
      "-1: 500-0.005-0.005-0.25: 0.843 (0.882)\n",
      "-1: 500-0.001-0.005-0.25: 0.824 (0.843)\n",
      "-1: 200-0.05-0.01-0.5: 0.824 (0.882)\n",
      "-1: 200-0.05-0.005-0.5: 0.784 (0.863)\n",
      "-1: 200-0.01-0.005-0.5: 0.804 (0.863)\n",
      "-1: 200-0.05-0.01-0: 0.804 (0.843)\n",
      "-1: 200-0.05-0.005-0: 0.745 (0.863)\n",
      "-1: 200-0.01-0.005-0: 0.804 (0.843)\n",
      "-1: 500-0.005-0.005-0.5: 0.824 (0.882)\n",
      "-1: 500-0.005-0.001-0.5: 0.824 (0.882)\n",
      "-1: 500-0.001-0.001-0.5: 0.804 (0.843)\n",
      "-1: 750-0.005-0.005-0.25: 0.804 (0.863)\n",
      "-1: 750-0.005-0.001-0.25: 0.784 (0.863)\n",
      "-1: 750-0.001-0.001-0.25: 0.804 (0.843)\n",
      "-2: 200-0.005-0.0001-0.25: 0.804 (0.863)\n",
      "-2: 200-0.005-0.0005-0.25: 0.863 (0.941)\n",
      "-2: 200-0.01-0.001-0.25: 0.824 (0.902)\n",
      "-2: 200-0.005-0.001-0.25: 0.843 (0.902)\n",
      "-2: 200-0.005-0.001-0.5: 0.843 (0.922)\n",
      "-2: 200-0.001-0.001-0.25: 0.765 (0.824)\n",
      "-2: 200-0.01-0.005-0.25: 0.843 (0.902)\n",
      "-2: 200-0.005-0.005-0.25: 0.882 (0.902)\n",
      "-2: 200-0.001-0.005-0.25: 0.843 (0.863)\n",
      "-2: 200-0.005-0.01-0.25: 0.882 (0.922)\n",
      "-2: 500-0.01-0.001-0.25: 0.824 (0.922)\n",
      "-2: 500-0.005-0.001-0.25: 0.863 (0.922)\n",
      "-2: 500-0.001-0.001-0.25: 0.824 (0.902)\n",
      "-2: 500-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-2: 500-0.001-0.005-0.25: 0.824 (0.902)\n",
      "-2: 200-0.05-0.01-0.5: 0.843 (0.922)\n",
      "-2: 200-0.05-0.005-0.5: 0.843 (0.922)\n",
      "-2: 200-0.01-0.005-0.5: 0.824 (0.922)\n",
      "-2: 200-0.05-0.01-0: 0.843 (0.902)\n",
      "-2: 200-0.05-0.005-0: 0.824 (0.902)\n",
      "-2: 200-0.01-0.005-0: 0.824 (0.882)\n",
      "-2: 500-0.005-0.005-0.5: 0.882 (0.902)\n",
      "-2: 500-0.005-0.001-0.5: 0.863 (0.902)\n",
      "-2: 500-0.001-0.001-0.5: 0.863 (0.882)\n",
      "-2: 750-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-2: 750-0.005-0.001-0.25: 0.824 (0.922)\n",
      "-2: 750-0.001-0.001-0.25: 0.843 (0.902)\n",
      "-3: 200-0.005-0.0001-0.25: 0.824 (0.843)\n",
      "-3: 200-0.005-0.0005-0.25: 0.824 (0.902)\n",
      "-3: 200-0.01-0.001-0.25: 0.863 (0.941)\n",
      "-3: 200-0.005-0.001-0.25: 0.863 (0.922)\n",
      "-3: 200-0.005-0.001-0.5: 0.863 (0.922)\n",
      "-3: 200-0.001-0.001-0.25: 0.686 (0.725)\n",
      "-3: 200-0.01-0.005-0.25: 0.863 (0.941)\n",
      "-3: 200-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-3: 200-0.001-0.005-0.25: 0.863 (0.882)\n",
      "-3: 200-0.005-0.01-0.25: 0.882 (0.922)\n",
      "-3: 500-0.01-0.001-0.25: 0.863 (0.941)\n",
      "-3: 500-0.005-0.001-0.25: 0.843 (0.941)\n",
      "-3: 500-0.001-0.001-0.25: 0.902 (0.902)\n",
      "-3: 500-0.005-0.005-0.25: 0.882 (0.922)\n",
      "-3: 500-0.001-0.005-0.25: 0.882 (0.922)\n",
      "-3: 200-0.05-0.01-0.5: 0.882 (0.902)\n",
      "-3: 200-0.05-0.005-0.5: 0.804 (0.882)\n",
      "-3: 200-0.01-0.005-0.5: 0.902 (0.941)\n",
      "-3: 200-0.05-0.01-0: 0.882 (0.902)\n",
      "-3: 200-0.05-0.005-0: 0.902 (0.902)\n",
      "-3: 200-0.01-0.005-0: 0.882 (0.882)\n",
      "-3: 500-0.005-0.005-0.5: 0.882 (0.941)\n",
      "-3: 500-0.005-0.001-0.5: 0.941 (0.941)\n",
      "-3: 500-0.001-0.001-0.5: 0.843 (0.863)\n",
      "-3: 750-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-3: 750-0.005-0.001-0.25: 0.843 (0.922)\n",
      "-3: 750-0.001-0.001-0.25: 0.863 (0.922)\n",
      "-4: 200-0.005-0.0001-0.25: 0.843 (0.882)\n",
      "-4: 200-0.005-0.0005-0.25: 0.882 (0.941)\n",
      "-4: 200-0.01-0.001-0.25: 0.882 (0.922)\n",
      "-4: 200-0.005-0.001-0.25: 0.863 (0.922)\n",
      "-4: 200-0.005-0.001-0.5: 0.843 (0.922)\n",
      "-4: 200-0.001-0.001-0.25: 0.765 (0.804)\n",
      "-4: 200-0.01-0.005-0.25: 0.882 (0.961)\n",
      "-4: 200-0.005-0.005-0.25: 0.922 (0.961)\n",
      "-4: 200-0.001-0.005-0.25: 0.882 (0.902)\n",
      "-4: 200-0.005-0.01-0.25: 0.902 (0.961)\n",
      "-4: 500-0.01-0.001-0.25: 0.843 (0.922)\n",
      "-4: 500-0.005-0.001-0.25: 0.941 (0.961)\n",
      "-4: 500-0.001-0.001-0.25: 0.922 (0.941)\n",
      "-4: 500-0.005-0.005-0.25: 0.941 (0.961)\n",
      "-4: 500-0.001-0.005-0.25: 0.902 (0.941)\n",
      "-4: 200-0.05-0.01-0.5: 0.745 (0.843)\n",
      "-4: 200-0.05-0.005-0.5: 0.745 (0.863)\n",
      "-4: 200-0.01-0.005-0.5: 0.882 (0.922)\n",
      "-4: 200-0.05-0.01-0: 0.902 (0.922)\n",
      "-4: 200-0.05-0.005-0: 0.902 (0.941)\n",
      "-4: 200-0.01-0.005-0: 0.922 (0.922)\n",
      "-4: 500-0.005-0.005-0.5: 0.941 (0.941)\n",
      "-4: 500-0.005-0.001-0.5: 0.843 (0.922)\n",
      "-4: 500-0.001-0.001-0.5: 0.902 (0.941)\n",
      "-4: 750-0.005-0.005-0.25: 0.922 (0.941)\n",
      "-4: 750-0.005-0.001-0.25: 0.922 (0.961)\n",
      "-4: 750-0.001-0.001-0.25: 0.882 (0.941)\n",
      "-5: 200-0.005-0.0001-0.25: 0.804 (0.863)\n",
      "-5: 200-0.005-0.0005-0.25: 0.902 (0.941)\n",
      "-5: 200-0.01-0.001-0.25: 0.824 (0.922)\n",
      "-5: 200-0.005-0.001-0.25: 0.843 (0.902)\n",
      "-5: 200-0.005-0.001-0.5: 0.882 (0.922)\n",
      "-5: 200-0.001-0.001-0.25: 0.765 (0.765)\n",
      "-5: 200-0.01-0.005-0.25: 0.863 (0.922)\n",
      "-5: 200-0.005-0.005-0.25: 0.804 (0.922)\n",
      "-5: 200-0.001-0.005-0.25: 0.745 (0.784)\n",
      "-5: 200-0.005-0.01-0.25: 0.843 (0.902)\n",
      "-5: 500-0.01-0.001-0.25: 0.824 (0.941)\n",
      "-5: 500-0.005-0.001-0.25: 0.863 (0.941)\n",
      "-5: 500-0.001-0.001-0.25: 0.824 (0.882)\n",
      "-5: 500-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-5: 500-0.001-0.005-0.25: 0.784 (0.902)\n",
      "-5: 200-0.05-0.01-0.5: 0.882 (0.882)\n",
      "-5: 200-0.05-0.005-0.5: 0.882 (0.902)\n",
      "-5: 200-0.01-0.005-0.5: 0.804 (0.902)\n",
      "-5: 200-0.05-0.01-0: 0.843 (0.922)\n",
      "-5: 200-0.05-0.005-0: 0.863 (0.902)\n",
      "-5: 200-0.01-0.005-0: 0.804 (0.843)\n",
      "-5: 500-0.005-0.005-0.5: 0.902 (0.922)\n",
      "-5: 500-0.005-0.001-0.5: 0.843 (0.902)\n",
      "-5: 500-0.001-0.001-0.5: 0.725 (0.843)\n",
      "-5: 750-0.005-0.005-0.25: 0.824 (0.902)\n",
      "-5: 750-0.005-0.001-0.25: 0.863 (0.941)\n",
      "-5: 750-0.001-0.001-0.25: 0.804 (0.902)\n",
      "-6: 200-0.005-0.0001-0.25: 0.882 (0.882)\n",
      "-6: 200-0.005-0.0005-0.25: 0.882 (0.902)\n",
      "-6: 200-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-6: 200-0.005-0.001-0.25: 0.882 (0.902)\n",
      "-6: 200-0.005-0.001-0.5: 0.882 (0.922)\n",
      "-6: 200-0.001-0.001-0.25: 0.706 (0.765)\n",
      "-6: 200-0.01-0.005-0.25: 0.882 (0.902)\n",
      "-6: 200-0.005-0.005-0.25: 0.863 (0.882)\n",
      "-6: 200-0.001-0.005-0.25: 0.804 (0.824)\n",
      "-6: 200-0.005-0.01-0.25: 0.863 (0.902)\n",
      "-6: 500-0.01-0.001-0.25: 0.863 (0.902)\n",
      "-6: 500-0.005-0.001-0.25: 0.902 (0.902)\n",
      "-6: 500-0.001-0.001-0.25: 0.863 (0.882)\n",
      "-6: 500-0.005-0.005-0.25: 0.843 (0.882)\n",
      "-6: 500-0.001-0.005-0.25: 0.843 (0.882)\n",
      "-6: 200-0.05-0.01-0.5: 0.804 (0.863)\n",
      "-6: 200-0.05-0.005-0.5: 0.804 (0.843)\n",
      "-6: 200-0.01-0.005-0.5: 0.902 (0.902)\n",
      "-6: 200-0.05-0.01-0: 0.882 (0.902)\n",
      "-6: 200-0.05-0.005-0: 0.745 (0.882)\n",
      "-6: 200-0.01-0.005-0: 0.843 (0.843)\n",
      "-6: 500-0.005-0.005-0.5: 0.863 (0.902)\n",
      "-6: 500-0.005-0.001-0.5: 0.843 (0.902)\n",
      "-6: 500-0.001-0.001-0.5: 0.784 (0.843)\n",
      "-6: 750-0.005-0.005-0.25: 0.882 (0.902)\n",
      "-6: 750-0.005-0.001-0.25: 0.882 (0.902)\n",
      "-6: 750-0.001-0.001-0.25: 0.843 (0.882)\n",
      "-7: 200-0.005-0.0001-0.25: 0.922 (0.922)\n",
      "-7: 200-0.005-0.0005-0.25: 0.784 (0.902)\n",
      "-7: 200-0.01-0.001-0.25: 0.863 (0.902)\n",
      "-7: 200-0.005-0.001-0.25: 0.882 (0.922)\n",
      "-7: 200-0.005-0.001-0.5: 0.863 (0.902)\n",
      "-7: 200-0.001-0.001-0.25: 0.745 (0.804)\n",
      "-7: 200-0.01-0.005-0.25: 0.843 (0.922)\n",
      "-7: 200-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-7: 200-0.001-0.005-0.25: 0.784 (0.843)\n",
      "-7: 200-0.005-0.01-0.25: 0.824 (0.882)\n",
      "-7: 500-0.01-0.001-0.25: 0.882 (0.941)\n",
      "-7: 500-0.005-0.001-0.25: 0.824 (0.902)\n",
      "-7: 500-0.001-0.001-0.25: 0.824 (0.882)\n",
      "-7: 500-0.005-0.005-0.25: 0.843 (0.902)\n",
      "-7: 500-0.001-0.005-0.25: 0.784 (0.863)\n",
      "-7: 200-0.05-0.01-0.5: 0.824 (0.902)\n",
      "-7: 200-0.05-0.005-0.5: 0.863 (0.902)\n",
      "-7: 200-0.01-0.005-0.5: 0.863 (0.922)\n",
      "-7: 200-0.05-0.01-0: 0.902 (0.902)\n",
      "-7: 200-0.05-0.005-0: 0.843 (0.863)\n",
      "-7: 200-0.01-0.005-0: 0.824 (0.843)\n",
      "-7: 500-0.005-0.005-0.5: 0.882 (0.902)\n",
      "-7: 500-0.005-0.001-0.5: 0.882 (0.902)\n",
      "-7: 500-0.001-0.001-0.5: 0.843 (0.882)\n",
      "-7: 750-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-7: 750-0.005-0.001-0.25: 0.863 (0.902)\n",
      "-7: 750-0.001-0.001-0.25: 0.863 (0.882)\n",
      "-8: 200-0.005-0.0001-0.25: 0.784 (0.824)\n",
      "-8: 200-0.005-0.0005-0.25: 0.824 (0.902)\n",
      "-8: 200-0.01-0.001-0.25: 0.784 (0.922)\n",
      "-8: 200-0.005-0.001-0.25: 0.824 (0.902)\n",
      "-8: 200-0.005-0.001-0.5: 0.784 (0.863)\n",
      "-8: 200-0.001-0.001-0.25: 0.549 (0.686)\n",
      "-8: 200-0.01-0.005-0.25: 0.882 (0.922)\n",
      "-8: 200-0.005-0.005-0.25: 0.843 (0.882)\n",
      "-8: 200-0.001-0.005-0.25: 0.765 (0.784)\n",
      "-8: 200-0.005-0.01-0.25: 0.824 (0.882)\n",
      "-8: 500-0.01-0.001-0.25: 0.843 (0.941)\n",
      "-8: 500-0.005-0.001-0.25: 0.824 (0.902)\n",
      "-8: 500-0.001-0.001-0.25: 0.804 (0.843)\n",
      "-8: 500-0.005-0.005-0.25: 0.843 (0.902)\n",
      "-8: 500-0.001-0.005-0.25: 0.804 (0.824)\n",
      "-8: 200-0.05-0.01-0.5: 0.784 (0.843)\n",
      "-8: 200-0.05-0.005-0.5: 0.784 (0.863)\n",
      "-8: 200-0.01-0.005-0.5: 0.843 (0.863)\n",
      "-8: 200-0.05-0.01-0: 0.804 (0.843)\n",
      "-8: 200-0.05-0.005-0: 0.784 (0.863)\n",
      "-8: 200-0.01-0.005-0: 0.804 (0.804)\n",
      "-8: 500-0.005-0.005-0.5: 0.784 (0.882)\n",
      "-8: 500-0.005-0.001-0.5: 0.843 (0.863)\n",
      "-8: 500-0.001-0.001-0.5: 0.784 (0.824)\n",
      "-8: 750-0.005-0.005-0.25: 0.882 (0.882)\n",
      "-8: 750-0.005-0.001-0.25: 0.863 (0.902)\n",
      "-8: 750-0.001-0.001-0.25: 0.863 (0.863)\n",
      "-9: 200-0.005-0.0001-0.25: 0.804 (0.882)\n",
      "-9: 200-0.005-0.0005-0.25: 0.843 (0.902)\n",
      "-9: 200-0.01-0.001-0.25: 0.863 (0.902)\n",
      "-9: 200-0.005-0.001-0.25: 0.863 (0.902)\n",
      "-9: 200-0.005-0.001-0.5: 0.863 (0.882)\n",
      "-9: 200-0.001-0.001-0.25: 0.745 (0.745)\n",
      "-9: 200-0.01-0.005-0.25: 0.863 (0.902)\n",
      "-9: 200-0.005-0.005-0.25: 0.863 (0.882)\n",
      "-9: 200-0.001-0.005-0.25: 0.765 (0.784)\n",
      "-9: 200-0.005-0.01-0.25: 0.804 (0.882)\n",
      "-9: 500-0.01-0.001-0.25: 0.804 (0.902)\n",
      "-9: 500-0.005-0.001-0.25: 0.824 (0.882)\n",
      "-9: 500-0.001-0.001-0.25: 0.804 (0.843)\n",
      "-9: 500-0.005-0.005-0.25: 0.863 (0.882)\n",
      "-9: 500-0.001-0.005-0.25: 0.824 (0.863)\n",
      "-9: 200-0.05-0.01-0.5: 0.824 (0.863)\n",
      "-9: 200-0.05-0.005-0.5: 0.784 (0.882)\n",
      "-9: 200-0.01-0.005-0.5: 0.843 (0.902)\n",
      "-9: 200-0.05-0.01-0: 0.863 (0.882)\n",
      "-9: 200-0.05-0.005-0: 0.843 (0.863)\n",
      "-9: 200-0.01-0.005-0: 0.765 (0.824)\n",
      "-9: 500-0.005-0.005-0.5: 0.863 (0.882)\n",
      "-9: 500-0.005-0.001-0.5: 0.863 (0.882)\n",
      "-9: 500-0.001-0.001-0.5: 0.824 (0.843)\n",
      "-9: 750-0.005-0.005-0.25: 0.843 (0.882)\n",
      "-9: 750-0.005-0.001-0.25: 0.843 (0.902)\n",
      "-9: 750-0.001-0.001-0.25: 0.843 (0.882)\n",
      "-10: 200-0.005-0.0001-0.25: 0.843 (0.882)\n",
      "-10: 200-0.005-0.0005-0.25: 0.902 (0.902)\n",
      "-10: 200-0.01-0.001-0.25: 0.922 (0.922)\n",
      "-10: 200-0.005-0.001-0.25: 0.882 (0.902)\n",
      "-10: 200-0.005-0.001-0.5: 0.824 (0.902)\n",
      "-10: 200-0.001-0.001-0.25: 0.706 (0.725)\n",
      "-10: 200-0.01-0.005-0.25: 0.863 (0.922)\n",
      "-10: 200-0.005-0.005-0.25: 0.882 (0.902)\n",
      "-10: 200-0.001-0.005-0.25: 0.784 (0.824)\n",
      "-10: 200-0.005-0.01-0.25: 0.882 (0.882)\n",
      "-10: 500-0.01-0.001-0.25: 0.902 (0.941)\n",
      "-10: 500-0.005-0.001-0.25: 0.902 (0.922)\n",
      "-10: 500-0.001-0.001-0.25: 0.843 (0.882)\n",
      "-10: 500-0.005-0.005-0.25: 0.902 (0.902)\n",
      "-10: 500-0.001-0.005-0.25: 0.824 (0.882)\n",
      "-10: 200-0.05-0.01-0.5: 0.824 (0.902)\n",
      "-10: 200-0.05-0.005-0.5: 0.882 (0.882)\n",
      "-10: 200-0.01-0.005-0.5: 0.902 (0.902)\n",
      "-10: 200-0.05-0.01-0: 0.843 (0.902)\n",
      "-10: 200-0.05-0.005-0: 0.843 (0.922)\n",
      "-10: 200-0.01-0.005-0: 0.843 (0.843)\n",
      "-10: 500-0.005-0.005-0.5: 0.843 (0.902)\n",
      "-10: 500-0.005-0.001-0.5: 0.863 (0.902)\n",
      "-10: 500-0.001-0.001-0.5: 0.843 (0.882)\n",
      "-10: 750-0.005-0.005-0.25: 0.824 (0.902)\n",
      "-10: 750-0.005-0.001-0.25: 0.863 (0.882)\n",
      "-10: 750-0.001-0.001-0.25: 0.882 (0.902)\n",
      "----- 36.65 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-4, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-4, 'drop': .25},\n",
    "\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "        \n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-2, 'drop': .25},\n",
    "\n",
    "        {'epochs': 500, 'lr': .01, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "\n",
    "\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': 0},\n",
    "\n",
    "        \n",
    "        {'epochs': 500, 'lr': .005, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 500, 'lr': .001, 'wd': 1e-3, 'drop': .5},\n",
    "\n",
    "        {'epochs': 750, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 750, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 750, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "        ]\n",
    "\n",
    "best_accs5 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs5 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=exp['drop'], init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S, K, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, exp['epochs'], exp['lr'], exp['wd'], epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs5[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs5[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}: {best_val_accs5[j,i]:.3f} ({best_accs5[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}' for exp in EXPS]\n",
    "table_over5 = summary_table(best_accs5, index_name)\n",
    "table5 = summary_table(best_val_accs5, index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0001-0.25</th>\n",
       "      <td>0.825490</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.047587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0005-0.25</th>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.039411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.25</th>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.038021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.25</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.025110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.5</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.051281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.001-0.25</th>\n",
       "      <td>0.709804</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.063112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.25</th>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.015188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.005-0.25</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.030941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.25</th>\n",
       "      <td>0.801961</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.043359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.01-0.25</th>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.034187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.01-0.001-0.25</th>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.028006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0.25</th>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.038223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.001-0.25</th>\n",
       "      <td>0.839216</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.041871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.005-0.25</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.030440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.005-0.25</th>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.036208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.5</th>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.039216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0.5</th>\n",
       "      <td>0.817647</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.044756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.5</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.037255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.034018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0</th>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.053375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0</th>\n",
       "      <td>0.831373</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.042237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.005-0.5</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.040942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0.5</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.030941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.001-0.5</th>\n",
       "      <td>0.821569</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.046772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750-0.005-0.005-0.25</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.032869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750-0.005-0.001-0.25</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.034187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750-0.001-0.001-0.25</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.026380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean accs       med       std\n",
       "200-0.005-0.0001-0.25   0.825490  0.813725  0.047587\n",
       "200-0.005-0.0005-0.25   0.850980  0.852941  0.039411\n",
       "200-0.01-0.001-0.25     0.847059  0.852941  0.038021\n",
       "200-0.005-0.001-0.25    0.854902  0.862745  0.025110\n",
       "200-0.005-0.001-0.5     0.835294  0.852941  0.051281\n",
       "200-0.001-0.001-0.25    0.709804  0.725490  0.063112\n",
       "200-0.01-0.005-0.25     0.862745  0.862745  0.015188\n",
       "200-0.005-0.005-0.25    0.860784  0.862745  0.030941\n",
       "200-0.001-0.005-0.25    0.801961  0.784314  0.043359\n",
       "200-0.005-0.01-0.25     0.850980  0.852941  0.034187\n",
       "500-0.01-0.001-0.25     0.850980  0.852941  0.028006\n",
       "500-0.005-0.001-0.25    0.862745  0.852941  0.038223\n",
       "500-0.001-0.001-0.25    0.839216  0.823529  0.041871\n",
       "500-0.005-0.005-0.25    0.868627  0.862745  0.030440\n",
       "500-0.001-0.005-0.25    0.829412  0.823529  0.036208\n",
       "200-0.05-0.01-0.5       0.823529  0.823529  0.039216\n",
       "200-0.05-0.005-0.5      0.817647  0.803922  0.044756\n",
       "200-0.01-0.005-0.5      0.856863  0.852941  0.037255\n",
       "200-0.05-0.01-0         0.856863  0.852941  0.034018\n",
       "200-0.05-0.005-0        0.829412  0.843137  0.053375\n",
       "200-0.01-0.005-0        0.831373  0.823529  0.042237\n",
       "500-0.005-0.005-0.5     0.866667  0.872549  0.040942\n",
       "500-0.005-0.001-0.5     0.860784  0.852941  0.030941\n",
       "500-0.001-0.001-0.5     0.821569  0.833333  0.046772\n",
       "750-0.005-0.005-0.25    0.856863  0.862745  0.032869\n",
       "750-0.005-0.001-0.25    0.854902  0.862745  0.034187\n",
       "750-0.001-0.001-0.25    0.849020  0.852941  0.026380"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 0.01-1-1-True: 0.824 (0.824)\n",
      "-1: 0.1-1-1-True: 0.784 (0.843)\n",
      "-1: 1-1-1-True: 0.824 (0.843)\n",
      "-1: 1-1-1-True: 0.824 (0.863)\n",
      "-1: 1-1-1-True: 0.804 (0.843)\n",
      "-1: 1-5-1-True: 0.824 (0.843)\n",
      "-1: 1-10-1-True: 0.824 (0.863)\n",
      "-1: 1-10-5-True: 0.824 (0.863)\n",
      "-1: 1-1-5-True: 0.745 (0.824)\n",
      "-1: 1-1-10-True: 0.824 (0.843)\n",
      "-1: 1-5-10-True: 0.804 (0.882)\n",
      "-1: 0.1-10-10-True: 0.804 (0.882)\n",
      "-1: 1-10-10-True: 0.784 (0.863)\n",
      "-1: 1-10-10-True: 0.804 (0.863)\n",
      "-1: 1-25-25-True: 0.686 (0.843)\n",
      "-1: 1-25-25-True: 0.804 (0.863)\n",
      "-1: 0.1-25-25-True: 0.784 (0.863)\n",
      "-1: 1-25-25-True: 0.804 (0.863)\n",
      "-1: 1-50-50-True: 0.843 (0.863)\n",
      "-2: 0.01-1-1-True: 0.863 (0.902)\n",
      "-2: 0.1-1-1-True: 0.882 (0.902)\n",
      "-2: 1-1-1-True: 0.863 (0.902)\n",
      "-2: 1-1-1-True: 0.863 (0.902)\n",
      "-2: 1-1-1-True: 0.863 (0.902)\n",
      "-2: 1-5-1-True: 0.902 (0.922)\n",
      "-2: 1-10-1-True: 0.863 (0.922)\n",
      "-2: 1-10-5-True: 0.843 (0.902)\n",
      "-2: 1-1-5-True: 0.882 (0.902)\n",
      "-2: 1-1-10-True: 0.784 (0.902)\n",
      "-2: 1-5-10-True: 0.863 (0.902)\n",
      "-2: 0.1-10-10-True: 0.863 (0.922)\n",
      "-2: 1-10-10-True: 0.843 (0.922)\n",
      "-2: 1-10-10-True: 0.863 (0.922)\n",
      "-2: 1-25-25-True: 0.882 (0.922)\n",
      "-2: 1-25-25-True: 0.863 (0.922)\n",
      "-2: 0.1-25-25-True: 0.863 (0.902)\n",
      "-2: 1-25-25-True: 0.843 (0.922)\n",
      "-2: 1-50-50-True: 0.902 (0.922)\n",
      "-3: 0.01-1-1-True: 0.843 (0.922)\n",
      "-3: 0.1-1-1-True: 0.882 (0.922)\n",
      "-3: 1-1-1-True: 0.863 (0.922)\n",
      "-3: 1-1-1-True: 0.863 (0.922)\n",
      "-3: 1-1-1-True: 0.863 (0.922)\n",
      "-3: 1-5-1-True: 0.843 (0.922)\n",
      "-3: 1-10-1-True: 0.902 (0.902)\n",
      "-3: 1-10-5-True: 0.863 (0.922)\n",
      "-3: 1-1-5-True: 0.843 (0.902)\n",
      "-3: 1-1-10-True: 0.863 (0.922)\n",
      "-3: 1-5-10-True: 0.882 (0.922)\n",
      "-3: 0.1-10-10-True: 0.863 (0.922)\n",
      "-3: 1-10-10-True: 0.843 (0.922)\n",
      "-3: 1-10-10-True: 0.922 (0.941)\n",
      "-3: 1-25-25-True: 0.804 (0.922)\n",
      "-3: 1-25-25-True: 0.863 (0.941)\n",
      "-3: 0.1-25-25-True: 0.882 (0.941)\n",
      "-3: 1-25-25-True: 0.843 (0.922)\n",
      "-3: 1-50-50-True: 0.863 (0.922)\n",
      "-4: 0.01-1-1-True: 0.784 (0.804)\n",
      "-4: 0.1-1-1-True: 0.863 (0.922)\n",
      "-4: 1-1-1-True: 0.941 (0.941)\n",
      "-4: 1-1-1-True: 0.922 (0.941)\n",
      "-4: 1-1-1-True: 0.922 (0.961)\n",
      "-4: 1-5-1-True: 0.902 (0.922)\n",
      "-4: 1-10-1-True: 0.922 (0.941)\n",
      "-4: 1-10-5-True: 0.902 (0.941)\n",
      "-4: 1-1-5-True: 0.882 (0.961)\n",
      "-4: 1-1-10-True: 0.922 (0.941)\n",
      "-4: 1-5-10-True: 0.863 (0.941)\n",
      "-4: 0.1-10-10-True: 0.882 (0.961)\n",
      "-4: 1-10-10-True: 0.882 (0.961)\n",
      "-4: 1-10-10-True: 0.882 (0.941)\n",
      "-4: 1-25-25-True: 0.902 (0.922)\n",
      "-4: 1-25-25-True: 0.863 (0.941)\n",
      "-4: 0.1-25-25-True: 0.863 (0.941)\n",
      "-4: 1-25-25-True: 0.882 (0.941)\n",
      "-4: 1-50-50-True: 0.863 (0.941)\n",
      "-5: 0.01-1-1-True: 0.784 (0.863)\n",
      "-5: 0.1-1-1-True: 0.863 (0.882)\n",
      "-5: 1-1-1-True: 0.784 (0.882)\n",
      "-5: 1-1-1-True: 0.804 (0.902)\n",
      "-5: 1-1-1-True: 0.882 (0.902)\n",
      "-5: 1-5-1-True: 0.882 (0.902)\n",
      "-5: 1-10-1-True: 0.784 (0.922)\n",
      "-5: 1-10-5-True: 0.863 (0.922)\n",
      "-5: 1-1-5-True: 0.882 (0.922)\n",
      "-5: 1-1-10-True: 0.863 (0.922)\n",
      "-5: 1-5-10-True: 0.863 (0.941)\n",
      "-5: 0.1-10-10-True: 0.784 (0.902)\n",
      "-5: 1-10-10-True: 0.765 (0.922)\n",
      "-5: 1-10-10-True: 0.843 (0.922)\n",
      "-5: 1-25-25-True: 0.863 (0.902)\n",
      "-5: 1-25-25-True: 0.902 (0.922)\n",
      "-5: 0.1-25-25-True: 0.882 (0.941)\n",
      "-5: 1-25-25-True: 0.863 (0.941)\n",
      "-5: 1-50-50-True: 0.824 (0.922)\n",
      "-6: 0.01-1-1-True: 0.863 (0.882)\n",
      "-6: 0.1-1-1-True: 0.804 (0.863)\n",
      "-6: 1-1-1-True: 0.863 (0.882)\n",
      "-6: 1-1-1-True: 0.863 (0.882)\n",
      "-6: 1-1-1-True: 0.882 (0.902)\n",
      "-6: 1-5-1-True: 0.882 (0.902)\n",
      "-6: 1-10-1-True: 0.882 (0.902)\n",
      "-6: 1-10-5-True: 0.843 (0.902)\n",
      "-6: 1-1-5-True: 0.863 (0.882)\n",
      "-6: 1-1-10-True: 0.863 (0.882)\n",
      "-6: 1-5-10-True: 0.843 (0.902)\n",
      "-6: 0.1-10-10-True: 0.882 (0.902)\n",
      "-6: 1-10-10-True: 0.863 (0.902)\n",
      "-6: 1-10-10-True: 0.843 (0.902)\n",
      "-6: 1-25-25-True: 0.863 (0.882)\n",
      "-6: 1-25-25-True: 0.863 (0.902)\n",
      "-6: 0.1-25-25-True: 0.902 (0.941)\n",
      "-6: 1-25-25-True: 0.863 (0.922)\n",
      "-6: 1-50-50-True: 0.863 (0.902)\n",
      "-7: 0.01-1-1-True: 0.863 (0.882)\n",
      "-7: 0.1-1-1-True: 0.863 (0.863)\n",
      "-7: 1-1-1-True: 0.863 (0.902)\n",
      "-7: 1-1-1-True: 0.882 (0.882)\n",
      "-7: 1-1-1-True: 0.863 (0.882)\n",
      "-7: 1-5-1-True: 0.882 (0.882)\n",
      "-7: 1-10-1-True: 0.843 (0.902)\n",
      "-7: 1-10-5-True: 0.843 (0.902)\n",
      "-7: 1-1-5-True: 0.804 (0.902)\n",
      "-7: 1-1-10-True: 0.843 (0.902)\n",
      "-7: 1-5-10-True: 0.882 (0.902)\n",
      "-7: 0.1-10-10-True: 0.824 (0.902)\n",
      "-7: 1-10-10-True: 0.843 (0.902)\n",
      "-7: 1-10-10-True: 0.882 (0.902)\n",
      "-7: 1-25-25-True: 0.784 (0.902)\n",
      "-7: 1-25-25-True: 0.843 (0.922)\n",
      "-7: 0.1-25-25-True: 0.863 (0.902)\n",
      "-7: 1-25-25-True: 0.863 (0.902)\n",
      "-7: 1-50-50-True: 0.843 (0.902)\n",
      "-8: 0.01-1-1-True: 0.784 (0.824)\n",
      "-8: 0.1-1-1-True: 0.804 (0.843)\n",
      "-8: 1-1-1-True: 0.804 (0.843)\n",
      "-8: 1-1-1-True: 0.843 (0.902)\n",
      "-8: 1-1-1-True: 0.804 (0.941)\n",
      "-8: 1-5-1-True: 0.824 (0.902)\n",
      "-8: 1-10-1-True: 0.804 (0.882)\n",
      "-8: 1-10-5-True: 0.863 (0.902)\n",
      "-8: 1-1-5-True: 0.843 (0.863)\n",
      "-8: 1-1-10-True: 0.765 (0.863)\n",
      "-8: 1-5-10-True: 0.882 (0.902)\n",
      "-8: 0.1-10-10-True: 0.843 (0.902)\n",
      "-8: 1-10-10-True: 0.843 (0.882)\n",
      "-8: 1-10-10-True: 0.843 (0.902)\n",
      "-8: 1-25-25-True: 0.824 (0.922)\n",
      "-8: 1-25-25-True: 0.824 (0.882)\n",
      "-8: 0.1-25-25-True: 0.902 (0.902)\n",
      "-8: 1-25-25-True: 0.922 (0.922)\n",
      "-8: 1-50-50-True: 0.843 (0.882)\n",
      "-9: 0.01-1-1-True: 0.804 (0.863)\n",
      "-9: 0.1-1-1-True: 0.824 (0.882)\n",
      "-9: 1-1-1-True: 0.824 (0.863)\n",
      "-9: 1-1-1-True: 0.824 (0.882)\n",
      "-9: 1-1-1-True: 0.804 (0.882)\n",
      "-9: 1-5-1-True: 0.863 (0.882)\n",
      "-9: 1-10-1-True: 0.843 (0.863)\n",
      "-9: 1-10-5-True: 0.824 (0.882)\n",
      "-9: 1-1-5-True: 0.824 (0.882)\n",
      "-9: 1-1-10-True: 0.824 (0.863)\n",
      "-9: 1-5-10-True: 0.804 (0.882)\n",
      "-9: 0.1-10-10-True: 0.824 (0.922)\n",
      "-9: 1-10-10-True: 0.824 (0.882)\n",
      "-9: 1-10-10-True: 0.882 (0.882)\n",
      "-9: 1-25-25-True: 0.902 (0.902)\n",
      "-9: 1-25-25-True: 0.824 (0.863)\n",
      "-9: 0.1-25-25-True: 0.843 (0.882)\n",
      "-9: 1-25-25-True: 0.863 (0.902)\n",
      "-9: 1-50-50-True: 0.863 (0.882)\n",
      "-10: 0.01-1-1-True: 0.824 (0.882)\n",
      "-10: 0.1-1-1-True: 0.843 (0.882)\n",
      "-10: 1-1-1-True: 0.863 (0.882)\n",
      "-10: 1-1-1-True: 0.882 (0.922)\n",
      "-10: 1-1-1-True: 0.863 (0.902)\n",
      "-10: 1-5-1-True: 0.863 (0.902)\n",
      "-10: 1-10-1-True: 0.863 (0.882)\n",
      "-10: 1-10-5-True: 0.863 (0.902)\n",
      "-10: 1-1-5-True: 0.882 (0.882)\n",
      "-10: 1-1-10-True: 0.882 (0.902)\n",
      "-10: 1-5-10-True: 0.902 (0.902)\n",
      "-10: 0.1-10-10-True: 0.863 (0.922)\n",
      "-10: 1-10-10-True: 0.882 (0.902)\n",
      "-10: 1-10-10-True: 0.863 (0.902)\n",
      "-10: 1-25-25-True: 0.882 (0.902)\n",
      "-10: 1-25-25-True: 0.863 (0.922)\n",
      "-10: 0.1-25-25-True: 0.863 (0.941)\n",
      "-10: 1-25-25-True: 0.882 (0.922)\n",
      "-10: 1-50-50-True: 0.863 (0.961)\n",
      "----- 32.06 mins -----\n"
     ]
    }
   ],
   "source": [
    "# h0, norm (norm, not norm, norm H), sep vs joint training, optimizers iters (W vs h)\n",
    "EXPS = [\n",
    "        # {'h0': .001, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': .01, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': .1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 1000, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 1500, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "\n",
    "        # {'h0': .1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        # {'h0': 1, 'epochs': 1000, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 5, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 1, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 5, 'alt': True},\n",
    "\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 5, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 10, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 5, 'epochs_W': 10, 'alt': True},\n",
    "        \n",
    "        {'h0': .1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 500, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 50, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 100, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': .1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 50, 'epochs_W': 50, 'alt': True},\n",
    "    ]\n",
    "\n",
    "\n",
    "best_accs6 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs6 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = Dual_GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, alpha=ALPHA, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=exp['h0'])\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        if not exp['alt']:\n",
    "            model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "            _, acc = model.train(feat, labels, exp['epochs'], LR, WD)\n",
    "        else:\n",
    "            model = GF_NodeClassModel(arch, S,  K, masks, LOSS_FN, device=device)\n",
    "            _, acc = model.train(feat, labels, exp['epochs'], LR, WD, epochs_h=exp['epochs_h'],\n",
    "                                 epochs_W=exp['epochs_W'])\n",
    "\n",
    "        best_accs6[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs6[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"h0\"]}-{exp[\"epochs_h\"]}-{exp[\"epochs_W\"]}-{exp[\"alt\"]}: {best_val_accs6[j,i]:.3f} ({best_accs6[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"h0\"]}-{exp[\"epochs_h\"]}-{exp[\"epochs_W\"]}-{exp[\"alt\"]}' for exp in EXPS]\n",
    "table_over6 = summary_table(best_accs6, index_name)\n",
    "table6 = summary_table(best_val_accs6, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.01-1-1-True</th>\n",
       "      <td>0.839216</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.049449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-1-1-True</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.039265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-1-True</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.029412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000-1-1-1-True</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.040232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500-1-1-1-True</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.023283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-5-1-True</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.040942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-1-True</th>\n",
       "      <td>0.831373</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.028006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-5-True</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.035130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-5-True</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.029672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-10-True</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.044063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-5-10-True</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.028074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-10-10-True</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.032159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-10-True</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.034467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-1-10-10-True</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.038273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50-1-25-25-True</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.033044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100-1-25-25-True</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.019706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-25-25-True</th>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.033962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-25-25-True</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.036208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-50-50-True</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.026013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mean accs       med       std\n",
       "200-0.01-1-1-True    0.839216  0.852941  0.049449\n",
       "200-0.1-1-1-True     0.837255  0.833333  0.039265\n",
       "200-1-1-1-True       0.852941  0.852941  0.029412\n",
       "1000-1-1-1-True      0.856863  0.862745  0.040232\n",
       "1500-1-1-1-True      0.849020  0.852941  0.023283\n",
       "200-1-5-1-True       0.858824  0.862745  0.040942\n",
       "200-1-10-1-True      0.831373  0.823529  0.028006\n",
       "200-1-10-5-True      0.868627  0.872549  0.035130\n",
       "200-1-1-5-True       0.841176  0.843137  0.029672\n",
       "200-1-1-10-True      0.852941  0.852941  0.044063\n",
       "200-1-5-10-True      0.852941  0.852941  0.028074\n",
       "200-0.1-10-10-True   0.860784  0.872549  0.032159\n",
       "200-1-10-10-True     0.860784  0.862745  0.034467\n",
       "500-1-10-10-True     0.849020  0.862745  0.038273\n",
       "50-1-25-25-True      0.854902  0.862745  0.033044\n",
       "100-1-25-25-True     0.868627  0.862745  0.019706\n",
       "200-0.1-25-25-True   0.862745  0.862745  0.033962\n",
       "200-1-25-25-True     0.849020  0.852941  0.036208\n",
       "200-1-50-50-True     0.858824  0.862745  0.026013"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 0.25: 0.843 (0.863)\n",
      "-1: 0.5: 0.824 (0.843)\n",
      "-1: 0.6: 0.824 (0.863)\n",
      "-1: 0.7: 0.804 (0.843)\n",
      "-1: 0.8: 0.824 (0.863)\n",
      "-1: 0.9: 0.804 (0.863)\n",
      "-1: 1: 0.843 (0.863)\n",
      "-1: None: 0.824 (0.863)\n",
      "-2: 0.25: 0.863 (0.902)\n",
      "-2: 0.5: 0.863 (0.902)\n",
      "-2: 0.6: 0.863 (0.902)\n",
      "-2: 0.7: 0.863 (0.922)\n",
      "-2: 0.8: 0.863 (0.922)\n",
      "-2: 0.9: 0.843 (0.922)\n",
      "-2: 1: 0.824 (0.922)\n",
      "-2: None: 0.863 (0.922)\n",
      "-3: 0.25: 0.902 (0.922)\n",
      "-3: 0.5: 0.863 (0.922)\n",
      "-3: 0.6: 0.863 (0.922)\n",
      "-3: 0.7: 0.902 (0.922)\n",
      "-3: 0.8: 0.863 (0.922)\n",
      "-3: 0.9: 0.863 (0.941)\n",
      "-3: 1: 0.882 (0.922)\n",
      "-3: None: 0.863 (0.922)\n",
      "-4: 0.25: 0.882 (0.941)\n",
      "-4: 0.5: 0.882 (0.922)\n",
      "-4: 0.6: 0.902 (0.922)\n",
      "-4: 0.7: 0.902 (0.961)\n",
      "-4: 0.8: 0.922 (0.961)\n",
      "-4: 0.9: 0.922 (0.961)\n",
      "-4: 1: 0.902 (0.961)\n",
      "-4: None: 0.843 (0.941)\n",
      "-5: 0.25: 0.863 (0.902)\n",
      "-5: 0.5: 0.784 (0.882)\n",
      "-5: 0.6: 0.882 (0.882)\n",
      "-5: 0.7: 0.863 (0.902)\n",
      "-5: 0.8: 0.882 (0.922)\n",
      "-5: 0.9: 0.882 (0.922)\n",
      "-5: 1: 0.863 (0.922)\n",
      "-5: None: 0.922 (0.922)\n",
      "-6: 0.25: 0.863 (0.902)\n",
      "-6: 0.5: 0.882 (0.882)\n",
      "-6: 0.6: 0.863 (0.882)\n",
      "-6: 0.7: 0.863 (0.902)\n",
      "-6: 0.8: 0.863 (0.902)\n",
      "-6: 0.9: 0.882 (0.902)\n",
      "-6: 1: 0.863 (0.902)\n",
      "-6: None: 0.863 (0.882)\n",
      "-7: 0.25: 0.804 (0.882)\n",
      "-7: 0.5: 0.882 (0.882)\n",
      "-7: 0.6: 0.843 (0.902)\n",
      "-7: 0.7: 0.863 (0.902)\n",
      "-7: 0.8: 0.863 (0.902)\n",
      "-7: 0.9: 0.843 (0.902)\n",
      "-7: 1: 0.843 (0.922)\n",
      "-7: None: 0.804 (0.882)\n",
      "-8: 0.25: 0.843 (0.863)\n",
      "-8: 0.5: 0.804 (0.863)\n",
      "-8: 0.6: 0.824 (0.863)\n",
      "-8: 0.7: 0.863 (0.882)\n",
      "-8: 0.8: 0.824 (0.882)\n",
      "-8: 0.9: 0.824 (0.882)\n",
      "-8: 1: 0.824 (0.922)\n",
      "-8: None: 0.765 (0.902)\n",
      "-9: 0.25: 0.824 (0.863)\n",
      "-9: 0.5: 0.804 (0.863)\n",
      "-9: 0.6: 0.824 (0.882)\n",
      "-9: 0.7: 0.824 (0.882)\n",
      "-9: 0.8: 0.824 (0.863)\n",
      "-9: 0.9: 0.863 (0.882)\n",
      "-9: 1: 0.843 (0.882)\n",
      "-9: None: 0.863 (0.882)\n",
      "-10: 0.25: 0.902 (0.902)\n",
      "-10: 0.5: 0.863 (0.902)\n",
      "-10: 0.6: 0.882 (0.882)\n",
      "-10: 0.7: 0.863 (0.902)\n",
      "-10: 0.8: 0.922 (0.922)\n",
      "-10: 0.9: 0.882 (0.882)\n",
      "-10: 1: 0.843 (0.922)\n",
      "-10: None: 0.863 (0.902)\n",
      "----- 34.19 mins -----\n"
     ]
    }
   ],
   "source": [
    "# h0, norm (norm, not norm, norm H), sep vs joint training, optimizers iters (W vs h)\n",
    "EXPS = [\n",
    "        # {'alpha': 0},\n",
    "        {'alpha': .25},\n",
    "        # {'alpha': .4},\n",
    "        {'alpha': .5},\n",
    "        {'alpha': .6},\n",
    "        {'alpha': .7},  # .87\n",
    "        {'alpha': .8},  # .87\n",
    "        {'alpha': .9},  # .87\n",
    "        {'alpha': 1},\n",
    "        {'alpha': None},\n",
    "    ]\n",
    "\n",
    "\n",
    "best_accs6b = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs6b = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = Dual_GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, alpha=exp['alpha'],\n",
    "                          last_act=LAST_ACT, dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  K, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs6b[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs6b[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"alpha\"]}: {best_val_accs6b[j,i]:.3f} ({best_accs6b[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"alpha\"]}' for exp in EXPS]\n",
    "table_over6b = summary_table(best_accs6b, index_name)\n",
    "table6b = summary_table(best_val_accs6b, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.25</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.030122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>0.845098</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.035565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.6</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.026380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.7</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.028347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>0.864706</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.034467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.9</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.032159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.023611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>None</th>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.039992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mean accs       med       std\n",
       "0.25   0.858824  0.862745  0.030122\n",
       "0.5    0.845098  0.862745  0.035565\n",
       "0.6    0.856863  0.862745  0.026380\n",
       "0.7    0.860784  0.862745  0.028347\n",
       "0.8    0.864706  0.862745  0.034467\n",
       "0.9    0.860784  0.862745  0.032159\n",
       "1      0.852941  0.843137  0.023611\n",
       "None   0.847059  0.862745  0.039992"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table6b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 2-2-16: 0.824 (0.843)\n",
      "-1: 2-3-16: 0.804 (0.863)\n",
      "-1: 3-2-16: 0.804 (0.843)\n",
      "-1: 3-3-16: 0.804 (0.863)\n",
      "-1: 2-2-32: 0.824 (0.843)\n",
      "-1: 2-2-64: 0.804 (0.863)\n",
      "-1: 2-2-128: 0.804 (0.863)\n",
      "-1: 2-3-32: 0.804 (0.863)\n",
      "-1: 2-3-64: 0.804 (0.863)\n",
      "-1: 2-3-128: 0.784 (0.863)\n",
      "-1: 3-2-32: 0.824 (0.882)\n",
      "-1: 3-2-64: 0.824 (0.863)\n",
      "-1: 3-2-128: 0.804 (0.863)\n",
      "-2: 2-2-16: 0.902 (0.922)\n",
      "-2: 2-3-16: 0.882 (0.922)\n",
      "-2: 3-2-16: 0.863 (0.902)\n",
      "-2: 3-3-16: 0.902 (0.922)\n",
      "-2: 2-2-32: 0.882 (0.922)\n",
      "-2: 2-2-64: 0.843 (0.922)\n",
      "-2: 2-2-128: 0.882 (0.922)\n",
      "-2: 2-3-32: 0.882 (0.941)\n",
      "-2: 2-3-64: 0.882 (0.922)\n",
      "-2: 2-3-128: 0.882 (0.902)\n",
      "-2: 3-2-32: 0.902 (0.922)\n",
      "-2: 3-2-64: 0.902 (0.922)\n",
      "-2: 3-2-128: 0.863 (0.922)\n",
      "-3: 2-2-16: 0.902 (0.922)\n",
      "-3: 2-3-16: 0.882 (0.941)\n",
      "-3: 3-2-16: 0.882 (0.941)\n",
      "-3: 3-3-16: 0.843 (0.902)\n",
      "-3: 2-2-32: 0.863 (0.922)\n",
      "-3: 2-2-64: 0.882 (0.922)\n",
      "-3: 2-2-128: 0.882 (0.922)\n",
      "-3: 2-3-32: 0.863 (0.922)\n",
      "-3: 2-3-64: 0.863 (0.922)\n",
      "-3: 2-3-128: 0.882 (0.941)\n",
      "-3: 3-2-32: 0.882 (0.922)\n",
      "-3: 3-2-64: 0.882 (0.922)\n",
      "-3: 3-2-128: 0.882 (0.922)\n",
      "-4: 2-2-16: 0.922 (0.941)\n",
      "-4: 2-3-16: 0.882 (0.941)\n",
      "-4: 3-2-16: 0.843 (0.922)\n",
      "-4: 3-3-16: 0.902 (0.941)\n",
      "-4: 2-2-32: 0.922 (0.961)\n",
      "-4: 2-2-64: 0.902 (0.961)\n",
      "-4: 2-2-128: 0.882 (0.961)\n",
      "-4: 2-3-32: 0.941 (0.941)\n",
      "-4: 2-3-64: 0.922 (0.961)\n",
      "-4: 2-3-128: 0.922 (0.961)\n",
      "-4: 3-2-32: 0.882 (0.922)\n",
      "-4: 3-2-64: 0.922 (0.961)\n",
      "-4: 3-2-128: 0.922 (0.941)\n",
      "-5: 2-2-16: 0.863 (0.882)\n",
      "-5: 2-3-16: 0.843 (0.922)\n",
      "-5: 3-2-16: 0.843 (0.922)\n",
      "-5: 3-3-16: 0.863 (0.941)\n",
      "-5: 2-2-32: 0.843 (0.922)\n",
      "-5: 2-2-64: 0.882 (0.902)\n",
      "-5: 2-2-128: 0.863 (0.902)\n",
      "-5: 2-3-32: 0.863 (0.902)\n",
      "-5: 2-3-64: 0.863 (0.902)\n",
      "-5: 2-3-128: 0.765 (0.902)\n",
      "-5: 3-2-32: 0.863 (0.941)\n",
      "-5: 3-2-64: 0.765 (0.941)\n",
      "-5: 3-2-128: 0.863 (0.922)\n",
      "-6: 2-2-16: 0.882 (0.902)\n",
      "-6: 2-3-16: 0.824 (0.882)\n",
      "-6: 3-2-16: 0.824 (0.863)\n",
      "-6: 3-3-16: 0.843 (0.882)\n",
      "-6: 2-2-32: 0.882 (0.902)\n",
      "-6: 2-2-64: 0.882 (0.882)\n",
      "-6: 2-2-128: 0.824 (0.882)\n",
      "-6: 2-3-32: 0.863 (0.902)\n",
      "-6: 2-3-64: 0.843 (0.882)\n",
      "-6: 2-3-128: 0.882 (0.902)\n",
      "-6: 3-2-32: 0.882 (0.902)\n",
      "-6: 3-2-64: 0.882 (0.922)\n",
      "-6: 3-2-128: 0.922 (0.922)\n",
      "-7: 2-2-16: 0.863 (0.882)\n",
      "-7: 2-3-16: 0.863 (0.882)\n",
      "-7: 3-2-16: 0.863 (0.922)\n",
      "-7: 3-3-16: 0.882 (0.922)\n",
      "-7: 2-2-32: 0.863 (0.902)\n",
      "-7: 2-2-64: 0.824 (0.902)\n",
      "-7: 2-2-128: 0.863 (0.902)\n",
      "-7: 2-3-32: 0.863 (0.902)\n",
      "-7: 2-3-64: 0.863 (0.902)\n",
      "-7: 2-3-128: 0.882 (0.922)\n",
      "-7: 3-2-32: 0.882 (0.922)\n",
      "-7: 3-2-64: 0.882 (0.902)\n",
      "-7: 3-2-128: 0.882 (0.902)\n",
      "-8: 2-2-16: 0.843 (0.882)\n",
      "-8: 2-3-16: 0.843 (0.882)\n",
      "-8: 3-2-16: 0.784 (0.882)\n",
      "-8: 3-3-16: 0.824 (0.902)\n",
      "-8: 2-2-32: 0.804 (0.863)\n",
      "-8: 2-2-64: 0.804 (0.863)\n",
      "-8: 2-2-128: 0.824 (0.902)\n",
      "-8: 2-3-32: 0.843 (0.882)\n",
      "-8: 2-3-64: 0.804 (0.863)\n",
      "-8: 2-3-128: 0.843 (0.882)\n",
      "-8: 3-2-32: 0.843 (0.902)\n",
      "-8: 3-2-64: 0.804 (0.882)\n",
      "-8: 3-2-128: 0.882 (0.902)\n",
      "-9: 2-2-16: 0.824 (0.902)\n",
      "-9: 2-3-16: 0.824 (0.863)\n",
      "-9: 3-2-16: 0.804 (0.882)\n",
      "-9: 3-3-16: 0.824 (0.882)\n",
      "-9: 2-2-32: 0.843 (0.882)\n",
      "-9: 2-2-64: 0.824 (0.882)\n",
      "-9: 2-2-128: 0.843 (0.882)\n",
      "-9: 2-3-32: 0.843 (0.882)\n",
      "-9: 2-3-64: 0.824 (0.882)\n",
      "-9: 2-3-128: 0.843 (0.882)\n",
      "-9: 3-2-32: 0.863 (0.882)\n",
      "-9: 3-2-64: 0.824 (0.882)\n",
      "-9: 3-2-128: 0.843 (0.902)\n",
      "-10: 2-2-16: 0.902 (0.941)\n",
      "-10: 2-3-16: 0.882 (0.902)\n",
      "-10: 3-2-16: 0.843 (0.882)\n",
      "-10: 3-3-16: 0.863 (0.902)\n",
      "-10: 2-2-32: 0.922 (0.941)\n",
      "-10: 2-2-64: 0.882 (0.941)\n",
      "-10: 2-2-128: 0.902 (0.941)\n",
      "-10: 2-3-32: 0.902 (0.902)\n",
      "-10: 2-3-64: 0.824 (0.902)\n",
      "-10: 2-3-128: 0.843 (0.882)\n",
      "-10: 3-2-32: 0.941 (0.941)\n",
      "-10: 3-2-64: 0.922 (0.941)\n",
      "-10: 3-2-128: 0.882 (0.941)\n",
      "----- 54.10 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'L': 2, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 16},\n",
    "        # {'L': 2, 'K': 4, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 16},\n",
    "        # {'L': 4, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 16},\n",
    "        # {'L': 4, 'K': 3, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 128},\n",
    "\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 64},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 128},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 128},\n",
    "        ]\n",
    "\n",
    "best_accs7 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs7 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = Dual_GFGCN(IN_DIM, exp['hid_dim'], OUT_DIM, exp['L'], exp['K'], alpha=ALPHA,act=ACT,\n",
    "                          last_act=LAST_ACT, dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  exp['K'], masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs7[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs7[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}: {best_val_accs7[j,i]:.3f} ({best_accs7[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}' for exp in EXPS]\n",
    "table_over7 = summary_table(best_accs7, index_name)\n",
    "table7 = summary_table(best_val_accs7, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.033102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.028074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.029346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-16</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.031859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.864706</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.036630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-64</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.035349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-128</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.030440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.034856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-64</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.035130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-128</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.045775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-32</th>\n",
       "      <td>0.876471</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.030440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-64</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.050716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-128</th>\n",
       "      <td>0.874510</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.033044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean accs       med       std\n",
       "2-2-16    0.872549  0.872549  0.033102\n",
       "2-3-16    0.852941  0.852941  0.028074\n",
       "3-2-16    0.835294  0.843137  0.029346\n",
       "3-3-16    0.854902  0.852941  0.031859\n",
       "2-2-32    0.864706  0.862745  0.036630\n",
       "2-2-64    0.852941  0.862745  0.035349\n",
       "2-2-128   0.856863  0.862745  0.030440\n",
       "2-3-32    0.866667  0.862745  0.034856\n",
       "2-3-64    0.849020  0.852941  0.035130\n",
       "2-3-128   0.852941  0.862745  0.045775\n",
       "3-2-32    0.876471  0.882353  0.030440\n",
       "3-2-64    0.860784  0.882353  0.050716\n",
       "3-2-128   0.874510  0.882353  0.033044"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSO, bias, and batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1: orig-True-False: 0.824 (0.863)\n",
      "- 1: orig-True-True: 0.745 (0.824)\n",
      "- 1: orig-False-True: 0.765 (0.824)\n",
      "- 1: orig-False-False: 0.804 (0.863)\n",
      "- 2: orig-True-False: 0.843 (0.941)\n",
      "- 2: orig-True-True: 0.765 (0.804)\n",
      "- 2: orig-False-True: 0.686 (0.784)\n",
      "- 2: orig-False-False: 0.863 (0.922)\n",
      "- 3: orig-True-False: 0.882 (0.922)\n",
      "- 3: orig-True-True: 0.843 (0.863)\n",
      "- 3: orig-False-True: 0.882 (0.922)\n",
      "- 3: orig-False-False: 0.902 (0.922)\n",
      "- 4: orig-True-False: 0.843 (0.941)\n",
      "- 4: orig-True-True: 0.863 (0.922)\n",
      "- 4: orig-False-True: 0.843 (0.902)\n",
      "- 4: orig-False-False: 0.902 (0.941)\n",
      "- 5: orig-True-False: 0.882 (0.922)\n",
      "- 5: orig-True-True: 0.902 (0.902)\n",
      "- 5: orig-False-True: 0.843 (0.902)\n",
      "- 5: orig-False-False: 0.922 (0.922)\n",
      "- 6: orig-True-False: 0.902 (0.922)\n",
      "- 6: orig-True-True: 0.804 (0.824)\n",
      "- 6: orig-False-True: 0.804 (0.863)\n",
      "- 6: orig-False-False: 0.882 (0.902)\n",
      "- 7: orig-True-False: 0.843 (0.902)\n",
      "- 7: orig-True-True: 0.765 (0.882)\n",
      "- 7: orig-False-True: 0.784 (0.804)\n",
      "- 7: orig-False-False: 0.843 (0.902)\n",
      "- 8: orig-True-False: 0.863 (0.882)\n",
      "- 8: orig-True-True: 0.725 (0.824)\n",
      "- 8: orig-False-True: 0.765 (0.843)\n",
      "- 8: orig-False-False: 0.843 (0.882)\n",
      "- 9: orig-True-False: 0.784 (0.902)\n",
      "- 9: orig-True-True: 0.745 (0.843)\n",
      "- 9: orig-False-True: 0.804 (0.882)\n",
      "- 9: orig-False-False: 0.863 (0.882)\n",
      "- 10: orig-True-False: 0.922 (0.941)\n",
      "- 10: orig-True-True: 0.804 (0.902)\n",
      "- 10: orig-False-True: 0.824 (0.902)\n",
      "- 10: orig-False-False: 0.882 (0.922)\n",
      "----- 18.58 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [{'GSO': 'orig', 'bias': True, 'bn': False},\n",
    "        {'GSO': 'orig', 'bias': True, 'bn': True},\n",
    "        {'GSO': 'orig', 'bias': False, 'bn': True},\n",
    "        {'GSO': 'orig', 'bias': False, 'bn': False},\n",
    "        ]\n",
    "\n",
    "best_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = Dual_GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, alpha=ALPHA, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0, bias=exp['bn'], batch_norm=exp['bn'])\n",
    "        \n",
    "        if exp['GSO'] == 'sym':\n",
    "            A_aux = (A + A.T)/2\n",
    "            A_aux = np.where(A_aux > 0, 1, 0)\n",
    "        elif exp['GSO'] == 'trans':\n",
    "            A_aux = A.T\n",
    "        else:\n",
    "            A_aux = A\n",
    "\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A_aux, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A_aux).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  K, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs3b[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs3b[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'- {i+1}: {exp[\"GSO\"]}-{exp[\"bias\"]}-{exp[\"bn\"]}: {best_val_accs3b[j,i]:.3f} ({best_accs3b[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"GSO\"]}-{exp[\"bias\"]}-{exp[\"bn\"]}' for exp in EXPS]\n",
    "table_over3b = summary_table(best_accs3b, index_name)\n",
    "table3b = summary_table(best_val_accs3b, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>orig-True-False</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.038021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig-True-True</th>\n",
       "      <td>0.796078</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.054902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig-False-True</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.051729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig-False-False</th>\n",
       "      <td>0.870588</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.033044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean accs       med       std\n",
       "orig-True-False    0.858824  0.852941  0.038021\n",
       "orig-True-True     0.796078  0.784314  0.054902\n",
       "orig-False-True    0.800000  0.803922  0.051729\n",
       "orig-False-False   0.870588  0.872549  0.033044"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearities and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.863)\n",
      "-1: ReLU()-Softmax(dim=1)-NLLLoss(): 0.804 (0.882)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.843 (0.863)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.765 (0.882)\n",
      "-1: ReLU()-Identity()-CrossEntropyLoss(): 0.824 (0.863)\n",
      "-1: ReLU()-Identity()-NLLLoss(): 0.373 (0.373)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.843)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.745 (0.843)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.863)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.824 (0.863)\n",
      "-1: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.863)\n",
      "-1: Identity()-Softmax(dim=1)-NLLLoss(): 0.804 (0.843)\n",
      "-1: Identity()-Identity()-CrossEntropyLoss(): 0.804 (0.843)\n",
      "-2: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-2: ReLU()-Softmax(dim=1)-NLLLoss(): 0.882 (0.902)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-2: ReLU()-Identity()-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-2: ReLU()-Identity()-NLLLoss(): 0.608 (0.627)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-2: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-2: Identity()-Softmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-2: Identity()-Identity()-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-3: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.922 (0.922)\n",
      "-3: ReLU()-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.902 (0.941)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.902 (0.941)\n",
      "-3: ReLU()-Identity()-CrossEntropyLoss(): 0.882 (0.941)\n",
      "-3: ReLU()-Identity()-NLLLoss(): 0.412 (0.412)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.922)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.941)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.922 (0.922)\n",
      "-3: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-3: Identity()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-3: Identity()-Identity()-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-4: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.961)\n",
      "-4: ReLU()-Softmax(dim=1)-NLLLoss(): 0.902 (0.941)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.863 (0.961)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.902 (0.961)\n",
      "-4: ReLU()-Identity()-CrossEntropyLoss(): 0.922 (0.941)\n",
      "-4: ReLU()-Identity()-NLLLoss(): 0.451 (0.471)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.941 (0.941)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.922 (0.922)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.922 (0.961)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.902 (0.961)\n",
      "-4: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.922 (0.941)\n",
      "-4: Identity()-Softmax(dim=1)-NLLLoss(): 0.902 (0.961)\n",
      "-4: Identity()-Identity()-CrossEntropyLoss(): 0.902 (0.941)\n",
      "-5: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.922)\n",
      "-5: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.902 (0.941)\n",
      "-5: ReLU()-Identity()-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-5: ReLU()-Identity()-NLLLoss(): 0.373 (0.392)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.902)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.922)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.824 (0.922)\n",
      "-5: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-5: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-5: Identity()-Identity()-CrossEntropyLoss(): 0.784 (0.902)\n",
      "-6: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-6: ReLU()-Softmax(dim=1)-NLLLoss(): 0.824 (0.902)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-6: ReLU()-Identity()-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-6: ReLU()-Identity()-NLLLoss(): 0.510 (0.569)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.882 (0.902)\n",
      "-6: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.882)\n",
      "-6: Identity()-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-6: Identity()-Identity()-CrossEntropyLoss(): 0.804 (0.882)\n",
      "-7: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.902)\n",
      "-7: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.843 (0.922)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.902 (0.902)\n",
      "-7: ReLU()-Identity()-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-7: ReLU()-Identity()-NLLLoss(): 0.490 (0.510)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-7: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-7: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-7: Identity()-Identity()-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-8: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-8: ReLU()-Softmax(dim=1)-NLLLoss(): 0.804 (0.863)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.824 (0.902)\n",
      "-8: ReLU()-Identity()-CrossEntropyLoss(): 0.824 (0.902)\n",
      "-8: ReLU()-Identity()-NLLLoss(): 0.510 (0.510)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.863)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.882)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.824 (0.902)\n",
      "-8: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-8: Identity()-Softmax(dim=1)-NLLLoss(): 0.784 (0.863)\n",
      "-8: Identity()-Identity()-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-9: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.882)\n",
      "-9: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.824 (0.882)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.863 (0.882)\n",
      "-9: ReLU()-Identity()-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-9: ReLU()-Identity()-NLLLoss(): 0.510 (0.510)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.863 (0.863)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.863 (0.882)\n",
      "-9: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-9: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.882)\n",
      "-9: Identity()-Identity()-CrossEntropyLoss(): 0.824 (0.843)\n",
      "-10: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.941 (0.941)\n",
      "-10: ReLU()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.902 (0.941)\n",
      "-10: ReLU()-Identity()-CrossEntropyLoss(): 0.902 (0.961)\n",
      "-10: ReLU()-Identity()-NLLLoss(): 0.490 (0.490)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.941)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.922 (0.941)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-10: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-10: Identity()-Softmax(dim=1)-NLLLoss(): 0.922 (0.922)\n",
      "-10: Identity()-Identity()-CrossEntropyLoss(): 0.863 (0.941)\n",
      "----- 58.38 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [{'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},]\n",
    "\n",
    "best_accs8 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs8 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = Dual_GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, alpha=ALPHA, act=exp['act'], last_act=exp['lact'],\n",
    "                          dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  K, masks, exp['loss'], device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs8[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs8[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}: {best_val_accs8[j,i]:.3f} ({best_accs8[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}' for exp in EXPS]\n",
    "table_over8 = summary_table(best_accs8, index_name)\n",
    "table8 = summary_table(best_val_accs8, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.050980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.035294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.025490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.043669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.864706</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.029672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Identity()-NLLLoss()</th>\n",
       "      <td>0.472549</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.068739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.046607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.044019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.037255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.031617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.030376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.039411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.845098</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.038673</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean accs       med  \\\n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()             0.866667  0.872549   \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                      0.854902  0.852941   \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()          0.860784  0.852941   \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                   0.866667  0.882353   \n",
       "ReLU()-Identity()-CrossEntropyLoss()                 0.864706  0.862745   \n",
       "ReLU()-Identity()-NLLLoss()                          0.472549  0.490196   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()     0.852941  0.843137   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()              0.850980  0.852941   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...   0.868627  0.862745   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...   0.862745  0.862745   \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()         0.862745  0.862745   \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                  0.854902  0.852941   \n",
       "Identity()-Identity()-CrossEntropyLoss()             0.845098  0.852941   \n",
       "\n",
       "                                                         std  \n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()            0.050980  \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                     0.035294  \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()         0.025490  \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                  0.043669  \n",
       "ReLU()-Identity()-CrossEntropyLoss()                0.029672  \n",
       "ReLU()-Identity()-NLLLoss()                         0.068739  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()    0.046607  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()             0.044019  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...  0.037255  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...  0.031617  \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()        0.030376  \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                 0.039411  \n",
       "Identity()-Identity()-CrossEntropyLoss()            0.038673  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters for norm A: True\n"
     ]
    }
   ],
   "source": [
    "EXPS = [\n",
    "        {'name': 'Kipf', 'norm': 'none'},\n",
    "        {'name': 'Kipf', 'norm': 'both'},\n",
    "\n",
    "        {'name': 'A-GCNN', 'norm': False},\n",
    "        {'name': 'A-GCNN', 'norm': True},\n",
    "\n",
    "        {'name': 'H-GCNN', 'norm': False}, # This should be the same as A-GCNN not norm\n",
    "        {'name': 'H-GCNN', 'norm': True},\n",
    "\n",
    "        {'name': 'Dual-GFCNN-b', 'norm': False, 'bias': True},\n",
    "        {'name': 'Dual-GFCNN-b', 'norm': True, 'bias': True},\n",
    "        {'name': 'Dual-GFCNN', 'norm': False, 'bias': False},\n",
    "        {'name': 'Dual-GFCNN', 'norm': True, 'bias': False},\n",
    "\n",
    "        {'name': 'W-GCN-A', 'norm': False},\n",
    "        {'name': 'W-GCN-A', 'norm': True},\n",
    "]\n",
    "\n",
    "print(\"Parameters for norm A:\", NORM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- RUN: 1\n",
      "\tKipf-none: acc = 0.588  -  acc2 = 0.588  -  acc (over) = 0.608\n",
      "\tKipf-both: acc = 0.588  -  acc2 = 0.529  -  acc (over) = 0.588\n",
      "\tA-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tA-GCNN-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.882\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tH-GCNN-True: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.863\n",
      "\tDual-GFCNN-b-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.882\n",
      "\tDual-GFCNN-b-True: acc = 0.804  -  acc2 = 0.804  -  acc (over) = 0.863\n",
      "\tDual-GFCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.863\n",
      "\tDual-GFCNN-True: acc = 0.843  -  acc2 = 0.824  -  acc (over) = 0.863\n",
      "\tW-GCN-A-False: acc = 0.667  -  acc2 = 0.667  -  acc (over) = 0.725\n",
      "\tW-GCN-A-True: acc = 0.804  -  acc2 = 0.804  -  acc (over) = 0.902\n",
      "- RUN: 2\n",
      "\tKipf-none: acc = 0.647  -  acc2 = 0.588  -  acc (over) = 0.706\n",
      "\tKipf-both: acc = 0.627  -  acc2 = 0.608  -  acc (over) = 0.706\n",
      "\tA-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.941\n",
      "\tA-GCNN-True: acc = 0.902  -  acc2 = 0.882  -  acc (over) = 0.922\n",
      "\tH-GCNN-False: acc = 0.882  -  acc2 = 0.843  -  acc (over) = 0.941\n",
      "\tH-GCNN-True: acc = 0.902  -  acc2 = 0.922  -  acc (over) = 0.922\n",
      "\tDual-GFCNN-b-False: acc = 0.902  -  acc2 = 0.902  -  acc (over) = 0.941\n",
      "\tDual-GFCNN-b-True: acc = 0.882  -  acc2 = 0.863  -  acc (over) = 0.922\n",
      "\tDual-GFCNN-False: acc = 0.902  -  acc2 = 0.922  -  acc (over) = 0.941\n",
      "\tDual-GFCNN-True: acc = 0.902  -  acc2 = 0.882  -  acc (over) = 0.922\n",
      "\tW-GCN-A-False: acc = 0.843  -  acc2 = 0.804  -  acc (over) = 0.863\n",
      "\tW-GCN-A-True: acc = 0.843  -  acc2 = 0.882  -  acc (over) = 0.922\n",
      "- RUN: 3\n",
      "\tKipf-none: acc = 0.471  -  acc2 = 0.471  -  acc (over) = 0.510\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.608  -  acc (over) = 0.627\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.902  -  acc (over) = 0.922\n",
      "\tA-GCNN-True: acc = 0.843  -  acc2 = 0.863  -  acc (over) = 0.922\n",
      "\tH-GCNN-False: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tDual-GFCNN-b-False: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.922\n",
      "\tDual-GFCNN-b-True: acc = 0.922  -  acc2 = 0.863  -  acc (over) = 0.922\n",
      "\tDual-GFCNN-False: acc = 0.882  -  acc2 = 0.902  -  acc (over) = 0.941\n",
      "\tDual-GFCNN-True: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.922\n",
      "\tW-GCN-A-False: acc = 0.784  -  acc2 = 0.745  -  acc (over) = 0.804\n",
      "\tW-GCN-A-True: acc = 0.765  -  acc2 = 0.824  -  acc (over) = 0.843\n",
      "- RUN: 4\n",
      "\tKipf-none: acc = 0.588  -  acc2 = 0.588  -  acc (over) = 0.608\n",
      "\tKipf-both: acc = 0.529  -  acc2 = 0.529  -  acc (over) = 0.569\n",
      "\tA-GCNN-False: acc = 0.922  -  acc2 = 0.941  -  acc (over) = 0.961\n",
      "\tA-GCNN-True: acc = 0.961  -  acc2 = 0.902  -  acc (over) = 0.961\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.922  -  acc (over) = 0.941\n",
      "\tH-GCNN-True: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.882\n",
      "\tDual-GFCNN-b-False: acc = 0.882  -  acc2 = 0.902  -  acc (over) = 0.961\n",
      "\tDual-GFCNN-b-True: acc = 0.902  -  acc2 = 0.922  -  acc (over) = 0.961\n",
      "\tDual-GFCNN-False: acc = 0.922  -  acc2 = 0.922  -  acc (over) = 0.941\n",
      "\tDual-GFCNN-True: acc = 0.941  -  acc2 = 0.941  -  acc (over) = 0.961\n",
      "\tW-GCN-A-False: acc = 0.745  -  acc2 = 0.804  -  acc (over) = 0.824\n",
      "\tW-GCN-A-True: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "- RUN: 5\n",
      "\tKipf-none: acc = 0.549  -  acc2 = 0.549  -  acc (over) = 0.588\n",
      "\tKipf-both: acc = 0.529  -  acc2 = 0.529  -  acc (over) = 0.647\n",
      "\tA-GCNN-False: acc = 0.902  -  acc2 = 0.882  -  acc (over) = 0.941\n",
      "\tA-GCNN-True: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.941\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.941\n",
      "\tH-GCNN-True: acc = 0.843  -  acc2 = 0.784  -  acc (over) = 0.941\n",
      "\tDual-GFCNN-b-False: acc = 0.863  -  acc2 = 0.902  -  acc (over) = 0.941\n",
      "\tDual-GFCNN-b-True: acc = 0.902  -  acc2 = 0.824  -  acc (over) = 0.922\n",
      "\tDual-GFCNN-False: acc = 0.922  -  acc2 = 0.882  -  acc (over) = 0.922\n",
      "\tDual-GFCNN-True: acc = 0.902  -  acc2 = 0.902  -  acc (over) = 0.922\n",
      "\tW-GCN-A-False: acc = 0.706  -  acc2 = 0.706  -  acc (over) = 0.765\n",
      "\tW-GCN-A-True: acc = 0.686  -  acc2 = 0.686  -  acc (over) = 0.882\n",
      "- RUN: 6\n",
      "\tKipf-none: acc = 0.529  -  acc2 = 0.529  -  acc (over) = 0.529\n",
      "\tKipf-both: acc = 0.588  -  acc2 = 0.569  -  acc (over) = 0.608\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.922\n",
      "\tA-GCNN-True: acc = 0.863  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tH-GCNN-False: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.765  -  acc2 = 0.765  -  acc (over) = 0.824\n",
      "\tDual-GFCNN-b-False: acc = 0.882  -  acc2 = 0.804  -  acc (over) = 0.902\n",
      "\tDual-GFCNN-b-True: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tDual-GFCNN-False: acc = 0.902  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tDual-GFCNN-True: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.647  -  acc2 = 0.627  -  acc (over) = 0.706\n",
      "\tW-GCN-A-True: acc = 0.765  -  acc2 = 0.765  -  acc (over) = 0.804\n",
      "- RUN: 7\n",
      "\tKipf-none: acc = 0.471  -  acc2 = 0.471  -  acc (over) = 0.471\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.627  -  acc (over) = 0.667\n",
      "\tA-GCNN-False: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.882  -  acc2 = 0.902  -  acc (over) = 0.922\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tDual-GFCNN-b-False: acc = 0.863  -  acc2 = 0.824  -  acc (over) = 0.922\n",
      "\tDual-GFCNN-b-True: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tDual-GFCNN-False: acc = 0.863  -  acc2 = 0.804  -  acc (over) = 0.882\n",
      "\tDual-GFCNN-True: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.706  -  acc2 = 0.765  -  acc (over) = 0.824\n",
      "\tW-GCN-A-True: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.882\n",
      "- RUN: 8\n",
      "\tKipf-none: acc = 0.549  -  acc2 = 0.549  -  acc (over) = 0.549\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.647  -  acc (over) = 0.686\n",
      "\tA-GCNN-False: acc = 0.863  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.863  -  acc2 = 0.882  -  acc (over) = 0.882\n",
      "\tH-GCNN-False: acc = 0.824  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tDual-GFCNN-b-False: acc = 0.863  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tDual-GFCNN-b-True: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tDual-GFCNN-False: acc = 0.843  -  acc2 = 0.824  -  acc (over) = 0.902\n",
      "\tDual-GFCNN-True: acc = 0.863  -  acc2 = 0.804  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.745  -  acc2 = 0.784  -  acc (over) = 0.843\n",
      "\tW-GCN-A-True: acc = 0.824  -  acc2 = 0.863  -  acc (over) = 0.863\n",
      "- RUN: 9\n",
      "\tKipf-none: acc = 0.627  -  acc2 = 0.627  -  acc (over) = 0.647\n",
      "\tKipf-both: acc = 0.569  -  acc2 = 0.608  -  acc (over) = 0.608\n",
      "\tA-GCNN-False: acc = 0.843  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.863  -  acc2 = 0.804  -  acc (over) = 0.902\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.804  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.804  -  acc2 = 0.824  -  acc (over) = 0.863\n",
      "\tDual-GFCNN-b-False: acc = 0.863  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tDual-GFCNN-b-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.882\n",
      "\tDual-GFCNN-False: acc = 0.843  -  acc2 = 0.804  -  acc (over) = 0.882\n",
      "\tDual-GFCNN-True: acc = 0.824  -  acc2 = 0.863  -  acc (over) = 0.882\n",
      "\tW-GCN-A-False: acc = 0.667  -  acc2 = 0.667  -  acc (over) = 0.745\n",
      "\tW-GCN-A-True: acc = 0.824  -  acc2 = 0.804  -  acc (over) = 0.843\n",
      "- RUN: 10\n",
      "\tKipf-none: acc = 0.608  -  acc2 = 0.608  -  acc (over) = 0.608\n",
      "\tKipf-both: acc = 0.569  -  acc2 = 0.510  -  acc (over) = 0.569\n",
      "\tA-GCNN-False: acc = 0.922  -  acc2 = 0.902  -  acc (over) = 0.922\n",
      "\tA-GCNN-True: acc = 0.902  -  acc2 = 0.941  -  acc (over) = 0.941\n",
      "\tH-GCNN-False: acc = 0.922  -  acc2 = 0.922  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.922\n",
      "\tDual-GFCNN-b-False: acc = 0.902  -  acc2 = 0.882  -  acc (over) = 0.922\n",
      "\tDual-GFCNN-b-True: acc = 0.922  -  acc2 = 0.922  -  acc (over) = 0.941\n",
      "\tDual-GFCNN-False: acc = 0.922  -  acc2 = 0.922  -  acc (over) = 0.922\n",
      "\tDual-GFCNN-True: acc = 0.902  -  acc2 = 0.922  -  acc (over) = 0.941\n",
      "\tW-GCN-A-False: acc = 0.784  -  acc2 = 0.686  -  acc (over) = 0.804\n",
      "\tW-GCN-A-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.922\n"
     ]
    }
   ],
   "source": [
    "N_RUNS = 10\n",
    "\n",
    "best_accs = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs2 = np.zeros((len(EXPS), N_RUNS))\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    print(f'- RUN: {i+1}')\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        # t_i = time.time()\n",
    "        if exp['name'] == 'Kipf':\n",
    "            arch = GCNN_2L(IN_DIM, HID_DIM, OUT_DIM, act=ACT, last_act=LAST_ACT,\n",
    "                           dropout=DROPOUT, norm=exp['norm'])\n",
    "            S = dgl.from_networkx(nx.from_numpy_array(A)).add_self_loop().to(device)\n",
    "            \n",
    "        elif exp['name'] == 'A-GCNN':\n",
    "            arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "            dropout=DROPOUT, diff_layer=GFGCNLayer, init_h0=h0)\n",
    "            if exp['norm']:\n",
    "                S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "            else:\n",
    "                S = torch.Tensor(A).to(device)\n",
    "\n",
    "        elif exp['name'] == 'H-GCNN':\n",
    "            arch = GFGCN_Spows(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                               dropout=DROPOUT, norm=exp['norm'], dev=device)\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        elif exp['name'] in 'Dual-GFCNN-b':\n",
    "            arch = Dual_GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, alpha=ALPHA,\n",
    "                              last_act=LAST_ACT, dropout=DROPOUT, init_h0=h0, bias=exp['bias'])\n",
    "            if exp['norm']:\n",
    "                S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "            else:\n",
    "                S = torch.Tensor(A).to(device)\n",
    "\n",
    "        elif exp['name'] == 'W-GCN-A':\n",
    "            arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                         dropout=DROPOUT, diff_layer=GFGCN_noh_Layer)\n",
    "            if exp['norm']:\n",
    "                S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "            else:\n",
    "                S = torch.Tensor(A).to(device)  \n",
    "            \n",
    "        else:\n",
    "            raise Exception(f'ERROR: Unknown architecture: {exp[\"name\"]}')\n",
    "\n",
    "        if exp['name'] in ['Kipf', 'W-GCN-A']:\n",
    "            model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "            loss, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "        else:\n",
    "            model = GF_NodeClassModel(arch, S, K, masks, LOSS_FN, device=device)\n",
    "            loss, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                                    epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs[j,i] = model.test(feat, model.S, labels, masks['test'])\n",
    "        best_val_accs2[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'\\t{exp[\"name\"]}-{exp[\"norm\"]}: acc = {best_val_accs[j,i]:.3f}  -  acc2 = {best_val_accs2[j,i]:.3f}  -  acc (over) = {best_accs[j,i]:.3f}')\n",
    "\n",
    "\n",
    "# Print results\n",
    "index_name = [f'{exp[\"name\"]}-{exp[\"norm\"]}' for exp in EXPS]\n",
    "table_comp_over = summary_table(best_accs, index_name)\n",
    "table_comp = summary_table(best_val_accs, index_name)\n",
    "table_comp2 = summary_table(best_val_accs2, index_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kipf-none</th>\n",
       "      <td>0.562745</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.057535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kipf-both</th>\n",
       "      <td>0.582353</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.031677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-False</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.035942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-True</th>\n",
       "      <td>0.874510</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.037409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-False</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.026013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-True</th>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.038423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dual-GFCNN-b-False</th>\n",
       "      <td>0.876471</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.015314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dual-GFCNN-b-True</th>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.038473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dual-GFCNN-False</th>\n",
       "      <td>0.884314</td>\n",
       "      <td>0.892157</td>\n",
       "      <td>0.032159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dual-GFCNN-True</th>\n",
       "      <td>0.874510</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.034187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-False</th>\n",
       "      <td>0.729412</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.059344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-True</th>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.056285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mean accs       med       std\n",
       "Kipf-none            0.562745  0.568627  0.057535\n",
       "Kipf-both            0.582353  0.588235  0.031677\n",
       "A-GCNN-False         0.866667  0.852941  0.035942\n",
       "A-GCNN-True          0.874510  0.862745  0.037409\n",
       "H-GCNN-False         0.866667  0.862745  0.026013\n",
       "H-GCNN-True          0.850980  0.862745  0.038423\n",
       "Dual-GFCNN-b-False   0.876471  0.872549  0.015314\n",
       "Dual-GFCNN-b-True    0.872549  0.872549  0.038473\n",
       "Dual-GFCNN-False     0.884314  0.892157  0.032159\n",
       "Dual-GFCNN-True      0.874510  0.862745  0.034187\n",
       "W-GCN-A-False        0.729412  0.725490  0.059344\n",
       "W-GCN-A-True         0.811765  0.823529  0.056285"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPS = [\n",
    "        {'name': 'Kipf', 'norm': 'none'},\n",
    "        {'name': 'Kipf', 'norm': 'both'},\n",
    "\n",
    "        {'name': 'A-GCNN', 'norm': False},\n",
    "        {'name': 'A-GCNN', 'norm': True},\n",
    "\n",
    "        {'name': 'H-GCNN', 'norm': False}, # This should be the same as A-GCNN not norm\n",
    "        {'name': 'H-GCNN', 'norm': True},\n",
    "\n",
    "        {'name': 'Dual-GFCNN-b', 'norm': False, 'bias': True},\n",
    "        {'name': 'Dual-GFCNN-b', 'norm': True, 'bias': True},\n",
    "        {'name': 'Dual-GFCNN', 'norm': False, 'bias': False},\n",
    "        {'name': 'Dual-GFCNN', 'norm': True, 'bias': False},\n",
    "\n",
    "        {'name': 'W-GCN-A', 'norm': False},\n",
    "        {'name': 'W-GCN-A', 'norm': True},\n",
    "]\n",
    "\n",
    "LAST_ACT = nn.LogSoftmax(dim=1)\n",
    "LOSS_FN = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- RUN: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tKipf-none: acc = 0.490  -  acc2 = 0.510  -  acc (over) = 0.569\n",
      "\tKipf-both: acc = 0.549  -  acc2 = 0.569  -  acc (over) = 0.608\n",
      "\tA-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tA-GCNN-True: acc = 0.824  -  acc2 = 0.804  -  acc (over) = 0.882\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tH-GCNN-True: acc = 0.725  -  acc2 = 0.725  -  acc (over) = 0.863\n",
      "\tDual-GFCNN-b-False: acc = 0.843  -  acc2 = 0.804  -  acc (over) = 0.863\n",
      "\tDual-GFCNN-b-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.863\n",
      "\tDual-GFCNN-False: acc = 0.353  -  acc2 = 0.725  -  acc (over) = 0.765\n",
      "\tDual-GFCNN-True: acc = 0.824  -  acc2 = 0.863  -  acc (over) = 0.863\n",
      "\tW-GCN-A-False: acc = 0.627  -  acc2 = 0.667  -  acc (over) = 0.745\n",
      "\tW-GCN-A-True: acc = 0.843  -  acc2 = 0.804  -  acc (over) = 0.882\n",
      "- RUN: 2\n",
      "\tKipf-none: acc = 0.549  -  acc2 = 0.745  -  acc (over) = 0.765\n",
      "\tKipf-both: acc = 0.627  -  acc2 = 0.549  -  acc (over) = 0.627\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.863  -  acc (over) = 0.922\n",
      "\tA-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.922\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.941\n",
      "\tH-GCNN-True: acc = 0.922  -  acc2 = 0.824  -  acc (over) = 0.922\n",
      "\tDual-GFCNN-b-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tDual-GFCNN-b-True: acc = 0.843  -  acc2 = 0.863  -  acc (over) = 0.922\n",
      "\tDual-GFCNN-False: acc = 0.863  -  acc2 = 0.843  -  acc (over) = 0.941\n",
      "\tDual-GFCNN-True: acc = 0.843  -  acc2 = 0.824  -  acc (over) = 0.922\n",
      "\tW-GCN-A-False: acc = 0.647  -  acc2 = 0.784  -  acc (over) = 0.863\n",
      "\tW-GCN-A-True: acc = 0.804  -  acc2 = 0.863  -  acc (over) = 0.922\n",
      "- RUN: 3\n",
      "\tKipf-none: acc = 0.529  -  acc2 = 0.588  -  acc (over) = 0.608\n",
      "\tKipf-both: acc = 0.510  -  acc2 = 0.510  -  acc (over) = 0.608\n",
      "\tA-GCNN-False: acc = 0.882  -  acc2 = 0.863  -  acc (over) = 0.922\n",
      "\tA-GCNN-True: acc = 0.882  -  acc2 = 0.804  -  acc (over) = 0.922\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.922  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.902  -  acc2 = 0.902  -  acc (over) = 0.902\n",
      "\tDual-GFCNN-b-False: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.922\n",
      "\tDual-GFCNN-b-True: acc = 0.882  -  acc2 = 0.843  -  acc (over) = 0.941\n",
      "\tDual-GFCNN-False: acc = 0.902  -  acc2 = 0.902  -  acc (over) = 0.922\n",
      "\tDual-GFCNN-True: acc = 0.922  -  acc2 = 0.882  -  acc (over) = 0.922\n",
      "\tW-GCN-A-False: acc = 0.725  -  acc2 = 0.784  -  acc (over) = 0.804\n",
      "\tW-GCN-A-True: acc = 0.863  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "- RUN: 4\n",
      "\tKipf-none: acc = 0.373  -  acc2 = 0.471  -  acc (over) = 0.471\n",
      "\tKipf-both: acc = 0.431  -  acc2 = 0.510  -  acc (over) = 0.549\n",
      "\tA-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.941\n",
      "\tA-GCNN-True: acc = 0.922  -  acc2 = 0.902  -  acc (over) = 0.941\n",
      "\tH-GCNN-False: acc = 0.922  -  acc2 = 0.902  -  acc (over) = 0.961\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tDual-GFCNN-b-False: acc = 0.941  -  acc2 = 0.902  -  acc (over) = 0.961\n",
      "\tDual-GFCNN-b-True: acc = 0.922  -  acc2 = 0.882  -  acc (over) = 0.941\n",
      "\tDual-GFCNN-False: acc = 0.882  -  acc2 = 0.922  -  acc (over) = 0.941\n",
      "\tDual-GFCNN-True: acc = 0.941  -  acc2 = 0.902  -  acc (over) = 0.961\n",
      "\tW-GCN-A-False: acc = 0.608  -  acc2 = 0.784  -  acc (over) = 0.804\n",
      "\tW-GCN-A-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.941\n",
      "- RUN: 5\n",
      "\tKipf-none: acc = 0.373  -  acc2 = 0.353  -  acc (over) = 0.412\n",
      "\tKipf-both: acc = 0.510  -  acc2 = 0.431  -  acc (over) = 0.588\n",
      "\tA-GCNN-False: acc = 0.882  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.902  -  acc2 = 0.902  -  acc (over) = 0.922\n",
      "\tH-GCNN-False: acc = 0.392  -  acc2 = 0.569  -  acc (over) = 0.608\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.804  -  acc (over) = 0.882\n",
      "\tDual-GFCNN-b-False: acc = 0.902  -  acc2 = 0.843  -  acc (over) = 0.941\n",
      "\tDual-GFCNN-b-True: acc = 0.882  -  acc2 = 0.902  -  acc (over) = 0.922\n",
      "\tDual-GFCNN-False: acc = 0.902  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tDual-GFCNN-True: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.941\n",
      "\tW-GCN-A-False: acc = 0.569  -  acc2 = 0.647  -  acc (over) = 0.725\n",
      "\tW-GCN-A-True: acc = 0.784  -  acc2 = 0.745  -  acc (over) = 0.882\n",
      "- RUN: 6\n",
      "\tKipf-none: acc = 0.549  -  acc2 = 0.549  -  acc (over) = 0.627\n",
      "\tKipf-both: acc = 0.471  -  acc2 = 0.569  -  acc (over) = 0.588\n",
      "\tA-GCNN-False: acc = 0.510  -  acc2 = 0.588  -  acc (over) = 0.647\n",
      "\tA-GCNN-True: acc = 0.843  -  acc2 = 0.863  -  acc (over) = 0.922\n",
      "\tH-GCNN-False: acc = 0.902  -  acc2 = 0.902  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.882\n",
      "\tDual-GFCNN-b-False: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.922\n",
      "\tDual-GFCNN-b-True: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tDual-GFCNN-False: acc = 0.843  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tDual-GFCNN-True: acc = 0.863  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.627  -  acc2 = 0.667  -  acc (over) = 0.706\n",
      "\tW-GCN-A-True: acc = 0.804  -  acc2 = 0.804  -  acc (over) = 0.863\n",
      "- RUN: 7\n",
      "\tKipf-none: acc = 0.588  -  acc2 = 0.647  -  acc (over) = 0.647\n",
      "\tKipf-both: acc = 0.549  -  acc2 = 0.608  -  acc (over) = 0.627\n",
      "\tA-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.843  -  acc2 = 0.824  -  acc (over) = 0.922\n",
      "\tH-GCNN-False: acc = 0.412  -  acc2 = 0.667  -  acc (over) = 0.667\n",
      "\tH-GCNN-True: acc = 0.804  -  acc2 = 0.804  -  acc (over) = 0.902\n",
      "\tDual-GFCNN-b-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.922\n",
      "\tDual-GFCNN-b-True: acc = 0.863  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tDual-GFCNN-False: acc = 0.863  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tDual-GFCNN-True: acc = 0.882  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.804  -  acc2 = 0.784  -  acc (over) = 0.824\n",
      "\tW-GCN-A-True: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "- RUN: 8\n",
      "\tKipf-none: acc = 0.588  -  acc2 = 0.569  -  acc (over) = 0.627\n",
      "\tKipf-both: acc = 0.627  -  acc2 = 0.549  -  acc (over) = 0.647\n",
      "\tA-GCNN-False: acc = 0.843  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.863  -  acc2 = 0.902  -  acc (over) = 0.922\n",
      "\tH-GCNN-False: acc = 0.882  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.882\n",
      "\tDual-GFCNN-b-False: acc = 0.882  -  acc2 = 0.804  -  acc (over) = 0.882\n",
      "\tDual-GFCNN-b-True: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tDual-GFCNN-False: acc = 0.882  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tDual-GFCNN-True: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tW-GCN-A-False: acc = 0.745  -  acc2 = 0.745  -  acc (over) = 0.765\n",
      "\tW-GCN-A-True: acc = 0.824  -  acc2 = 0.784  -  acc (over) = 0.863\n",
      "- RUN: 9\n",
      "\tKipf-none: acc = 0.451  -  acc2 = 0.451  -  acc (over) = 0.451\n",
      "\tKipf-both: acc = 0.510  -  acc2 = 0.529  -  acc (over) = 0.549\n",
      "\tA-GCNN-False: acc = 0.490  -  acc2 = 0.451  -  acc (over) = 0.510\n",
      "\tA-GCNN-True: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tH-GCNN-False: acc = 0.412  -  acc2 = 0.667  -  acc (over) = 0.706\n",
      "\tH-GCNN-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.882\n",
      "\tDual-GFCNN-b-False: acc = 0.569  -  acc2 = 0.745  -  acc (over) = 0.784\n",
      "\tDual-GFCNN-b-True: acc = 0.863  -  acc2 = 0.824  -  acc (over) = 0.902\n",
      "\tDual-GFCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.882\n",
      "\tDual-GFCNN-True: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.667  -  acc2 = 0.745  -  acc (over) = 0.765\n",
      "\tW-GCN-A-True: acc = 0.745  -  acc2 = 0.745  -  acc (over) = 0.843\n",
      "- RUN: 10\n",
      "\tKipf-none: acc = 0.451  -  acc2 = 0.510  -  acc (over) = 0.588\n",
      "\tKipf-both: acc = 0.549  -  acc2 = 0.549  -  acc (over) = 0.549\n",
      "\tA-GCNN-False: acc = 0.471  -  acc2 = 0.529  -  acc (over) = 0.627\n",
      "\tA-GCNN-True: acc = 0.922  -  acc2 = 0.902  -  acc (over) = 0.941\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.902  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.922  -  acc2 = 0.922  -  acc (over) = 0.922\n",
      "\tDual-GFCNN-b-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tDual-GFCNN-b-True: acc = 0.902  -  acc2 = 0.902  -  acc (over) = 0.941\n",
      "\tDual-GFCNN-False: acc = 0.373  -  acc2 = 0.510  -  acc (over) = 0.569\n",
      "\tDual-GFCNN-True: acc = 0.882  -  acc2 = 0.922  -  acc (over) = 0.941\n",
      "\tW-GCN-A-False: acc = 0.627  -  acc2 = 0.804  -  acc (over) = 0.824\n",
      "\tW-GCN-A-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.922\n"
     ]
    }
   ],
   "source": [
    "N_RUNS = 10\n",
    "\n",
    "best_accs = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs2 = np.zeros((len(EXPS), N_RUNS))\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    print(f'- RUN: {i+1}')\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        # t_i = time.time()\n",
    "        if exp['name'] == 'Kipf':\n",
    "            arch = GCNN_2L(IN_DIM, HID_DIM, OUT_DIM, act=ACT, last_act=LAST_ACT,\n",
    "                           dropout=DROPOUT, norm=exp['norm'])\n",
    "            S = dgl.from_networkx(nx.from_numpy_array(A)).add_self_loop().to(device)\n",
    "            \n",
    "        elif exp['name'] == 'A-GCNN':\n",
    "            arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "            dropout=DROPOUT, diff_layer=GFGCNLayer, init_h0=h0)\n",
    "            if exp['norm']:\n",
    "                S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "            else:\n",
    "                S = torch.Tensor(A).to(device)\n",
    "\n",
    "        elif exp['name'] == 'H-GCNN':\n",
    "            arch = GFGCN_Spows(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                               dropout=DROPOUT, norm=exp['norm'], dev=device)\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        elif exp['name'] in 'Dual-GFCNN-b':\n",
    "            arch = Dual_GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, alpha=ALPHA,\n",
    "                              last_act=LAST_ACT, dropout=DROPOUT, init_h0=h0, bias=exp['bias'])\n",
    "            if exp['norm']:\n",
    "                S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "            else:\n",
    "                S = torch.Tensor(A).to(device)\n",
    "\n",
    "        elif exp['name'] == 'W-GCN-A':\n",
    "            arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                         dropout=DROPOUT, diff_layer=GFGCN_noh_Layer)\n",
    "            if exp['norm']:\n",
    "                S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "            else:\n",
    "                S = torch.Tensor(A).to(device)  \n",
    "            \n",
    "        else:\n",
    "            raise Exception(f'ERROR: Unknown architecture: {exp[\"name\"]}')\n",
    "\n",
    "        if exp['name'] in ['Kipf', 'W-GCN-A']:\n",
    "            model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "            loss, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "        else:\n",
    "            model = GF_NodeClassModel(arch, S, K, masks, LOSS_FN, device=device)\n",
    "            loss, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                                    epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs[j,i] = model.test(feat, model.S, labels, masks['test'])\n",
    "        best_val_accs2[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'\\t{exp[\"name\"]}-{exp[\"norm\"]}: acc = {best_val_accs[j,i]:.3f}  -  acc2 = {best_val_accs2[j,i]:.3f}  -  acc (over) = {best_accs[j,i]:.3f}')\n",
    "\n",
    "\n",
    "# Print results\n",
    "index_name = [f'{exp[\"name\"]}-{exp[\"norm\"]}' for exp in EXPS]\n",
    "table_comp_over = summary_table(best_accs, index_name)\n",
    "table_comp = summary_table(best_val_accs, index_name)\n",
    "table_comp2 = summary_table(best_val_accs2, index_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kipf-none</th>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.076345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kipf-both</th>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.058693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-False</th>\n",
       "      <td>0.747059</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.169231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-True</th>\n",
       "      <td>0.874510</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.031859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-False</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.216078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-True</th>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.058954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dual-GFCNN-b-False</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.096875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dual-GFCNN-b-True</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.027799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dual-GFCNN-False</th>\n",
       "      <td>0.772549</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.205686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dual-GFCNN-True</th>\n",
       "      <td>0.870588</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.035294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-False</th>\n",
       "      <td>0.664706</td>\n",
       "      <td>0.637255</td>\n",
       "      <td>0.068178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-True</th>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.039216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mean accs       med       std\n",
       "Kipf-none            0.494118  0.509804  0.076345\n",
       "Kipf-both            0.533333  0.529412  0.058693\n",
       "A-GCNN-False         0.747059  0.843137  0.169231\n",
       "A-GCNN-True          0.874510  0.872549  0.031859\n",
       "H-GCNN-False         0.733333  0.852941  0.216078\n",
       "H-GCNN-True          0.850980  0.852941  0.058954\n",
       "Dual-GFCNN-b-False   0.849020  0.872549  0.096875\n",
       "Dual-GFCNN-b-True    0.868627  0.862745  0.027799\n",
       "Dual-GFCNN-False     0.772549  0.862745  0.205686\n",
       "Dual-GFCNN-True      0.870588  0.862745  0.035294\n",
       "W-GCN-A-False        0.664706  0.637255  0.068178\n",
       "W-GCN-A-True         0.823529  0.823529  0.039216"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_comp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
