{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fdc2eb478d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import utils\n",
    "from gsp_utils.baselines_archs import GCNN_2L\n",
    "from gsp_utils.baselines_models import NodeClassModel, GF_NodeClassModel\n",
    "from gsp_utils.data import normalize_gso\n",
    "from src.arch import GFGCN, GFGCNLayer, GFGCN_noh_Layer, GFGCN_Spows\n",
    "\n",
    "SEED = 15\n",
    "# PATH = 'results/diff_filters/'\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "def summary_table(acc, index_name):\n",
    "    mean_accs = acc.mean(axis=1)\n",
    "    med_accs = np.median(acc, axis=1)\n",
    "    std_accs = acc.std(axis=1)\n",
    "    return DataFrame(np.vstack((mean_accs, med_accs, std_accs)).T, columns=['mean accs', 'med', 'std'], index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ChameleonDataset\n",
      "Number of nodes: 2277\n",
      "Number of features: 2325\n",
      "Shape of signals: torch.Size([2277, 2325])\n",
      "Number of classes: 5\n",
      "Norm of A: 190.00262451171875\n",
      "Max value of A: 1.0\n",
      "Proportion of validation data: 0.32\n",
      "Proportion of test data: 0.20\n",
      "Node homophily: 0.25\n",
      "Edge homophily: 0.24\n"
     ]
    }
   ],
   "source": [
    "# Dataset must be from DGL\n",
    "dataset_name = 'ChameleonDataset'\n",
    "\n",
    "A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device,\n",
    "                                                     verb=True)\n",
    "N = A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without normalizing the GSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # BEST PARAMETERS  - WisconsinDataset\n",
    "# ## Reaining params\n",
    "# N_RUNS = 20\n",
    "# N_EPOCHS = 500  # 500\n",
    "# LR = .005  # .01\n",
    "# WD = .01  # .005\n",
    "# DROPOUT = .25\n",
    "\n",
    "# # BEST PARAMETERS\n",
    "# ## Architecture params\n",
    "# N_LAYERS = 2\n",
    "# K = 2  # 2\n",
    "# HID_DIM = 32 # 8\n",
    "\n",
    "# ## Model params\n",
    "# h0 = 1  # 1\n",
    "# NORM = False\n",
    "\n",
    "# IN_DIM = feat.shape[1]\n",
    "# OUT_DIM = n_class\n",
    "\n",
    "# ACT = nn.LeakyReLU()\n",
    "# LAST_ACT = nn.Softmax(dim=1)\n",
    "# LOSS_FN = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PARAMETERS  - 0.61\n",
    "## Reaining params\n",
    "N_RUNS = 20\n",
    "N_EPOCHS = 500  # 500\n",
    "LR = .001  # .01\n",
    "WD = .01  # .005\n",
    "DROPOUT = .5\n",
    "\n",
    "# BEST PARAMETERS\n",
    "## Architecture params\n",
    "N_LAYERS = 2\n",
    "K = 2  # 2\n",
    "HID_DIM = 64 # 8\n",
    "\n",
    "## Model params\n",
    "h0 = 1  # 1\n",
    "NORM = False\n",
    "\n",
    "IN_DIM = feat.shape[1]\n",
    "OUT_DIM = n_class\n",
    "\n",
    "ACT = nn.ReLU()\n",
    "LAST_ACT = nn.Identity(dim=1)\n",
    "LOSS_FN = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting eval/test acc/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc at best val: 0.480  -  Best test acc: 0.529\n",
      "Test acc (based on loss): 0.482\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = N_EPOCHS\n",
    "lr = LR\n",
    "wd = WD\n",
    "drop = DROPOUT\n",
    "L = N_LAYERS\n",
    "K_aux = K\n",
    "hid_dim = HID_DIM\n",
    "norm = False\n",
    "act = ACT\n",
    "lact = LAST_ACT\n",
    "loss = LOSS_FN\n",
    "patience = 300\n",
    "\n",
    "iters_aux = 1\n",
    "\n",
    "err1 = np.zeros(iters_aux)\n",
    "err2 = np.zeros(iters_aux)\n",
    "for i in range(iters_aux):\n",
    "    # Create model\n",
    "    arch = GFGCN(IN_DIM, hid_dim, OUT_DIM, L, K_aux, act=act, last_act=lact,\n",
    "                dropout=drop, diff_layer=GFGCN_noh_Layer)\n",
    "\n",
    "    S = torch.Tensor(A).to(device)\n",
    "    model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "    loss, acc = model.train(feat, labels, epochs, lr, wd, patience=patience)\n",
    "\n",
    "    idx_max_acc = np.argmax(acc[\"val\"])\n",
    "    print(f'Test acc at best val: {acc[\"test\"][idx_max_acc]:.3f}  -  Best test acc: {np.max(acc[\"test\"]):.3f}')\n",
    "    acc_val = model.test(feat, model.S, labels, masks['val'])\n",
    "    acc_test = model.test(feat, model.S, labels, masks['test'])\n",
    "    print(f'Test acc (based on loss): {acc_test:.3f}')\n",
    "\n",
    "    err1[i] = acc[\"test\"][idx_max_acc]\n",
    "    err2[i] = acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc at best val acc: 0.480 +- 0.000\n",
      "Acc at test: 0.482 +- 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fdb5244aeb0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAFlCAYAAADYnoD9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAAsTAAALEwEAmpwYAABpAElEQVR4nO3dd3hUxdfA8e8kJCR0CL0ISO8dQUBRAbFQFFQECyqgIihWQHw1KthQUYoIioj8pEgVBKQoUQREeu899E4KqXvePybJJiGBhCzZZHM+z7PP7i177+xks+dOuTNGRFBKKaWU+3i5OwFKKaVUTqfBWCmllHIzDcZKKaWUm2kwVkoppdxMg7FSSinlZhqMlVJKKTfL5a4TFy1aVCpUqOCy44WFhZE3b16XHS+n0nzMOM3DjNM8dA3Nx4xzZR6uX7/+rIgUS2mb24JxhQoVWLduncuOFxQUROvWrV12vJxK8zHjNA8zTvPQNTQfM86VeWiMOZzaNq2mVkoppdxMg7FSSinlZhqMlVJKKTdzW5txSqKjowkODiYiIiLd7y1YsCA7d+68CanKHvz8/Chbtiw+Pj7uTopSSql0ylLBODg4mPz581OhQgWMMel6b0hICPnz579JKcvaRIRz584RHBxMxYoV3Z0cpZRS6ZSlqqkjIiIICAhIdyDO6YwxBAQE3FCNglJKKffLUsEY0EB8gzTflFIq+8pywTgrmDt3LsYYdu3aleFjHTp0iClTptzQe2+//fYMn18ppVTWp8E4BVOnTqVly5ZMnTo1w8e6VjCOiYm55ntXrVqV4fMrpZTK+jQYJxMaGso///zDhAkTmDZtWsL62NhY3njjDWrXrk3dunUZNWoUAGvXruX222+nXr16NG3alJCQkCTHGzRoECtWrKB+/fqMGDGCH3/8kY4dO3L33Xdzzz33EBoayj333EPDhg2pU6cOv/76a8J78+XLBzhHgOnatSvVq1enR48eiEgm5IZSSqnMkKV6Uyc2YABs2pT2/WNj/fH2vvY+9evDV19de59ff/2V9u3bU7VqVQICAli/fj2NGjVi/PjxHDp0iE2bNpErVy7Onz9PVFQUjz32GNOnT6dJkyZcvnwZf3//JMf75JNP+Pzzz/ntt98A+PHHH9mwYQNbtmyhSJEixMTEMGfOHAoUKMDZs2dp1qwZHTt2vKoNeOPGjWzfvp3SpUvTokULVq5cScuWLdOeQUoppbIsLRknM3XqVLp16wZAt27dEqqqly1bxvPPP0+uXPb6pUiRIuzevZtSpUrRpEkTAAoUKJCw/Vratm1LkSJFAHtb0ttvv03dunVp06YNx44d49SpU1e9p2nTppQtWxYvLy/q16/PoUOHXPFxlVLqpjl1Ck6ccHcqbsyePbB6deadL8uWjK9Xgk0uJORKhu8zPn/+PH/++Sdbt27FGENsbCzGGIYPH56h4yaXeAaQn3/+mTNnzrB+/Xp8fHyoUKFCirco5c6dO+G1t7f3ddublVLKnWJioGxZ8PWFkBAYMQJmzYKiRaF1a3jtNdi4Ef77D8qXh7ffhldegdy57T5t2tzYeUXg4EEoXhziWvoAWLIEli2D33+HO++E224DhwMefRT8/ODQIXve+Pe89RYEBcGUKdepcnWRLBuM3WHmzJk8+eSTjBs3LmHdnXfeyYoVK2jbti3jxo3jrrvuSqimrlatGidOnGDt2rU0adKEkJAQ/P39k5SO8+fPf1U7cmKXLl2iePHi+Pj4sHz5cg4fTnVSD6WUylKiouC33+zzww/bpsWPP4YLF6BECRuQY2KgUSO7zdsbYmNh/nyYNg3WrnUey9sbeva0r42xpepixWxw/b//gxo1oEcP5/4HD8KQIRAYaM9x/jw0bQqPPw6zZ9tg/N57NriWKgX33mvfV6wYjBsHo0fb5YEDbfoWLLDbPv4YJk6ElSth6FDIkyf25mckGoyTmDp1KgMHDkyyrkuXLkydOpVRo0axZ88e6tati4+PD71796Zfv35Mnz6d/v37c+XKFfz9/Vm2bFlCxyuAunXr4u3tTb169ejZsyeFCxdOcvwePXrQoUMH6tSpQ+PGjalevXqmfFallEpuxQqYMwe6dIEWLey6mBjYsQPWrYPTp6FlS2jWDH79FV5+GY4ft/tVqgT799sgXLIk/P03PPUULF9uA/FLL8Gbb8Kff8Kzz9pAnCcPjBpln++4A/791wbhfv2gXTsYMwYWLYJhw+w58ueHDh0gPNzuHxwM+/bZ9IWFOT/HwIE22L/0knNdyZL2XHfeCZGRNphHRdm0LFhgg/2VK9CrFwQE2PUDBiS9YLipRMQtj0aNGklyO3bsuGpdWl2+fPmG3+spMpJ/8ZYvX57xhORwmocZp3l4Y2JjRcLDncszZqyUDz4Qee89kQ8+ENm3z7lt82aRJk1EXn9dZNo0kRdfFLHlUBFjRIKCRI4eFXngAef6+EfjxiLe3iING4osXCgyfryIv79IoUIihw6JOBwiV67Y84SFiZw6lTSds2eLHDlit6Vk6lSRUqWc56tXz6YJ7LYPPrCvGzSwz9WqifTvb1/37WuPERFh079unU3fmTMpn2vzZpG2bUX27hW5fFnku+9E9u93bnfldxFYJ6nERA3GHkSDcdageZhxmofp43CITJwoUqWKSIkSziD38MNHkwTRYsVE1q8XCQ4WCQgQKVzYBtX47d262cBZtapI6dIiBQva7Z07i3z8sciuXSKffy7i52fPdfKkMw3nz4scO+a6z3T4sL1QWL3aLh88aIN9/MVC1642re++a88tInLggL0gcaXMCsZaTa2UUjdJYCCMHw+TJ9sq3uBguHwZmjeH++6zbaPxoqNtVXD16lC4MEREwIQJdn2vXrbt8+RJ2+Hozjvhww+d7503D555BipXttW8Y8fCCy/AqlUBPPigbaPdswfatrXto8aAvz9s2GA7WO3fD3fdRcLtoT//bNOYLx9s3w7VqjnPVa0avP761Z+1cGH7cJVbboHPP3cuV6gAn30GffrYtuPx4+1neP995z7Zep6c1KJ04gfQHtgN7AMGpbC9PPAHsAUIAspe75haMnY9LRlnDZqHGZcZeRhfjZoWoaEi338v8sUX9nVikZEpv+e//66u3k38aNYsaRq6dLHr773XLr/2mnPfHj3susRVxvHvPX3allKrVbNpKVIk6Xm++855juBgkfffFxkyROSff679mZcuFVm7Nu15lBkcDpFLlzL3nFmmmhrwBvYDtwK+wGagZrJ9ZgBPx72+G5h8veNqMHY9DcZZg+Zhxt3MPIyNFfnmG1v92rSpSPv2tk1x1arUA+tddzmD24MP2urTK1ds26Wvrw3Uyc/RubNIvnwiP/0kkj+/yIABItu3ixw/LjJqlD3Wxx/b/X//PWkAfeIJ+9yli31frlwiy5fbdS1b2ufBg0VCQkSaN7fttfFZ9uuvIvXr231KlLgiFy7cpIzMIbJSMG4OLE60PBgYnGyf7UC5uNcGuHy942owdj0NxlmD5mHGuToPDx8W2bbNlqzuv9/+8pUubQNZ5crOIPjcc873OBw2SL78st32f/8nUr361SXckiVFvLzs8evVE8mTx7apgsjQoamnKT4djRqJ5M4tUrOmyMyZSY89d67tTOTtLZI3r93v2DHne/Pmtc8TJiQ9tsMhsnixyLx5K1yajzlRVmozLgMcTbQcDNyWbJ/NwMPA18BDQH5jTICInEu8kzGmD9AHoESJEgQFBSU5SMGCBa95T+61xMbG3vB7PUVERMRVeZpeoaGhGT5GTqd5mHGuyMO//y7K5MnliY724vBhO9BOmTLhHDuWh549D/LEE0fw9hauXPHinXfqsGFDYSZMgGLFdtK8+TkGDqzLrl0FEo5XqdJ/tGxZlF27bqVTp2Ns316ARo0ucP/9J3n66abUrg3GCA89dAwfHwcVK4Zx++2nSO1j9O/vRcWKpVi0qBR33BHKSy/tx9c3FriDwoWj6NnzIPnyneDIEXjqqfJMnFiRfv32smfPMd58E+6/vwDTppUjKsqL8uW3XnUeX1+IitLvYkZl2v9zalE6/gF0Bb5PtPwkMDrZPqWB2cBGbEAOBgpd67hZuWQ8Z84cAWTnzp0uO2ZaHTx4UGrVqnVD79WScdageZhxGc3DAwdsqbFECduGev/9tmq6enVbeo2Kuvo9UVH2Vp/EJdPx40UefdTeyuNwiERHi2zYcPV742+x6dUrQ8kWEZGdO69ul3Y4RLZutc/pod/FjMtKJeNjQLlEy2Xj1iUO6MexJWOMMfmALiJy8UYvENwt8RSK7yfuqqeUcrtjx+wIT8WK2QEmwIbOuXPhn39sT+ETJ+wgENu2Qbly4OVl17/4ot032TwsAPj42MEfpk2zA1O0bQu9e9uezPHvyZULGjS4+r1Tp9pBJNq1y/jnS2ncH2Ogdu2MH1tlXWmZKGItUMUYU9EY4wt0A+Yl3sEYU9QYE3+swcAPrk1m5nH1FIrdunVjwYIFCcs9e/Zk5syZHDp0iFatWtGwYUMaNmyocxcrlQKHw470dOWKXT5wwN6+UqeODbI//mhv6+ne3Q7H+M03dlz7KVOgbl17O4y3d9Lgm1IgjlesGPTvb0eh6tvXub/XdX4pq1WD9u2vv59SqbluyVhEYowx/YDF2J7VP4jIdmPMB9gi9zygNfCxMUaAv4GXUj1gGg34fQCbTm5K8/6xsbF4X2cOxfol6/NV+6+uuY+rp1B87LHH+OWXX3jggQeIiorijz/+YOzYsYgIS5cuxc/Pj7179/L444+zbt26NH9epTzNiRN2WMSCBe3ysWN2bOEJE+CTT+wQh2PG2FLq5Mn2PtNnnnG+/6mn7L4ffWTfV6OGez6HUjciTddxIrJQRKqKSCURGRa37t24QIyIzBSRKnH79BKRyJuZ6JvJ1VMo3nfffSxfvpzIyEgWLVrEHXfcgb+/P9HR0fTu3Zs6derwyCOPsGPHjkz8lErdHH//bQfhdzhS3j51KjRubEuyAwfaquRevexgFaVL24Aar0sXG1wBFi+G0FC73LUrPPGErUqeM8dOVDBhgh3oIlcuO65x27Z2FiClsossOwLX9UqwyYWEhGTJKRT9/Pxo3bo1ixcvZvr06QmBfsSIEZQoUYLNmzfjcDjw8/PLUNqVcrcFC+DBB+3rLVvs6FOvvWZHa6pfH5YutSMneXnB+vV2P19fG0jr1bPL8+bZAB0dbdiwwc60U7cufPkldO4Mly7ZyQnABt7Ona9OR5Eidro8pbITbeFIJH4KxcOHD3Po0CGOHj1KxYoVk0yhGD+PcPIpFMFeEKQ0z/Bjjz3GxIkTWbFiBe3btwfs1ImlSpXCy8uLyZMnExubOdN0KZVeEybAwoXX32/4cDsv7ZNP2irkzz6D//3Pdnh69FF45BFbdXzggK12BjtF3T332Fl9li2z6xYvhkOH8hIdbWfO6dvXlpz/+MMO0dis2U37qEq5jQbjRKZOncpDDz2UZF38FIq9evXilltuoW7dutSrV48pU6bg6+ubMIVivXr1aNu2LREREVcdt127dvz111+0adMGX19fAPr27cukSZOoV68eu3btIm/evJnyGZWKFxtrp8eLFxZmHz/8YKfSGz4cnn/eViM/8IAttcY7dswGxooVbZDdtg3++ssGzjfftMf9/HM7uXz37nYautatbWesW26xJebixe2x3n3XPt9xh+1w9eabsHlzIcAG8goV7BR5R4/a6fiu1QFLqWwrtXuebvYjK99nnF3pfcZZQ3bJw+eft6NQbdkiMmKEnQmnYMGrR5jy8REpV06kUiXncJHPPGOHgezUye5TuLAdherECbv9zjvt8X76KfXzHzsmVw3V+NdfzvMWKOD6GXhymuzyXczKstJ9xkopD/Dnn/b+3BYtYPBge1sQ2DbZeA8+aKuUp0yxpeJChWzb7NGjtnT8/fcQEgITJ9q24OHDoVUrWLkS2rSxE7iDLcGKXPtWn9Klr153xx1QpQrs3Wvv8dVbhVROocFYKQ9y+jQ89hh88QU0bGjbX2vXhrx5bdtsckuW2NuAqlaFESOcU+B17Jh0v9q17bR9L8XdtPj44/Dxx7bK+PffbS/pVq2c+xtz49XJ334L/fpd5u23C1x/Z6U8hAZjpTzIL79AUBB062bvt33kEahUyc6dG69ePRusy5Sxoz21bXv94xoD330Hw4bZwD1woHPu23z5bCnWVe6+G775ZgNFirR23UGVyuI0GCuVDVy+bEd4atnS9kResAAOHbLrn37adpSaO9eOHhUQYKt5H3nEvnf/fhg92t4S9NVXN15irVLFWbWtlHItDcZKZVEOh20zdThs9fDq1fYxZYrtzRxv3z74+muIvxGgTx9b3Tx/vr21aPZsiIiAAQO0J7JSWZUGY6WyoH/+scG1dm0SpsYLDLRVwx9+CM89By+8YKuNf/zRWWL94gu7Pk8ee4sQ2CCslMratK9iCubOnYsxhl27dmX4WIcOHWLKlCk3/P6PPvoow2lQWcf69bajU/zsQf/3f3D77XD4sL1Xd8gQW8rt2hXOnnUG4qeestveecfeC/z993ZYyS+/tB2rwA6MMWCADcRKqexFS8YpcOUUivHBuHv37jf0/o8++oi3dZDdbCvxdH0xMXYAjD17bMn30CHbE9nHxwboEyeSDsIxa5adlu+ZZ+wQj/ESD39esaIN2FFRdmYjvRVIqexJ/3WTcfUUioMGDWLFihXUr1+fESNGEBsby5tvvkmTJk2oW7cu48aNA+DEiRPccccd1K9fn9q1a7NixQoGDRrElStXqF+/Pj169Mi8TFAZ5nDYzlSFC9uqZIfD3tu7Z48Nsk2a2EBcqZK9vSg42E4JeOqUHW/5tttsNfXrrycNxKnx9XXOdqSUyn6ybsl4wAA7YG0a+cfGOu+1SE39+rY76TW4egrFTz75hM8//5zffvsNgPHjx1OwYEHWrl1LZGQkLVq0oF27dsyePZt7772XIUOGEBsbS3h4OK1atWL06NFsSkc+KPcLD/fmm29sD+aiRW3V8owZsHmzHUjjoYds4GzTxg4L2bq1vR+4UiU7ROTs2XaoSu1spVTOkXWDsZtMnTqVV155BXBOodioUSOWLVvGCy+8kGQKxa1bt141heL1LFmyhC1btjBz5kzAThixd+9emjRpwrPPPkt0dDSdO3emfv36N+cDqptq9Wro1q0ZISG26vmvv2x77//+Z7ePGmWD7D332AkTKlSw6xPf62tM0qpopZTny7r/8tcpwSZ3JYtOoZiciDBq1Cjuvffeq7b9/fffLFiwgJ49e/Laa6/xVOLJXVWWc/GiHS2qQgXbiWroUNur2dvbDis5bJgNrB9/DD//bGcx8vFxvr9iRTclXCmV5WibcSI3YwrF/PnzJ2lHvvfeexk7dizR0dEA7Nmzh7CwMA4fPkyJEiXo3bs3vXr1YsOGDQD4+Pgk7KuyhhUr7FCTJUrYduDHH7djKn/zjZ3J6Mcf1zJ/vnPM57JlITTUtv8qpVRKNBgncjOmUKxbty7e3t7Uq1ePESNG0KtXL2rWrEnDhg2pXbs2zz//PDExMQQFBVGvXj0aNGjA9OnTE6rK+/TpQ926dbUDVxaxaBG0awcbN9pJFJYutWNB79tnpwJctgyKFIm66n158mgbsFIqdcbO6pT5GjduLOvWrUuybufOndSoUeOGjhfigmrq7C4j+RcvKCiI1q1buyZBHiYszJZyK1a0Ha6KFrUBNjbWBuG777bV0JqHGad56Bqajxnnyjw0xqwXkcYpbcu6bcZKZTGTJ9t24tGjoVgx53pvb0ihC4BSSqWZVlMrlYIVK2x7cGSkc91330GDBrZdWCmlXElLxkrFiYiwI2b5+dmJGbZuhbVr7WxIR4/Chg0Zm/VIKaVSk+VKxu5qw87uNN/SbvFiOyxlTIx9TJhgx3j297eDcvzzjw3EXbrYoSa7d7e3LHl52c5aSinlalmqZOzn58e5c+cICAjAaPEjzUSEc+fO4efn5+6kZBlHj9oezAEBV2974gk7CUPr1nZ7r17ObZMn25GwjIGJE+3AHQMG2OkIu3SBkiUz6xMopXKSLBWMy5YtS3BwMGfOnEn3eyMiInJ0MPLz86Ns2bLuTkaWEBQEd91lRz/9/XeYMwd++cV2vGrf3gZisIN0tG0L+fPD/v0wdiy89559f7Vqdv3LL9vbmCZNgldfdeOHUkp5tCwVjH18fKh4g8MSBQUF0aBBAxenSGU3ItCvn329aVPSkmytWs7XI0faQPvDD9Cxo+0d3bCh3RYUBPG3dceXkD/7zI4brZRSN0OWazNWKiOCgmD7djvf7yuvQKNGUKAAPPmkc5/oaOjb15Z+wc6SBM4Rs8AZmMEGZA3ESqmbKUuVjJXKqN9/t9MJ9uhhe0WDDb7BwbY9uFAh5yQMmzZBSIjznuFy5eCBB+DwYbj/fnekXimVU2kwVh5lyxaoUcMZiMGOilWxom0TbtrUud7PL+l+xkDcTJdKKZWpNBirbC0qCs6fd7YNb91qh6VMyQsvZF66lFIqPTQYq2xn0CA4dAiaNLEdsU6csAE5MhKOHUva9quUUtmBBmOVraxdC59+al9Pn+5cv3gxFC5sX9epk/npUkqpjEhTb2pjTHtjzG5jzD5jzKAUtt9ijFlujNlojNlijNHuL8olliyxPaPjBxgbOtQO1PHrr7Bjhy0NFy4MTz0Fw4fbdt/bbnNvmpVSKr2uWzI2xngDY4C2QDCw1hgzT0R2JNrtHeAXERlrjKkJLAQq3IT0qhxi71545hlYvRocDjh50g5LOX8+DBli7w2O99JLNkj//rutoi5UyG3JVkqpG5KWauqmwD4ROQBgjJkGdAISB2MBCsS9Lggcd2UiVc6yZw907Wo7Y/XrZ9uEhw61AdqYqztiffghlC8PvXsnndpQKaWyi7QE4zLA0UTLwUDyisBAYIkxpj+QF2jjktSpHGfHDjtQR0SEHfXqzTftfb/z58NPP9nhK8uUufp9PXrYKu0338z8NCulVEaZ6832Y4zpCrQXkV5xy08Ct4lIv0T7vBZ3rC+MMc2BCUBtEXEkO1YfoA9AiRIlGk2bNs1lHyQ0NJR8+fK57Hg5lTvzMSrKi759G3LmTG6GDt1GnTqXErZNnVqO8eMrMXDgLtq3P+mW9KWVfhczTvPQNTQfM86VeXjXXXetF5HGKW4UkWs+gObA4kTLg4HByfbZDpRLtHwAKH6t4zZq1Ehcafny5S49Xk7ljnw8fVrkxRdF6tYVAZHffrt6H4dDZNUqkdjYTE9euul3MeM0D11D8zHjXJmHwDpJJSampZp6LVDFGFMROAZ0A7on2+cIcA/wozGmBuAHpH/qJZXjhIbaqQx3xPVA+OQTOyRlcsZA8+aZmjSllMo01w3GIhJjjOkHLAa8gR9EZLsx5gNslJ8HvA58Z4x5FduZq2fcVYBS1/TGG7Bzp71PuEoVO2ylUkrlNGka9ENEFmJvV0q87t1Er3cALVybNOXpjh+HCRPsDErt2rk7NUop5T46haJyi+Bgex9xbCwMGODu1CillHvpcJgq0wQH20fhwvYWpfPn7ahZlSu7O2VKKeVeGoxVpnnuOXsvMEDx4rByJdSr5940KaVUVqDV1CpThIbaQFy4MAQG2mEuNRArpZSlJWOVKZYvt88zZsA997g3LUopldVoyVhlikWLIG9eaNnS3SlRSqmsR4OxuulEYOFCaNMGcud2d2qUUirr0WCsbqroaOjc2U72cN997k6NUkplTRqM1U0VFATz5kHp0vDQQ+5OjVJKZU3agUvdNCIwfTr4+8O+ffZZKaXU1bRkrG6a55+3w122a6eBWCmlrkWDsbppNmywz1984d50KKVUVqfBWLnc3r3QowccOQI9e0KlSu5OkVJKZW3aZqxcavhweOst53Lx4u5Li1JKZRdaMlYuc/Ro0kAMGoyVUiotNBgrl1m27Op1GoyVUur6NBgrlxg6FJ59FgoUsHMUx9NgrJRS16fBWGVYSAh88ol9/dZb4JXoW6XBWCmlrk87cKkMiY2FV1+FsDA7LWKzZkm3azBWSqnr02CsbpgIPPUUTJkCgwdfHYgBihXL/HQppVR2o9XU6oYFBdlA/O678NFHSbeNH2+Ds6+vW5KmlFLZigZjdcNGjoSiRW2pOLnevW21tVJKqevTamqVbgcPwpw5sGAB9O8Pfn7uTpFSSmVvGoxVur30EixaZF/rtIhKKZVxWk2t0u3IEefr5s3dlw6llPIUGoxVuoSGwq5d0KkT/PMPeHu7O0VKKZX9aTBW6bJ2rb23+PnnoUULd6dGKaU8gwZjlS6rVtnnlO4pVkopdWM0GKt0Wb0aatSAwoXdnRKllPIcGoxVmkVF2WB8++3uTolSSnkWDcYqTQ4ehDp14Px5aN/e3alRSinPovcZq+vavh3atoWICJg3Dx580N0pUkopz5KmkrExpr0xZrcxZp8xZlAK20cYYzbFPfYYYy66PKUq0/31Fzz6KLRqZZf//hs6dABj3JsupZTyNNctGRtjvIExQFsgGFhrjJknIjvi9xGRVxPt3x9ocBPSqjJRSAjce6/tqNW0KXzzDdx6q7tTpZRSniktJeOmwD4ROSAiUcA0oNM19n8cmOqKxCn3Wb0aIiPhp5/g9981ECul1M2UlmBcBjiaaDk4bt1VjDHlgYrAnxlPmnKnFSvAy0vvJ1ZKqczg6g5c3YCZIhKb0kZjTB+gD0CJEiUICgpy2YlDQ0NderycKj4fZ89uQOXKXqxfv97dScp29LuYcZqHrqH5mHGZlYdpCcbHgHKJlsvGrUtJN+Cl1A4kIuOB8QCNGzeW1q1bpy2VaRAUFIQrj5dTBQUFERXVmh074Kuv0Dy9AfpdzDjNQ9fQfMy4zMrDtFRTrwWqGGMqGmN8sQF3XvKdjDHVgcKATimfjR08mIdu3Wwb8QsvuDs1SimVM1w3GItIDNAPWAzsBH4Rke3GmA+MMR0T7doNmCYicnOSqjLD3LlliIqCZcsgd253p0YppXKGNLUZi8hCYGGyde8mWw50XbKUOzgcsGpVUdq3h4oV3Z0apZTKOXQ4TAXA3r3w7LNw9mxuOna8/v5KKaVcR4fDVCxbBp0723mKH3oomG7dyro7SUoplaNoMM7h/vwTHngAqlWDRYtg7959+PpqMFZKqcyk1dQ5VEwMfPEFdOsGZcrYcajLpDiUi1JKqZtNS8Y51KxZ8MYb9vWQIXYMaqWUUu6hJeMcatQo+/zmm/Dcc+5Ni1JK5XRaMs6BjhyBlSvh449h0FUTYiqllMpsWjLOgebFjZ/20EPuTYdSSilLg3EOc/o0jBlje09Xq+bu1CillAINxjlGRAQMHw4tW8Lhw842Y6WUUu6nbcY5xFtv2QBcqhQsXQotWrg7RUoppeJpyTgH2LDBBuKXX4bjxzUQK6VUVqPB2MM5HPZ+4oAA+OADd6dGKaVUSrSa2sONHQvLl8P48VCwoLtTo5RSKiVaMvZw06dDw4bQq5e7U6KUUio1Gow9WEwMrF8PrVqBMe5OjVJKqdRoMPZg27ZBeDjcdpu7U6KUUupaNBh7sJUr7bMGY6WUyto0GHuwGTOgalWoWNHdKVFKKXUtGow91IIFdo7iJ57Q9mKllMrqNBh7oL17oUMHqFRJp0dUSqnsQIOxB/r5Z/v8119QurR706KUUur6NBh7GIcDJk+Gu+6CMmXcnRqllFJpocHYwyxaBAcOQJ8+7k6JUkqptNJg7EGioyEw0FZNP/ywu1OjlFIqrXRsag8yejSsWwe//AI+Pu5OjVJKqbTSkrEHmT4dGjeGRx5xd0qUUkqlhwZjD3HiBKxZA506uTslSiml0kurqbOxK1fg5Ek70tYvv9jBPbp0cXeqlFJKpZcG42zq0iXo2BH+/tsuV6xo7y+uUcO96VJKKZV+GoyzoaVLoX17e09xrVowYIDOV6yUUtmZBuNsJiYGeva0A3qMHGnbiHXsaaWUyt40GGcTMTGQKxfs2gXHj9tRtjp3dneqlFJKuUKaelMbY9obY3YbY/YZYwalss+jxpgdxpjtxpgprk1mzhUbCz16QN688N13sGGDXd+okXvTpZRSynWuWzI2xngDY4C2QDCw1hgzT0R2JNqnCjAYaCEiF4wxxW9WgnOaf/+FKVNsVfQLL9hOW3ny2HmKlVJKeYa0lIybAvtE5ICIRAHTgOR3s/YGxojIBQAROe3aZOZcy5bZQLx2re2wNXcu1K8P3t7uTplSSilXSUubcRngaKLlYOC2ZPtUBTDGrAS8gUAR+T35gYwxfYA+ACVKlCAoKOgGkpyy0NBQlx4vq5g5swHVqhlCQjZwyy1NOHIkL61a7SIo6ORNOZ+n5mNm0jzMOM1D19B8zLjMykNXdeDKBVQBWgNlgb+NMXVE5GLinURkPDAeoHHjxtK6dWsXnR6CgoJw5fGygsuXYedOGDgQWrduzR9/QHAwtG5dHah+U87pifmY2TQPM07z0DU0HzMus/IwLcH4GFAu0XLZuHWJBQNrRCQaOGiM2YMNzmtdksocKijIduBq29YuV65sH0oppTxLWtqM1wJVjDEVjTG+QDdgXrJ95mJLxRhjimKrrQ+4Lpk50/z5trNW8+buTolSSqmb6brBWERigH7AYmAn8IuIbDfGfGCM6Ri322LgnDFmB7AceFNEzt2sROcEU6fC999Dt26QO7e7U6OUUupmSlObsYgsBBYmW/duotcCvBb3UDdg4UJYtQqGDrXLkyZBlSrw7bfuTZdSSqmbT0fgyiIeeMA+N20KrVvbCSB69QIfH7cmSymlVCbQYJwFhIU5X3fqBMWL2+kR4ztuKaWU8mxpGg5T3Vzx0yA+/bRtHz59GgICoE0b96ZLKaVU5tBgnAUsXWqD8NixdhKI7t1hyRLw93d3ypRSSmUGrabOApYtg5YtbfD194eff3Z3ipRSSmUmLRm72cmTsHWrtg8rpVROpsHYzdats88tW7o3HUoppdxHg7Gb7dpln2vUcG86lFJKuY+2GWfAhg1w9iwULQoNGtipDtNr1y57K1ORIq5Pn1JKqexBg/EN2r8fGjcGEbv86KMwbVr6A/KuXVD95kzApJRSKpvQauobtHixDcSzZ8Mbb8Avv9jbkdJDxE6RqMFYKaVyNg3GN2jZMihfHjp3hmHDoGJFO++ww5H2Y5w9C+fPa3uxUkrldBqMb9DKlXYMaWPA19dO8LB5M8yalfZjxHfe0pKxUkrlbBqMb4DDAWfOQLlyznXdukG+fLBiRdqPo8FYKaUUaDC+IZcu2fbegADnOi8vW928Y0faj7Nzpx1x65ZbXJ9GpZRS2YcG4xtw7px9Tn47Uq1asH172o+zaxdUq2YDuVJKqZxLw8ANOH/ePicPxjVr2uEt47dfj97WpJRSCjQY35D4YJu4mhpsyRjSVlV95QocOqTBWCmllAbjG5JaNXW9evZ506brH2PPHtvurMFYKaWUBuMbkFo1denSdmjLDRuufwwdk1oppVQ8HQ4zjYKD4cEHoX178POz6woXTrqPMXaM6rQGY2OgShXXp1UppVT2oiXjNNi2DZ5+2g7q8emnEBQEBQtCrhQuZRo2tD2qIyKufcytW6FCBXtrk1JKqZxNg/F1iEC7dvDnn3b4S4C1a1OfZalePYiJgd27Uz+mw2EDeqtWLk+uUkqpbEiDcSoOHYLmzWHePDhxAu66CxYssNvCw1MPxmnpUb1xo+0E1ratS5Ossroff4SWLSEkxN0pUUplMdpmnMyRIxAaCp99Bv/+C08+adt2p02znbMKF4YLF1LveFWlCnh7XzsYBwXZ5zZtXJ58lVVt3w7PPGNfL1kCXbq4Nz1KqSxFS8bJNG5sS7fr1tnlkBBb9Vy8uF2OjbXPqVUx584NlStfeySu7duhZEn7UDnEzJnO1/Pnuy8dSqksSYNxIvETQEDSYNq4sfP15cv2+Y47Uj9OrVp2woj//kt5++7ddhhMlUWEh9sAGX+/mauEhtp2jr174bffbLtH9+6waJHtjKCUUnE0GCeyZ4/zddeuzoDZsKFz/aef2vbiawXT118HHx946qmUf3M1GGchK1fCa69Bx45wzz2uDZKDBkGnTradeN06e2/cHXfA6dNw4MC137thAxw86Lq0KKWyNA3GiaxZY5+3bYMZM2xJGZwjawG89ZbtfGVM6se5/XZ4/30bdJPfc3z2rH2/BuMsYO9eGyjHjbPLx49fPwD+/jts2XL9Y587Bz/8YF+fPm2fO3SwXw6AVatSf29ICNx9N/Tuff3zKKU8ggbjRHbsAF9f5xCV48ZB06Z2II/06trVlo5nzUq6Pr70rcNgutH27dC3Lzz+uF02BiZNsq+vFSRPn4bOneGJJ65fgl6wwA5AvnChXS5fHmrXtrOJ5M8Pq1en/t4ffrDzdP71l31WSnm8HBGMRWD9+uvvFxwMZcrY3tBgb2das+bGBuYoXBjq17/6Nze+dvLWW9N/TJVBkZHw8su2xDl2rP1S5Mtnq0B69LCvn3zS+UebPBkeegj++MMujxhhj7F1Kyxb5jzuu+/arveJrVplR4a5917bi/rll23Q9/aGJk2cPQSTi4mBr76CEiXs619/dXk2KKWynhwRjGfNsp2wpk699n7Hjtlg7Cq33WYHCInvgQ321imAW25x3XlUGi1ZAqNG2WD7+OO2dPz993abtze88op9/cYb9vnjj2HuXBtIf/vNdhh49FHbDf6LL+w+u3fDhx/azlmJS8urV0OzZnay6h9+sO3S8erWtaXzxF8MsFXk1arZm9xHj4Y6daB/f3uf3YABNyFDlFJZRZqCsTGmvTFmtzFmnzFmUArbexpjzhhjNsU9erk+qTdu/377HN8m7HDA7NkQFZV0v+BgKFvWdee97TYIC0vaM/vwYShWDPLkcd151DWsWGFv/q5QwXbSitexI4wZA4895lw3dCiMHGlLtXPm2EBbsaJtv+jQAapWtdXZ/frB4sW2c8FvvznfH1c69jt+3JaemzdPOU1169oe3AcO2M4JnTrZwPzpp3Zdz562RP7bb7YkPnAgfP31tau2VY4VHRtNVGzU9XcEQiJ1wJms6rrB2BjjDYwB7gNqAo8bY2qmsOt0Eakf9/jexenMkNy57XNoqH0eMcKOuTBxIpw8aR8iri8Zx/8W//23c92RI85hNVU63UhP58BA2+4a3wuvenUYMsS2/abk2Wdt2+7DD9urtmHDnCO8vP66nSXkhRfsc506thRdoYLtbDBrFsTGUvudd6BQIdu2nJK6de3z6tX2YmDePDvQ+bhx0KuX/WJ6e9vqkyeftPt6e9uAnJWsWmXbW44edXdKXE5EuBRxia///ZqwqLD0HyAkBC5evOo76xAHQ/8eypA/hhARk/oA9nvO7eHDvz7kbPjZa6Zx8b7FlP+qPI/Pevyayfltz288P/95Cn1aiA5TOzBx48R0fRx186VlBK6mwD4ROQBgjJkGdAKuMcZU1hI///CFC7YZLr6Gcdcu+7vq42OHvIyIcG3JuFIl+1i0yBamwJaMddrEG7B5M7RoAUuXpl7iTG7ePDuo+KefsubxOyj1QCtuaduF08XzsvnYCtpWSmE80rx5bbVJ1ap2uVEjW7qePdvOFgIQEGCruSdO5Mzdzehb+xCfrS5P0VlTyNeiBfkOHrRDtlWqlHK6ata07cfxx4vXv7+9HSqxYcPsvXUbN9p2lshI59WlC12OvMz/tvyPMvnL0LFaR8y1bheIN2GCrVp/7TXbvh4/nVk2djzkONO3Tefb9d+y55ztbXkx4iLvtX4vyX5bT23FIQ6qBlRl3u55dKzWEX+fuM4lU6bYPgjA6qr+jH3vAcqXrM5ve3/j0zaf8n/L/w+A+iXr80itR65Kw9FLR2k4riFh0WFM2jyJnS/txMfbJ8k++87vo/639QmLDsPHy4ff9vxGWFQYeX3zXnW8GEcMHaZ2SFhesGcBv+35jXy++VI8v3KPtFRTlwESX/oGx61LrosxZosxZqYxppxLUuci8QN5HDpk+92cOGGXJ0+2z9HRzrtVXBmMAe6/H5Yvt4FexMNKxkeP2rE91693zf25V67Yq5WUfPaZrfOfNi1tx1q1ypY6GzdmUot8NPuhOY0vfYZUqMDjsx6n3f/acfjiYbac2kLbyW3Zf36/871Vqji/CJUq2eDbu7ct/cb7+muCfx5Lhbs3M7PIST4vtpf8h04Q9WJvrpQsee3hLv39bRV0fMCfPNneAzdypJ0UO7HixeHFF221emio7WENfLbyM179/dVrZsG7y9/lgynP88B3dzF27dhr7jth3Xd8PPUlOk/vzBtL3rjmvgniJ/aeOdPWPMTfwpWN9V/Un9eWvEZkTCT5ffMD8PWar1l11NnL/vDFw7Sa2IqWE1vSamIrus3qRqPxjdh0cpPdYepUYkuX5KcHytJ8zxVK/zCToSuGsunkJl5b/Bp5ffLin8ufRfsWpZiGyVsmExYdxkd3f8T+C/tZdmDZVfvM3jmbsOgw3mn1DnMem0NUbBR/HvwzxeOtPmqbNzpU7cCRAUeIfCeS28rcRp/f+nDk0pE0583OMzu556d7qDqqKiPXjOTd5e9e9zuYHjGOGNYeW4vk0AFxXDU29XxgqohEGmOeByYBdyffyRjTB+gDUKJECYLiB2l2gdDQ0FSPt2NHLaAYu3fH8Pnn58ifvwgtWpzl999LJezz2WcngFKcOrWBoKDLLktXmTJFuHKlLp98sp3ISC/CwmoQFbWXoKBjLjuHK10rHxMzUVE0696d3HHVDoe7d+dgBu+LrTJiBCWXLOHfqVOJLlQIAN/z58m7fz91p00DYzAjR7Kudm1Cq1ShcFyP5AuNGpH34EGiChcmunBh8hw5QoP+/YkOCGDD4MG8u8r+YJwJP0P54eU5esVeW77/6/ssPrmY4xHHeXH6i7xd4+2EtOT65htynz5N2IoVKaY1yhFFvxNfkgsfAmsO5JPoQLpvhUbHzzH29Y40/Oefa3/YPHkwo0eT99AhQsuW5acto8i3Ix+dS3fGyyS9Rv7n7D8sODiTIF9vzo74gr2+vgz8ayAAdWLqcGu+pF3zvaKi8D24nwmHh3Ps4wh+rAfPPBSE7ylfKuW7urReYMcO7vjqQ17dC70HNmTc2nHc53Mfl2MuU8Q3lRlRgNv++4+IBg0417w5t373HSeffZY9b6QxkKci/65dhN9yC6EOB0FBQYREh5AnVx68jXeGjptcaEwoeb3zJtQAHA47zDf7v+G/C//RtUxXXqj0AgDB4cEM2jaIVj+04uEyD3My4iTbLm8j2hGNl/Fi/Yn13F38bjZf3Ez7H9szrd4PtFyymO8aGV69zdBhTUE6hBXhU+z969vPbOeBUg8QFhPGnO1zaJ+7PcX9iiek63zUecZsHEPdgnVpHN2Y6mH5+HnaUPxbJb2lY+qmqVTKW4l7vO4hKjgKf29/vg36lvwn8ifscy7yHGP2jCFyWyTexps+xfqwf+N+9rOf/mX60/tkbzpM7MCX9b5MyN8LURf49sC39K3Ul4I+BROOtfrcagJ3BOLn5UeFvBV45fdXErY9mPvBJH+f0JhQxuwbQ5sSbWhUuFGa/yaj941m1rFZVM5XmUanclGxQgva1uh+1f9DRokIF6IvXPO7nVhafxMzTESu+QCaA4sTLQ8GBl9jf2/g0vWO26hRI3Gl5cuXp7qtVSsRW3QT8fUVef55kWnT7PL774vkyyfi52eXz551abIkPNyeM/78tWqJnD7t2nO40rXyURYvFvnqK5EffhD57jv7gcaOFXn2Wfu6Vy+RrVtv7MSnTonkzm2P07KlyIEDIseOiZQt68y8wED7nDevyKJFzvWtW9vnhx+2x+rQQSQgQGT/fvkv+D8hEBm+crgQSIqPW7++Vbzf95YTISfE4XAkJMnhcMiZsDPy7p/vyqELh5Ik96vVXwmByNydc0VE5Fz4Oenzy1NS9lV7zKOXjqb5o58MOZmQlnf+eEe+XPVlQjoiYyKl3JflhEDkm8ZIdC4vuXhwV8L+7Sa3k7CoMImJjbEHmztXpGpVEZBfq9r8OVLA7vvDhh+uPnloqDhy5UrIyyMPtBLeQ958sbLkeRt5ZdErKSf6559FQBwffmiXH31UpGRJkdjYVD9n4rxN0Y8/2nQ0by5/LV4s58PPS5FPCsuEdx60/0gZFBUTJeFR4bLpxCbJ91E+eX7+8yIicvDCQSn0SSEp8mkRuf/n++VsWNIfgUsRl6Tn3J5CIFLss2LSYUoH2Xxys5wNOyvbT28Xh8MhU7ZMEQKRTZM+EwF5+Nl8suvMLvvdvP12ORd+Tm4ZcYsQiJw8d0R2jHpPCn6QR6qMrCKXIy7LB0EfyPh14yXg0wDx+zC3rB0XKHLpUsLfJSQyJCE9xy4fk1wf5JJBSwclrHtqzlNCIPLe8vckOjZaRET6/tY34XvSb0G/q/Jj0qZJQiAydevUhHUvL3xZCETu//l+GfnvSImKiRIRkdu+u02qjKwixy8fF4fDIePXjRefD3yEQGTbqW0iIhITGyNj/hsjVUZWSTjvsL+HSe95vaX/wv5y6MKhVL8Dry9+XfIPQl7o6C3fdS4vAvJLTWTE6hHp/0Nfw5XoK9J5WmchEHnxtxclNDJU5NdfRQ4dElmwIOnOf/0lMnGiBC1d6rLzA+sktdiZ2oaEHWzp+QBQEfAFNgO1ku1TKtHrh4B/r3fczAzG1avb33cfH/uJV6wQcThEjsb9Xt55p11fvLhLk5SgVCl7/CFDRMLCbs45XCXVfAwOFkn0oy3589srC4dDJDpa5JFH7PrGjZO+b+ZMkS1bREaPtvslt3SpyPLlInfdZY/fvLk9TsmS9pi+vvY8zz9vz/X113a7l5dInjwijz3mTNMtt4h89JF93b+/RMVESePxjaXwJ4XlfPh52Xlmpzw45UEhEPkg6AMhEAn4NEB2nN6RsO7Wr2+Vh6c/LM/Pf16qj64uVUdVFQKRxuMbJ/lBbDiuoTQen/SzXoq4JL/u+lW83/eWAYsGpDnP438UEz+2n94uIiI/b/lZCEQW7lkob499RCK9keONqknuIchD0x5K2P++V4pKaMf7nHmR6HG8ZD7JMyyPTdMvv4j83/+JHD5sT/7vvyIg/dsjm59uLwLyYwMjAjK9XRkhENl3bp8zsdu2iXz8ccKx7++TT8auHSvy00923X//pfgZQyNDpcrIKvL4hPslfOQXIjExzo2HD9uLu4AAkfLlRUC2vv++fPflEzK6iT3P5XcHpjk/U7Jo7yLJMyxPQn55ve8lBCIf/f2RVB9dXfJ/lD/p50zBv0f/lXPh55wrYmPtxeiZMxIWFSb5Psono+8rKjEGeeWX5+w+PXuKlC4tIiJHLh6RA+cPiEyZIgJyuNNdwntIqx9aJaSr0bhGcnTSqKv+hj9tnJRw2tcXvy7e73vL/vP7E9Yt2bck4Rh/HvhTdp/dLbk/zC2tx7SWVUdWpfh5YmJjJO+wvNJ/YX8RsRdLNUbXSPI9fO7X52TP2T0JF7SJxf/fjFs3Tr7+92u5Y+IdCe9r/n3zhNcFPy4ovh/6CoFI+RHl5eKVi0mO88/hfyT3EGRX7ZJJPvO5Aj5S6vNSciX6yrX/uGm0bP8yuevHu4RA+79jAo288k6TpHl95ozzDY89JlKihCz/4w+XnF8kg8HYvp/7gT3AfmBI3LoPgI5xrz8GtscF6uVA9esdMzODcUCASN++Is89J1KjxtUX76++6ixg3QzLlon073/NQkOWkWo+vvaaDYAbNtiSKYi8+27SfQYOtOt79hQ5edL+UCX+ov/yS9L9Q0JEChUS8fa227//3mbS6tUi/v523b332iAcf0UdHu483qef2nUXL4r06ZPkXId/+V5G/jtSCESmbJmScMrwqHD53+b/SawjVv469Jf9cRSRRuMaJfx4xF/xxz+6TO8iJtBI7W9qS1RMlGw/vV0IRL5a/VWKWXXv2HvFf6i/nA49LYHLA6Xk5yWl3JflkpRAEntsxmNSYngJ6fpL14Rzxv/wPTrjUSn5eUlxOByy79w+6fmorWbZWAIJGTNCtvbqJP91aS6n89oAGtuju3y26P+kf3vkdN1KcqT7g+Lw9pbm3zSSR0e0cOZ1sWIib78tYaO/EgG5452ycunSaZF77knIQ4ePj7x0H/L8/OflZMhJ+zcoVixhe4VXnHm0ZM1UW7Px4ovOD3b5skS+2EcWPFJfenVA7uuODGsZ9zf6IVEpvUsX59/up59EvLzk93trJfl77mjXUGTePJE5c1L+fu7eLZcGvSpfv9RYJg17NMmmsKgwKT68uNQcU1M+WfGJfLLiE9l0YpM0Gd9ECERKfV5K/jzwZ8rHvZYVK2z6+vYVEZGZ22fKn1VyyaaSyNpja+0+779v91m+3NYgTZ0q8tJLCZ9r3H3FE/Jw0qZJEhl1xXlBmi9fwn5N3y8r07ZOk9k7ZkveYXnlidlPJEmKw+GQsWvHCoHIp/98Ki0mtJAinxaRX35P9j+XTKsfWknz75uLiMgfB/6QAoPs36jAIKTE8BLiN9RPnp37rHi/7y3Bl4KTvDcmNkbyfZQvIf3+Q/1l4saJMmP7DDl++bh89s9nUmN0DTkbdlbm7pybsF/HqR3lTJgNerGOWGkz8S75tU5c9eGkSSLnzydc8BV901lydzgcsmDPAll5ZOVVn+P45eMyfdt0mbtzrkRER1y1ffy68Qnnf3vZ2yIi8uFfH8qs6skuXtesEblyReSNN+zys89eu7YwnTIcjG/GI7OCcUyMiDEi771nC2Yp1XZNnpzkfypHSzEff/3VZtAzz9jlhx5KuRS0ZYvzS/3++yKFCyf9on/2WdL9R450bqtWLenVSqdOIiCRn34k58PPJ6yOiomS9+9AvmmRO2H/yxGX5UjgawnHWlW3sPi+Y//x8g7Lm1Btdy1/HvhTCn5cUO7/+X7ZfHKzvPPHO+I/1F8KfFxAwqPCE0qvC/YskLeXvS3e73vbAJWCH3/7UUygkWfnPiu5P8wtTb9rKvXG1pM8w/LYqstEomOjpfAnheXpOU/LoKWDEn4w6n9bX9YEr5FCnxSSZ+Y+k7D/jtM7ZPpLrSUin99VpafuD9sqdwKRJuOb2DfEfbmHjOkiw1oisV5GRg5oLtGFC4mAnC6RTy77IqsO/WP3v3TJ5n3FignHrdIPqTmmpuycNtoGaWNk4D32R/XghYNSd2xd8fnARw52ucdeRMW19Vz8ZkSS9EV5G1nVzDY7xNSrKzJokMjw4fYiL36/S5cktNItcsU77oKgaFERkBB/b3EYIyEF/GTmlulJ8vDM7o1yKiBpflw6d1xEREIiQ+R/m/8nFV9Gjj7a3lZHfvONyIwZcvjiYQlcHuislt6xwwbKi0lLbkns22cvOM6edV58xn/mmBhxFCggMc/3ce4fX2MQXy2XL5/N27vuEnniCYn1ySWlXktU6hw82HlhKmJrjkDaPJvLWap/Fznb+wkbOJKp+FVFafZ9MyEQ+eyfz+z/8//+Z38AU6gifvX3V8VvqJ9ExUTJPZPukc/a2QuAv3vdKyuPrEw452fvtRV5++2r3v/KoleEQKT66OrOppJEkjf7xNdMVR9dXS5FXJKBSwfKJy3i/m6ff+58499/i4C8+GQReXDKgyIiMnXr1IT0/LLNXmScDTsr58PPS9kvyyZsGzj8XnG88kqS35M2P7WRmmNq2mrpQ4dEOnWS8LZ3iYBsvauW88Jn2jR70Rf/XZozR4NxeqWWYcHB9lOOHZv6e3futPt8+61Lk5QtXZWPDodIzZq2qvhKXHXRX3/ZwJy8qO9w2CqAxFf18e28IPLkk0n3v/12kfr1bVtz8vaaw4dFHnpIbv+4iuQZlkdERPaf359QmiGQhCDbYUoHad/DniPSC6n9TW1pOK5hQrtQWoVFhSWpEhu/bryMWzdORGzbbZFPi0iX6V2k/Ijycu/ke1M9zvLly6XL9C4J6dx7bq8EXwqWIp8WkWKfFZOv//06Yd/F+xYLgcj0bdPlVOgp6Tm3p4xaM0pyfeD88Z21Y9bVJ4mKEnnhBVu9u2iRyLPPyid/figm0AiByOBlg+1+a9aIgGwfP0x2Vcwvf1S0x+w182lxlCsnArK3WgrtM4cP279P3N+uf3tkfEPkYm4k9xBbmoz/oT0bdlaqjqoqvd5rYPefOFFERHbdWUuO5kd2ffS6MyAXzC8xJllppFgxkY8+kvC335I3Fr8hM+rY0ntM89tERGT+c7bTx+k8dv/bn8We+/vvRerXl8uF8sglX2TKw1XlfGUb7FcP7CGrjqySEsNLyNhGSFiuuHPVrOk8b2JHjzr7J3z8cepfkm7d7D7Nm4tUrmwfIDJ0qPNi9KefnPvHl55vuUXkt9+c5x4yxAZ2YyT8jQE2kEVGihQoYNvfE/8dQC5/PVy2n94uw1cOl1nDnrTHaNXK/v379UsItI/88kiSpo6NX3xhSyMgEt++n8j0bdOFQFv7UfRNJKxQXK1X8eLiOH1anp//vDw4+X6JrFLJrl+VtMo7PCpc+i3oJ7sX/U+ka1eR0FD73LSpbdJ45x2Rt95KSF94VLhM2DBBTKCRAh8XkFKv2X4Qjp49kyYsIkKkcmUJy+8n1V+y3+daY2pJ8eHFpc43dSTg04CEdvrKIytLrg9yyW+7f5MhfwyRSXXj8vjffyUyJlJCIkOk0CeFpM+8uIuk995L8v2r/X8BEn7+tPNvH3dBNLUWsmn/Kg3G6ZVahi1fbj/l9drgly51xpqc7Kp8XLLk6h+Y64lvx73tNhs0BgwQqVLFrnv4YZvRZ86IGCOHXukpa4KvvsIPiwqTfgv6JfywHLt8LEmHFAKRrae2SlhUmOT+MLdU7m//sf4rayQ8ylZ/bDu1zV4Ju8ibS95MOHd8x62ULF++XLaf3i4tf2gp83fPT1i/aO+ihPcfv3xcJm6cmNB+mbj0LyKy++xumbdrnizdv1RiHWlv39hzdo/cPuF2Zwk8LMz2TuzXT8TPT8692FPunHinlP6itOz54XOZWw1Z8eWA1A8Y36kOJCa3r6xvWUkqflVRPl6RNGANXjZYeA8Jzm/33dGyuoT6GplzZ4m4D+/scDfgfi+Z8Vp7Wyp++WWRDRskNDJUaoyuId7ve8uoDnFth7PsRciuDUtlTuuS0n/0AxLlhXzcAll/fL1I1aoSVjJA5tT0ksAP24iILX1trOAnBwK8pezwUtK0lz3n4WK+zvao+Mfu3ba67JlnbMDKn1+kXj3bxhsZeXVebNhgq/lbtLD7G2NLUffeazucxFdJ793rfM+VK7YkvSvu7/Hee7Zafvduu/zww/Y9NWrYtnywnfDixcbaknePHnZ52DC7T3yAjX/8/beIiMzaMUsIRAp8XEAcDoccffhh27eiY0f7+S5dSvKRImMibRPNe8g/FXOJw89PZNw4+53x9rZ/n/nznefMl8+2689KdoH44IN2n3vvdaYpvrMM2BqHihVtX5CSJWX3c52l6fgmsvX5h+xx96XQXr9vnziKF5MzxfJJqdeQXB/kkn9+eF8uN6glBQcm7WPRe15vERGJiYqUC/ls35YDL/WQkp+XlK6PIKvLINNmx12MNGpkLzQnT5bdn74lBCITNkyQ6ABnTd7Wcn5CIPLYjMc0GKdXahk2frz9lIcOufR0HishH6OjbcDs08f+A0Zc3Q6TqvhOVonbiPv2df5j9u0rMmGCCEij3vafKXkvy2F/D0vyz/bTpp8k4NMAeWzGY7LzzE4hEHl85uMJ25+a/aS82g55cuRdGc+EVIRHhUvLH1qm2DM1sWv988anvfe83pJnWB6p/U1tWbhnoYtTmswDDzjzfuJE+WHDD0IgCb1Kk7cFJrFihe1wdF9c57BJk1Le7fAKIRD5ubYzQFzKjcyf9Ynd4cKFhPVtP6sjBCJtf2orhy8elnPh5+S+/90nJtDI4n2LRYKDZV+fPkk7ecWJat1Kthf3kkffulUE5PUOftJ4fGM5fvl4wj5/fma/a1FeSFid6hKdP58cPLzF/ggkDmDg7InfsaPtzLZwofPic+dOW/KdP1/k/vvt+nLlbH+I2bNt50QRG6Tz5LHbixZNsTo4VStXJk2Pj48tXSb2WlwTzBtv2A6NJUvapqPXXhN5801nH4433hBxOGTjsfWy6eg6ERE5X7++vSheu9bu87WzVkbOnxcpX14uzJ0ms7952W4fPdpuW75cpG1bu65AAVtrMGOG/T2oV8+uv/VWkf1xncji9wWRChVsEAfbf6RaNee2p55yfh+//97WVNx9d+r5s26dSL58ElK1gkRULJdwnJ961JFWP7SS79aNl5F355XIqpVEGjRI+HuG57L9Khq8nFsivbj67/7RRyJiL95KfV5Kus/qLlHezgucH+shJtBImS/KyMJlrvv/zNHB+M037fc3hf9rlYLlf/xhfyASV+fF3zKUVpcu2X+0xJkeHCwyaZKEP/uUCEisTy7ZUcJLeM8G0zH/jUloc9p0YpMEfBog9/98v0THRkveYXkTgu7ifYsl1hErpb8onbCu5Ocl5eilo5Lvo3wyceNE12XGDbrelXSLCS0SSi/HLh+7+Qn69lvn33L9erkUcSmhja32N7XTdozQUBugoqJS3BzriJXPV34uA6c8K089lluO/vytDVKJTZ8usmePfPT3R0Igkv+j/FJlZBVp8G0D8f3Q1/bKjpNqHn75ZZIf1fKvIEv3J632ioq8Istf6SRRZeJKZhMm2A0Oh/O9X34pUru2JFQZx4tvlqlXz1lSjb838Y47nMEnuYULbUmyU6fU8zAlDoe9aP3vP1vVPXv21fvExCTt5LZpU9Ltf/zhvFjo31+kWTP72Vavlqj8+UV621Kj1Kpl26rj/fmnJFS5d+1qLyQSd6pxOGxTCCTt73Hhgq0FKFDA1nidPm0vEBo2tJ06V6+2vwE//mi/L50722P4+9uSvsNhq7HjP8+IEdfOoyVLkt7J4e8vUqaMrb2YOdOu8/Jy9j1o1kyOvGE7dF5o2Vhi/f1k2pi+4nj3XZu+oUPthUicR2c8KoU+KSQ7i9rj97sPafh2UTlw/oBcib6iJeP0Si3DHnrI1gCptNkTf0WbO7f98YkvJbjA8cvHpdlbRRL+qXp2svexxt9ycsfEO2TE6hHi84GPlPy8pGw+uVlERL5c9aV0nNpRZm6fmXCsvef2Su95vWXnmZ0J7bwhkSHXv5c1E1zvn/dM2Bnpv7D/jfXgvRFhYc42/Li2mHXH1skL81+4dqn4BsU3E6QmJjZGIqIjZMGeBQkXVN+uTdphI9U83LdPBCS6XFnp+pSf+Hzgk2LHIRGx964nvy0lvupUROTyZdtXIfl35vvvnT/89evb0nHhwrb3/7WsWiVy8OC197lRERH2/3D+/JS3x8aKdO/uDEyJBzcYNcruM2iQDWrz5tlq8sR3O+TKZQNvcjExtokhpdsSV6601dllyjgvcFLy1lt2e926znW//OI8d0pV1MmtWWOr+hctcnawmjzZ3kpZpYrI9u0iGzfai6KLF50dgSBpD/8UjFozSghEyg1AVs8dLW8teStJj20NxumUUoZ9840k1ECptDlfv779R961y/5IbdqUvmq3a+g5t6f4DfWT3kPqSvseyOcr7NX2rjO7ZPSa0Qn3frb9qW3CrQ/ZkSv/eV3m8mWRzZvdnYqrbDu1TdYdW3fV+mvm4Y4dIhERcj78fLoGVxER2+s5tdJtvCtXnEH7iy9sKerAgfSdxx2iokR+/93+z+7c6SxNx9/18N9/zgCVP79tK/f3d971kLwTZVrE32kBtkkjJfEXN4884lzncNhBhFZefZvSdcXG2hJW8eL2uN98c/U+DocN0gUKiJw7d/X2RA5fPCyNxjWSdpPbSWTM1X0FNBinU0oZFjcQ0Q19x3Ich0NkzBibYYMHu/zwMbExUuTTIvL0nKclJjZGlu1fdlWJ5vOVn0uT8U2u6syU3WTJYJzNuD0Pjx2zwSulEmF24XDI6qnJ7m3fujVpqbRePXvHQ7FiNz7K2cGD9kIvtYv2uNuUkjQHZFT8CIABAamPpHT+fPr6uqQis4Kx180YYjMrOHUK9uyxU8Tef7+7U5OFbNxoJ7sXscv798N338EHH8BLL9l1jz7q8tP+d+w/zl85z32V78Pby5t7br0Hb6+k4w2/fvvr/Nf7Pwr7F3b5+ZVKl9KloUkTO7VldmUMESVLJl1XuzY88gi0b2+XCxSA//s/O4uOv/9Vh0iTChXstKCpzfRVp46dm/bOO2/s+Cl54gk7octbb6U+OXzhwjdlhrObJRt/064tfpz+Vq3cmw632rMHfv4Z3n0X3n8f9u2z8zlevGhn2ilWDBYssHNMenlBt2789eyz3Fm/vsuTsvTAUgwm5WkLlVKZ66ef4IEH4Jln7P9+agHNFQoVguBg1x7Tz8/+vnkQjw3GmzfbC7VGaZ80xPMMGGCDr78/fPihvdqvXdtOqBwUBEuWQGysc/+PPkJSm8Iwg9YeX0uNYjUo4l/kphxfKZUOxYrBf/+5OxUqEY8NxiEhdp74xFPQ5jjx88sOHmyf//0XyiWaarpjR5g/H+6+G9q0gYoVU59P+BouRVxi+vbpRMZEktc3L8NWDOPLdl/SqXonwPZLWHd8He0qtcvoJ1JKKY/kscE4PNwG4xwrIgK2bIFbboEjRyBfvqSBGGD8ePjsMxg69Iaqqfad38fus7v5as1XV02A/syvz/Dovke5HHmZB6s+yMnQkzQu1Tgjn0gppTyWxwbjsLAcHoxXrYLoaPjqK1tfn7wjB9h1X36Z6iEuRlzk1cWv8n7r97ml4C1JtsU4Ymg1sRUnQ08CMLL9SLrU7MK3676lRtEa9PmtD+PWjwNg6rapALS8paVrPptSSnkYDcaeavRoKFIE2rW74Yz4fNXn/LjpR3KZXHzX8TvAloanbJ1CzWI1ORl6kpHtR9KuUjuqFa0GwAd3fQBAPt98LNy7kMGtBnMq9BT5c+enetHqrvlsSinlYTQYe6Jff4W5c+Htt284ExbsWcCo/0YBsHDfQiJiIvhp80+8uvhVwqPDASjkV4g+jfqQO9fVtw90qNaBDtU6AFxVqlZKKZWUxwbj8PCb21s/y9i921ZDV60Kly7B6tXQsyc0buzsuJVO205vo+uMrlQvWp3nGjxH/0X9KT68OCFRIbS5tQ31StRj0uZJfN/h+xQDsVJKqfTx2GAcFmZraT1e9biq36lTYdQo21YM9nUKpeJYRywhUSEU8iuUsC48Opzo2GgK+hVk1+VdDPphEAVyF+D3Hr9TIl8JKhepzKwds2hapinPNXwOL+PF8LbDMand5K+UUipdPDoYZ/tqahF7c379+lCv3tXbY2Kcrx9/3Pm6c2ccTZtcNbyaiNB1RlcW7V1E9zrdyeWVi4EtBvL4rMfZfW43Mx6ZwfTg6eTyysWqZ1dRIl8JANpXbk/7yu2THEsDsVJKuY4G46zs88/tcG9168Idd8CwYXb4unh79ybdv2lTmDWL3X5h3D68GF+3/5on6j6RsHnGjhnM3TUXgImbJgIwectkImIiAHjhtxc4dvkYfRr1oVKRSjf1oymllHLy2LGps30wnjfPBuJChez9wqNHw+zZSffZvNk+N2pkh7dcs4aoUsXpPrs756+c59t13ybZfeKmiZQvWJ61vdey6tlVtCjXgoiYCFre0pKv23/NwYsHiXJE8UyDZzLnMyqllAI8OBhn+w5cP/9sB1efNcu5bsOGpPssX24Hsl+1Crp0AeCdP99hw4kN3FPxHlYeXcnS/UtxiIOuv3Tl932/06NODxqXbkzzcs2pX7I+AA9Vf4jHaj1GLq9cPFTmIRqWaphJH1IppRR4aDV1dLR9ZOuS8apV0LKlnenkvffsTEujRtkB17/+Gho2hLNnoXfvhDE/lx1YxvBVw3mh0Qu8f9f73DHxDu77+T6G3j2UWTtnUblIZV5o/ELCKd5u9TZRsVH0btib/Lnzc+bNM2xcvdFdn1gppXIsjwzGYWH2OdsG4+Bg+2jeHLy9CX37DQ7vWUmtqUdhzhzIn98G4r597QhbwLnwczw15ylqFK3BF/d+QR6fPKzptYYG4xow+A97i9Om5zeR19eZKaXzl2Z8h/EJy4X8CmnHLKWUcgOPrKZ2eTA+d87ew5tZvvnGPrdoAcDTc5/mzrLLWPBedyha1PawLlLEtiP7+ADw2crPOBV2iildppDHx9bPF/QryILuC2hWthlv3f5WkkCslFIq6/DIknG4HSDKdcG4WDEb/M6eddEBr2HcOPj4Y3jmGc7VqMDGA8uYvXM25IUHmcLMu2rRZcZZaNYsYTLvf4P/5Zt13/BYrccS2oHj1ShWg9XPrb756VZKKXXDPDIYx5eMXdKBKzra3u977hxcvpz01iJXmzcP6duXnU0qsur5Jrw28lZCokIAWPLEEsauG8urEQtp75+LPPfcw/82T2b3ud2MXDOSonmKJowLrZRSKnvx6GDskpLx7t3O10uWQNeuLjhoCiIj4fnnOVyhEE3aHiT8974Jm5qWaUrbSm1pW6ktQ4oOoRwf806TWF6f+1TCPgt7LKRykco3J21KKaVuKm0zvp4tW5yv33sPPvkEoqJgwgRYu9YFJ4gzdSqcPMnzLS9Qv9Lt1Cleh+19t/Nfr/+Y89ichN161O3BBT/h9T/fonT+0gnrby93u+vSopRSKlNpyfh6tmyxnaQGDoShQ+3kC/nzQ79+druIC04CLFhATLkyLKl4jO/qP0Ovhr1S3K1msZp0qNqB+Xvm06tBL+6ueDe5vHLhZTzyukoppXIEjwzG587ZZ5dMFLFnD1SuDK++aoefnD4dBg1ybg8JscE5Ay5FXKLgli1cqFUJzDHKFyx/zf0ndZ7Eh39/SL+m/SiWt1iGzq2UUsr9PLI4Fd/puWhRFxzs4EGoWNFG9mnTYOJECA11bv/99wwdfv3x9ZT6sBCydy/HKwQAUL7QtYNxYf/CfHnvlxqIlVLKQ3hsMM6Tx0W9qeODcbzHH4eSJaFSJbjlFnuvbwbM3jmbWmfAiLCjlL1nuFyBchk6plJKqewlTcHYGNPeGLPbGLPPGDPoGvt1McaIMaax65KYfmfO2FuDr+n8+avXxcY667gvXYILF+xz4mCcO7cdL/rHH+GVV+Dvv+0kDTdg77m9fLXmK5oftcvvnPuFQn6F8Pfxv6HjKaWUyp6uG4yNMd7AGOA+oCbwuDGmZgr75QdeAda4OpHpdfbsdaqo//0XAgJg7tyk67/80r5xwAA7W9JLL9n1iYMxwO2323GjX3zRvn76aTvSyL59tqd1aCgcOXLNNIoIbSa34UpkOO/tKMbZGhU4UBguRlxM34dVSimV7aWlZNwU2CciB0QkCpgGdEphvw+BT4EIF6bvhlw3GMdPRbgx2aQI8TMkff21fZ461T5XrIik1Gva3x/eftsG4sGDoWpVaNXKTmlYvTqsSf26ZOvprRy5dIQRAd0JOHKGooPe55M2n/B9h+/T9iGVUkp5jLQE4zLA0UTLwXHrEhhjGgLlRGSBC9N2w65bTb1rl31OPJqWw2HX9+gBK1fCwoV2vZ8fS81BSn5Rkvt/vh/zvuFEyAkAzoafZZJf3KAgI0dC+fL23uPDh+19Ve++m2oS5u2eB8CTBVvZFQ0bMrDlQJ5r+NyNfGSllFLZWIZvbTLGeAFfAj3TsG8foA9AiRIlCAoKyujpE4SGhiYc79SplkREnCAoaH+K+962bh3+wOFNmzgY9548Bw/S9NIldpYty6moKPD3x3fGDBy+vvRe+jKnw0+zaN8iAEYtHMWfp/9k08VNRDoieTruuBvefJMrJUsiuXJR5euvKbBtG2tS+Iyrz63m3W3v0vd4RaL/ngHAP4cOEZMZY19fR+J8VDdG8zDjNA9dQ/Mx4zItD0Xkmg+gObA40fJgYHCi5YLAWeBQ3CMCOA40vtZxGzVqJK60fPlyERGJiBABkaFDU9nx0CG7A4i8+KJz/Y8/2nU7dybZPTo2Wnw+8JEinxYRAhECkcbjGwuBSP1v68vkzZPlm46l5JeayIrDK5xvHDxYxMdHJCbmqiQ89+tzUvTjIs505Msn4nBkMAdcIz4f1Y3TPMw4zUPX0HzMOFfmIbBOUomJaSkZrwWqGGMqAseAbkD3RMH8EpDQQmuMCQLeEJF1GbpKuEHXvcd4/nz7bEzSHtVbtoCfH1SpkmT3o5eOEu2I5v/u+D+2ntrKv8f+Zd3xdeT2zs3yp5dTyK8QYTMeouLXFfGd2Y1JnSdxz633QIUKdpKJY8dstfXixXYEL2DjyY10NNWBVfYkZcsmzMCklFIq57lum7GIxAD9gMXATuAXEdlujPnAGNPxZicwvQ4dss+33JLKDvPnQ7Vq0LSpvXUp3pYtULs2eHsn2X3v+b0ANCjZgAmdJvDene9RuUhlXm32KoX8CgGQ1zcvvz/xO/l889Fmchte/f1VIsuVdibojjtg2DCIiCA6NpqAf7cwYdAq50n8/DL8uZVSSmVfaWozFpGFwMJk61LsnSQirTOerBu3b599rpzSBEYhIRAUBP37w44dtqdXvC1b4MEHrz7eeXvAKgG2xPxorUd5tNajV+3XsFRDNjy/gYFLB/LVmq84lGsZc8AOGhLv2DF25A2lz78xSd+c+KJAKaVUjuNxI3Dt3WsLtxUqpLBx6VJ7H3CHDlC4sA2CMTH2nuDTp6FOnavesuXUFvL65KVUvlLXPXcenzyMun8Uv3T9hQUR24j1MkmHyzx6lI0nN1L7NDjy5HGO3nXrrTf2YZVSSnkEjwzGFSvaiZausmQJFCxoB+ooUsS2GbdrZ29JAmjdmhnbZzDs72FExkSyaO8iJmycwEM1HsKko033kVqP0KJya6a0K2nHs44Tc+QQO3f9Q/VzwJAhdlCRX39Nso9SSqmcx+Nmbdq7N5UqarAzMNWqZSN1kSK2ZLx8ecLmV47/wMi1owCYvn06W09vpVaxWoxsPzLd6ahQqAJD2uzlySLdYcoUAD6b1p9dBe0kE14tWtgdO2a5ZnellFKZzONKxgcO2Dkcftz0IyU+L0GnaZ0YvnK43Zh40ofChZO87+87KjBy7ShqF6/NkFZD2Hp6Kw1LNWRt77UU9i9MepXNX5ZjYSf4dfDDRKxZhaNwIQqdCeWBPRDunwuaN8/oR1VKKeUhPKpkHBVl53UoUQJeW/waFyIuMG/3PA5dPMSbt70KR486g3HVqvY9AYUo98xFLvod4tFajzK963REhMpFKtPm1jY3PGlD2QJlcYiDzrO68nS9p/m6RCHuPHyRYmFwrlUj8vj6uupjK6WUyuY8qmQcf9twQICz9zPA1lNbubxvu52VKb5n1/33w6lTPDC0JucL5OKpJr34tM2nABhj6Fm/J2ULlL3htCR+76TNk5hZ6Di1zkCxCC/K9R18w8dVSinleTwqGMfPfhgQACXzlUxYLwjrV8ZNc5hoBqaYokVYeW4j/Zr047uO31GhUAWXpaVcQeecxM3KNqPXvVE0HlMPExUFnVKaZ0MppVRO5VHV1ImDccixEABK5y+Nr7cvc2YO5S7gcukA4qeH2HlmJ1dirtC4tOunX05cMl7UYxEL9y6kYamGVw0qopRSSnlsML64/yJlC5Rlapep1D58Bf+B97O+VAxbQ9fRk3psPLGRDSc2ANCkTBOXp6Wwn+30NbDFQAr5FaJ7ne7XeYdSSqmcynODccRFWldoTcsSTaBpeaR0OV561sGaBb1YeHAxM3bMIK9PXgrmLkjlIqndC3XjjDHIeynMgayUUkol47FtxhcjLlIodyE7NvSpU5jAQAZ0/oRcXrmYscNOWxgWHcbt5W7Hy3hUNiillMpmPCoKnTsHuXODn7+DS5GXKBHrb8ecBihfnm61u/Ffr/+SvKfVLa3ckFKllFLKyeOCcUAAhEWH4hAHrzw7Dh6Nm9ShrO1Q1aBUA37t9isvNXkJgFblNRgrpZRyL49rM46vojYOyH/2snNjWWfv5o7VOtKsbDPK5C9D87I6EpZSSin38qhgfOGCHeXyYsRFqp5LtjF37iSLxfMWZ3ArHXxDKaWU+3lUNXV4OOTNC5ciLnH7UXenRimllEobjwrGERHg5wcXIi5Q/SyIl0d9PKWUUh7Ko6qp44PxscvHCLgCjhLF8H7mOahf391JU0oppVLlccHY3x+OXDpC8ysGr6LFYdgwdydLKaWUuiaPqse9csWWjA9fOkypKF9MQIC7k6SUUkpdl0cF4/hq6iOXjlAswtve56SUUkplcR4ZjA9fOkzhMIcGY6WUUtmCx7QZx8YaYmPBkfs8wZeCyRfqBUWLujtZSiml1HV5TMk4KsoLfEMZIeUpEAnesVoyVkoplT14TDCOjPSCRuOJJJT3avW1KzUYK6WUygY8JhhHRXlB9TmU92nMa1Wftis1GCullMoGPCsY5w4hwKd00omNlVJKqSzOs4KxTzj+ufLAgQN2Zbly7k2UUkoplQYeFozDyOubF7ZssdM3lSnj7mQppZRS1+UxtzZFRnqBbxhjP50HR89A69ZgjLuTpZRSSl2XxwTjqCgvTK4wbj16ya6oVMm9CVJKKaXSyGOqqa9ExpIvJsa5ok4d9yVGKaWUSgePCcZh0VEUiohb6NcPXnrJrelRSiml0ipNwdgY094Ys9sYs88YMyiF7S8YY7YaYzYZY/4xxtR0fVKvLSwq0hmMW7eGXB5TA6+UUsrDXTcYG2O8gTHAfUBN4PEUgu0UEakjIvWBz4AvXZ3Q6wmLjnAG40KFMvv0Siml1A1LS8m4KbBPRA6ISBQwDeiUeAcRuZxoMS8grkti2lyJjtRgrJRSKltKS11uGeBoouVg4LbkOxljXgJeA3yBu1M6kDGmD9AHoESJEgQFBaUzuam7HBFKQKR9/e+uXUSEhLjs2DlJaGioS/8uOZHmYcZpHrqG5mPGZVYeuqxhVUTGAGOMMd2Bd4CnU9hnPDAeoHHjxtK6dWtXnZ6Pp/+UUDJudu+9On3iDQoKCsKVf5ecSPMw4zQPXUPzMeMyKw/TUk19DEg8rmTZuHWpmQZ0zkCabkhEbKJq6oIFM/v0Siml1A1LSzBeC1QxxlQ0xvgC3YB5iXcwxlRJtPgAsNd1SUybK7G2A5cjTx7w8cns0yullFI37LrV1CISY4zpBywGvIEfRGS7MeYDYJ2IzAP6GWPaANHABVKoor7ZIuKDcaECnnPztFJKqRwhTW3GIrIQWJhs3buJXr/i4nSlW0I1tfakVkoplc14TCEyUq5QKAK8ChV2d1KUUkqpdPGcYOyIpGAEGA3GSimlshmPCcZRXKFQhMFoNbVSSqlsxmOCcTTh2maslFIqW/KgYHyFQhGiwVgppVS24zHB2C82hFyCBmOllFLZjscE4/yxcXNVaDBWSimVzXhEMI6KgoKOULugQ2EqpZTKZjwiGIeGJgrGWjJWSimVzXhEMA4Lg0Kx4XZBg7FSSqlsxiOCcWgoFIq9Yhc0GCullMpmPCcYx8TNn6jBWCmlVDbjOcE4OsouaAcupZRS2YxHBONLITEUioolyicX5M7t7uQopZRS6eIRwfjc5TDyR0KUvwZipZRS2Y9HBOPzoeHkiYYYfz93J0UppZRKN48Ixg8+FEbeaJB8GoyVUkplPx4RjKMkjDzRIP7+7k6KUkoplW4eEYzDo8PJGwWSN4+7k6KUUkqlm0cE47BoWzI2eTQYK6WUyn48IhiHR4eTNxpMnrzuTopSSimVbh4RjMOibMnYK19+dydFKaWUSjfPCMbR8cE4n7uTopRSSqWbRwTjh6o/RKEYH/IULObupCillFLplsvdCXCFAL/CEBUNWk2tlFIqG/KIkjFX4qZPzKsduJRSSmU/nhGMw8Pts97apJRSKhvyjGAcFmaftWSslFIqG/KMYKwlY6WUUtmYZwTj+JKxBmOllFLZkGcE4/iSsVZTK6WUyoY8KxhryVgppVQ2lKZgbIxpb4zZbYzZZ4wZlML214wxO4wxW4wxfxhjyrs+qdegHbiUUkplY9cNxsYYb2AMcB9QE3jcGFMz2W4bgcYiUheYCXzm6oRek5aMlVJKZWNpKRk3BfaJyAERiQKmAZ0S7yAiy0UkLiLyL1DWtcm8jmLFON+oERQsmKmnVUoppVwhLcNhlgGOJloOBm67xv7PAYsykqh0u+8+tvj707po0Uw9rVJKKeUKLh2b2hjzBNAYuDOV7X2APgAlSpQgKCjIZecODQ116fFyKs3HjNM8zDjNQ9fQfMy4zMrDtATjY0C5RMtl49YlYYxpAwwB7hSRyJQOJCLjgfEAjRs3ltatW6c3vakKCgrClcfLqTQfM07zMOM0D11D8zHjMisP09JmvBaoYoypaIzxBboB8xLvYIxpAIwDOorIadcnUymllPJc1w3GIhID9AMWAzuBX0RkuzHmA2NMx7jdhgP5gBnGmE3GmHmpHE4ppZRSyaSpzVhEFgILk617N9HrNi5Ol1JKKZVjeMYIXEoppVQ2psFYKaWUcjMNxkoppZSbaTBWSiml3EyDsVJKKeVmGoyVUkopN9NgrJRSSrmZBmOllFLKzYyIuOfExpwBDrvwkEWBsy48Xk6l+ZhxmocZp3noGpqPGefKPCwvIsVS2uC2YOxqxph1ItLY3enI7jQfM07zMOM0D11D8zHjMisPtZpaKaWUcjMNxkoppZSbeVIwHu/uBHgIzceM0zzMOM1D19B8zLhMyUOPaTNWSimlsitPKhkrpZRS2ZJHBGNjTHtjzG5jzD5jzCB3pyerMsb8YIw5bYzZlmhdEWPMUmPM3rjnwnHrjTFmZFyebjHGNHRfyrMOY0w5Y8xyY8wOY8x2Y8wrces1H9PBGONnjPnPGLM5Lh/fj1tf0RizJi6/phtjfOPW545b3he3vYJbP0AWYozxNsZsNMb8FreseZgOxphDxpitxphNxph1cesy/f852wdjY4w3MAa4D6gJPG6MqeneVGVZPwLtk60bBPwhIlWAP+KWweZnlbhHH2BsJqUxq4sBXheRmkAz4KW475vmY/pEAneLSD2gPtDeGNMM+BQYISKVgQvAc3H7PwdciFs/Im4/Zb0C7Ey0rHmYfneJSP1EtzBl+v9ztg/GQFNgn4gcEJEoYBrQyc1pypJE5G/gfLLVnYBJca8nAZ0Trf9JrH+BQsaYUpmS0CxMRE6IyIa41yHYH8EyaD6mS1x+hMYt+sQ9BLgbmBm3Pnk+xufvTOAeY4zJnNRmXcaYssADwPdxywbNQ1fI9P9nTwjGZYCjiZaD49aptCkhIifiXp8ESsS91ny9jrhqvgbAGjQf0y2uenUTcBpYCuwHLopITNwuifMqIR/jtl8CAjI1wVnTV8BbgCNuOQDNw/QSYIkxZr0xpk/cukz/f87lioMozyAiYozR7vVpYIzJB8wCBojI5cQFDM3HtBGRWKC+MaYQMAeo7t4UZS/GmAeB0yKy3hjT2s3Jyc5aisgxY0xxYKkxZlfijZn1/+wJJeNjQLlEy2Xj1qm0ORVfzRL3fDpuveZrKowxPthA/LOIzI5brfl4g0TkIrAcaI6t9osvJCTOq4R8jNteEDiXuSnNcloAHY0xh7DNc3cDX6N5mC4icizu+TT2orApbvh/9oRgvBaoEteD0BfoBsxzc5qyk3nA03GvnwZ+TbT+qbjeg82AS4mqbXKsuDa2CcBOEfky0SbNx3QwxhSLKxFjjPEH2mLb35cDXeN2S56P8fnbFfhTcvggCSIyWETKikgF7O/enyLSA83DNDPG5DXG5I9/DbQDtuGO/2cRyfYP4H5gD7bNaYi705NVH8BU4AQQjW3reA7bZvQHsBdYBhSJ29dge6nvB7YCjd2d/qzwAFpi25i2AJviHvdrPqY7H+sCG+PycRvwbtz6W4H/gH3ADCB33Hq/uOV9cdtvdfdnyEoPoDXwm+ZhuvPtVmBz3GN7fPxwx/+zjsCllFJKuZknVFMrpZRS2ZoGY6WUUsrNNBgrpZRSbqbBWCmllHIzDcZKKaWUm2kwVkoppdxMg7FSSinlZhqMlVJKKTf7f+EoHMHMC0lZAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'Acc at best val acc: {err1.mean():.3f} +- {err1.std():.3f}')\n",
    "print(f'Acc at test: {err2.mean():.3f} +- {err1.std():.3f}')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(acc['train'], 'b-', label='Acc train')\n",
    "plt.plot(acc['val'], 'g-', label='Acc val')\n",
    "plt.plot(acc['test'], 'r-', label='Acc test')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(loss['train'], 'b-', label='Loss train')\n",
    "plt.plot(loss['val'], 'g-', label='Loss val')\n",
    "plt.plot(loss['test'], 'r-', label='Loss test')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training paramerters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 200-0.01-0.0005-0.5: 0.434 (0.458)\n",
      "-1: 200-0.005-0.0005-0.5: 0.445 (0.489)\n",
      "-1: 200-0.001-0.0005-0.5: 0.480 (0.507)\n",
      "-1: 300-0.0005-0.0005-0.5: 0.489 (0.511)\n",
      "-1: 200-0.01-0.001-0.5: 0.417 (0.474)\n",
      "-1: 200-0.005-0.001-0.5: 0.436 (0.489)\n",
      "-1: 200-0.001-0.001-0.5: 0.504 (0.535)\n",
      "-1: 300-0.0005-0.001-0.5: 0.428 (0.485)\n",
      "-1: 200-0.01-0.005-0.5: 0.465 (0.504)\n",
      "-1: 200-0.005-0.005-0.5: 0.482 (0.489)\n",
      "-1: 200-0.001-0.005-0.5: 0.476 (0.504)\n",
      "-1: 200-0.05-0.01-0.5: 0.452 (0.452)\n",
      "-1: 200-0.01-0.01-0.5: 0.469 (0.500)\n",
      "-1: 200-0.005-0.01-0.5: 0.482 (0.502)\n",
      "-1: 200-0.001-0.01-0.5: 0.500 (0.502)\n",
      "-1: 200-0.01-0.001-0: 0.399 (0.463)\n",
      "-1: 200-0.01-0.001-0.25: 0.465 (0.491)\n",
      "-1: 200-0.01-0.001-0.75: 0.452 (0.480)\n",
      "-1: 200-0.001-0.005-0: 0.441 (0.491)\n",
      "-1: 200-0.001-0.005-0.25: 0.439 (0.502)\n",
      "-1: 200-0.001-0.005-0.75: 0.467 (0.500)\n",
      "-1: 100-0.001-0.005-0.5: 0.480 (0.491)\n",
      "-1: 500-0.001-0.005-0.5: 0.355 (0.509)\n",
      "-1: 1000-0.001-0.005-0.5: 0.351 (0.485)\n",
      "-2: 200-0.01-0.0005-0.5: 0.478 (0.500)\n",
      "-2: 200-0.005-0.0005-0.5: 0.390 (0.498)\n",
      "-2: 200-0.001-0.0005-0.5: 0.491 (0.524)\n",
      "-2: 300-0.0005-0.0005-0.5: 0.443 (0.509)\n",
      "-2: 200-0.01-0.001-0.5: 0.465 (0.509)\n",
      "-2: 200-0.005-0.001-0.5: 0.463 (0.487)\n",
      "-2: 200-0.001-0.001-0.5: 0.322 (0.480)\n",
      "-2: 300-0.0005-0.001-0.5: 0.351 (0.461)\n",
      "-2: 200-0.01-0.005-0.5: 0.474 (0.509)\n",
      "-2: 200-0.005-0.005-0.5: 0.480 (0.496)\n",
      "-2: 200-0.001-0.005-0.5: 0.476 (0.496)\n",
      "-2: 200-0.05-0.01-0.5: 0.417 (0.489)\n",
      "-2: 200-0.01-0.01-0.5: 0.487 (0.493)\n",
      "-2: 200-0.005-0.01-0.5: 0.480 (0.511)\n",
      "-2: 200-0.001-0.01-0.5: 0.461 (0.480)\n",
      "-2: 200-0.01-0.001-0: 0.406 (0.471)\n",
      "-2: 200-0.01-0.001-0.25: 0.465 (0.489)\n",
      "-2: 200-0.01-0.001-0.75: 0.478 (0.502)\n",
      "-2: 200-0.001-0.005-0: 0.298 (0.456)\n",
      "-2: 200-0.001-0.005-0.25: 0.498 (0.513)\n",
      "-2: 200-0.001-0.005-0.75: 0.456 (0.487)\n",
      "-2: 100-0.001-0.005-0.5: 0.368 (0.421)\n",
      "-2: 500-0.001-0.005-0.5: 0.482 (0.509)\n",
      "-2: 1000-0.001-0.005-0.5: 0.502 (0.520)\n",
      "-3: 200-0.01-0.0005-0.5: 0.366 (0.414)\n",
      "-3: 200-0.005-0.0005-0.5: 0.406 (0.474)\n",
      "-3: 200-0.001-0.0005-0.5: 0.458 (0.465)\n",
      "-3: 300-0.0005-0.0005-0.5: 0.436 (0.452)\n",
      "-3: 200-0.01-0.001-0.5: 0.408 (0.458)\n",
      "-3: 200-0.005-0.001-0.5: 0.397 (0.471)\n",
      "-3: 200-0.001-0.001-0.5: 0.406 (0.461)\n",
      "-3: 300-0.0005-0.001-0.5: 0.463 (0.463)\n",
      "-3: 200-0.01-0.005-0.5: 0.430 (0.487)\n",
      "-3: 200-0.005-0.005-0.5: 0.441 (0.467)\n",
      "-3: 200-0.001-0.005-0.5: 0.439 (0.452)\n",
      "-3: 200-0.05-0.01-0.5: 0.364 (0.425)\n",
      "-3: 200-0.01-0.01-0.5: 0.423 (0.450)\n",
      "-3: 200-0.005-0.01-0.5: 0.430 (0.452)\n",
      "-3: 200-0.001-0.01-0.5: 0.434 (0.434)\n",
      "-3: 200-0.01-0.001-0: 0.364 (0.439)\n",
      "-3: 200-0.01-0.001-0.25: 0.382 (0.452)\n",
      "-3: 200-0.01-0.001-0.75: 0.410 (0.461)\n",
      "-3: 200-0.001-0.005-0: 0.430 (0.458)\n",
      "-3: 200-0.001-0.005-0.25: 0.443 (0.456)\n",
      "-3: 200-0.001-0.005-0.75: 0.414 (0.428)\n",
      "-3: 100-0.001-0.005-0.5: 0.406 (0.406)\n",
      "-3: 500-0.001-0.005-0.5: 0.454 (0.461)\n",
      "-3: 1000-0.001-0.005-0.5: 0.452 (0.480)\n",
      "-4: 200-0.01-0.0005-0.5: 0.471 (0.502)\n",
      "-4: 200-0.005-0.0005-0.5: 0.482 (0.502)\n",
      "-4: 200-0.001-0.0005-0.5: 0.450 (0.507)\n",
      "-4: 300-0.0005-0.0005-0.5: 0.498 (0.533)\n",
      "-4: 200-0.01-0.001-0.5: 0.441 (0.500)\n",
      "-4: 200-0.005-0.001-0.5: 0.463 (0.500)\n",
      "-4: 200-0.001-0.001-0.5: 0.491 (0.515)\n",
      "-4: 300-0.0005-0.001-0.5: 0.518 (0.531)\n",
      "-4: 200-0.01-0.005-0.5: 0.489 (0.502)\n",
      "-4: 200-0.005-0.005-0.5: 0.482 (0.491)\n",
      "-4: 200-0.001-0.005-0.5: 0.504 (0.513)\n",
      "-4: 200-0.05-0.01-0.5: 0.454 (0.467)\n",
      "-4: 200-0.01-0.01-0.5: 0.476 (0.485)\n",
      "-4: 200-0.005-0.01-0.5: 0.458 (0.482)\n",
      "-4: 200-0.001-0.01-0.5: 0.489 (0.491)\n",
      "-4: 200-0.01-0.001-0: 0.463 (0.491)\n",
      "-4: 200-0.01-0.001-0.25: 0.423 (0.500)\n",
      "-4: 200-0.01-0.001-0.75: 0.465 (0.498)\n",
      "-4: 200-0.001-0.005-0: 0.482 (0.496)\n",
      "-4: 200-0.001-0.005-0.25: 0.507 (0.509)\n",
      "-4: 200-0.001-0.005-0.75: 0.489 (0.504)\n",
      "-4: 100-0.001-0.005-0.5: 0.430 (0.454)\n",
      "-4: 500-0.001-0.005-0.5: 0.493 (0.509)\n",
      "-4: 1000-0.001-0.005-0.5: 0.482 (0.515)\n",
      "-5: 200-0.01-0.0005-0.5: 0.404 (0.467)\n",
      "-5: 200-0.005-0.0005-0.5: 0.404 (0.500)\n",
      "-5: 200-0.001-0.0005-0.5: 0.467 (0.491)\n",
      "-5: 300-0.0005-0.0005-0.5: 0.489 (0.502)\n",
      "-5: 200-0.01-0.001-0.5: 0.461 (0.509)\n",
      "-5: 200-0.005-0.001-0.5: 0.441 (0.489)\n",
      "-5: 200-0.001-0.001-0.5: 0.469 (0.491)\n",
      "-5: 300-0.0005-0.001-0.5: 0.502 (0.509)\n",
      "-5: 200-0.01-0.005-0.5: 0.465 (0.491)\n",
      "-5: 200-0.005-0.005-0.5: 0.465 (0.489)\n",
      "-5: 200-0.001-0.005-0.5: 0.454 (0.476)\n",
      "-5: 200-0.05-0.01-0.5: 0.399 (0.458)\n",
      "-5: 200-0.01-0.01-0.5: 0.480 (0.498)\n",
      "-5: 200-0.005-0.01-0.5: 0.467 (0.489)\n",
      "-5: 200-0.001-0.01-0.5: 0.467 (0.478)\n",
      "-5: 200-0.01-0.001-0: 0.395 (0.461)\n",
      "-5: 200-0.01-0.001-0.25: 0.439 (0.491)\n",
      "-5: 200-0.01-0.001-0.75: 0.478 (0.500)\n",
      "-5: 200-0.001-0.005-0: 0.469 (0.474)\n",
      "-5: 200-0.001-0.005-0.25: 0.474 (0.480)\n",
      "-5: 200-0.001-0.005-0.75: 0.487 (0.500)\n",
      "-5: 100-0.001-0.005-0.5: 0.458 (0.461)\n",
      "-5: 500-0.001-0.005-0.5: 0.471 (0.487)\n",
      "-5: 1000-0.001-0.005-0.5: 0.491 (0.504)\n",
      "-6: 200-0.01-0.0005-0.5: 0.480 (0.509)\n",
      "-6: 200-0.005-0.0005-0.5: 0.423 (0.465)\n",
      "-6: 200-0.001-0.0005-0.5: 0.456 (0.485)\n",
      "-6: 300-0.0005-0.0005-0.5: 0.469 (0.493)\n",
      "-6: 200-0.01-0.001-0.5: 0.390 (0.436)\n",
      "-6: 200-0.005-0.001-0.5: 0.428 (0.465)\n",
      "-6: 200-0.001-0.001-0.5: 0.476 (0.491)\n",
      "-6: 300-0.0005-0.001-0.5: 0.469 (0.487)\n",
      "-6: 200-0.01-0.005-0.5: 0.465 (0.502)\n",
      "-6: 200-0.005-0.005-0.5: 0.461 (0.478)\n",
      "-6: 200-0.001-0.005-0.5: 0.476 (0.485)\n",
      "-6: 200-0.05-0.01-0.5: 0.445 (0.461)\n",
      "-6: 200-0.01-0.01-0.5: 0.474 (0.489)\n",
      "-6: 200-0.005-0.01-0.5: 0.425 (0.478)\n",
      "-6: 200-0.001-0.01-0.5: 0.480 (0.487)\n",
      "-6: 200-0.01-0.001-0: 0.408 (0.458)\n",
      "-6: 200-0.01-0.001-0.25: 0.412 (0.487)\n",
      "-6: 200-0.01-0.001-0.75: 0.454 (0.504)\n",
      "-6: 200-0.001-0.005-0: 0.458 (0.480)\n",
      "-6: 200-0.001-0.005-0.25: 0.476 (0.487)\n",
      "-6: 200-0.001-0.005-0.75: 0.463 (0.469)\n",
      "-6: 100-0.001-0.005-0.5: 0.425 (0.428)\n",
      "-6: 500-0.001-0.005-0.5: 0.469 (0.480)\n",
      "-6: 1000-0.001-0.005-0.5: 0.493 (0.502)\n",
      "-7: 200-0.01-0.0005-0.5: 0.428 (0.476)\n",
      "-7: 200-0.005-0.0005-0.5: 0.428 (0.465)\n",
      "-7: 200-0.001-0.0005-0.5: 0.454 (0.469)\n",
      "-7: 300-0.0005-0.0005-0.5: 0.410 (0.447)\n",
      "-7: 200-0.01-0.001-0.5: 0.414 (0.461)\n",
      "-7: 200-0.005-0.001-0.5: 0.417 (0.461)\n",
      "-7: 200-0.001-0.001-0.5: 0.452 (0.463)\n",
      "-7: 300-0.0005-0.001-0.5: 0.401 (0.465)\n",
      "-7: 200-0.01-0.005-0.5: 0.434 (0.487)\n",
      "-7: 200-0.005-0.005-0.5: 0.474 (0.491)\n",
      "-7: 200-0.001-0.005-0.5: 0.436 (0.456)\n",
      "-7: 200-0.05-0.01-0.5: 0.419 (0.461)\n",
      "-7: 200-0.01-0.01-0.5: 0.432 (0.498)\n",
      "-7: 200-0.005-0.01-0.5: 0.463 (0.478)\n",
      "-7: 200-0.001-0.01-0.5: 0.445 (0.480)\n",
      "-7: 200-0.01-0.001-0: 0.338 (0.452)\n",
      "-7: 200-0.01-0.001-0.25: 0.425 (0.456)\n",
      "-7: 200-0.01-0.001-0.75: 0.436 (0.454)\n",
      "-7: 200-0.001-0.005-0: 0.452 (0.485)\n",
      "-7: 200-0.001-0.005-0.25: 0.300 (0.441)\n",
      "-7: 200-0.001-0.005-0.75: 0.447 (0.458)\n",
      "-7: 100-0.001-0.005-0.5: 0.404 (0.436)\n",
      "-7: 500-0.001-0.005-0.5: 0.432 (0.463)\n",
      "-7: 1000-0.001-0.005-0.5: 0.445 (0.458)\n",
      "-8: 200-0.01-0.0005-0.5: 0.456 (0.474)\n",
      "-8: 200-0.005-0.0005-0.5: 0.454 (0.476)\n",
      "-8: 200-0.001-0.0005-0.5: 0.439 (0.465)\n",
      "-8: 300-0.0005-0.0005-0.5: 0.434 (0.469)\n",
      "-8: 200-0.01-0.001-0.5: 0.465 (0.467)\n",
      "-8: 200-0.005-0.001-0.5: 0.480 (0.509)\n",
      "-8: 200-0.001-0.001-0.5: 0.414 (0.507)\n",
      "-8: 300-0.0005-0.001-0.5: 0.482 (0.496)\n",
      "-8: 200-0.01-0.005-0.5: 0.441 (0.487)\n",
      "-8: 200-0.005-0.005-0.5: 0.458 (0.487)\n",
      "-8: 200-0.001-0.005-0.5: 0.496 (0.515)\n",
      "-8: 200-0.05-0.01-0.5: 0.493 (0.502)\n",
      "-8: 200-0.01-0.01-0.5: 0.476 (0.511)\n",
      "-8: 200-0.005-0.01-0.5: 0.491 (0.518)\n",
      "-8: 200-0.001-0.01-0.5: 0.471 (0.487)\n",
      "-8: 200-0.01-0.001-0: 0.364 (0.432)\n",
      "-8: 200-0.01-0.001-0.25: 0.439 (0.491)\n",
      "-8: 200-0.01-0.001-0.75: 0.443 (0.476)\n",
      "-8: 200-0.001-0.005-0: 0.465 (0.478)\n",
      "-8: 200-0.001-0.005-0.25: 0.461 (0.474)\n",
      "-8: 200-0.001-0.005-0.75: 0.493 (0.511)\n",
      "-8: 100-0.001-0.005-0.5: 0.493 (0.504)\n",
      "-8: 500-0.001-0.005-0.5: 0.467 (0.531)\n",
      "-8: 1000-0.001-0.005-0.5: 0.485 (0.509)\n",
      "-9: 200-0.01-0.0005-0.5: 0.410 (0.458)\n",
      "-9: 200-0.005-0.0005-0.5: 0.395 (0.461)\n",
      "-9: 200-0.001-0.0005-0.5: 0.458 (0.485)\n",
      "-9: 300-0.0005-0.0005-0.5: 0.463 (0.482)\n",
      "-9: 200-0.01-0.001-0.5: 0.423 (0.478)\n",
      "-9: 200-0.005-0.001-0.5: 0.456 (0.474)\n",
      "-9: 200-0.001-0.001-0.5: 0.478 (0.482)\n",
      "-9: 300-0.0005-0.001-0.5: 0.430 (0.461)\n",
      "-9: 200-0.01-0.005-0.5: 0.445 (0.482)\n",
      "-9: 200-0.005-0.005-0.5: 0.447 (0.480)\n",
      "-9: 200-0.001-0.005-0.5: 0.463 (0.471)\n",
      "-9: 200-0.05-0.01-0.5: 0.417 (0.456)\n",
      "-9: 200-0.01-0.01-0.5: 0.491 (0.509)\n",
      "-9: 200-0.005-0.01-0.5: 0.487 (0.502)\n",
      "-9: 200-0.001-0.01-0.5: 0.434 (0.439)\n",
      "-9: 200-0.01-0.001-0: 0.397 (0.425)\n",
      "-9: 200-0.01-0.001-0.25: 0.443 (0.458)\n",
      "-9: 200-0.01-0.001-0.75: 0.450 (0.482)\n",
      "-9: 200-0.001-0.005-0: 0.445 (0.471)\n",
      "-9: 200-0.001-0.005-0.25: 0.454 (0.471)\n",
      "-9: 200-0.001-0.005-0.75: 0.439 (0.474)\n",
      "-9: 100-0.001-0.005-0.5: 0.428 (0.432)\n",
      "-9: 500-0.001-0.005-0.5: 0.471 (0.480)\n",
      "-9: 1000-0.001-0.005-0.5: 0.454 (0.469)\n",
      "-10: 200-0.01-0.0005-0.5: 0.458 (0.471)\n",
      "-10: 200-0.005-0.0005-0.5: 0.467 (0.485)\n",
      "-10: 200-0.001-0.0005-0.5: 0.465 (0.489)\n",
      "-10: 300-0.0005-0.0005-0.5: 0.485 (0.507)\n",
      "-10: 200-0.01-0.001-0.5: 0.458 (0.491)\n",
      "-10: 200-0.005-0.001-0.5: 0.458 (0.489)\n",
      "-10: 200-0.001-0.001-0.5: 0.371 (0.507)\n",
      "-10: 300-0.0005-0.001-0.5: 0.344 (0.496)\n",
      "-10: 200-0.01-0.005-0.5: 0.463 (0.500)\n",
      "-10: 200-0.005-0.005-0.5: 0.469 (0.491)\n",
      "-10: 200-0.001-0.005-0.5: 0.465 (0.496)\n",
      "-10: 200-0.05-0.01-0.5: 0.436 (0.471)\n",
      "-10: 200-0.01-0.01-0.5: 0.480 (0.493)\n",
      "-10: 200-0.005-0.01-0.5: 0.447 (0.493)\n",
      "-10: 200-0.001-0.01-0.5: 0.476 (0.493)\n",
      "-10: 200-0.01-0.001-0: 0.327 (0.445)\n",
      "-10: 200-0.01-0.001-0.25: 0.397 (0.456)\n",
      "-10: 200-0.01-0.001-0.75: 0.491 (0.531)\n",
      "-10: 200-0.001-0.005-0: 0.346 (0.493)\n",
      "-10: 200-0.001-0.005-0.25: 0.482 (0.509)\n",
      "-10: 200-0.001-0.005-0.75: 0.450 (0.463)\n",
      "-10: 100-0.001-0.005-0.5: 0.421 (0.436)\n",
      "-10: 500-0.001-0.005-0.5: 0.480 (0.498)\n",
      "-10: 1000-0.001-0.005-0.5: 0.463 (0.504)\n",
      "-11: 200-0.01-0.0005-0.5: 0.406 (0.452)\n",
      "-11: 200-0.005-0.0005-0.5: 0.454 (0.496)\n",
      "-11: 200-0.001-0.0005-0.5: 0.384 (0.496)\n",
      "-11: 300-0.0005-0.0005-0.5: 0.436 (0.504)\n",
      "-11: 200-0.01-0.001-0.5: 0.450 (0.474)\n",
      "-11: 200-0.005-0.001-0.5: 0.390 (0.489)\n",
      "-11: 200-0.001-0.001-0.5: 0.351 (0.491)\n",
      "-11: 300-0.0005-0.001-0.5: 0.441 (0.500)\n",
      "-11: 200-0.01-0.005-0.5: 0.461 (0.487)\n",
      "-11: 200-0.005-0.005-0.5: 0.456 (0.496)\n",
      "-11: 200-0.001-0.005-0.5: 0.489 (0.496)\n",
      "-11: 200-0.05-0.01-0.5: 0.410 (0.474)\n",
      "-11: 200-0.01-0.01-0.5: 0.502 (0.502)\n",
      "-11: 200-0.005-0.01-0.5: 0.465 (0.496)\n",
      "-11: 200-0.001-0.01-0.5: 0.502 (0.515)\n",
      "-11: 200-0.01-0.001-0: 0.327 (0.439)\n",
      "-11: 200-0.01-0.001-0.25: 0.465 (0.489)\n",
      "-11: 200-0.01-0.001-0.75: 0.458 (0.478)\n",
      "-11: 200-0.001-0.005-0: 0.428 (0.504)\n",
      "-11: 200-0.001-0.005-0.25: 0.307 (0.482)\n",
      "-11: 200-0.001-0.005-0.75: 0.485 (0.493)\n",
      "-11: 100-0.001-0.005-0.5: 0.485 (0.491)\n",
      "-11: 500-0.001-0.005-0.5: 0.487 (0.511)\n",
      "-11: 1000-0.001-0.005-0.5: 0.498 (0.515)\n",
      "-12: 200-0.01-0.0005-0.5: 0.452 (0.487)\n",
      "-12: 200-0.005-0.0005-0.5: 0.432 (0.489)\n",
      "-12: 200-0.001-0.0005-0.5: 0.412 (0.498)\n",
      "-12: 300-0.0005-0.0005-0.5: 0.441 (0.522)\n",
      "-12: 200-0.01-0.001-0.5: 0.496 (0.513)\n",
      "-12: 200-0.005-0.001-0.5: 0.454 (0.471)\n",
      "-12: 200-0.001-0.001-0.5: 0.500 (0.524)\n",
      "-12: 300-0.0005-0.001-0.5: 0.360 (0.454)\n",
      "-12: 200-0.01-0.005-0.5: 0.480 (0.496)\n",
      "-12: 200-0.005-0.005-0.5: 0.489 (0.507)\n",
      "-12: 200-0.001-0.005-0.5: 0.478 (0.518)\n",
      "-12: 200-0.05-0.01-0.5: 0.445 (0.474)\n",
      "-12: 200-0.01-0.01-0.5: 0.478 (0.496)\n",
      "-12: 200-0.005-0.01-0.5: 0.482 (0.493)\n",
      "-12: 200-0.001-0.01-0.5: 0.476 (0.487)\n",
      "-12: 200-0.01-0.001-0: 0.445 (0.452)\n",
      "-12: 200-0.01-0.001-0.25: 0.524 (0.524)\n",
      "-12: 200-0.01-0.001-0.75: 0.498 (0.513)\n",
      "-12: 200-0.001-0.005-0: 0.294 (0.485)\n",
      "-12: 200-0.001-0.005-0.25: 0.498 (0.507)\n",
      "-12: 200-0.001-0.005-0.75: 0.480 (0.498)\n",
      "-12: 100-0.001-0.005-0.5: 0.377 (0.458)\n",
      "-12: 500-0.001-0.005-0.5: 0.412 (0.489)\n",
      "-12: 1000-0.001-0.005-0.5: 0.463 (0.498)\n",
      "-13: 200-0.01-0.0005-0.5: 0.395 (0.441)\n",
      "-13: 200-0.005-0.0005-0.5: 0.368 (0.434)\n",
      "-13: 200-0.001-0.0005-0.5: 0.395 (0.425)\n",
      "-13: 300-0.0005-0.0005-0.5: 0.414 (0.441)\n",
      "-13: 200-0.01-0.001-0.5: 0.357 (0.423)\n",
      "-13: 200-0.005-0.001-0.5: 0.408 (0.471)\n",
      "-13: 200-0.001-0.001-0.5: 0.430 (0.436)\n",
      "-13: 300-0.0005-0.001-0.5: 0.452 (0.469)\n",
      "-13: 200-0.01-0.005-0.5: 0.439 (0.480)\n",
      "-13: 200-0.005-0.005-0.5: 0.404 (0.458)\n",
      "-13: 200-0.001-0.005-0.5: 0.461 (0.469)\n",
      "-13: 200-0.05-0.01-0.5: 0.360 (0.423)\n",
      "-13: 200-0.01-0.01-0.5: 0.417 (0.439)\n",
      "-13: 200-0.005-0.01-0.5: 0.421 (0.441)\n",
      "-13: 200-0.001-0.01-0.5: 0.452 (0.458)\n",
      "-13: 200-0.01-0.001-0: 0.388 (0.439)\n",
      "-13: 200-0.01-0.001-0.25: 0.399 (0.447)\n",
      "-13: 200-0.01-0.001-0.75: 0.452 (0.458)\n",
      "-13: 200-0.001-0.005-0: 0.434 (0.456)\n",
      "-13: 200-0.001-0.005-0.25: 0.436 (0.452)\n",
      "-13: 200-0.001-0.005-0.75: 0.439 (0.454)\n",
      "-13: 100-0.001-0.005-0.5: 0.371 (0.373)\n",
      "-13: 500-0.001-0.005-0.5: 0.434 (0.467)\n",
      "-13: 1000-0.001-0.005-0.5: 0.456 (0.471)\n",
      "-14: 200-0.01-0.0005-0.5: 0.450 (0.500)\n",
      "-14: 200-0.005-0.0005-0.5: 0.489 (0.502)\n",
      "-14: 200-0.001-0.0005-0.5: 0.496 (0.513)\n",
      "-14: 300-0.0005-0.0005-0.5: 0.478 (0.502)\n",
      "-14: 200-0.01-0.001-0.5: 0.476 (0.513)\n",
      "-14: 200-0.005-0.001-0.5: 0.482 (0.498)\n",
      "-14: 200-0.001-0.001-0.5: 0.511 (0.531)\n",
      "-14: 300-0.0005-0.001-0.5: 0.504 (0.511)\n",
      "-14: 200-0.01-0.005-0.5: 0.478 (0.491)\n",
      "-14: 200-0.005-0.005-0.5: 0.458 (0.487)\n",
      "-14: 200-0.001-0.005-0.5: 0.493 (0.515)\n",
      "-14: 200-0.05-0.01-0.5: 0.436 (0.456)\n",
      "-14: 200-0.01-0.01-0.5: 0.487 (0.498)\n",
      "-14: 200-0.005-0.01-0.5: 0.480 (0.489)\n",
      "-14: 200-0.001-0.01-0.5: 0.487 (0.500)\n",
      "-14: 200-0.01-0.001-0: 0.447 (0.500)\n",
      "-14: 200-0.01-0.001-0.25: 0.450 (0.469)\n",
      "-14: 200-0.01-0.001-0.75: 0.478 (0.507)\n",
      "-14: 200-0.001-0.005-0: 0.467 (0.496)\n",
      "-14: 200-0.001-0.005-0.25: 0.498 (0.504)\n",
      "-14: 200-0.001-0.005-0.75: 0.456 (0.474)\n",
      "-14: 100-0.001-0.005-0.5: 0.469 (0.469)\n",
      "-14: 500-0.001-0.005-0.5: 0.471 (0.496)\n",
      "-14: 1000-0.001-0.005-0.5: 0.491 (0.504)\n",
      "-15: 200-0.01-0.0005-0.5: 0.434 (0.489)\n",
      "-15: 200-0.005-0.0005-0.5: 0.428 (0.489)\n",
      "-15: 200-0.001-0.0005-0.5: 0.463 (0.502)\n",
      "-15: 300-0.0005-0.0005-0.5: 0.474 (0.513)\n",
      "-15: 200-0.01-0.001-0.5: 0.445 (0.513)\n",
      "-15: 200-0.005-0.001-0.5: 0.447 (0.496)\n",
      "-15: 200-0.001-0.001-0.5: 0.461 (0.504)\n",
      "-15: 300-0.0005-0.001-0.5: 0.487 (0.491)\n",
      "-15: 200-0.01-0.005-0.5: 0.474 (0.504)\n",
      "-15: 200-0.005-0.005-0.5: 0.476 (0.498)\n",
      "-15: 200-0.001-0.005-0.5: 0.463 (0.476)\n",
      "-15: 200-0.05-0.01-0.5: 0.379 (0.425)\n",
      "-15: 200-0.01-0.01-0.5: 0.480 (0.489)\n",
      "-15: 200-0.005-0.01-0.5: 0.467 (0.491)\n",
      "-15: 200-0.001-0.01-0.5: 0.474 (0.485)\n",
      "-15: 200-0.01-0.001-0: 0.357 (0.456)\n",
      "-15: 200-0.01-0.001-0.25: 0.443 (0.480)\n",
      "-15: 200-0.01-0.001-0.75: 0.496 (0.502)\n",
      "-15: 200-0.001-0.005-0: 0.480 (0.485)\n",
      "-15: 200-0.001-0.005-0.25: 0.489 (0.500)\n",
      "-15: 200-0.001-0.005-0.75: 0.476 (0.478)\n",
      "-15: 100-0.001-0.005-0.5: 0.463 (0.465)\n",
      "-15: 500-0.001-0.005-0.5: 0.467 (0.485)\n",
      "-15: 1000-0.001-0.005-0.5: 0.463 (0.482)\n",
      "-16: 200-0.01-0.0005-0.5: 0.410 (0.476)\n",
      "-16: 200-0.005-0.0005-0.5: 0.445 (0.500)\n",
      "-16: 200-0.001-0.0005-0.5: 0.467 (0.478)\n",
      "-16: 300-0.0005-0.0005-0.5: 0.436 (0.467)\n",
      "-16: 200-0.01-0.001-0.5: 0.412 (0.467)\n",
      "-16: 200-0.005-0.001-0.5: 0.447 (0.476)\n",
      "-16: 200-0.001-0.001-0.5: 0.471 (0.485)\n",
      "-16: 300-0.0005-0.001-0.5: 0.476 (0.487)\n",
      "-16: 200-0.01-0.005-0.5: 0.485 (0.498)\n",
      "-16: 200-0.005-0.005-0.5: 0.474 (0.489)\n",
      "-16: 200-0.001-0.005-0.5: 0.487 (0.498)\n",
      "-16: 200-0.05-0.01-0.5: 0.410 (0.458)\n",
      "-16: 200-0.01-0.01-0.5: 0.469 (0.489)\n",
      "-16: 200-0.005-0.01-0.5: 0.463 (0.500)\n",
      "-16: 200-0.001-0.01-0.5: 0.480 (0.491)\n",
      "-16: 200-0.01-0.001-0: 0.342 (0.447)\n",
      "-16: 200-0.01-0.001-0.25: 0.414 (0.480)\n",
      "-16: 200-0.01-0.001-0.75: 0.428 (0.467)\n",
      "-16: 200-0.001-0.005-0: 0.471 (0.489)\n",
      "-16: 200-0.001-0.005-0.25: 0.461 (0.476)\n",
      "-16: 200-0.001-0.005-0.75: 0.463 (0.471)\n",
      "-16: 100-0.001-0.005-0.5: 0.443 (0.450)\n",
      "-16: 500-0.001-0.005-0.5: 0.456 (0.485)\n",
      "-16: 1000-0.001-0.005-0.5: 0.465 (0.489)\n",
      "-17: 200-0.01-0.0005-0.5: 0.346 (0.443)\n",
      "-17: 200-0.005-0.0005-0.5: 0.375 (0.463)\n",
      "-17: 200-0.001-0.0005-0.5: 0.447 (0.454)\n",
      "-17: 300-0.0005-0.0005-0.5: 0.439 (0.474)\n",
      "-17: 200-0.01-0.001-0.5: 0.465 (0.498)\n",
      "-17: 200-0.005-0.001-0.5: 0.439 (0.485)\n",
      "-17: 200-0.001-0.001-0.5: 0.375 (0.465)\n",
      "-17: 300-0.0005-0.001-0.5: 0.336 (0.428)\n",
      "-17: 200-0.01-0.005-0.5: 0.461 (0.491)\n",
      "-17: 200-0.005-0.005-0.5: 0.450 (0.482)\n",
      "-17: 200-0.001-0.005-0.5: 0.463 (0.485)\n",
      "-17: 200-0.05-0.01-0.5: 0.421 (0.454)\n",
      "-17: 200-0.01-0.01-0.5: 0.469 (0.496)\n",
      "-17: 200-0.005-0.01-0.5: 0.458 (0.491)\n",
      "-17: 200-0.001-0.01-0.5: 0.469 (0.469)\n",
      "-17: 200-0.01-0.001-0: 0.454 (0.471)\n",
      "-17: 200-0.01-0.001-0.25: 0.375 (0.452)\n",
      "-17: 200-0.01-0.001-0.75: 0.432 (0.471)\n",
      "-17: 200-0.001-0.005-0: 0.362 (0.471)\n",
      "-17: 200-0.001-0.005-0.25: 0.456 (0.476)\n",
      "-17: 200-0.001-0.005-0.75: 0.456 (0.461)\n",
      "-17: 100-0.001-0.005-0.5: 0.366 (0.450)\n",
      "-17: 500-0.001-0.005-0.5: 0.445 (0.474)\n",
      "-17: 1000-0.001-0.005-0.5: 0.360 (0.430)\n",
      "-18: 200-0.01-0.0005-0.5: 0.478 (0.482)\n",
      "-18: 200-0.005-0.0005-0.5: 0.443 (0.458)\n",
      "-18: 200-0.001-0.0005-0.5: 0.480 (0.507)\n",
      "-18: 300-0.0005-0.0005-0.5: 0.493 (0.518)\n",
      "-18: 200-0.01-0.001-0.5: 0.463 (0.493)\n",
      "-18: 200-0.005-0.001-0.5: 0.441 (0.467)\n",
      "-18: 200-0.001-0.001-0.5: 0.513 (0.518)\n",
      "-18: 300-0.0005-0.001-0.5: 0.465 (0.493)\n",
      "-18: 200-0.01-0.005-0.5: 0.461 (0.493)\n",
      "-18: 200-0.005-0.005-0.5: 0.480 (0.489)\n",
      "-18: 200-0.001-0.005-0.5: 0.461 (0.474)\n",
      "-18: 200-0.05-0.01-0.5: 0.417 (0.456)\n",
      "-18: 200-0.01-0.01-0.5: 0.487 (0.513)\n",
      "-18: 200-0.005-0.01-0.5: 0.482 (0.515)\n",
      "-18: 200-0.001-0.01-0.5: 0.498 (0.504)\n",
      "-18: 200-0.01-0.001-0: 0.447 (0.491)\n",
      "-18: 200-0.01-0.001-0.25: 0.456 (0.487)\n",
      "-18: 200-0.01-0.001-0.75: 0.454 (0.487)\n",
      "-18: 200-0.001-0.005-0: 0.480 (0.504)\n",
      "-18: 200-0.001-0.005-0.25: 0.487 (0.507)\n",
      "-18: 200-0.001-0.005-0.75: 0.465 (0.478)\n",
      "-18: 100-0.001-0.005-0.5: 0.478 (0.485)\n",
      "-18: 500-0.001-0.005-0.5: 0.458 (0.507)\n",
      "-18: 1000-0.001-0.005-0.5: 0.482 (0.502)\n",
      "-19: 200-0.01-0.0005-0.5: 0.445 (0.467)\n",
      "-19: 200-0.005-0.0005-0.5: 0.436 (0.482)\n",
      "-19: 200-0.001-0.0005-0.5: 0.447 (0.454)\n",
      "-19: 300-0.0005-0.0005-0.5: 0.456 (0.471)\n",
      "-19: 200-0.01-0.001-0.5: 0.447 (0.493)\n",
      "-19: 200-0.005-0.001-0.5: 0.432 (0.480)\n",
      "-19: 200-0.001-0.001-0.5: 0.493 (0.496)\n",
      "-19: 300-0.0005-0.001-0.5: 0.454 (0.502)\n",
      "-19: 200-0.01-0.005-0.5: 0.465 (0.487)\n",
      "-19: 200-0.005-0.005-0.5: 0.469 (0.482)\n",
      "-19: 200-0.001-0.005-0.5: 0.463 (0.489)\n",
      "-19: 200-0.05-0.01-0.5: 0.428 (0.456)\n",
      "-19: 200-0.01-0.01-0.5: 0.489 (0.496)\n",
      "-19: 200-0.005-0.01-0.5: 0.461 (0.478)\n",
      "-19: 200-0.001-0.01-0.5: 0.474 (0.491)\n",
      "-19: 200-0.01-0.001-0: 0.382 (0.441)\n",
      "-19: 200-0.01-0.001-0.25: 0.441 (0.443)\n",
      "-19: 200-0.01-0.001-0.75: 0.419 (0.465)\n",
      "-19: 200-0.001-0.005-0: 0.450 (0.474)\n",
      "-19: 200-0.001-0.005-0.25: 0.441 (0.461)\n",
      "-19: 200-0.001-0.005-0.75: 0.443 (0.445)\n",
      "-19: 100-0.001-0.005-0.5: 0.377 (0.397)\n",
      "-19: 500-0.001-0.005-0.5: 0.471 (0.491)\n",
      "-19: 1000-0.001-0.005-0.5: 0.443 (0.467)\n",
      "-20: 200-0.01-0.0005-0.5: 0.491 (0.500)\n",
      "-20: 200-0.005-0.0005-0.5: 0.491 (0.513)\n",
      "-20: 200-0.001-0.0005-0.5: 0.474 (0.502)\n",
      "-20: 300-0.0005-0.0005-0.5: 0.485 (0.502)\n",
      "-20: 200-0.01-0.001-0.5: 0.467 (0.502)\n",
      "-20: 200-0.005-0.001-0.5: 0.471 (0.507)\n",
      "-20: 200-0.001-0.001-0.5: 0.496 (0.509)\n",
      "-20: 300-0.0005-0.001-0.5: 0.461 (0.502)\n",
      "-20: 200-0.01-0.005-0.5: 0.461 (0.491)\n",
      "-20: 200-0.005-0.005-0.5: 0.463 (0.491)\n",
      "-20: 200-0.001-0.005-0.5: 0.474 (0.491)\n",
      "-20: 200-0.05-0.01-0.5: 0.432 (0.480)\n",
      "-20: 200-0.01-0.01-0.5: 0.487 (0.487)\n",
      "-20: 200-0.005-0.01-0.5: 0.463 (0.482)\n",
      "-20: 200-0.001-0.01-0.5: 0.487 (0.502)\n",
      "-20: 200-0.01-0.001-0: 0.351 (0.489)\n",
      "-20: 200-0.01-0.001-0.25: 0.441 (0.500)\n",
      "-20: 200-0.01-0.001-0.75: 0.458 (0.502)\n",
      "-20: 200-0.001-0.005-0: 0.478 (0.491)\n",
      "-20: 200-0.001-0.005-0.25: 0.511 (0.515)\n",
      "-20: 200-0.001-0.005-0.75: 0.485 (0.496)\n",
      "-20: 100-0.001-0.005-0.5: 0.357 (0.452)\n",
      "-20: 500-0.001-0.005-0.5: 0.454 (0.498)\n",
      "-20: 1000-0.001-0.005-0.5: 0.471 (0.502)\n",
      "----- 25.93 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-4, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-4, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 5e-4, 'drop': .5},\n",
    "        {'epochs': 300, 'lr': .0005, 'wd': 5e-4, 'drop': .5},\n",
    "        \n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 300, 'lr': .0005, 'wd': 1e-3, 'drop': .5},\n",
    "\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 5e-3, 'drop': .5},\n",
    "\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-2, 'drop': .5},\n",
    "\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-2, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-2, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-2, 'drop': .75},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-2, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-2, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-2, 'drop': .75},\n",
    "\n",
    "        {'epochs': 100, 'lr': .001, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 500, 'lr': .001, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 1000, 'lr': .001, 'wd': 1e-2, 'drop': .5},\n",
    "        ]\n",
    "\n",
    "best_accs1 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs1 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=exp['drop'], init_h0=h0, diff_layer=GFGCN_noh_Layer)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, exp['epochs'], exp['lr'], exp['wd'])\n",
    "\n",
    "        best_accs1[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs1[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}: {best_val_accs1[j,i]:.3f} ({best_accs1[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}' for exp in EXPS]\n",
    "table_over1 = summary_table(best_accs1, index_name)\n",
    "table1 = summary_table(best_val_accs1, index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.0005-0.5</th>\n",
       "      <td>0.434649</td>\n",
       "      <td>0.439693</td>\n",
       "      <td>0.038189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0005-0.5</th>\n",
       "      <td>0.432785</td>\n",
       "      <td>0.434211</td>\n",
       "      <td>0.034648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.0005-0.5</th>\n",
       "      <td>0.454167</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.028279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300-0.0005-0.0005-0.5</th>\n",
       "      <td>0.458443</td>\n",
       "      <td>0.459430</td>\n",
       "      <td>0.026493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.5</th>\n",
       "      <td>0.441009</td>\n",
       "      <td>0.448465</td>\n",
       "      <td>0.032371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.5</th>\n",
       "      <td>0.442544</td>\n",
       "      <td>0.444079</td>\n",
       "      <td>0.024846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.001-0.5</th>\n",
       "      <td>0.449232</td>\n",
       "      <td>0.470395</td>\n",
       "      <td>0.056079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300-0.0005-0.001-0.5</th>\n",
       "      <td>0.441118</td>\n",
       "      <td>0.457237</td>\n",
       "      <td>0.053973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.5</th>\n",
       "      <td>0.461623</td>\n",
       "      <td>0.463816</td>\n",
       "      <td>0.016182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.005-0.5</th>\n",
       "      <td>0.463925</td>\n",
       "      <td>0.467105</td>\n",
       "      <td>0.018769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.5</th>\n",
       "      <td>0.470724</td>\n",
       "      <td>0.469298</td>\n",
       "      <td>0.017313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.5</th>\n",
       "      <td>0.421711</td>\n",
       "      <td>0.419956</td>\n",
       "      <td>0.030490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.01-0.5</th>\n",
       "      <td>0.472697</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.022110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.01-0.5</th>\n",
       "      <td>0.463706</td>\n",
       "      <td>0.463816</td>\n",
       "      <td>0.019608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.01-0.5</th>\n",
       "      <td>0.472807</td>\n",
       "      <td>0.474781</td>\n",
       "      <td>0.019236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0</th>\n",
       "      <td>0.390022</td>\n",
       "      <td>0.391447</td>\n",
       "      <td>0.042874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.25</th>\n",
       "      <td>0.434868</td>\n",
       "      <td>0.439693</td>\n",
       "      <td>0.033063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.75</th>\n",
       "      <td>0.456469</td>\n",
       "      <td>0.453947</td>\n",
       "      <td>0.024096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0</th>\n",
       "      <td>0.431579</td>\n",
       "      <td>0.450658</td>\n",
       "      <td>0.057214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.25</th>\n",
       "      <td>0.455811</td>\n",
       "      <td>0.467105</td>\n",
       "      <td>0.055588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.75</th>\n",
       "      <td>0.462610</td>\n",
       "      <td>0.462719</td>\n",
       "      <td>0.020129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100-0.001-0.005-0.5</th>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.426535</td>\n",
       "      <td>0.043907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.005-0.5</th>\n",
       "      <td>0.456689</td>\n",
       "      <td>0.467105</td>\n",
       "      <td>0.030211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000-0.001-0.005-0.5</th>\n",
       "      <td>0.460526</td>\n",
       "      <td>0.463816</td>\n",
       "      <td>0.039162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean accs       med       std\n",
       "200-0.01-0.0005-0.5     0.434649  0.439693  0.038189\n",
       "200-0.005-0.0005-0.5    0.432785  0.434211  0.034648\n",
       "200-0.001-0.0005-0.5    0.454167  0.458333  0.028279\n",
       "300-0.0005-0.0005-0.5   0.458443  0.459430  0.026493\n",
       "200-0.01-0.001-0.5      0.441009  0.448465  0.032371\n",
       "200-0.005-0.001-0.5     0.442544  0.444079  0.024846\n",
       "200-0.001-0.001-0.5     0.449232  0.470395  0.056079\n",
       "300-0.0005-0.001-0.5    0.441118  0.457237  0.053973\n",
       "200-0.01-0.005-0.5      0.461623  0.463816  0.016182\n",
       "200-0.005-0.005-0.5     0.463925  0.467105  0.018769\n",
       "200-0.001-0.005-0.5     0.470724  0.469298  0.017313\n",
       "200-0.05-0.01-0.5       0.421711  0.419956  0.030490\n",
       "200-0.01-0.01-0.5       0.472697  0.479167  0.022110\n",
       "200-0.005-0.01-0.5      0.463706  0.463816  0.019608\n",
       "200-0.001-0.01-0.5      0.472807  0.474781  0.019236\n",
       "200-0.01-0.001-0        0.390022  0.391447  0.042874\n",
       "200-0.01-0.001-0.25     0.434868  0.439693  0.033063\n",
       "200-0.01-0.001-0.75     0.456469  0.453947  0.024096\n",
       "200-0.001-0.005-0       0.431579  0.450658  0.057214\n",
       "200-0.001-0.005-0.25    0.455811  0.467105  0.055588\n",
       "200-0.001-0.005-0.75    0.462610  0.462719  0.020129\n",
       "100-0.001-0.005-0.5     0.425000  0.426535  0.043907\n",
       "500-0.001-0.005-0.5     0.456689  0.467105  0.030211\n",
       "1000-0.001-0.005-0.5    0.460526  0.463816  0.039162"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.0005-0.5</th>\n",
       "      <td>0.473355</td>\n",
       "      <td>0.474781</td>\n",
       "      <td>0.023652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0005-0.5</th>\n",
       "      <td>0.482018</td>\n",
       "      <td>0.486842</td>\n",
       "      <td>0.019226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.0005-0.5</th>\n",
       "      <td>0.485746</td>\n",
       "      <td>0.490132</td>\n",
       "      <td>0.023675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300-0.0005-0.0005-0.5</th>\n",
       "      <td>0.491009</td>\n",
       "      <td>0.502193</td>\n",
       "      <td>0.025677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.5</th>\n",
       "      <td>0.483662</td>\n",
       "      <td>0.492325</td>\n",
       "      <td>0.025267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.5</th>\n",
       "      <td>0.483662</td>\n",
       "      <td>0.485746</td>\n",
       "      <td>0.013545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.001-0.5</th>\n",
       "      <td>0.494518</td>\n",
       "      <td>0.493421</td>\n",
       "      <td>0.024787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300-0.0005-0.001-0.5</th>\n",
       "      <td>0.484430</td>\n",
       "      <td>0.489035</td>\n",
       "      <td>0.023537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.5</th>\n",
       "      <td>0.493531</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.007830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.005-0.5</th>\n",
       "      <td>0.486952</td>\n",
       "      <td>0.489035</td>\n",
       "      <td>0.010297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.5</th>\n",
       "      <td>0.488706</td>\n",
       "      <td>0.490132</td>\n",
       "      <td>0.018728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.5</th>\n",
       "      <td>0.459978</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.019366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.01-0.5</th>\n",
       "      <td>0.491447</td>\n",
       "      <td>0.495614</td>\n",
       "      <td>0.017542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.01-0.5</th>\n",
       "      <td>0.489145</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.018315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.01-0.5</th>\n",
       "      <td>0.483772</td>\n",
       "      <td>0.486842</td>\n",
       "      <td>0.020080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0</th>\n",
       "      <td>0.458114</td>\n",
       "      <td>0.453947</td>\n",
       "      <td>0.021033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.25</th>\n",
       "      <td>0.477193</td>\n",
       "      <td>0.483553</td>\n",
       "      <td>0.021187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.75</th>\n",
       "      <td>0.486952</td>\n",
       "      <td>0.484649</td>\n",
       "      <td>0.020331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0</th>\n",
       "      <td>0.481908</td>\n",
       "      <td>0.484649</td>\n",
       "      <td>0.014176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.25</th>\n",
       "      <td>0.486075</td>\n",
       "      <td>0.484649</td>\n",
       "      <td>0.021966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.75</th>\n",
       "      <td>0.477083</td>\n",
       "      <td>0.475877</td>\n",
       "      <td>0.021244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100-0.001-0.005-0.5</th>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.450658</td>\n",
       "      <td>0.032430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.005-0.5</th>\n",
       "      <td>0.491338</td>\n",
       "      <td>0.490132</td>\n",
       "      <td>0.017619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000-0.001-0.005-0.5</th>\n",
       "      <td>0.490461</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.022075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean accs       med       std\n",
       "200-0.01-0.0005-0.5     0.473355  0.474781  0.023652\n",
       "200-0.005-0.0005-0.5    0.482018  0.486842  0.019226\n",
       "200-0.001-0.0005-0.5    0.485746  0.490132  0.023675\n",
       "300-0.0005-0.0005-0.5   0.491009  0.502193  0.025677\n",
       "200-0.01-0.001-0.5      0.483662  0.492325  0.025267\n",
       "200-0.005-0.001-0.5     0.483662  0.485746  0.013545\n",
       "200-0.001-0.001-0.5     0.494518  0.493421  0.024787\n",
       "300-0.0005-0.001-0.5    0.484430  0.489035  0.023537\n",
       "200-0.01-0.005-0.5      0.493531  0.491228  0.007830\n",
       "200-0.005-0.005-0.5     0.486952  0.489035  0.010297\n",
       "200-0.001-0.005-0.5     0.488706  0.490132  0.018728\n",
       "200-0.05-0.01-0.5       0.459978  0.458333  0.019366\n",
       "200-0.01-0.01-0.5       0.491447  0.495614  0.017542\n",
       "200-0.005-0.01-0.5      0.489145  0.491228  0.018315\n",
       "200-0.001-0.01-0.5      0.483772  0.486842  0.020080\n",
       "200-0.01-0.001-0        0.458114  0.453947  0.021033\n",
       "200-0.01-0.001-0.25     0.477193  0.483553  0.021187\n",
       "200-0.01-0.001-0.75     0.486952  0.484649  0.020331\n",
       "200-0.001-0.005-0       0.481908  0.484649  0.014176\n",
       "200-0.001-0.005-0.25    0.486075  0.484649  0.021966\n",
       "200-0.001-0.005-0.75    0.477083  0.475877  0.021244\n",
       "100-0.001-0.005-0.5     0.447917  0.450658  0.032430\n",
       "500-0.001-0.005-0.5     0.491338  0.490132  0.017619\n",
       "1000-0.001-0.005-0.5    0.490461  0.500000  0.022075"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_over1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design - Bias True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 2-2-16: 0.248 (0.303)\n",
      "-1: 2-3-16: 0.283 (0.320)\n",
      "-1: 2-4-16: 0.272 (0.322)\n",
      "-1: 3-2-16: 0.410 (0.423)\n",
      "-1: 3-3-16: 0.202 (0.261)\n",
      "-1: 2-2-32: 0.318 (0.487)\n",
      "-1: 2-3-32: 0.279 (0.292)\n",
      "-1: 2-4-32: 0.285 (0.298)\n",
      "-1: 3-2-32: 0.292 (0.327)\n",
      "-1: 3-3-32: 0.248 (0.287)\n",
      "-1: 2-2-64: 0.469 (0.518)\n",
      "-1: 2-3-64: 0.340 (0.349)\n",
      "-1: 3-2-64: 0.274 (0.294)\n",
      "-1: 3-3-64: 0.217 (0.254)\n",
      "-1: 2-2-128: 0.489 (0.522)\n",
      "-1: 2-3-128: 0.300 (0.322)\n",
      "-1: 3-2-128: 0.294 (0.386)\n",
      "-1: 3-3-128: 0.263 (0.268)\n",
      "-1: 2-2-256: 0.471 (0.498)\n",
      "-2: 2-2-16: 0.270 (0.371)\n",
      "-2: 2-3-16: 0.204 (0.272)\n",
      "-2: 2-4-16: 0.268 (0.285)\n",
      "-2: 3-2-16: 0.265 (0.292)\n",
      "-2: 3-3-16: 0.228 (0.298)\n",
      "-2: 2-2-32: 0.498 (0.513)\n",
      "-2: 2-3-32: 0.265 (0.333)\n",
      "-2: 2-4-32: 0.217 (0.265)\n",
      "-2: 3-2-32: 0.353 (0.371)\n",
      "-2: 3-3-32: 0.200 (0.333)\n",
      "-2: 2-2-64: 0.474 (0.502)\n",
      "-2: 2-3-64: 0.287 (0.325)\n",
      "-2: 3-2-64: 0.467 (0.480)\n",
      "-2: 3-3-64: 0.246 (0.279)\n",
      "-2: 2-2-128: 0.480 (0.502)\n",
      "-2: 2-3-128: 0.252 (0.311)\n",
      "-2: 3-2-128: 0.265 (0.344)\n",
      "-2: 3-3-128: 0.228 (0.344)\n",
      "-2: 2-2-256: 0.493 (0.509)\n",
      "-3: 2-2-16: 0.404 (0.436)\n",
      "-3: 2-3-16: 0.268 (0.311)\n",
      "-3: 2-4-16: 0.206 (0.296)\n",
      "-3: 3-2-16: 0.285 (0.311)\n",
      "-3: 3-3-16: 0.241 (0.248)\n",
      "-3: 2-2-32: 0.434 (0.458)\n",
      "-3: 2-3-32: 0.268 (0.287)\n",
      "-3: 2-4-32: 0.184 (0.268)\n",
      "-3: 3-2-32: 0.320 (0.349)\n",
      "-3: 3-3-32: 0.219 (0.259)\n",
      "-3: 2-2-64: 0.445 (0.456)\n",
      "-3: 2-3-64: 0.289 (0.322)\n",
      "-3: 3-2-64: 0.371 (0.388)\n",
      "-3: 3-3-64: 0.226 (0.303)\n",
      "-3: 2-2-128: 0.441 (0.458)\n",
      "-3: 2-3-128: 0.294 (0.331)\n",
      "-3: 3-2-128: 0.342 (0.382)\n",
      "-3: 3-3-128: 0.221 (0.318)\n",
      "-3: 2-2-256: 0.447 (0.471)\n",
      "-4: 2-2-16: 0.432 (0.456)\n",
      "-4: 2-3-16: 0.276 (0.289)\n",
      "-4: 2-4-16: 0.298 (0.305)\n",
      "-4: 3-2-16: 0.298 (0.298)\n",
      "-4: 3-3-16: 0.164 (0.281)\n",
      "-4: 2-2-32: 0.487 (0.502)\n",
      "-4: 2-3-32: 0.294 (0.320)\n",
      "-4: 2-4-32: 0.270 (0.292)\n",
      "-4: 3-2-32: 0.399 (0.404)\n",
      "-4: 3-3-32: 0.243 (0.281)\n",
      "-4: 2-2-64: 0.496 (0.509)\n",
      "-4: 2-3-64: 0.294 (0.327)\n",
      "-4: 3-2-64: 0.428 (0.463)\n",
      "-4: 3-3-64: 0.283 (0.300)\n",
      "-4: 2-2-128: 0.511 (0.511)\n",
      "-4: 2-3-128: 0.393 (0.428)\n",
      "-4: 3-2-128: 0.436 (0.465)\n",
      "-4: 3-3-128: 0.248 (0.294)\n",
      "-4: 2-2-256: 0.509 (0.518)\n",
      "-5: 2-2-16: 0.461 (0.471)\n",
      "-5: 2-3-16: 0.261 (0.272)\n",
      "-5: 2-4-16: 0.193 (0.292)\n",
      "-5: 3-2-16: 0.261 (0.316)\n",
      "-5: 3-3-16: 0.191 (0.287)\n",
      "-5: 2-2-32: 0.476 (0.487)\n",
      "-5: 2-3-32: 0.279 (0.287)\n",
      "-5: 2-4-32: 0.232 (0.254)\n",
      "-5: 3-2-32: 0.353 (0.366)\n",
      "-5: 3-3-32: 0.195 (0.261)\n",
      "-5: 2-2-64: 0.480 (0.502)\n",
      "-5: 2-3-64: 0.287 (0.311)\n",
      "-5: 3-2-64: 0.474 (0.478)\n",
      "-5: 3-3-64: 0.224 (0.272)\n",
      "-5: 2-2-128: 0.502 (0.502)\n",
      "-5: 2-3-128: 0.204 (0.320)\n",
      "-5: 3-2-128: 0.421 (0.447)\n",
      "-5: 3-3-128: 0.213 (0.294)\n",
      "-5: 2-2-256: 0.476 (0.493)\n",
      "-6: 2-2-16: 0.432 (0.447)\n",
      "-6: 2-3-16: 0.292 (0.298)\n",
      "-6: 2-4-16: 0.261 (0.283)\n",
      "-6: 3-2-16: 0.349 (0.353)\n",
      "-6: 3-3-16: 0.182 (0.289)\n",
      "-6: 2-2-32: 0.465 (0.476)\n",
      "-6: 2-3-32: 0.261 (0.292)\n",
      "-6: 2-4-32: 0.171 (0.257)\n",
      "-6: 3-2-32: 0.283 (0.362)\n",
      "-6: 3-3-32: 0.215 (0.274)\n",
      "-6: 2-2-64: 0.480 (0.485)\n",
      "-6: 2-3-64: 0.279 (0.342)\n",
      "-6: 3-2-64: 0.386 (0.404)\n",
      "-6: 3-3-64: 0.250 (0.307)\n",
      "-6: 2-2-128: 0.454 (0.498)\n",
      "-6: 2-3-128: 0.377 (0.386)\n",
      "-6: 3-2-128: 0.436 (0.447)\n",
      "-6: 3-3-128: 0.268 (0.285)\n",
      "-6: 2-2-256: 0.469 (0.487)\n",
      "-7: 2-2-16: 0.228 (0.432)\n",
      "-7: 2-3-16: 0.246 (0.296)\n",
      "-7: 2-4-16: 0.219 (0.289)\n",
      "-7: 3-2-16: 0.237 (0.287)\n",
      "-7: 3-3-16: 0.200 (0.329)\n",
      "-7: 2-2-32: 0.349 (0.458)\n",
      "-7: 2-3-32: 0.232 (0.296)\n",
      "-7: 2-4-32: 0.228 (0.300)\n",
      "-7: 3-2-32: 0.283 (0.386)\n",
      "-7: 3-3-32: 0.235 (0.272)\n",
      "-7: 2-2-64: 0.441 (0.474)\n",
      "-7: 2-3-64: 0.235 (0.292)\n",
      "-7: 3-2-64: 0.241 (0.344)\n",
      "-7: 3-3-64: 0.230 (0.309)\n",
      "-7: 2-2-128: 0.441 (0.474)\n",
      "-7: 2-3-128: 0.208 (0.298)\n",
      "-7: 3-2-128: 0.430 (0.447)\n",
      "-7: 3-3-128: 0.221 (0.263)\n",
      "-7: 2-2-256: 0.454 (0.485)\n",
      "-8: 2-2-16: 0.478 (0.487)\n",
      "-8: 2-3-16: 0.250 (0.281)\n",
      "-8: 2-4-16: 0.268 (0.300)\n",
      "-8: 3-2-16: 0.364 (0.368)\n",
      "-8: 3-3-16: 0.226 (0.289)\n",
      "-8: 2-2-32: 0.465 (0.493)\n",
      "-8: 2-3-32: 0.254 (0.263)\n",
      "-8: 2-4-32: 0.202 (0.285)\n",
      "-8: 3-2-32: 0.434 (0.445)\n",
      "-8: 3-3-32: 0.197 (0.292)\n",
      "-8: 2-2-64: 0.474 (0.502)\n",
      "-8: 2-3-64: 0.252 (0.274)\n",
      "-8: 3-2-64: 0.425 (0.469)\n",
      "-8: 3-3-64: 0.237 (0.318)\n",
      "-8: 2-2-128: 0.487 (0.502)\n",
      "-8: 2-3-128: 0.211 (0.314)\n",
      "-8: 3-2-128: 0.441 (0.487)\n",
      "-8: 3-3-128: 0.204 (0.318)\n",
      "-8: 2-2-256: 0.463 (0.511)\n",
      "-9: 2-2-16: 0.281 (0.366)\n",
      "-9: 2-3-16: 0.270 (0.292)\n",
      "-9: 2-4-16: 0.259 (0.265)\n",
      "-9: 3-2-16: 0.281 (0.303)\n",
      "-9: 3-3-16: 0.230 (0.259)\n",
      "-9: 2-2-32: 0.476 (0.496)\n",
      "-9: 2-3-32: 0.276 (0.285)\n",
      "-9: 2-4-32: 0.272 (0.279)\n",
      "-9: 3-2-32: 0.283 (0.303)\n",
      "-9: 3-3-32: 0.237 (0.276)\n",
      "-9: 2-2-64: 0.461 (0.469)\n",
      "-9: 2-3-64: 0.204 (0.305)\n",
      "-9: 3-2-64: 0.276 (0.300)\n",
      "-9: 3-3-64: 0.254 (0.272)\n",
      "-9: 2-2-128: 0.471 (0.502)\n",
      "-9: 2-3-128: 0.377 (0.388)\n",
      "-9: 3-2-128: 0.274 (0.311)\n",
      "-9: 3-3-128: 0.261 (0.279)\n",
      "-9: 2-2-256: 0.469 (0.502)\n",
      "-10: 2-2-16: 0.289 (0.428)\n",
      "-10: 2-3-16: 0.300 (0.322)\n",
      "-10: 2-4-16: 0.217 (0.320)\n",
      "-10: 3-2-16: 0.294 (0.331)\n",
      "-10: 3-3-16: 0.219 (0.283)\n",
      "-10: 2-2-32: 0.386 (0.461)\n",
      "-10: 2-3-32: 0.235 (0.325)\n",
      "-10: 2-4-32: 0.235 (0.316)\n",
      "-10: 3-2-32: 0.241 (0.331)\n",
      "-10: 3-3-32: 0.254 (0.292)\n",
      "-10: 2-2-64: 0.476 (0.507)\n",
      "-10: 2-3-64: 0.279 (0.320)\n",
      "-10: 3-2-64: 0.333 (0.388)\n",
      "-10: 3-3-64: 0.261 (0.281)\n",
      "-10: 2-2-128: 0.489 (0.500)\n",
      "-10: 2-3-128: 0.355 (0.406)\n",
      "-10: 3-2-128: 0.325 (0.417)\n",
      "-10: 3-3-128: 0.246 (0.270)\n",
      "-10: 2-2-256: 0.467 (0.498)\n",
      "-11: 2-2-16: 0.357 (0.465)\n",
      "-11: 2-3-16: 0.283 (0.316)\n",
      "-11: 2-4-16: 0.265 (0.287)\n",
      "-11: 3-2-16: 0.320 (0.382)\n",
      "-11: 3-3-16: 0.259 (0.272)\n",
      "-11: 2-2-32: 0.338 (0.474)\n",
      "-11: 2-3-32: 0.287 (0.307)\n",
      "-11: 2-4-32: 0.270 (0.283)\n",
      "-11: 3-2-32: 0.285 (0.340)\n",
      "-11: 3-3-32: 0.226 (0.296)\n",
      "-11: 2-2-64: 0.349 (0.515)\n",
      "-11: 2-3-64: 0.279 (0.331)\n",
      "-11: 3-2-64: 0.272 (0.292)\n",
      "-11: 3-3-64: 0.224 (0.252)\n",
      "-11: 2-2-128: 0.469 (0.511)\n",
      "-11: 2-3-128: 0.322 (0.362)\n",
      "-11: 3-2-128: 0.270 (0.311)\n",
      "-11: 3-3-128: 0.250 (0.289)\n",
      "-11: 2-2-256: 0.485 (0.515)\n",
      "-12: 2-2-16: 0.265 (0.349)\n",
      "-12: 2-3-16: 0.213 (0.248)\n",
      "-12: 2-4-16: 0.213 (0.230)\n",
      "-12: 3-2-16: 0.228 (0.272)\n",
      "-12: 3-3-16: 0.320 (0.338)\n",
      "-12: 2-2-32: 0.406 (0.489)\n",
      "-12: 2-3-32: 0.206 (0.285)\n",
      "-12: 2-4-32: 0.204 (0.296)\n",
      "-12: 3-2-32: 0.331 (0.351)\n",
      "-12: 3-3-32: 0.248 (0.276)\n",
      "-12: 2-2-64: 0.493 (0.507)\n",
      "-12: 2-3-64: 0.289 (0.355)\n",
      "-12: 3-2-64: 0.465 (0.480)\n",
      "-12: 3-3-64: 0.200 (0.289)\n",
      "-12: 2-2-128: 0.491 (0.511)\n",
      "-12: 2-3-128: 0.232 (0.254)\n",
      "-12: 3-2-128: 0.263 (0.342)\n",
      "-12: 3-3-128: 0.246 (0.322)\n",
      "-12: 2-2-256: 0.485 (0.509)\n",
      "-13: 2-2-16: 0.399 (0.423)\n",
      "-13: 2-3-16: 0.246 (0.270)\n",
      "-13: 2-4-16: 0.221 (0.289)\n",
      "-13: 3-2-16: 0.294 (0.329)\n",
      "-13: 3-3-16: 0.217 (0.261)\n",
      "-13: 2-2-32: 0.447 (0.463)\n",
      "-13: 2-3-32: 0.211 (0.265)\n",
      "-13: 2-4-32: 0.283 (0.285)\n",
      "-13: 3-2-32: 0.289 (0.316)\n",
      "-13: 3-3-32: 0.169 (0.246)\n",
      "-13: 2-2-64: 0.443 (0.467)\n",
      "-13: 2-3-64: 0.281 (0.349)\n",
      "-13: 3-2-64: 0.351 (0.382)\n",
      "-13: 3-3-64: 0.241 (0.272)\n",
      "-13: 2-2-128: 0.434 (0.465)\n",
      "-13: 2-3-128: 0.338 (0.357)\n",
      "-13: 3-2-128: 0.412 (0.443)\n",
      "-13: 3-3-128: 0.224 (0.265)\n",
      "-13: 2-2-256: 0.441 (0.456)\n",
      "-14: 2-2-16: 0.476 (0.496)\n",
      "-14: 2-3-16: 0.300 (0.316)\n",
      "-14: 2-4-16: 0.285 (0.318)\n",
      "-14: 3-2-16: 0.292 (0.300)\n",
      "-14: 3-3-16: 0.226 (0.300)\n",
      "-14: 2-2-32: 0.491 (0.498)\n",
      "-14: 2-3-32: 0.289 (0.316)\n",
      "-14: 2-4-32: 0.300 (0.307)\n",
      "-14: 3-2-32: 0.434 (0.436)\n",
      "-14: 3-3-32: 0.265 (0.309)\n",
      "-14: 2-2-64: 0.493 (0.504)\n",
      "-14: 2-3-64: 0.215 (0.325)\n",
      "-14: 3-2-64: 0.454 (0.469)\n",
      "-14: 3-3-64: 0.294 (0.336)\n",
      "-14: 2-2-128: 0.482 (0.511)\n",
      "-14: 2-3-128: 0.226 (0.325)\n",
      "-14: 3-2-128: 0.456 (0.474)\n",
      "-14: 3-3-128: 0.250 (0.294)\n",
      "-14: 2-2-256: 0.504 (0.511)\n",
      "-15: 2-2-16: 0.412 (0.445)\n",
      "-15: 2-3-16: 0.246 (0.289)\n",
      "-15: 2-4-16: 0.228 (0.270)\n",
      "-15: 3-2-16: 0.303 (0.307)\n",
      "-15: 3-3-16: 0.226 (0.307)\n",
      "-15: 2-2-32: 0.482 (0.496)\n",
      "-15: 2-3-32: 0.261 (0.292)\n",
      "-15: 2-4-32: 0.228 (0.327)\n",
      "-15: 3-2-32: 0.272 (0.274)\n",
      "-15: 3-3-32: 0.202 (0.272)\n",
      "-15: 2-2-64: 0.485 (0.511)\n",
      "-15: 2-3-64: 0.276 (0.322)\n",
      "-15: 3-2-64: 0.456 (0.458)\n",
      "-15: 3-3-64: 0.221 (0.276)\n",
      "-15: 2-2-128: 0.465 (0.502)\n",
      "-15: 2-3-128: 0.303 (0.382)\n",
      "-15: 3-2-128: 0.450 (0.489)\n",
      "-15: 3-3-128: 0.182 (0.322)\n",
      "-15: 2-2-256: 0.463 (0.489)\n",
      "-16: 2-2-16: 0.443 (0.465)\n",
      "-16: 2-3-16: 0.202 (0.294)\n",
      "-16: 2-4-16: 0.252 (0.274)\n",
      "-16: 3-2-16: 0.270 (0.303)\n",
      "-16: 3-3-16: 0.272 (0.276)\n",
      "-16: 2-2-32: 0.480 (0.487)\n",
      "-16: 2-3-32: 0.298 (0.307)\n",
      "-16: 2-4-32: 0.268 (0.283)\n",
      "-16: 3-2-32: 0.397 (0.408)\n",
      "-16: 3-3-32: 0.235 (0.351)\n",
      "-16: 2-2-64: 0.463 (0.489)\n",
      "-16: 2-3-64: 0.327 (0.333)\n",
      "-16: 3-2-64: 0.397 (0.397)\n",
      "-16: 3-3-64: 0.217 (0.281)\n",
      "-16: 2-2-128: 0.474 (0.482)\n",
      "-16: 2-3-128: 0.298 (0.311)\n",
      "-16: 3-2-128: 0.441 (0.461)\n",
      "-16: 3-3-128: 0.184 (0.254)\n",
      "-16: 2-2-256: 0.480 (0.498)\n",
      "-17: 2-2-16: 0.338 (0.428)\n",
      "-17: 2-3-16: 0.215 (0.270)\n",
      "-17: 2-4-16: 0.221 (0.292)\n",
      "-17: 3-2-16: 0.314 (0.364)\n",
      "-17: 3-3-16: 0.226 (0.259)\n",
      "-17: 2-2-32: 0.458 (0.469)\n",
      "-17: 2-3-32: 0.248 (0.283)\n",
      "-17: 2-4-32: 0.186 (0.289)\n",
      "-17: 3-2-32: 0.276 (0.375)\n",
      "-17: 3-3-32: 0.232 (0.263)\n",
      "-17: 2-2-64: 0.456 (0.474)\n",
      "-17: 2-3-64: 0.259 (0.327)\n",
      "-17: 3-2-64: 0.281 (0.384)\n",
      "-17: 3-3-64: 0.237 (0.281)\n",
      "-17: 2-2-128: 0.452 (0.478)\n",
      "-17: 2-3-128: 0.204 (0.285)\n",
      "-17: 3-2-128: 0.362 (0.445)\n",
      "-17: 3-3-128: 0.221 (0.283)\n",
      "-17: 2-2-256: 0.467 (0.485)\n",
      "-18: 2-2-16: 0.465 (0.487)\n",
      "-18: 2-3-16: 0.252 (0.270)\n",
      "-18: 2-4-16: 0.241 (0.268)\n",
      "-18: 3-2-16: 0.384 (0.399)\n",
      "-18: 3-3-16: 0.167 (0.211)\n",
      "-18: 2-2-32: 0.469 (0.498)\n",
      "-18: 2-3-32: 0.261 (0.272)\n",
      "-18: 2-4-32: 0.263 (0.296)\n",
      "-18: 3-2-32: 0.375 (0.419)\n",
      "-18: 3-3-32: 0.186 (0.283)\n",
      "-18: 2-2-64: 0.456 (0.485)\n",
      "-18: 2-3-64: 0.314 (0.342)\n",
      "-18: 3-2-64: 0.463 (0.498)\n",
      "-18: 3-3-64: 0.208 (0.298)\n",
      "-18: 2-2-128: 0.471 (0.502)\n",
      "-18: 2-3-128: 0.164 (0.274)\n",
      "-18: 3-2-128: 0.443 (0.507)\n",
      "-18: 3-3-128: 0.252 (0.257)\n",
      "-18: 2-2-256: 0.491 (0.498)\n",
      "-19: 2-2-16: 0.458 (0.458)\n",
      "-19: 2-3-16: 0.265 (0.289)\n",
      "-19: 2-4-16: 0.259 (0.318)\n",
      "-19: 3-2-16: 0.270 (0.336)\n",
      "-19: 3-3-16: 0.237 (0.281)\n",
      "-19: 2-2-32: 0.471 (0.500)\n",
      "-19: 2-3-32: 0.296 (0.311)\n",
      "-19: 2-4-32: 0.279 (0.283)\n",
      "-19: 3-2-32: 0.289 (0.305)\n",
      "-19: 3-3-32: 0.246 (0.257)\n",
      "-19: 2-2-64: 0.447 (0.471)\n",
      "-19: 2-3-64: 0.285 (0.316)\n",
      "-19: 3-2-64: 0.401 (0.410)\n",
      "-19: 3-3-64: 0.259 (0.316)\n",
      "-19: 2-2-128: 0.469 (0.489)\n",
      "-19: 2-3-128: 0.371 (0.393)\n",
      "-19: 3-2-128: 0.279 (0.333)\n",
      "-19: 3-3-128: 0.272 (0.272)\n",
      "-19: 2-2-256: 0.467 (0.500)\n",
      "-20: 2-2-16: 0.450 (0.478)\n",
      "-20: 2-3-16: 0.250 (0.263)\n",
      "-20: 2-4-16: 0.268 (0.292)\n",
      "-20: 3-2-16: 0.235 (0.283)\n",
      "-20: 3-3-16: 0.241 (0.265)\n",
      "-20: 2-2-32: 0.331 (0.458)\n",
      "-20: 2-3-32: 0.239 (0.303)\n",
      "-20: 2-4-32: 0.261 (0.309)\n",
      "-20: 3-2-32: 0.237 (0.309)\n",
      "-20: 3-3-32: 0.250 (0.305)\n",
      "-20: 2-2-64: 0.498 (0.507)\n",
      "-20: 2-3-64: 0.285 (0.320)\n",
      "-20: 3-2-64: 0.309 (0.368)\n",
      "-20: 3-3-64: 0.237 (0.373)\n",
      "-20: 2-2-128: 0.480 (0.513)\n",
      "-20: 2-3-128: 0.327 (0.375)\n",
      "-20: 3-2-128: 0.303 (0.408)\n",
      "-20: 3-3-128: 0.248 (0.276)\n",
      "-20: 2-2-256: 0.480 (0.502)\n",
      "----- 57.13 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [\n",
    "        # {'L': 2, 'K': 2, 'hid_dim': 8},\n",
    "        # {'L': 2, 'K': 3, 'hid_dim': 8},\n",
    "        # {'L': 2, 'K': 4, 'hid_dim': 8},\n",
    "\n",
    "        # {'L': 3, 'K': 2, 'hid_dim': 8},\n",
    "        # {'L': 4, 'K': 2, 'hid_dim': 8},\n",
    "        # {'L': 3, 'K': 3, 'hid_dim': 8},\n",
    "        # {'L': 4, 'K': 3, 'hid_dim': 8},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 16},\n",
    "        # {'L': 4, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 16},\n",
    "        # {'L': 4, 'K': 3, 'hid_dim': 16},\n",
    "    \n",
    "        {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 32},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 32},\n",
    "        # {'L': 4, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 32},\n",
    "        # {'L': 4, 'K': 3, 'hid_dim': 32},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 64},\n",
    "        # {'L': 2, 'K': 4, 'hid_dim': 64},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 64},\n",
    "        # {'L': 4, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 64},\n",
    "        # {'L': 4, 'K': 3, 'hid_dim': 64},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 128},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 128},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 128},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 128},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 256},\n",
    "        ]\n",
    "\n",
    "best_accs3 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs3 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, exp['hid_dim'], OUT_DIM, exp['L'], exp['K'], act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0, diff_layer=GFGCN_noh_Layer)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "\n",
    "        best_accs3[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs3[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}: {best_val_accs3[j,i]:.3f} ({best_accs3[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}' for exp in EXPS]\n",
    "table_over3 = summary_table(best_accs3, index_name)\n",
    "table3 = summary_table(best_val_accs3, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.379276</td>\n",
       "      <td>0.407895</td>\n",
       "      <td>0.083970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.256031</td>\n",
       "      <td>0.256579</td>\n",
       "      <td>0.029273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-16</th>\n",
       "      <td>0.245724</td>\n",
       "      <td>0.255482</td>\n",
       "      <td>0.028182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.297588</td>\n",
       "      <td>0.292763</td>\n",
       "      <td>0.047340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-16</th>\n",
       "      <td>0.223684</td>\n",
       "      <td>0.225877</td>\n",
       "      <td>0.034957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.436404</td>\n",
       "      <td>0.464912</td>\n",
       "      <td>0.057901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.261952</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.026320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-32</th>\n",
       "      <td>0.241886</td>\n",
       "      <td>0.247807</td>\n",
       "      <td>0.037320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-32</th>\n",
       "      <td>0.321382</td>\n",
       "      <td>0.290570</td>\n",
       "      <td>0.058703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-32</th>\n",
       "      <td>0.225110</td>\n",
       "      <td>0.233553</td>\n",
       "      <td>0.025417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-64</th>\n",
       "      <td>0.463925</td>\n",
       "      <td>0.471491</td>\n",
       "      <td>0.031806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-64</th>\n",
       "      <td>0.277741</td>\n",
       "      <td>0.282895</td>\n",
       "      <td>0.032135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-64</th>\n",
       "      <td>0.376206</td>\n",
       "      <td>0.391447</td>\n",
       "      <td>0.076612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-64</th>\n",
       "      <td>0.238268</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.023128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-128</th>\n",
       "      <td>0.472697</td>\n",
       "      <td>0.472588</td>\n",
       "      <td>0.020021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-128</th>\n",
       "      <td>0.287829</td>\n",
       "      <td>0.299342</td>\n",
       "      <td>0.068489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-128</th>\n",
       "      <td>0.367105</td>\n",
       "      <td>0.387061</td>\n",
       "      <td>0.073807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-128</th>\n",
       "      <td>0.235088</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>0.025258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-256</th>\n",
       "      <td>0.474123</td>\n",
       "      <td>0.470395</td>\n",
       "      <td>0.016995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean accs       med       std\n",
       "2-2-16    0.379276  0.407895  0.083970\n",
       "2-3-16    0.256031  0.256579  0.029273\n",
       "2-4-16    0.245724  0.255482  0.028182\n",
       "3-2-16    0.297588  0.292763  0.047340\n",
       "3-3-16    0.223684  0.225877  0.034957\n",
       "2-2-32    0.436404  0.464912  0.057901\n",
       "2-3-32    0.261952  0.263158  0.026320\n",
       "2-4-32    0.241886  0.247807  0.037320\n",
       "3-2-32    0.321382  0.290570  0.058703\n",
       "3-3-32    0.225110  0.233553  0.025417\n",
       "2-2-64    0.463925  0.471491  0.031806\n",
       "2-3-64    0.277741  0.282895  0.032135\n",
       "3-2-64    0.376206  0.391447  0.076612\n",
       "3-3-64    0.238268  0.236842  0.023128\n",
       "2-2-128   0.472697  0.472588  0.020021\n",
       "2-3-128   0.287829  0.299342  0.068489\n",
       "3-2-128   0.367105  0.387061  0.073807\n",
       "3-3-128   0.235088  0.245614  0.025258\n",
       "2-2-256   0.474123  0.470395  0.016995"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.434539</td>\n",
       "      <td>0.446272</td>\n",
       "      <td>0.049686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.288925</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.020394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-16</th>\n",
       "      <td>0.289803</td>\n",
       "      <td>0.290570</td>\n",
       "      <td>0.021590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.327851</td>\n",
       "      <td>0.313596</td>\n",
       "      <td>0.040362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-16</th>\n",
       "      <td>0.279715</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.027596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.483114</td>\n",
       "      <td>0.486842</td>\n",
       "      <td>0.016703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.296053</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.018608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-32</th>\n",
       "      <td>0.288596</td>\n",
       "      <td>0.287281</td>\n",
       "      <td>0.018327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-32</th>\n",
       "      <td>0.358772</td>\n",
       "      <td>0.356360</td>\n",
       "      <td>0.046133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-32</th>\n",
       "      <td>0.284211</td>\n",
       "      <td>0.278509</td>\n",
       "      <td>0.025094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-64</th>\n",
       "      <td>0.492654</td>\n",
       "      <td>0.502193</td>\n",
       "      <td>0.018114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-64</th>\n",
       "      <td>0.324342</td>\n",
       "      <td>0.324561</td>\n",
       "      <td>0.018927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-64</th>\n",
       "      <td>0.407346</td>\n",
       "      <td>0.400219</td>\n",
       "      <td>0.064198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-64</th>\n",
       "      <td>0.293421</td>\n",
       "      <td>0.285088</td>\n",
       "      <td>0.027744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-128</th>\n",
       "      <td>0.496820</td>\n",
       "      <td>0.502193</td>\n",
       "      <td>0.016636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-128</th>\n",
       "      <td>0.341118</td>\n",
       "      <td>0.327851</td>\n",
       "      <td>0.046196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-128</th>\n",
       "      <td>0.417325</td>\n",
       "      <td>0.444079</td>\n",
       "      <td>0.060094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-128</th>\n",
       "      <td>0.288377</td>\n",
       "      <td>0.283991</td>\n",
       "      <td>0.024445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-256</th>\n",
       "      <td>0.496711</td>\n",
       "      <td>0.497807</td>\n",
       "      <td>0.014654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean accs       med       std\n",
       "2-2-16    0.434539  0.446272  0.049686\n",
       "2-3-16    0.288925  0.289474  0.020394\n",
       "2-4-16    0.289803  0.290570  0.021590\n",
       "3-2-16    0.327851  0.313596  0.040362\n",
       "3-3-16    0.279715  0.280702  0.027596\n",
       "2-2-32    0.483114  0.486842  0.016703\n",
       "2-3-32    0.296053  0.291667  0.018608\n",
       "2-4-32    0.288596  0.287281  0.018327\n",
       "3-2-32    0.358772  0.356360  0.046133\n",
       "3-3-32    0.284211  0.278509  0.025094\n",
       "2-2-64    0.492654  0.502193  0.018114\n",
       "2-3-64    0.324342  0.324561  0.018927\n",
       "3-2-64    0.407346  0.400219  0.064198\n",
       "3-3-64    0.293421  0.285088  0.027744\n",
       "2-2-128   0.496820  0.502193  0.016636\n",
       "2-3-128   0.341118  0.327851  0.046196\n",
       "3-2-128   0.417325  0.444079  0.060094\n",
       "3-3-128   0.288377  0.283991  0.024445\n",
       "2-2-256   0.496711  0.497807  0.014654"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_over3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design - Bias False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 2-2-16: 0.248 (0.303)\n",
      "-1: 2-3-16: 0.283 (0.320)\n",
      "-1: 2-4-16: 0.272 (0.322)\n",
      "-1: 3-2-16: 0.410 (0.423)\n",
      "-1: 3-3-16: 0.202 (0.261)\n",
      "-1: 2-2-32: 0.318 (0.487)\n",
      "-1: 2-3-32: 0.279 (0.292)\n",
      "-1: 2-4-32: 0.285 (0.298)\n",
      "-2: 2-2-16: 0.270 (0.371)\n",
      "-2: 2-3-16: 0.204 (0.272)\n",
      "-2: 2-4-16: 0.268 (0.285)\n",
      "-2: 3-2-16: 0.265 (0.292)\n",
      "-2: 3-3-16: 0.228 (0.298)\n",
      "-2: 2-2-32: 0.498 (0.513)\n",
      "-2: 2-3-32: 0.265 (0.333)\n",
      "-2: 2-4-32: 0.217 (0.265)\n",
      "-3: 2-2-16: 0.404 (0.436)\n",
      "-3: 2-3-16: 0.268 (0.311)\n",
      "-3: 2-4-16: 0.206 (0.296)\n",
      "-3: 3-2-16: 0.285 (0.311)\n",
      "-3: 3-3-16: 0.241 (0.248)\n",
      "-3: 2-2-32: 0.434 (0.458)\n",
      "-3: 2-3-32: 0.268 (0.287)\n",
      "-3: 2-4-32: 0.184 (0.268)\n",
      "-4: 2-2-16: 0.432 (0.456)\n",
      "-4: 2-3-16: 0.276 (0.289)\n",
      "-4: 2-4-16: 0.298 (0.305)\n",
      "-4: 3-2-16: 0.298 (0.298)\n",
      "-4: 3-3-16: 0.164 (0.281)\n",
      "-4: 2-2-32: 0.487 (0.502)\n",
      "-4: 2-3-32: 0.294 (0.320)\n",
      "-4: 2-4-32: 0.270 (0.292)\n",
      "-5: 2-2-16: 0.461 (0.471)\n",
      "-5: 2-3-16: 0.261 (0.272)\n",
      "-5: 2-4-16: 0.193 (0.292)\n",
      "-5: 3-2-16: 0.261 (0.316)\n",
      "-5: 3-3-16: 0.191 (0.287)\n",
      "-5: 2-2-32: 0.476 (0.487)\n",
      "-5: 2-3-32: 0.279 (0.287)\n",
      "-5: 2-4-32: 0.232 (0.254)\n",
      "-6: 2-2-16: 0.432 (0.447)\n",
      "-6: 2-3-16: 0.292 (0.298)\n",
      "-6: 2-4-16: 0.261 (0.283)\n",
      "-6: 3-2-16: 0.349 (0.353)\n",
      "-6: 3-3-16: 0.182 (0.289)\n",
      "-6: 2-2-32: 0.465 (0.476)\n",
      "-6: 2-3-32: 0.261 (0.292)\n",
      "-6: 2-4-32: 0.171 (0.257)\n",
      "-7: 2-2-16: 0.228 (0.432)\n",
      "-7: 2-3-16: 0.246 (0.296)\n",
      "-7: 2-4-16: 0.219 (0.289)\n",
      "-7: 3-2-16: 0.237 (0.287)\n",
      "-7: 3-3-16: 0.200 (0.329)\n",
      "-7: 2-2-32: 0.349 (0.458)\n",
      "-7: 2-3-32: 0.232 (0.296)\n",
      "-7: 2-4-32: 0.228 (0.300)\n",
      "-8: 2-2-16: 0.478 (0.487)\n",
      "-8: 2-3-16: 0.250 (0.281)\n",
      "-8: 2-4-16: 0.268 (0.300)\n",
      "-8: 3-2-16: 0.364 (0.368)\n",
      "-8: 3-3-16: 0.226 (0.289)\n",
      "-8: 2-2-32: 0.465 (0.493)\n",
      "-8: 2-3-32: 0.254 (0.263)\n",
      "-8: 2-4-32: 0.202 (0.285)\n",
      "-9: 2-2-16: 0.281 (0.366)\n",
      "-9: 2-3-16: 0.270 (0.292)\n",
      "-9: 2-4-16: 0.259 (0.265)\n",
      "-9: 3-2-16: 0.281 (0.303)\n",
      "-9: 3-3-16: 0.230 (0.259)\n",
      "-9: 2-2-32: 0.476 (0.496)\n",
      "-9: 2-3-32: 0.276 (0.285)\n",
      "-9: 2-4-32: 0.272 (0.279)\n",
      "-10: 2-2-16: 0.289 (0.428)\n",
      "-10: 2-3-16: 0.300 (0.322)\n",
      "-10: 2-4-16: 0.217 (0.320)\n",
      "-10: 3-2-16: 0.294 (0.331)\n",
      "-10: 3-3-16: 0.219 (0.283)\n",
      "-10: 2-2-32: 0.386 (0.461)\n",
      "-10: 2-3-32: 0.235 (0.325)\n",
      "-10: 2-4-32: 0.235 (0.316)\n",
      "-11: 2-2-16: 0.357 (0.465)\n",
      "-11: 2-3-16: 0.283 (0.316)\n",
      "-11: 2-4-16: 0.265 (0.287)\n",
      "-11: 3-2-16: 0.320 (0.382)\n",
      "-11: 3-3-16: 0.259 (0.272)\n",
      "-11: 2-2-32: 0.338 (0.474)\n",
      "-11: 2-3-32: 0.287 (0.307)\n",
      "-11: 2-4-32: 0.270 (0.283)\n",
      "-12: 2-2-16: 0.265 (0.349)\n",
      "-12: 2-3-16: 0.213 (0.248)\n",
      "-12: 2-4-16: 0.213 (0.230)\n",
      "-12: 3-2-16: 0.228 (0.272)\n",
      "-12: 3-3-16: 0.320 (0.338)\n",
      "-12: 2-2-32: 0.406 (0.489)\n",
      "-12: 2-3-32: 0.206 (0.285)\n",
      "-12: 2-4-32: 0.204 (0.296)\n",
      "-13: 2-2-16: 0.399 (0.423)\n",
      "-13: 2-3-16: 0.246 (0.270)\n",
      "-13: 2-4-16: 0.221 (0.289)\n",
      "-13: 3-2-16: 0.294 (0.329)\n",
      "-13: 3-3-16: 0.217 (0.261)\n",
      "-13: 2-2-32: 0.447 (0.463)\n",
      "-13: 2-3-32: 0.211 (0.265)\n",
      "-13: 2-4-32: 0.283 (0.285)\n",
      "-14: 2-2-16: 0.476 (0.496)\n",
      "-14: 2-3-16: 0.300 (0.316)\n",
      "-14: 2-4-16: 0.285 (0.318)\n",
      "-14: 3-2-16: 0.292 (0.300)\n",
      "-14: 3-3-16: 0.226 (0.300)\n",
      "-14: 2-2-32: 0.491 (0.498)\n",
      "-14: 2-3-32: 0.289 (0.316)\n",
      "-14: 2-4-32: 0.300 (0.307)\n",
      "-15: 2-2-16: 0.412 (0.445)\n",
      "-15: 2-3-16: 0.246 (0.289)\n",
      "-15: 2-4-16: 0.228 (0.270)\n",
      "-15: 3-2-16: 0.303 (0.307)\n",
      "-15: 3-3-16: 0.226 (0.307)\n",
      "-15: 2-2-32: 0.482 (0.496)\n",
      "-15: 2-3-32: 0.261 (0.292)\n",
      "-15: 2-4-32: 0.228 (0.327)\n",
      "-16: 2-2-16: 0.443 (0.465)\n",
      "-16: 2-3-16: 0.202 (0.294)\n",
      "-16: 2-4-16: 0.252 (0.274)\n",
      "-16: 3-2-16: 0.270 (0.303)\n",
      "-16: 3-3-16: 0.272 (0.276)\n",
      "-16: 2-2-32: 0.480 (0.487)\n",
      "-16: 2-3-32: 0.298 (0.307)\n",
      "-16: 2-4-32: 0.268 (0.283)\n",
      "-17: 2-2-16: 0.338 (0.428)\n",
      "-17: 2-3-16: 0.215 (0.270)\n",
      "-17: 2-4-16: 0.221 (0.292)\n",
      "-17: 3-2-16: 0.314 (0.364)\n",
      "-17: 3-3-16: 0.226 (0.259)\n",
      "-17: 2-2-32: 0.458 (0.469)\n",
      "-17: 2-3-32: 0.248 (0.283)\n",
      "-17: 2-4-32: 0.186 (0.289)\n",
      "-18: 2-2-16: 0.465 (0.487)\n",
      "-18: 2-3-16: 0.252 (0.270)\n",
      "-18: 2-4-16: 0.241 (0.268)\n",
      "-18: 3-2-16: 0.384 (0.399)\n",
      "-18: 3-3-16: 0.167 (0.211)\n",
      "-18: 2-2-32: 0.469 (0.498)\n",
      "-18: 2-3-32: 0.261 (0.272)\n",
      "-18: 2-4-32: 0.263 (0.296)\n",
      "-19: 2-2-16: 0.458 (0.458)\n",
      "-19: 2-3-16: 0.265 (0.289)\n",
      "-19: 2-4-16: 0.259 (0.318)\n",
      "-19: 3-2-16: 0.270 (0.336)\n",
      "-19: 3-3-16: 0.237 (0.281)\n",
      "-19: 2-2-32: 0.471 (0.500)\n",
      "-19: 2-3-32: 0.296 (0.311)\n",
      "-19: 2-4-32: 0.279 (0.283)\n",
      "-20: 2-2-16: 0.450 (0.478)\n",
      "-20: 2-3-16: 0.250 (0.263)\n",
      "-20: 2-4-16: 0.268 (0.292)\n",
      "-20: 3-2-16: 0.235 (0.283)\n",
      "-20: 3-3-16: 0.241 (0.265)\n",
      "-20: 2-2-32: 0.331 (0.458)\n",
      "-20: 2-3-32: 0.239 (0.303)\n",
      "-20: 2-4-32: 0.261 (0.309)\n",
      "----- 27.62 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 16},\n",
    "        # {'L': 4, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 16},\n",
    "        # {'L': 4, 'K': 3, 'hid_dim': 16},\n",
    "    \n",
    "        {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 32},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 32},\n",
    "        # {'L': 4, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 32},\n",
    "        # {'L': 4, 'K': 3, 'hid_dim': 32},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 64},\n",
    "        # {'L': 2, 'K': 4, 'hid_dim': 64},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 64},\n",
    "        # {'L': 4, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 64},\n",
    "        # {'L': 4, 'K': 3, 'hid_dim': 64},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 128},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 128},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 128},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 128},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 256},\n",
    "        ]\n",
    "\n",
    "best_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, exp['hid_dim'], OUT_DIM, exp['L'], exp['K'], act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0, diff_layer=GFGCN_noh_Layer, bias=False)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "\n",
    "        best_accs3b[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs3b[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}: {best_val_accs3[j,i]:.3f} ({best_accs3[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}' for exp in EXPS]\n",
    "table_over3b = summary_table(best_accs3b, index_name)\n",
    "table3b = summary_table(best_val_accs3b, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.365461</td>\n",
       "      <td>0.404605</td>\n",
       "      <td>0.075214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.230592</td>\n",
       "      <td>0.228070</td>\n",
       "      <td>0.027454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-16</th>\n",
       "      <td>0.199123</td>\n",
       "      <td>0.195175</td>\n",
       "      <td>0.023969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.285965</td>\n",
       "      <td>0.292763</td>\n",
       "      <td>0.048788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-16</th>\n",
       "      <td>0.206360</td>\n",
       "      <td>0.211623</td>\n",
       "      <td>0.025356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.428728</td>\n",
       "      <td>0.430921</td>\n",
       "      <td>0.030224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.262390</td>\n",
       "      <td>0.256579</td>\n",
       "      <td>0.034600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-32</th>\n",
       "      <td>0.202632</td>\n",
       "      <td>0.205044</td>\n",
       "      <td>0.022720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean accs       med       std\n",
       "2-2-16   0.365461  0.404605  0.075214\n",
       "2-3-16   0.230592  0.228070  0.027454\n",
       "2-4-16   0.199123  0.195175  0.023969\n",
       "3-2-16   0.285965  0.292763  0.048788\n",
       "3-3-16   0.206360  0.211623  0.025356\n",
       "2-2-32   0.428728  0.430921  0.030224\n",
       "2-3-32   0.262390  0.256579  0.034600\n",
       "2-4-32   0.202632  0.205044  0.022720"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_over3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearities and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.450 (0.487)\n",
      "-1: ReLU()-Softmax(dim=1)-NLLLoss(): 0.471 (0.485)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.480 (0.504)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.485 (0.507)\n",
      "-1: ReLU()-Identity()-CrossEntropyLoss(): 0.498 (0.511)\n",
      "-1: ReLU()-Identity()-NLLLoss(): 0.211 (0.248)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.458 (0.480)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.445 (0.485)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.456 (0.496)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.461 (0.491)\n",
      "-1: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.452 (0.485)\n",
      "-1: Identity()-Softmax(dim=1)-NLLLoss(): 0.439 (0.476)\n",
      "-1: Identity()-Identity()-CrossEntropyLoss(): 0.450 (0.491)\n",
      "-2: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.465 (0.482)\n",
      "-2: ReLU()-Softmax(dim=1)-NLLLoss(): 0.461 (0.478)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.485 (0.504)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.436 (0.487)\n",
      "-2: ReLU()-Identity()-CrossEntropyLoss(): 0.476 (0.500)\n",
      "-2: ReLU()-Identity()-NLLLoss(): 0.217 (0.252)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.458 (0.467)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.452 (0.478)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.452 (0.469)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.458 (0.471)\n",
      "-2: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.452 (0.480)\n",
      "-2: Identity()-Softmax(dim=1)-NLLLoss(): 0.450 (0.482)\n",
      "-2: Identity()-Identity()-CrossEntropyLoss(): 0.518 (0.537)\n",
      "-3: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.408 (0.421)\n",
      "-3: ReLU()-Softmax(dim=1)-NLLLoss(): 0.384 (0.417)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.445 (0.458)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.441 (0.445)\n",
      "-3: ReLU()-Identity()-CrossEntropyLoss(): 0.439 (0.467)\n",
      "-3: ReLU()-Identity()-NLLLoss(): 0.195 (0.268)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.408 (0.428)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.404 (0.434)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.430 (0.445)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.428 (0.439)\n",
      "-3: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.417 (0.458)\n",
      "-3: Identity()-Softmax(dim=1)-NLLLoss(): 0.421 (0.452)\n",
      "-3: Identity()-Identity()-CrossEntropyLoss(): 0.414 (0.456)\n",
      "-4: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.476 (0.491)\n",
      "-4: ReLU()-Softmax(dim=1)-NLLLoss(): 0.452 (0.458)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.502 (0.511)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.474 (0.491)\n",
      "-4: ReLU()-Identity()-CrossEntropyLoss(): 0.493 (0.511)\n",
      "-4: ReLU()-Identity()-NLLLoss(): 0.178 (0.257)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.441 (0.482)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.450 (0.476)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.452 (0.469)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.458 (0.478)\n",
      "-4: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.469 (0.485)\n",
      "-4: Identity()-Softmax(dim=1)-NLLLoss(): 0.474 (0.491)\n",
      "-4: Identity()-Identity()-CrossEntropyLoss(): 0.498 (0.507)\n",
      "-5: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.423 (0.456)\n",
      "-5: ReLU()-Softmax(dim=1)-NLLLoss(): 0.414 (0.454)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.471 (0.491)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.461 (0.485)\n",
      "-5: ReLU()-Identity()-CrossEntropyLoss(): 0.476 (0.496)\n",
      "-5: ReLU()-Identity()-NLLLoss(): 0.232 (0.314)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.436 (0.461)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.414 (0.465)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.432 (0.454)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.423 (0.461)\n",
      "-5: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.408 (0.458)\n",
      "-5: Identity()-Softmax(dim=1)-NLLLoss(): 0.406 (0.456)\n",
      "-5: Identity()-Identity()-CrossEntropyLoss(): 0.480 (0.487)\n",
      "-6: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.456 (0.480)\n",
      "-6: ReLU()-Softmax(dim=1)-NLLLoss(): 0.445 (0.474)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.456 (0.480)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.452 (0.471)\n",
      "-6: ReLU()-Identity()-CrossEntropyLoss(): 0.480 (0.485)\n",
      "-6: ReLU()-Identity()-NLLLoss(): 0.237 (0.270)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.467 (0.485)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.447 (0.476)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.452 (0.485)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.452 (0.489)\n",
      "-6: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.452 (0.482)\n",
      "-6: Identity()-Softmax(dim=1)-NLLLoss(): 0.456 (0.478)\n",
      "-6: Identity()-Identity()-CrossEntropyLoss(): 0.454 (0.474)\n",
      "-7: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.441 (0.471)\n",
      "-7: ReLU()-Softmax(dim=1)-NLLLoss(): 0.452 (0.465)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.461 (0.485)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.450 (0.485)\n",
      "-7: ReLU()-Identity()-CrossEntropyLoss(): 0.443 (0.476)\n",
      "-7: ReLU()-Identity()-NLLLoss(): 0.219 (0.261)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.445 (0.467)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.450 (0.476)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.447 (0.476)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.465 (0.471)\n",
      "-7: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.454 (0.467)\n",
      "-7: Identity()-Softmax(dim=1)-NLLLoss(): 0.447 (0.467)\n",
      "-7: Identity()-Identity()-CrossEntropyLoss(): 0.412 (0.478)\n",
      "-8: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.478 (0.511)\n",
      "-8: ReLU()-Softmax(dim=1)-NLLLoss(): 0.474 (0.496)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.482 (0.507)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.485 (0.502)\n",
      "-8: ReLU()-Identity()-CrossEntropyLoss(): 0.471 (0.489)\n",
      "-8: ReLU()-Identity()-NLLLoss(): 0.213 (0.215)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.493 (0.504)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.471 (0.509)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.487 (0.502)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.478 (0.500)\n",
      "-8: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.496 (0.513)\n",
      "-8: Identity()-Softmax(dim=1)-NLLLoss(): 0.489 (0.509)\n",
      "-8: Identity()-Identity()-CrossEntropyLoss(): 0.436 (0.480)\n",
      "-9: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.465 (0.478)\n",
      "-9: ReLU()-Softmax(dim=1)-NLLLoss(): 0.458 (0.463)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.480 (0.496)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.456 (0.478)\n",
      "-9: ReLU()-Identity()-CrossEntropyLoss(): 0.487 (0.502)\n",
      "-9: ReLU()-Identity()-NLLLoss(): 0.226 (0.230)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.458 (0.474)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.432 (0.463)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.474 (0.485)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.456 (0.476)\n",
      "-9: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.463 (0.476)\n",
      "-9: Identity()-Softmax(dim=1)-NLLLoss(): 0.458 (0.463)\n",
      "-9: Identity()-Identity()-CrossEntropyLoss(): 0.423 (0.458)\n",
      "-10: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.439 (0.461)\n",
      "-10: ReLU()-Softmax(dim=1)-NLLLoss(): 0.430 (0.461)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.461 (0.504)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.482 (0.515)\n",
      "-10: ReLU()-Identity()-CrossEntropyLoss(): 0.485 (0.513)\n",
      "-10: ReLU()-Identity()-NLLLoss(): 0.254 (0.254)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.454 (0.463)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.432 (0.454)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.439 (0.461)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.441 (0.447)\n",
      "-10: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.445 (0.454)\n",
      "-10: Identity()-Softmax(dim=1)-NLLLoss(): 0.450 (0.456)\n",
      "-10: Identity()-Identity()-CrossEntropyLoss(): 0.511 (0.518)\n",
      "-11: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.461 (0.478)\n",
      "-11: ReLU()-Softmax(dim=1)-NLLLoss(): 0.447 (0.471)\n",
      "-11: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.480 (0.507)\n",
      "-11: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.408 (0.507)\n",
      "-11: ReLU()-Identity()-CrossEntropyLoss(): 0.485 (0.513)\n",
      "-11: ReLU()-Identity()-NLLLoss(): 0.213 (0.257)\n",
      "-11: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.450 (0.487)\n",
      "-11: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.434 (0.458)\n",
      "-11: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.456 (0.487)\n",
      "-11: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.456 (0.485)\n",
      "-11: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.465 (0.482)\n",
      "-11: Identity()-Softmax(dim=1)-NLLLoss(): 0.456 (0.487)\n",
      "-11: Identity()-Identity()-CrossEntropyLoss(): 0.485 (0.504)\n",
      "-12: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.456 (0.485)\n",
      "-12: ReLU()-Softmax(dim=1)-NLLLoss(): 0.454 (0.469)\n",
      "-12: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.327 (0.502)\n",
      "-12: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.480 (0.498)\n",
      "-12: ReLU()-Identity()-CrossEntropyLoss(): 0.491 (0.515)\n",
      "-12: ReLU()-Identity()-NLLLoss(): 0.213 (0.300)\n",
      "-12: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.458 (0.482)\n",
      "-12: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.450 (0.480)\n",
      "-12: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.467 (0.482)\n",
      "-12: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.454 (0.476)\n",
      "-12: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.463 (0.474)\n",
      "-12: Identity()-Softmax(dim=1)-NLLLoss(): 0.478 (0.480)\n",
      "-12: Identity()-Identity()-CrossEntropyLoss(): 0.482 (0.504)\n",
      "-13: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.417 (0.425)\n",
      "-13: ReLU()-Softmax(dim=1)-NLLLoss(): 0.412 (0.423)\n",
      "-13: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.439 (0.450)\n",
      "-13: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.443 (0.458)\n",
      "-13: ReLU()-Identity()-CrossEntropyLoss(): 0.423 (0.439)\n",
      "-13: ReLU()-Identity()-NLLLoss(): 0.195 (0.228)\n",
      "-13: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.423 (0.436)\n",
      "-13: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.417 (0.430)\n",
      "-13: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.417 (0.458)\n",
      "-13: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.423 (0.425)\n",
      "-13: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.408 (0.434)\n",
      "-13: Identity()-Softmax(dim=1)-NLLLoss(): 0.417 (0.443)\n",
      "-13: Identity()-Identity()-CrossEntropyLoss(): 0.465 (0.480)\n",
      "-14: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.454 (0.465)\n",
      "-14: ReLU()-Softmax(dim=1)-NLLLoss(): 0.447 (0.465)\n",
      "-14: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.463 (0.482)\n",
      "-14: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.493 (0.504)\n",
      "-14: ReLU()-Identity()-CrossEntropyLoss(): 0.487 (0.504)\n",
      "-14: ReLU()-Identity()-NLLLoss(): 0.221 (0.261)\n",
      "-14: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.450 (0.509)\n",
      "-14: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.458 (0.478)\n",
      "-14: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.456 (0.471)\n",
      "-14: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.452 (0.454)\n",
      "-14: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.480 (0.493)\n",
      "-14: Identity()-Softmax(dim=1)-NLLLoss(): 0.458 (0.474)\n",
      "-14: Identity()-Identity()-CrossEntropyLoss(): 0.502 (0.515)\n",
      "-15: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.421 (0.450)\n",
      "-15: ReLU()-Softmax(dim=1)-NLLLoss(): 0.428 (0.439)\n",
      "-15: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.467 (0.489)\n",
      "-15: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.485 (0.493)\n",
      "-15: ReLU()-Identity()-CrossEntropyLoss(): 0.485 (0.496)\n",
      "-15: ReLU()-Identity()-NLLLoss(): 0.215 (0.303)\n",
      "-15: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.412 (0.461)\n",
      "-15: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.404 (0.465)\n",
      "-15: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.423 (0.465)\n",
      "-15: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.412 (0.463)\n",
      "-15: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.421 (0.467)\n",
      "-15: Identity()-Softmax(dim=1)-NLLLoss(): 0.421 (0.447)\n",
      "-15: Identity()-Identity()-CrossEntropyLoss(): 0.489 (0.489)\n",
      "-16: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.454 (0.476)\n",
      "-16: ReLU()-Softmax(dim=1)-NLLLoss(): 0.456 (0.467)\n",
      "-16: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.463 (0.476)\n",
      "-16: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.467 (0.485)\n",
      "-16: ReLU()-Identity()-CrossEntropyLoss(): 0.443 (0.463)\n",
      "-16: ReLU()-Identity()-NLLLoss(): 0.169 (0.200)\n",
      "-16: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.463 (0.480)\n",
      "-16: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.447 (0.480)\n",
      "-16: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.461 (0.487)\n",
      "-16: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.447 (0.476)\n",
      "-16: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.463 (0.480)\n",
      "-16: Identity()-Softmax(dim=1)-NLLLoss(): 0.441 (0.469)\n",
      "-16: Identity()-Identity()-CrossEntropyLoss(): 0.463 (0.476)\n",
      "-17: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.469 (0.480)\n",
      "-17: ReLU()-Softmax(dim=1)-NLLLoss(): 0.432 (0.458)\n",
      "-17: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.285 (0.445)\n",
      "-17: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.452 (0.482)\n",
      "-17: ReLU()-Identity()-CrossEntropyLoss(): 0.425 (0.461)\n",
      "-17: ReLU()-Identity()-NLLLoss(): 0.154 (0.230)\n",
      "-17: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.452 (0.465)\n",
      "-17: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.476 (0.485)\n",
      "-17: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.434 (0.469)\n",
      "-17: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.482 (0.489)\n",
      "-17: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.461 (0.476)\n",
      "-17: Identity()-Softmax(dim=1)-NLLLoss(): 0.456 (0.467)\n",
      "-17: Identity()-Identity()-CrossEntropyLoss(): 0.480 (0.507)\n",
      "-18: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.487 (0.507)\n",
      "-18: ReLU()-Softmax(dim=1)-NLLLoss(): 0.476 (0.487)\n",
      "-18: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.476 (0.504)\n",
      "-18: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.493 (0.513)\n",
      "-18: ReLU()-Identity()-CrossEntropyLoss(): 0.458 (0.482)\n",
      "-18: ReLU()-Identity()-NLLLoss(): 0.193 (0.239)\n",
      "-18: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.480 (0.496)\n",
      "-18: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.491 (0.502)\n",
      "-18: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.513 (0.513)\n",
      "-18: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.487 (0.504)\n",
      "-18: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.500 (0.502)\n",
      "-18: Identity()-Softmax(dim=1)-NLLLoss(): 0.485 (0.500)\n",
      "-18: Identity()-Identity()-CrossEntropyLoss(): 0.443 (0.498)\n",
      "-19: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.456 (0.467)\n",
      "-19: ReLU()-Softmax(dim=1)-NLLLoss(): 0.458 (0.463)\n",
      "-19: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.469 (0.482)\n",
      "-19: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.463 (0.487)\n",
      "-19: ReLU()-Identity()-CrossEntropyLoss(): 0.454 (0.478)\n",
      "-19: ReLU()-Identity()-NLLLoss(): 0.169 (0.189)\n",
      "-19: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.467 (0.485)\n",
      "-19: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.452 (0.469)\n",
      "-19: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.463 (0.489)\n",
      "-19: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.461 (0.487)\n",
      "-19: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.454 (0.478)\n",
      "-19: Identity()-Softmax(dim=1)-NLLLoss(): 0.454 (0.463)\n",
      "-19: Identity()-Identity()-CrossEntropyLoss(): 0.436 (0.478)\n",
      "-20: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.445 (0.465)\n",
      "-20: ReLU()-Softmax(dim=1)-NLLLoss(): 0.419 (0.461)\n",
      "-20: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.487 (0.507)\n",
      "-20: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.471 (0.493)\n",
      "-20: ReLU()-Identity()-CrossEntropyLoss(): 0.480 (0.504)\n",
      "-20: ReLU()-Identity()-NLLLoss(): 0.182 (0.211)\n",
      "-20: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.436 (0.461)\n",
      "-20: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.445 (0.463)\n",
      "-20: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.443 (0.458)\n",
      "-20: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.441 (0.454)\n",
      "-20: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.452 (0.465)\n",
      "-20: Identity()-Softmax(dim=1)-NLLLoss(): 0.443 (0.452)\n",
      "-20: Identity()-Identity()-CrossEntropyLoss(): 0.493 (0.511)\n",
      "----- 30.24 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},]\n",
    "\n",
    "best_accs4 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs4 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=exp['act'], last_act=exp['lact'],\n",
    "                     dropout=DROPOUT, init_h0=h0, diff_layer=GFGCN_noh_Layer)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = NodeClassModel(arch, S, masks, exp['loss'], device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "\n",
    "        best_accs4[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs4[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}: {best_val_accs4[j,i]:.3f} ({best_accs4[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}' for exp in EXPS]\n",
    "table_over4 = summary_table(best_accs4, index_name)\n",
    "table4 = summary_table(best_val_accs4, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.450987</td>\n",
       "      <td>0.455044</td>\n",
       "      <td>0.020668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.443531</td>\n",
       "      <td>0.449561</td>\n",
       "      <td>0.022884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.453947</td>\n",
       "      <td>0.468202</td>\n",
       "      <td>0.051812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.463816</td>\n",
       "      <td>0.464912</td>\n",
       "      <td>0.021504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.468969</td>\n",
       "      <td>0.478070</td>\n",
       "      <td>0.022645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Identity()-NLLLoss()</th>\n",
       "      <td>0.205263</td>\n",
       "      <td>0.212719</td>\n",
       "      <td>0.024950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.450548</td>\n",
       "      <td>0.452851</td>\n",
       "      <td>0.020283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.443531</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.022049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.452632</td>\n",
       "      <td>0.451754</td>\n",
       "      <td>0.021736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.451754</td>\n",
       "      <td>0.455044</td>\n",
       "      <td>0.019168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.453618</td>\n",
       "      <td>0.453947</td>\n",
       "      <td>0.024482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.449890</td>\n",
       "      <td>0.451754</td>\n",
       "      <td>0.021635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.466776</td>\n",
       "      <td>0.472588</td>\n",
       "      <td>0.031124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean accs       med  \\\n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()             0.450987  0.455044   \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                      0.443531  0.449561   \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()          0.453947  0.468202   \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                   0.463816  0.464912   \n",
       "ReLU()-Identity()-CrossEntropyLoss()                 0.468969  0.478070   \n",
       "ReLU()-Identity()-NLLLoss()                          0.205263  0.212719   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()     0.450548  0.452851   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()              0.443531  0.447368   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...   0.452632  0.451754   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...   0.451754  0.455044   \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()         0.453618  0.453947   \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                  0.449890  0.451754   \n",
       "Identity()-Identity()-CrossEntropyLoss()             0.466776  0.472588   \n",
       "\n",
       "                                                         std  \n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()            0.020668  \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                     0.022884  \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()         0.051812  \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                  0.021504  \n",
       "ReLU()-Identity()-CrossEntropyLoss()                0.022645  \n",
       "ReLU()-Identity()-NLLLoss()                         0.024950  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()    0.020283  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()             0.022049  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...  0.021736  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...  0.019168  \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()        0.024482  \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                 0.021635  \n",
       "Identity()-Identity()-CrossEntropyLoss()            0.031124  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.471820</td>\n",
       "      <td>0.476974</td>\n",
       "      <td>0.021878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.462610</td>\n",
       "      <td>0.463816</td>\n",
       "      <td>0.018756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.489254</td>\n",
       "      <td>0.493421</td>\n",
       "      <td>0.019167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.489364</td>\n",
       "      <td>0.489035</td>\n",
       "      <td>0.017061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.490241</td>\n",
       "      <td>0.495614</td>\n",
       "      <td>0.020648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Identity()-NLLLoss()</th>\n",
       "      <td>0.249232</td>\n",
       "      <td>0.253289</td>\n",
       "      <td>0.032314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.473575</td>\n",
       "      <td>0.476974</td>\n",
       "      <td>0.019522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.471272</td>\n",
       "      <td>0.475877</td>\n",
       "      <td>0.018477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.476096</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.016772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.471820</td>\n",
       "      <td>0.475877</td>\n",
       "      <td>0.020006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.475548</td>\n",
       "      <td>0.476974</td>\n",
       "      <td>0.017005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.470614</td>\n",
       "      <td>0.468202</td>\n",
       "      <td>0.017203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.492434</td>\n",
       "      <td>0.490132</td>\n",
       "      <td>0.020105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean accs       med  \\\n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()             0.471820  0.476974   \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                      0.462610  0.463816   \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()          0.489254  0.493421   \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                   0.489364  0.489035   \n",
       "ReLU()-Identity()-CrossEntropyLoss()                 0.490241  0.495614   \n",
       "ReLU()-Identity()-NLLLoss()                          0.249232  0.253289   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()     0.473575  0.476974   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()              0.471272  0.475877   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...   0.476096  0.473684   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...   0.471820  0.475877   \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()         0.475548  0.476974   \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                  0.470614  0.468202   \n",
       "Identity()-Identity()-CrossEntropyLoss()             0.492434  0.490132   \n",
       "\n",
       "                                                         std  \n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()            0.021878  \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                     0.018756  \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()         0.019167  \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                  0.017061  \n",
       "ReLU()-Identity()-CrossEntropyLoss()                0.020648  \n",
       "ReLU()-Identity()-NLLLoss()                         0.032314  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()    0.019522  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()             0.018477  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...  0.016772  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...  0.020006  \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()        0.017005  \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                 0.017203  \n",
       "Identity()-Identity()-CrossEntropyLoss()            0.020105  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_over4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the GSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PARAMETERS  - 0.61\n",
    "## Reaining params\n",
    "N_RUNS = 10\n",
    "N_EPOCHS = 200  # 500\n",
    "LR = .005  # .01\n",
    "WD = .01  # .005\n",
    "DROPOUT = .5\n",
    "\n",
    "# BEST PARAMETERS\n",
    "## Architecture params\n",
    "N_LAYERS = 2\n",
    "K = 2  # 2\n",
    "HID_DIM = 32 # 8\n",
    "\n",
    "## Model params\n",
    "h0 = 1  # 1\n",
    "NORM = False\n",
    "\n",
    "IN_DIM = feat.shape[1]\n",
    "OUT_DIM = n_class\n",
    "\n",
    "ACT = nn.LeakyReLU()\n",
    "LAST_ACT = nn.Softmax(dim=1)\n",
    "LOSS_FN = nn.CrossEntropyLoss()\n",
    "\n",
    "NORM = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training paramerters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:43: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(1/S.sum(1))\n",
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:51: RuntimeWarning: invalid value encountered in matmul\n",
      "  return D_inv_sqr @ S @ D_inv_sqr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 200-0.05-0.0005-0.5: 0.182 (0.182)\n",
      "-1: 200-0.01-0.0005-0.5: 0.182 (0.182)\n",
      "-1: 200-0.005-0.0005-0.5: 0.182 (0.182)\n",
      "-1: 200-0.001-0.0005-0.5: 0.182 (0.182)\n",
      "-1: 200-0.05-0.001-0.5: 0.182 (0.182)\n",
      "-1: 200-0.01-0.001-0.5: 0.182 (0.182)\n",
      "-1: 200-0.005-0.001-0.5: 0.182 (0.182)\n",
      "-1: 200-0.001-0.001-0.5: 0.182 (0.182)\n",
      "-1: 200-0.05-0.005-0.5: 0.182 (0.182)\n",
      "-1: 200-0.01-0.005-0.5: 0.182 (0.182)\n",
      "-1: 200-0.005-0.005-0.5: 0.182 (0.182)\n",
      "-1: 200-0.001-0.005-0.5: 0.182 (0.182)\n",
      "-1: 200-0.05-0.01-0.5: 0.182 (0.182)\n",
      "-1: 200-0.01-0.01-0.5: 0.182 (0.182)\n",
      "-1: 200-0.005-0.01-0.5: 0.182 (0.182)\n",
      "-1: 200-0.001-0.01-0.5: 0.182 (0.182)\n",
      "-1: 200-0.005-0.001-0: 0.182 (0.182)\n",
      "-1: 200-0.005-0.001-0.25: 0.182 (0.182)\n",
      "-1: 200-0.001-0.001-0: 0.182 (0.182)\n",
      "-1: 200-0.001-0.001-0.25: 0.182 (0.182)\n",
      "-1: 100-0.005-0.001-0.5: 0.182 (0.182)\n",
      "-1: 500-0.005-0.001-0.5: 0.182 (0.182)\n",
      "-1: 1000-0.005-0.001-0.5: 0.182 (0.182)\n",
      "-2: 200-0.05-0.0005-0.5: 0.189 (0.189)\n",
      "-2: 200-0.01-0.0005-0.5: 0.189 (0.189)\n",
      "-2: 200-0.005-0.0005-0.5: 0.189 (0.189)\n",
      "-2: 200-0.001-0.0005-0.5: 0.189 (0.189)\n",
      "-2: 200-0.05-0.001-0.5: 0.189 (0.189)\n",
      "-2: 200-0.01-0.001-0.5: 0.189 (0.189)\n",
      "-2: 200-0.005-0.001-0.5: 0.189 (0.189)\n",
      "-2: 200-0.001-0.001-0.5: 0.189 (0.189)\n",
      "-2: 200-0.05-0.005-0.5: 0.189 (0.189)\n",
      "-2: 200-0.01-0.005-0.5: 0.189 (0.189)\n",
      "-2: 200-0.005-0.005-0.5: 0.189 (0.189)\n",
      "-2: 200-0.001-0.005-0.5: 0.189 (0.189)\n",
      "-2: 200-0.05-0.01-0.5: 0.189 (0.189)\n",
      "-2: 200-0.01-0.01-0.5: 0.189 (0.189)\n",
      "-2: 200-0.005-0.01-0.5: 0.189 (0.189)\n",
      "-2: 200-0.001-0.01-0.5: 0.189 (0.189)\n",
      "-2: 200-0.005-0.001-0: 0.189 (0.189)\n",
      "-2: 200-0.005-0.001-0.25: 0.189 (0.189)\n",
      "-2: 200-0.001-0.001-0: 0.189 (0.189)\n",
      "-2: 200-0.001-0.001-0.25: 0.189 (0.189)\n",
      "-2: 100-0.005-0.001-0.5: 0.189 (0.189)\n",
      "-2: 500-0.005-0.001-0.5: 0.189 (0.189)\n",
      "-2: 1000-0.005-0.001-0.5: 0.189 (0.189)\n",
      "-3: 200-0.05-0.0005-0.5: 0.195 (0.195)\n",
      "-3: 200-0.01-0.0005-0.5: 0.195 (0.195)\n",
      "-3: 200-0.005-0.0005-0.5: 0.195 (0.195)\n",
      "-3: 200-0.001-0.0005-0.5: 0.195 (0.195)\n",
      "-3: 200-0.05-0.001-0.5: 0.195 (0.195)\n",
      "-3: 200-0.01-0.001-0.5: 0.195 (0.195)\n",
      "-3: 200-0.005-0.001-0.5: 0.195 (0.195)\n",
      "-3: 200-0.001-0.001-0.5: 0.195 (0.195)\n",
      "-3: 200-0.05-0.005-0.5: 0.195 (0.195)\n",
      "-3: 200-0.01-0.005-0.5: 0.195 (0.195)\n",
      "-3: 200-0.005-0.005-0.5: 0.195 (0.195)\n",
      "-3: 200-0.001-0.005-0.5: 0.195 (0.195)\n",
      "-3: 200-0.05-0.01-0.5: 0.195 (0.195)\n",
      "-3: 200-0.01-0.01-0.5: 0.195 (0.195)\n",
      "-3: 200-0.005-0.01-0.5: 0.195 (0.195)\n",
      "-3: 200-0.001-0.01-0.5: 0.195 (0.195)\n",
      "-3: 200-0.005-0.001-0: 0.195 (0.195)\n",
      "-3: 200-0.005-0.001-0.25: 0.195 (0.195)\n",
      "-3: 200-0.001-0.001-0: 0.195 (0.195)\n",
      "-3: 200-0.001-0.001-0.25: 0.195 (0.195)\n",
      "-3: 100-0.005-0.001-0.5: 0.195 (0.195)\n",
      "-3: 500-0.005-0.001-0.5: 0.195 (0.195)\n",
      "-3: 1000-0.005-0.001-0.5: 0.195 (0.195)\n",
      "-4: 200-0.05-0.0005-0.5: 0.211 (0.211)\n",
      "-4: 200-0.01-0.0005-0.5: 0.211 (0.211)\n",
      "-4: 200-0.005-0.0005-0.5: 0.211 (0.211)\n",
      "-4: 200-0.001-0.0005-0.5: 0.211 (0.211)\n",
      "-4: 200-0.05-0.001-0.5: 0.211 (0.211)\n",
      "-4: 200-0.01-0.001-0.5: 0.211 (0.211)\n",
      "-4: 200-0.005-0.001-0.5: 0.211 (0.211)\n",
      "-4: 200-0.001-0.001-0.5: 0.211 (0.211)\n",
      "-4: 200-0.05-0.005-0.5: 0.211 (0.211)\n",
      "-4: 200-0.01-0.005-0.5: 0.211 (0.211)\n",
      "-4: 200-0.005-0.005-0.5: 0.211 (0.211)\n",
      "-4: 200-0.001-0.005-0.5: 0.211 (0.211)\n",
      "-4: 200-0.05-0.01-0.5: 0.211 (0.211)\n",
      "-4: 200-0.01-0.01-0.5: 0.211 (0.211)\n",
      "-4: 200-0.005-0.01-0.5: 0.211 (0.211)\n",
      "-4: 200-0.001-0.01-0.5: 0.211 (0.211)\n",
      "-4: 200-0.005-0.001-0: 0.211 (0.211)\n",
      "-4: 200-0.005-0.001-0.25: 0.211 (0.211)\n",
      "-4: 200-0.001-0.001-0: 0.211 (0.211)\n",
      "-4: 200-0.001-0.001-0.25: 0.211 (0.211)\n",
      "-4: 100-0.005-0.001-0.5: 0.211 (0.211)\n",
      "-4: 500-0.005-0.001-0.5: 0.211 (0.211)\n",
      "-4: 1000-0.005-0.001-0.5: 0.211 (0.211)\n",
      "-5: 200-0.05-0.0005-0.5: 0.235 (0.235)\n",
      "-5: 200-0.01-0.0005-0.5: 0.235 (0.235)\n",
      "-5: 200-0.005-0.0005-0.5: 0.235 (0.235)\n",
      "-5: 200-0.001-0.0005-0.5: 0.235 (0.235)\n",
      "-5: 200-0.05-0.001-0.5: 0.235 (0.235)\n",
      "-5: 200-0.01-0.001-0.5: 0.235 (0.235)\n",
      "-5: 200-0.005-0.001-0.5: 0.235 (0.235)\n",
      "-5: 200-0.001-0.001-0.5: 0.235 (0.235)\n",
      "-5: 200-0.05-0.005-0.5: 0.235 (0.235)\n",
      "-5: 200-0.01-0.005-0.5: 0.235 (0.235)\n",
      "-5: 200-0.005-0.005-0.5: 0.235 (0.235)\n",
      "-5: 200-0.001-0.005-0.5: 0.235 (0.235)\n",
      "-5: 200-0.05-0.01-0.5: 0.235 (0.235)\n",
      "-5: 200-0.01-0.01-0.5: 0.235 (0.235)\n",
      "-5: 200-0.005-0.01-0.5: 0.235 (0.235)\n",
      "-5: 200-0.001-0.01-0.5: 0.235 (0.235)\n",
      "-5: 200-0.005-0.001-0: 0.235 (0.235)\n",
      "-5: 200-0.005-0.001-0.25: 0.235 (0.235)\n",
      "-5: 200-0.001-0.001-0: 0.235 (0.235)\n",
      "-5: 200-0.001-0.001-0.25: 0.235 (0.235)\n",
      "-5: 100-0.005-0.001-0.5: 0.235 (0.235)\n",
      "-5: 500-0.005-0.001-0.5: 0.235 (0.235)\n",
      "-5: 1000-0.005-0.001-0.5: 0.235 (0.235)\n",
      "-6: 200-0.05-0.0005-0.5: 0.182 (0.182)\n",
      "-6: 200-0.01-0.0005-0.5: 0.182 (0.182)\n",
      "-6: 200-0.005-0.0005-0.5: 0.182 (0.182)\n",
      "-6: 200-0.001-0.0005-0.5: 0.182 (0.182)\n",
      "-6: 200-0.05-0.001-0.5: 0.182 (0.182)\n",
      "-6: 200-0.01-0.001-0.5: 0.182 (0.182)\n",
      "-6: 200-0.005-0.001-0.5: 0.182 (0.182)\n",
      "-6: 200-0.001-0.001-0.5: 0.182 (0.182)\n",
      "-6: 200-0.05-0.005-0.5: 0.182 (0.182)\n",
      "-6: 200-0.01-0.005-0.5: 0.182 (0.182)\n",
      "-6: 200-0.005-0.005-0.5: 0.182 (0.182)\n",
      "-6: 200-0.001-0.005-0.5: 0.182 (0.182)\n",
      "-6: 200-0.05-0.01-0.5: 0.182 (0.182)\n",
      "-6: 200-0.01-0.01-0.5: 0.182 (0.182)\n",
      "-6: 200-0.005-0.01-0.5: 0.182 (0.182)\n",
      "-6: 200-0.001-0.01-0.5: 0.182 (0.182)\n",
      "-6: 200-0.005-0.001-0: 0.182 (0.182)\n",
      "-6: 200-0.005-0.001-0.25: 0.182 (0.182)\n",
      "-6: 200-0.001-0.001-0: 0.182 (0.182)\n",
      "-6: 200-0.001-0.001-0.25: 0.182 (0.182)\n",
      "-6: 100-0.005-0.001-0.5: 0.182 (0.182)\n",
      "-6: 500-0.005-0.001-0.5: 0.182 (0.182)\n",
      "-6: 1000-0.005-0.001-0.5: 0.182 (0.182)\n",
      "-7: 200-0.05-0.0005-0.5: 0.202 (0.202)\n",
      "-7: 200-0.01-0.0005-0.5: 0.202 (0.202)\n",
      "-7: 200-0.005-0.0005-0.5: 0.202 (0.202)\n",
      "-7: 200-0.001-0.0005-0.5: 0.202 (0.202)\n",
      "-7: 200-0.05-0.001-0.5: 0.202 (0.202)\n",
      "-7: 200-0.01-0.001-0.5: 0.202 (0.202)\n",
      "-7: 200-0.005-0.001-0.5: 0.202 (0.202)\n",
      "-7: 200-0.001-0.001-0.5: 0.202 (0.202)\n",
      "-7: 200-0.05-0.005-0.5: 0.202 (0.202)\n",
      "-7: 200-0.01-0.005-0.5: 0.202 (0.202)\n",
      "-7: 200-0.005-0.005-0.5: 0.202 (0.202)\n",
      "-7: 200-0.001-0.005-0.5: 0.202 (0.202)\n",
      "-7: 200-0.05-0.01-0.5: 0.202 (0.202)\n",
      "-7: 200-0.01-0.01-0.5: 0.202 (0.202)\n",
      "-7: 200-0.005-0.01-0.5: 0.202 (0.202)\n",
      "-7: 200-0.001-0.01-0.5: 0.202 (0.202)\n",
      "-7: 200-0.005-0.001-0: 0.202 (0.202)\n",
      "-7: 200-0.005-0.001-0.25: 0.202 (0.202)\n",
      "-7: 200-0.001-0.001-0: 0.202 (0.202)\n",
      "-7: 200-0.001-0.001-0.25: 0.202 (0.202)\n",
      "-7: 100-0.005-0.001-0.5: 0.202 (0.202)\n",
      "-7: 500-0.005-0.001-0.5: 0.202 (0.202)\n",
      "-7: 1000-0.005-0.001-0.5: 0.202 (0.202)\n",
      "-8: 200-0.05-0.0005-0.5: 0.186 (0.186)\n",
      "-8: 200-0.01-0.0005-0.5: 0.186 (0.186)\n",
      "-8: 200-0.005-0.0005-0.5: 0.186 (0.186)\n",
      "-8: 200-0.001-0.0005-0.5: 0.186 (0.186)\n",
      "-8: 200-0.05-0.001-0.5: 0.186 (0.186)\n",
      "-8: 200-0.01-0.001-0.5: 0.186 (0.186)\n",
      "-8: 200-0.005-0.001-0.5: 0.186 (0.186)\n",
      "-8: 200-0.001-0.001-0.5: 0.186 (0.186)\n",
      "-8: 200-0.05-0.005-0.5: 0.186 (0.186)\n",
      "-8: 200-0.01-0.005-0.5: 0.186 (0.186)\n",
      "-8: 200-0.005-0.005-0.5: 0.186 (0.186)\n",
      "-8: 200-0.001-0.005-0.5: 0.186 (0.186)\n",
      "-8: 200-0.05-0.01-0.5: 0.186 (0.186)\n",
      "-8: 200-0.01-0.01-0.5: 0.186 (0.186)\n",
      "-8: 200-0.005-0.01-0.5: 0.186 (0.186)\n",
      "-8: 200-0.001-0.01-0.5: 0.186 (0.186)\n",
      "-8: 200-0.005-0.001-0: 0.186 (0.186)\n",
      "-8: 200-0.005-0.001-0.25: 0.186 (0.186)\n",
      "-8: 200-0.001-0.001-0: 0.186 (0.186)\n",
      "-8: 200-0.001-0.001-0.25: 0.186 (0.186)\n",
      "-8: 100-0.005-0.001-0.5: 0.186 (0.186)\n",
      "-8: 500-0.005-0.001-0.5: 0.186 (0.186)\n",
      "-8: 1000-0.005-0.001-0.5: 0.186 (0.186)\n",
      "-9: 200-0.05-0.0005-0.5: 0.206 (0.206)\n",
      "-9: 200-0.01-0.0005-0.5: 0.206 (0.206)\n",
      "-9: 200-0.005-0.0005-0.5: 0.206 (0.206)\n",
      "-9: 200-0.001-0.0005-0.5: 0.206 (0.206)\n",
      "-9: 200-0.05-0.001-0.5: 0.206 (0.206)\n",
      "-9: 200-0.01-0.001-0.5: 0.206 (0.206)\n",
      "-9: 200-0.005-0.001-0.5: 0.206 (0.206)\n",
      "-9: 200-0.001-0.001-0.5: 0.206 (0.206)\n",
      "-9: 200-0.05-0.005-0.5: 0.206 (0.206)\n",
      "-9: 200-0.01-0.005-0.5: 0.206 (0.206)\n",
      "-9: 200-0.005-0.005-0.5: 0.206 (0.206)\n",
      "-9: 200-0.001-0.005-0.5: 0.206 (0.206)\n",
      "-9: 200-0.05-0.01-0.5: 0.206 (0.206)\n",
      "-9: 200-0.01-0.01-0.5: 0.206 (0.206)\n",
      "-9: 200-0.005-0.01-0.5: 0.206 (0.206)\n",
      "-9: 200-0.001-0.01-0.5: 0.206 (0.206)\n",
      "-9: 200-0.005-0.001-0: 0.206 (0.206)\n",
      "-9: 200-0.005-0.001-0.25: 0.206 (0.206)\n",
      "-9: 200-0.001-0.001-0: 0.206 (0.206)\n",
      "-9: 200-0.001-0.001-0.25: 0.206 (0.206)\n",
      "-9: 100-0.005-0.001-0.5: 0.206 (0.206)\n",
      "-9: 500-0.005-0.001-0.5: 0.206 (0.206)\n",
      "-9: 1000-0.005-0.001-0.5: 0.206 (0.206)\n",
      "-10: 200-0.05-0.0005-0.5: 0.171 (0.171)\n",
      "-10: 200-0.01-0.0005-0.5: 0.171 (0.171)\n",
      "-10: 200-0.005-0.0005-0.5: 0.171 (0.171)\n",
      "-10: 200-0.001-0.0005-0.5: 0.171 (0.171)\n",
      "-10: 200-0.05-0.001-0.5: 0.171 (0.171)\n",
      "-10: 200-0.01-0.001-0.5: 0.171 (0.171)\n",
      "-10: 200-0.005-0.001-0.5: 0.171 (0.171)\n",
      "-10: 200-0.001-0.001-0.5: 0.171 (0.171)\n",
      "-10: 200-0.05-0.005-0.5: 0.171 (0.171)\n",
      "-10: 200-0.01-0.005-0.5: 0.171 (0.171)\n",
      "-10: 200-0.005-0.005-0.5: 0.171 (0.171)\n",
      "-10: 200-0.001-0.005-0.5: 0.171 (0.171)\n",
      "-10: 200-0.05-0.01-0.5: 0.171 (0.171)\n",
      "-10: 200-0.01-0.01-0.5: 0.171 (0.171)\n",
      "-10: 200-0.005-0.01-0.5: 0.171 (0.171)\n",
      "-10: 200-0.001-0.01-0.5: 0.171 (0.171)\n",
      "-10: 200-0.005-0.001-0: 0.171 (0.171)\n",
      "-10: 200-0.005-0.001-0.25: 0.171 (0.171)\n",
      "-10: 200-0.001-0.001-0: 0.171 (0.171)\n",
      "-10: 200-0.001-0.001-0.25: 0.171 (0.171)\n",
      "-10: 100-0.005-0.001-0.5: 0.171 (0.171)\n",
      "-10: 500-0.005-0.001-0.5: 0.171 (0.171)\n",
      "-10: 1000-0.005-0.001-0.5: 0.171 (0.171)\n",
      "----- 2.14 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-4, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-4, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-4, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 5e-4, 'drop': .5},\n",
    "        \n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-3, 'drop': .5},\n",
    "\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 5e-3, 'drop': .5},\n",
    "\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-2, 'drop': .5},\n",
    "\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-3, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "\n",
    "        {'epochs': 100, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 1000, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        ]\n",
    "\n",
    "best_accs5 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs5 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=exp['drop'], init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, exp['epochs'], exp['lr'], exp['wd'])\n",
    "\n",
    "        best_accs5[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs5[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}: {best_val_accs5[j,i]:.3f} ({best_accs5[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}' for exp in EXPS]\n",
    "table_over5 = summary_table(best_accs5, index_name)\n",
    "table5 = summary_table(best_val_accs5, index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.0005-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.0005-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0005-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.0005-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.001-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.001-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.005-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.01-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.01-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.01-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.25</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.001-0</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.001-0.25</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100-0.005-0.001-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000-0.005-0.001-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      mean accs       med       std\n",
       "200-0.05-0.0005-0.5    0.195833  0.191886  0.017325\n",
       "200-0.01-0.0005-0.5    0.195833  0.191886  0.017325\n",
       "200-0.005-0.0005-0.5   0.195833  0.191886  0.017325\n",
       "200-0.001-0.0005-0.5   0.195833  0.191886  0.017325\n",
       "200-0.05-0.001-0.5     0.195833  0.191886  0.017325\n",
       "200-0.01-0.001-0.5     0.195833  0.191886  0.017325\n",
       "200-0.005-0.001-0.5    0.195833  0.191886  0.017325\n",
       "200-0.001-0.001-0.5    0.195833  0.191886  0.017325\n",
       "200-0.05-0.005-0.5     0.195833  0.191886  0.017325\n",
       "200-0.01-0.005-0.5     0.195833  0.191886  0.017325\n",
       "200-0.005-0.005-0.5    0.195833  0.191886  0.017325\n",
       "200-0.001-0.005-0.5    0.195833  0.191886  0.017325\n",
       "200-0.05-0.01-0.5      0.195833  0.191886  0.017325\n",
       "200-0.01-0.01-0.5      0.195833  0.191886  0.017325\n",
       "200-0.005-0.01-0.5     0.195833  0.191886  0.017325\n",
       "200-0.001-0.01-0.5     0.195833  0.191886  0.017325\n",
       "200-0.005-0.001-0      0.195833  0.191886  0.017325\n",
       "200-0.005-0.001-0.25   0.195833  0.191886  0.017325\n",
       "200-0.001-0.001-0      0.195833  0.191886  0.017325\n",
       "200-0.001-0.001-0.25   0.195833  0.191886  0.017325\n",
       "100-0.005-0.001-0.5    0.195833  0.191886  0.017325\n",
       "500-0.005-0.001-0.5    0.195833  0.191886  0.017325\n",
       "1000-0.005-0.001-0.5   0.195833  0.191886  0.017325"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:43: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(1/S.sum(1))\n",
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:51: RuntimeWarning: invalid value encountered in matmul\n",
      "  return D_inv_sqr @ S @ D_inv_sqr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 2-2-8: 0.182 (0.182)\n",
      "-1: 2-3-8: 0.182 (0.182)\n",
      "-1: 2-4-8: 0.182 (0.182)\n",
      "-1: 3-2-8: 0.182 (0.182)\n",
      "-1: 4-2-8: 0.182 (0.182)\n",
      "-1: 3-3-8: 0.182 (0.182)\n",
      "-1: 4-3-8: 0.182 (0.182)\n",
      "-1: 2-2-16: 0.182 (0.182)\n",
      "-1: 2-3-16: 0.182 (0.182)\n",
      "-1: 2-4-16: 0.182 (0.182)\n",
      "-1: 3-2-16: 0.182 (0.182)\n",
      "-1: 4-2-16: 0.182 (0.182)\n",
      "-1: 3-3-16: 0.182 (0.182)\n",
      "-1: 4-3-16: 0.182 (0.182)\n",
      "-1: 2-2-32: 0.182 (0.182)\n",
      "-1: 2-3-32: 0.182 (0.182)\n",
      "-1: 2-4-32: 0.182 (0.182)\n",
      "-1: 3-2-32: 0.182 (0.182)\n",
      "-1: 4-2-32: 0.182 (0.182)\n",
      "-1: 3-3-32: 0.182 (0.182)\n",
      "-1: 4-3-32: 0.182 (0.182)\n",
      "-1: 2-2-64: 0.182 (0.182)\n",
      "-1: 2-3-64: 0.182 (0.182)\n",
      "-1: 2-4-64: 0.182 (0.182)\n",
      "-1: 3-2-64: 0.182 (0.182)\n",
      "-1: 4-2-64: 0.182 (0.182)\n",
      "-1: 3-3-64: 0.182 (0.182)\n",
      "-1: 4-3-64: 0.182 (0.182)\n",
      "-1: 2-2-100: 0.182 (0.182)\n",
      "-1: 2-3-100: 0.182 (0.182)\n",
      "-1: 2-4-100: 0.182 (0.182)\n",
      "-1: 3-2-100: 0.182 (0.182)\n",
      "-1: 4-2-100: 0.182 (0.182)\n",
      "-1: 3-3-100: 0.182 (0.182)\n",
      "-1: 4-3-100: 0.182 (0.182)\n",
      "-2: 2-2-8: 0.189 (0.189)\n",
      "-2: 2-3-8: 0.189 (0.189)\n",
      "-2: 2-4-8: 0.189 (0.189)\n",
      "-2: 3-2-8: 0.189 (0.189)\n",
      "-2: 4-2-8: 0.189 (0.189)\n",
      "-2: 3-3-8: 0.189 (0.189)\n",
      "-2: 4-3-8: 0.189 (0.189)\n",
      "-2: 2-2-16: 0.189 (0.189)\n",
      "-2: 2-3-16: 0.189 (0.189)\n",
      "-2: 2-4-16: 0.189 (0.189)\n",
      "-2: 3-2-16: 0.189 (0.189)\n",
      "-2: 4-2-16: 0.189 (0.189)\n",
      "-2: 3-3-16: 0.189 (0.189)\n",
      "-2: 4-3-16: 0.189 (0.189)\n",
      "-2: 2-2-32: 0.189 (0.189)\n",
      "-2: 2-3-32: 0.189 (0.189)\n",
      "-2: 2-4-32: 0.189 (0.189)\n",
      "-2: 3-2-32: 0.189 (0.189)\n",
      "-2: 4-2-32: 0.189 (0.189)\n",
      "-2: 3-3-32: 0.189 (0.189)\n",
      "-2: 4-3-32: 0.189 (0.189)\n",
      "-2: 2-2-64: 0.189 (0.189)\n",
      "-2: 2-3-64: 0.189 (0.189)\n",
      "-2: 2-4-64: 0.189 (0.189)\n",
      "-2: 3-2-64: 0.189 (0.189)\n",
      "-2: 4-2-64: 0.189 (0.189)\n",
      "-2: 3-3-64: 0.189 (0.189)\n",
      "-2: 4-3-64: 0.189 (0.189)\n",
      "-2: 2-2-100: 0.189 (0.189)\n",
      "-2: 2-3-100: 0.189 (0.189)\n",
      "-2: 2-4-100: 0.189 (0.189)\n",
      "-2: 3-2-100: 0.189 (0.189)\n",
      "-2: 4-2-100: 0.189 (0.189)\n",
      "-2: 3-3-100: 0.189 (0.189)\n",
      "-2: 4-3-100: 0.189 (0.189)\n",
      "-3: 2-2-8: 0.195 (0.195)\n",
      "-3: 2-3-8: 0.195 (0.195)\n",
      "-3: 2-4-8: 0.195 (0.195)\n",
      "-3: 3-2-8: 0.195 (0.195)\n",
      "-3: 4-2-8: 0.195 (0.195)\n",
      "-3: 3-3-8: 0.195 (0.195)\n",
      "-3: 4-3-8: 0.195 (0.195)\n",
      "-3: 2-2-16: 0.195 (0.195)\n",
      "-3: 2-3-16: 0.195 (0.195)\n",
      "-3: 2-4-16: 0.195 (0.195)\n",
      "-3: 3-2-16: 0.195 (0.195)\n",
      "-3: 4-2-16: 0.195 (0.195)\n",
      "-3: 3-3-16: 0.195 (0.195)\n",
      "-3: 4-3-16: 0.195 (0.195)\n",
      "-3: 2-2-32: 0.195 (0.195)\n",
      "-3: 2-3-32: 0.195 (0.195)\n",
      "-3: 2-4-32: 0.195 (0.195)\n",
      "-3: 3-2-32: 0.195 (0.195)\n",
      "-3: 4-2-32: 0.195 (0.195)\n",
      "-3: 3-3-32: 0.195 (0.195)\n",
      "-3: 4-3-32: 0.195 (0.195)\n",
      "-3: 2-2-64: 0.195 (0.195)\n",
      "-3: 2-3-64: 0.195 (0.195)\n",
      "-3: 2-4-64: 0.195 (0.195)\n",
      "-3: 3-2-64: 0.195 (0.195)\n",
      "-3: 4-2-64: 0.195 (0.195)\n",
      "-3: 3-3-64: 0.195 (0.195)\n",
      "-3: 4-3-64: 0.195 (0.195)\n",
      "-3: 2-2-100: 0.195 (0.195)\n",
      "-3: 2-3-100: 0.195 (0.195)\n",
      "-3: 2-4-100: 0.195 (0.195)\n",
      "-3: 3-2-100: 0.195 (0.195)\n",
      "-3: 4-2-100: 0.195 (0.195)\n",
      "-3: 3-3-100: 0.195 (0.195)\n",
      "-3: 4-3-100: 0.195 (0.195)\n",
      "-4: 2-2-8: 0.211 (0.211)\n",
      "-4: 2-3-8: 0.211 (0.211)\n",
      "-4: 2-4-8: 0.211 (0.211)\n",
      "-4: 3-2-8: 0.211 (0.211)\n",
      "-4: 4-2-8: 0.211 (0.211)\n",
      "-4: 3-3-8: 0.211 (0.211)\n",
      "-4: 4-3-8: 0.211 (0.211)\n",
      "-4: 2-2-16: 0.211 (0.211)\n",
      "-4: 2-3-16: 0.211 (0.211)\n",
      "-4: 2-4-16: 0.211 (0.211)\n",
      "-4: 3-2-16: 0.211 (0.211)\n",
      "-4: 4-2-16: 0.211 (0.211)\n",
      "-4: 3-3-16: 0.211 (0.211)\n",
      "-4: 4-3-16: 0.211 (0.211)\n",
      "-4: 2-2-32: 0.211 (0.211)\n",
      "-4: 2-3-32: 0.211 (0.211)\n",
      "-4: 2-4-32: 0.211 (0.211)\n",
      "-4: 3-2-32: 0.211 (0.211)\n",
      "-4: 4-2-32: 0.211 (0.211)\n",
      "-4: 3-3-32: 0.211 (0.211)\n",
      "-4: 4-3-32: 0.211 (0.211)\n",
      "-4: 2-2-64: 0.211 (0.211)\n",
      "-4: 2-3-64: 0.211 (0.211)\n",
      "-4: 2-4-64: 0.211 (0.211)\n",
      "-4: 3-2-64: 0.211 (0.211)\n",
      "-4: 4-2-64: 0.211 (0.211)\n",
      "-4: 3-3-64: 0.211 (0.211)\n",
      "-4: 4-3-64: 0.211 (0.211)\n",
      "-4: 2-2-100: 0.211 (0.211)\n",
      "-4: 2-3-100: 0.211 (0.211)\n",
      "-4: 2-4-100: 0.211 (0.211)\n",
      "-4: 3-2-100: 0.211 (0.211)\n",
      "-4: 4-2-100: 0.211 (0.211)\n",
      "-4: 3-3-100: 0.211 (0.211)\n",
      "-4: 4-3-100: 0.211 (0.211)\n",
      "-5: 2-2-8: 0.235 (0.235)\n",
      "-5: 2-3-8: 0.235 (0.235)\n",
      "-5: 2-4-8: 0.235 (0.235)\n",
      "-5: 3-2-8: 0.235 (0.235)\n",
      "-5: 4-2-8: 0.235 (0.235)\n",
      "-5: 3-3-8: 0.235 (0.235)\n",
      "-5: 4-3-8: 0.235 (0.235)\n",
      "-5: 2-2-16: 0.235 (0.235)\n",
      "-5: 2-3-16: 0.235 (0.235)\n",
      "-5: 2-4-16: 0.235 (0.235)\n",
      "-5: 3-2-16: 0.235 (0.235)\n",
      "-5: 4-2-16: 0.235 (0.235)\n",
      "-5: 3-3-16: 0.235 (0.235)\n",
      "-5: 4-3-16: 0.235 (0.235)\n",
      "-5: 2-2-32: 0.235 (0.235)\n",
      "-5: 2-3-32: 0.235 (0.235)\n",
      "-5: 2-4-32: 0.235 (0.235)\n",
      "-5: 3-2-32: 0.235 (0.235)\n",
      "-5: 4-2-32: 0.235 (0.235)\n",
      "-5: 3-3-32: 0.235 (0.235)\n",
      "-5: 4-3-32: 0.235 (0.235)\n",
      "-5: 2-2-64: 0.235 (0.235)\n",
      "-5: 2-3-64: 0.235 (0.235)\n",
      "-5: 2-4-64: 0.235 (0.235)\n",
      "-5: 3-2-64: 0.235 (0.235)\n",
      "-5: 4-2-64: 0.235 (0.235)\n",
      "-5: 3-3-64: 0.235 (0.235)\n",
      "-5: 4-3-64: 0.235 (0.235)\n",
      "-5: 2-2-100: 0.235 (0.235)\n",
      "-5: 2-3-100: 0.235 (0.235)\n",
      "-5: 2-4-100: 0.235 (0.235)\n",
      "-5: 3-2-100: 0.235 (0.235)\n",
      "-5: 4-2-100: 0.235 (0.235)\n",
      "-5: 3-3-100: 0.235 (0.235)\n",
      "-5: 4-3-100: 0.235 (0.235)\n",
      "-6: 2-2-8: 0.182 (0.182)\n",
      "-6: 2-3-8: 0.182 (0.182)\n",
      "-6: 2-4-8: 0.182 (0.182)\n",
      "-6: 3-2-8: 0.182 (0.182)\n",
      "-6: 4-2-8: 0.182 (0.182)\n",
      "-6: 3-3-8: 0.182 (0.182)\n",
      "-6: 4-3-8: 0.182 (0.182)\n",
      "-6: 2-2-16: 0.182 (0.182)\n",
      "-6: 2-3-16: 0.182 (0.182)\n",
      "-6: 2-4-16: 0.182 (0.182)\n",
      "-6: 3-2-16: 0.182 (0.182)\n",
      "-6: 4-2-16: 0.182 (0.182)\n",
      "-6: 3-3-16: 0.182 (0.182)\n",
      "-6: 4-3-16: 0.182 (0.182)\n",
      "-6: 2-2-32: 0.182 (0.182)\n",
      "-6: 2-3-32: 0.182 (0.182)\n",
      "-6: 2-4-32: 0.182 (0.182)\n",
      "-6: 3-2-32: 0.182 (0.182)\n",
      "-6: 4-2-32: 0.182 (0.182)\n",
      "-6: 3-3-32: 0.182 (0.182)\n",
      "-6: 4-3-32: 0.182 (0.182)\n",
      "-6: 2-2-64: 0.182 (0.182)\n",
      "-6: 2-3-64: 0.182 (0.182)\n",
      "-6: 2-4-64: 0.182 (0.182)\n",
      "-6: 3-2-64: 0.182 (0.182)\n",
      "-6: 4-2-64: 0.182 (0.182)\n",
      "-6: 3-3-64: 0.182 (0.182)\n",
      "-6: 4-3-64: 0.182 (0.182)\n",
      "-6: 2-2-100: 0.182 (0.182)\n",
      "-6: 2-3-100: 0.182 (0.182)\n",
      "-6: 2-4-100: 0.182 (0.182)\n",
      "-6: 3-2-100: 0.182 (0.182)\n",
      "-6: 4-2-100: 0.182 (0.182)\n",
      "-6: 3-3-100: 0.182 (0.182)\n",
      "-6: 4-3-100: 0.182 (0.182)\n",
      "-7: 2-2-8: 0.202 (0.202)\n",
      "-7: 2-3-8: 0.202 (0.202)\n",
      "-7: 2-4-8: 0.202 (0.202)\n",
      "-7: 3-2-8: 0.202 (0.202)\n",
      "-7: 4-2-8: 0.202 (0.202)\n",
      "-7: 3-3-8: 0.202 (0.202)\n",
      "-7: 4-3-8: 0.202 (0.202)\n",
      "-7: 2-2-16: 0.202 (0.202)\n",
      "-7: 2-3-16: 0.202 (0.202)\n",
      "-7: 2-4-16: 0.202 (0.202)\n",
      "-7: 3-2-16: 0.202 (0.202)\n",
      "-7: 4-2-16: 0.202 (0.202)\n",
      "-7: 3-3-16: 0.202 (0.202)\n",
      "-7: 4-3-16: 0.202 (0.202)\n",
      "-7: 2-2-32: 0.202 (0.202)\n",
      "-7: 2-3-32: 0.202 (0.202)\n",
      "-7: 2-4-32: 0.202 (0.202)\n",
      "-7: 3-2-32: 0.202 (0.202)\n",
      "-7: 4-2-32: 0.202 (0.202)\n",
      "-7: 3-3-32: 0.202 (0.202)\n",
      "-7: 4-3-32: 0.202 (0.202)\n",
      "-7: 2-2-64: 0.202 (0.202)\n",
      "-7: 2-3-64: 0.202 (0.202)\n",
      "-7: 2-4-64: 0.202 (0.202)\n",
      "-7: 3-2-64: 0.202 (0.202)\n",
      "-7: 4-2-64: 0.202 (0.202)\n",
      "-7: 3-3-64: 0.202 (0.202)\n",
      "-7: 4-3-64: 0.202 (0.202)\n",
      "-7: 2-2-100: 0.202 (0.202)\n",
      "-7: 2-3-100: 0.202 (0.202)\n",
      "-7: 2-4-100: 0.202 (0.202)\n",
      "-7: 3-2-100: 0.202 (0.202)\n",
      "-7: 4-2-100: 0.202 (0.202)\n",
      "-7: 3-3-100: 0.202 (0.202)\n",
      "-7: 4-3-100: 0.202 (0.202)\n",
      "-8: 2-2-8: 0.186 (0.186)\n",
      "-8: 2-3-8: 0.186 (0.186)\n",
      "-8: 2-4-8: 0.186 (0.186)\n",
      "-8: 3-2-8: 0.186 (0.186)\n",
      "-8: 4-2-8: 0.186 (0.186)\n",
      "-8: 3-3-8: 0.186 (0.186)\n",
      "-8: 4-3-8: 0.186 (0.186)\n",
      "-8: 2-2-16: 0.186 (0.186)\n",
      "-8: 2-3-16: 0.186 (0.186)\n",
      "-8: 2-4-16: 0.186 (0.186)\n",
      "-8: 3-2-16: 0.186 (0.186)\n",
      "-8: 4-2-16: 0.186 (0.186)\n",
      "-8: 3-3-16: 0.186 (0.186)\n",
      "-8: 4-3-16: 0.186 (0.186)\n",
      "-8: 2-2-32: 0.186 (0.186)\n",
      "-8: 2-3-32: 0.186 (0.186)\n",
      "-8: 2-4-32: 0.186 (0.186)\n",
      "-8: 3-2-32: 0.186 (0.186)\n",
      "-8: 4-2-32: 0.186 (0.186)\n",
      "-8: 3-3-32: 0.186 (0.186)\n",
      "-8: 4-3-32: 0.186 (0.186)\n",
      "-8: 2-2-64: 0.186 (0.186)\n",
      "-8: 2-3-64: 0.186 (0.186)\n",
      "-8: 2-4-64: 0.186 (0.186)\n",
      "-8: 3-2-64: 0.186 (0.186)\n",
      "-8: 4-2-64: 0.186 (0.186)\n",
      "-8: 3-3-64: 0.186 (0.186)\n",
      "-8: 4-3-64: 0.186 (0.186)\n",
      "-8: 2-2-100: 0.186 (0.186)\n",
      "-8: 2-3-100: 0.186 (0.186)\n",
      "-8: 2-4-100: 0.186 (0.186)\n",
      "-8: 3-2-100: 0.186 (0.186)\n",
      "-8: 4-2-100: 0.186 (0.186)\n",
      "-8: 3-3-100: 0.186 (0.186)\n",
      "-8: 4-3-100: 0.186 (0.186)\n",
      "-9: 2-2-8: 0.206 (0.206)\n",
      "-9: 2-3-8: 0.206 (0.206)\n",
      "-9: 2-4-8: 0.206 (0.206)\n",
      "-9: 3-2-8: 0.206 (0.206)\n",
      "-9: 4-2-8: 0.206 (0.206)\n",
      "-9: 3-3-8: 0.206 (0.206)\n",
      "-9: 4-3-8: 0.206 (0.206)\n",
      "-9: 2-2-16: 0.206 (0.206)\n",
      "-9: 2-3-16: 0.206 (0.206)\n",
      "-9: 2-4-16: 0.206 (0.206)\n",
      "-9: 3-2-16: 0.206 (0.206)\n",
      "-9: 4-2-16: 0.206 (0.206)\n",
      "-9: 3-3-16: 0.206 (0.206)\n",
      "-9: 4-3-16: 0.206 (0.206)\n",
      "-9: 2-2-32: 0.206 (0.206)\n",
      "-9: 2-3-32: 0.206 (0.206)\n",
      "-9: 2-4-32: 0.206 (0.206)\n",
      "-9: 3-2-32: 0.206 (0.206)\n",
      "-9: 4-2-32: 0.206 (0.206)\n",
      "-9: 3-3-32: 0.206 (0.206)\n",
      "-9: 4-3-32: 0.206 (0.206)\n",
      "-9: 2-2-64: 0.206 (0.206)\n",
      "-9: 2-3-64: 0.206 (0.206)\n",
      "-9: 2-4-64: 0.206 (0.206)\n",
      "-9: 3-2-64: 0.206 (0.206)\n",
      "-9: 4-2-64: 0.206 (0.206)\n",
      "-9: 3-3-64: 0.206 (0.206)\n",
      "-9: 4-3-64: 0.206 (0.206)\n",
      "-9: 2-2-100: 0.206 (0.206)\n",
      "-9: 2-3-100: 0.206 (0.206)\n",
      "-9: 2-4-100: 0.206 (0.206)\n",
      "-9: 3-2-100: 0.206 (0.206)\n",
      "-9: 4-2-100: 0.206 (0.206)\n",
      "-9: 3-3-100: 0.206 (0.206)\n",
      "-9: 4-3-100: 0.206 (0.206)\n",
      "-10: 2-2-8: 0.171 (0.171)\n",
      "-10: 2-3-8: 0.171 (0.171)\n",
      "-10: 2-4-8: 0.171 (0.171)\n",
      "-10: 3-2-8: 0.171 (0.171)\n",
      "-10: 4-2-8: 0.171 (0.171)\n",
      "-10: 3-3-8: 0.171 (0.171)\n",
      "-10: 4-3-8: 0.171 (0.171)\n",
      "-10: 2-2-16: 0.171 (0.171)\n",
      "-10: 2-3-16: 0.171 (0.171)\n",
      "-10: 2-4-16: 0.171 (0.171)\n",
      "-10: 3-2-16: 0.171 (0.171)\n",
      "-10: 4-2-16: 0.171 (0.171)\n",
      "-10: 3-3-16: 0.171 (0.171)\n",
      "-10: 4-3-16: 0.171 (0.171)\n",
      "-10: 2-2-32: 0.171 (0.171)\n",
      "-10: 2-3-32: 0.171 (0.171)\n",
      "-10: 2-4-32: 0.171 (0.171)\n",
      "-10: 3-2-32: 0.171 (0.171)\n",
      "-10: 4-2-32: 0.171 (0.171)\n",
      "-10: 3-3-32: 0.171 (0.171)\n",
      "-10: 4-3-32: 0.171 (0.171)\n",
      "-10: 2-2-64: 0.171 (0.171)\n",
      "-10: 2-3-64: 0.171 (0.171)\n",
      "-10: 2-4-64: 0.171 (0.171)\n",
      "-10: 3-2-64: 0.171 (0.171)\n",
      "-10: 4-2-64: 0.171 (0.171)\n",
      "-10: 3-3-64: 0.171 (0.171)\n",
      "-10: 4-3-64: 0.171 (0.171)\n",
      "-10: 2-2-100: 0.171 (0.171)\n",
      "-10: 2-3-100: 0.171 (0.171)\n",
      "-10: 2-4-100: 0.171 (0.171)\n",
      "-10: 3-2-100: 0.171 (0.171)\n",
      "-10: 4-2-100: 0.171 (0.171)\n",
      "-10: 3-3-100: 0.171 (0.171)\n",
      "-10: 4-3-100: 0.171 (0.171)\n",
      "----- 4.62 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'L': 2, 'K': 2, 'hid_dim': 8},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 8},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 8},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 8},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 8},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 8},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 8},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 16},\n",
    "    \n",
    "        {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 32},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 32},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 64},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 64},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 64},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 64},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 100},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 100},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 100},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 100},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 100},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 100},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 100},\n",
    "        ]\n",
    "\n",
    "best_accs7 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs7 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, exp['hid_dim'], OUT_DIM, exp['L'], exp['K'], act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "\n",
    "        best_accs7[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs7[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}: {best_val_accs7[j,i]:.3f} ({best_accs7[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}' for exp in EXPS]\n",
    "table_over7 = summary_table(best_accs7, index_name)\n",
    "table7 = summary_table(best_val_accs7, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-8</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-8</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-8</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-8</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-8</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-8</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-8</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-16</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-16</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-16</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-16</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-32</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-32</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-32</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-32</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-32</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-64</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-64</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-64</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-64</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-64</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-64</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-64</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-100</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-100</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-100</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-100</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-100</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-100</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-100</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean accs       med       std\n",
       "2-2-8     0.195833  0.191886  0.017325\n",
       "2-3-8     0.195833  0.191886  0.017325\n",
       "2-4-8     0.195833  0.191886  0.017325\n",
       "3-2-8     0.195833  0.191886  0.017325\n",
       "4-2-8     0.195833  0.191886  0.017325\n",
       "3-3-8     0.195833  0.191886  0.017325\n",
       "4-3-8     0.195833  0.191886  0.017325\n",
       "2-2-16    0.195833  0.191886  0.017325\n",
       "2-3-16    0.195833  0.191886  0.017325\n",
       "2-4-16    0.195833  0.191886  0.017325\n",
       "3-2-16    0.195833  0.191886  0.017325\n",
       "4-2-16    0.195833  0.191886  0.017325\n",
       "3-3-16    0.195833  0.191886  0.017325\n",
       "4-3-16    0.195833  0.191886  0.017325\n",
       "2-2-32    0.195833  0.191886  0.017325\n",
       "2-3-32    0.195833  0.191886  0.017325\n",
       "2-4-32    0.195833  0.191886  0.017325\n",
       "3-2-32    0.195833  0.191886  0.017325\n",
       "4-2-32    0.195833  0.191886  0.017325\n",
       "3-3-32    0.195833  0.191886  0.017325\n",
       "4-3-32    0.195833  0.191886  0.017325\n",
       "2-2-64    0.195833  0.191886  0.017325\n",
       "2-3-64    0.195833  0.191886  0.017325\n",
       "2-4-64    0.195833  0.191886  0.017325\n",
       "3-2-64    0.195833  0.191886  0.017325\n",
       "4-2-64    0.195833  0.191886  0.017325\n",
       "3-3-64    0.195833  0.191886  0.017325\n",
       "4-3-64    0.195833  0.191886  0.017325\n",
       "2-2-100   0.195833  0.191886  0.017325\n",
       "2-3-100   0.195833  0.191886  0.017325\n",
       "2-4-100   0.195833  0.191886  0.017325\n",
       "3-2-100   0.195833  0.191886  0.017325\n",
       "4-2-100   0.195833  0.191886  0.017325\n",
       "3-3-100   0.195833  0.191886  0.017325\n",
       "4-3-100   0.195833  0.191886  0.017325"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearities and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:43: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(1/S.sum(1))\n",
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:51: RuntimeWarning: invalid value encountered in matmul\n",
      "  return D_inv_sqr @ S @ D_inv_sqr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-1: ReLU()-Softmax(dim=1)-NLLLoss(): 0.182 (0.182)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.182 (0.182)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.182 (0.182)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.182 (0.182)\n",
      "-1: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-1: Identity()-Softmax(dim=1)-NLLLoss(): 0.182 (0.182)\n",
      "-1: Identity()-Identity()-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-2: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.189 (0.189)\n",
      "-2: ReLU()-Softmax(dim=1)-NLLLoss(): 0.189 (0.189)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.189 (0.189)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.189 (0.189)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.189 (0.189)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.189 (0.189)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.189 (0.189)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.189 (0.189)\n",
      "-2: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.189 (0.189)\n",
      "-2: Identity()-Softmax(dim=1)-NLLLoss(): 0.189 (0.189)\n",
      "-2: Identity()-Identity()-CrossEntropyLoss(): 0.189 (0.189)\n",
      "-3: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.195 (0.195)\n",
      "-3: ReLU()-Softmax(dim=1)-NLLLoss(): 0.195 (0.195)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.195 (0.195)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.195 (0.195)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.195 (0.195)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.195 (0.195)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.195 (0.195)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.195 (0.195)\n",
      "-3: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.195 (0.195)\n",
      "-3: Identity()-Softmax(dim=1)-NLLLoss(): 0.195 (0.195)\n",
      "-3: Identity()-Identity()-CrossEntropyLoss(): 0.195 (0.195)\n",
      "-4: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.211 (0.211)\n",
      "-4: ReLU()-Softmax(dim=1)-NLLLoss(): 0.211 (0.211)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.211 (0.211)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.211 (0.211)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.211 (0.211)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.211 (0.211)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.211 (0.211)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.211 (0.211)\n",
      "-4: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.211 (0.211)\n",
      "-4: Identity()-Softmax(dim=1)-NLLLoss(): 0.211 (0.211)\n",
      "-4: Identity()-Identity()-CrossEntropyLoss(): 0.211 (0.211)\n",
      "-5: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.235 (0.235)\n",
      "-5: ReLU()-Softmax(dim=1)-NLLLoss(): 0.235 (0.235)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.235 (0.235)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.235 (0.235)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.235 (0.235)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.235 (0.235)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.235 (0.235)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.235 (0.235)\n",
      "-5: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.235 (0.235)\n",
      "-5: Identity()-Softmax(dim=1)-NLLLoss(): 0.235 (0.235)\n",
      "-5: Identity()-Identity()-CrossEntropyLoss(): 0.235 (0.235)\n",
      "-6: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-6: ReLU()-Softmax(dim=1)-NLLLoss(): 0.182 (0.182)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.182 (0.182)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.182 (0.182)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.182 (0.182)\n",
      "-6: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-6: Identity()-Softmax(dim=1)-NLLLoss(): 0.182 (0.182)\n",
      "-6: Identity()-Identity()-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-7: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.202 (0.202)\n",
      "-7: ReLU()-Softmax(dim=1)-NLLLoss(): 0.202 (0.202)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.202 (0.202)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.202 (0.202)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.202 (0.202)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.202 (0.202)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.202 (0.202)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.202 (0.202)\n",
      "-7: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.202 (0.202)\n",
      "-7: Identity()-Softmax(dim=1)-NLLLoss(): 0.202 (0.202)\n",
      "-7: Identity()-Identity()-CrossEntropyLoss(): 0.202 (0.202)\n",
      "-8: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.186 (0.186)\n",
      "-8: ReLU()-Softmax(dim=1)-NLLLoss(): 0.186 (0.186)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.186 (0.186)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.186 (0.186)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.186 (0.186)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.186 (0.186)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.186 (0.186)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.186 (0.186)\n",
      "-8: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.186 (0.186)\n",
      "-8: Identity()-Softmax(dim=1)-NLLLoss(): 0.186 (0.186)\n",
      "-8: Identity()-Identity()-CrossEntropyLoss(): 0.186 (0.186)\n",
      "-9: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.206 (0.206)\n",
      "-9: ReLU()-Softmax(dim=1)-NLLLoss(): 0.206 (0.206)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.206 (0.206)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.206 (0.206)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.206 (0.206)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.206 (0.206)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.206 (0.206)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.206 (0.206)\n",
      "-9: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.206 (0.206)\n",
      "-9: Identity()-Softmax(dim=1)-NLLLoss(): 0.206 (0.206)\n",
      "-9: Identity()-Identity()-CrossEntropyLoss(): 0.206 (0.206)\n",
      "-10: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.171 (0.171)\n",
      "-10: ReLU()-Softmax(dim=1)-NLLLoss(): 0.171 (0.171)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.171 (0.171)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.171 (0.171)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.171 (0.171)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.171 (0.171)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.171 (0.171)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.171 (0.171)\n",
      "-10: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.171 (0.171)\n",
      "-10: Identity()-Softmax(dim=1)-NLLLoss(): 0.171 (0.171)\n",
      "-10: Identity()-Identity()-CrossEntropyLoss(): 0.171 (0.171)\n",
      "----- 1.10 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},]\n",
    "\n",
    "best_accs8 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs8 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=exp['act'], last_act=exp['lact'],\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = NodeClassModel(arch, S, masks, exp['loss'], device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "\n",
    "        best_accs8[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs8[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}: {best_val_accs8[j,i]:.3f} ({best_accs8[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}' for exp in EXPS]\n",
    "table_over8 = summary_table(best_accs8, index_name)\n",
    "table8 = summary_table(best_val_accs8, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean accs       med  \\\n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()             0.195833  0.191886   \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                      0.195833  0.191886   \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()          0.195833  0.191886   \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                   0.195833  0.191886   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()     0.195833  0.191886   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()              0.195833  0.191886   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...   0.195833  0.191886   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...   0.195833  0.191886   \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()         0.195833  0.191886   \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                  0.195833  0.191886   \n",
       "Identity()-Identity()-CrossEntropyLoss()             0.195833  0.191886   \n",
       "\n",
       "                                                         std  \n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()            0.017325  \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                     0.017325  \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()         0.017325  \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                  0.017325  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()    0.017325  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()             0.017325  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...  0.017325  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...  0.017325  \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()        0.017325  \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                 0.017325  \n",
       "Identity()-Identity()-CrossEntropyLoss()            0.017325  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPS = [\n",
    "        {'name': 'Kipf', 'norm': 'none'},\n",
    "        {'name': 'Kipf', 'norm': 'both'},\n",
    "\n",
    "        {'name': 'A-GCNN', 'norm': False},\n",
    "        {'name': 'A-GCNN', 'norm': True},\n",
    "\n",
    "        {'name': 'H-GCNN', 'norm': False}, # This should be the same as A-GCNN not norm\n",
    "        {'name': 'H-GCNN', 'norm': True},\n",
    "\n",
    "        {'name': 'W-GCN-A', 'norm': False},\n",
    "        {'name': 'W-GCN-A', 'norm': True},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- RUN: 1\n",
      "\tKipf-none: acc = 0.325  -  acc (over) = 0.353\n",
      "\tKipf-both: acc = 0.360  -  acc (over) = 0.410\n",
      "\tA-GCNN-False: acc = 0.454  -  acc (over) = 0.467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:43: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(1/S.sum(1))\n",
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:51: RuntimeWarning: invalid value encountered in matmul\n",
      "  return D_inv_sqr @ S @ D_inv_sqr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tA-GCNN-True: acc = 0.182  -  acc (over) = 0.182\n",
      "\tH-GCNN-False: acc = 0.441  -  acc (over) = 0.463\n",
      "\tH-GCNN-True: acc = 0.425  -  acc (over) = 0.454\n",
      "\tW-GCN-A-False: acc = 0.421  -  acc (over) = 0.450\n",
      "\tW-GCN-A-True: acc = 0.182  -  acc (over) = 0.182\n",
      "- RUN: 2\n",
      "\tKipf-none: acc = 0.384  -  acc (over) = 0.384\n",
      "\tKipf-both: acc = 0.377  -  acc (over) = 0.384\n",
      "\tA-GCNN-False: acc = 0.465  -  acc (over) = 0.465\n",
      "\tA-GCNN-True: acc = 0.189  -  acc (over) = 0.189\n",
      "\tH-GCNN-False: acc = 0.436  -  acc (over) = 0.439\n",
      "\tH-GCNN-True: acc = 0.390  -  acc (over) = 0.390\n",
      "\tW-GCN-A-False: acc = 0.373  -  acc (over) = 0.482\n",
      "\tW-GCN-A-True: acc = 0.189  -  acc (over) = 0.189\n",
      "- RUN: 3\n",
      "\tKipf-none: acc = 0.366  -  acc (over) = 0.384\n",
      "\tKipf-both: acc = 0.393  -  acc (over) = 0.395\n",
      "\tA-GCNN-False: acc = 0.404  -  acc (over) = 0.430\n",
      "\tA-GCNN-True: acc = 0.195  -  acc (over) = 0.195\n",
      "\tH-GCNN-False: acc = 0.428  -  acc (over) = 0.441\n",
      "\tH-GCNN-True: acc = 0.386  -  acc (over) = 0.445\n",
      "\tW-GCN-A-False: acc = 0.377  -  acc (over) = 0.419\n",
      "\tW-GCN-A-True: acc = 0.195  -  acc (over) = 0.195\n",
      "- RUN: 4\n",
      "\tKipf-none: acc = 0.357  -  acc (over) = 0.368\n",
      "\tKipf-both: acc = 0.355  -  acc (over) = 0.362\n",
      "\tA-GCNN-False: acc = 0.434  -  acc (over) = 0.456\n",
      "\tA-GCNN-True: acc = 0.211  -  acc (over) = 0.211\n",
      "\tH-GCNN-False: acc = 0.441  -  acc (over) = 0.445\n",
      "\tH-GCNN-True: acc = 0.428  -  acc (over) = 0.441\n",
      "\tW-GCN-A-False: acc = 0.469  -  acc (over) = 0.485\n",
      "\tW-GCN-A-True: acc = 0.211  -  acc (over) = 0.211\n",
      "- RUN: 5\n",
      "\tKipf-none: acc = 0.399  -  acc (over) = 0.419\n",
      "\tKipf-both: acc = 0.340  -  acc (over) = 0.351\n",
      "\tA-GCNN-False: acc = 0.430  -  acc (over) = 0.430\n",
      "\tA-GCNN-True: acc = 0.235  -  acc (over) = 0.235\n",
      "\tH-GCNN-False: acc = 0.419  -  acc (over) = 0.425\n",
      "\tH-GCNN-True: acc = 0.412  -  acc (over) = 0.439\n",
      "\tW-GCN-A-False: acc = 0.425  -  acc (over) = 0.463\n",
      "\tW-GCN-A-True: acc = 0.235  -  acc (over) = 0.235\n",
      "- RUN: 6\n",
      "\tKipf-none: acc = 0.331  -  acc (over) = 0.349\n",
      "\tKipf-both: acc = 0.362  -  acc (over) = 0.371\n",
      "\tA-GCNN-False: acc = 0.441  -  acc (over) = 0.441\n",
      "\tA-GCNN-True: acc = 0.182  -  acc (over) = 0.182\n",
      "\tH-GCNN-False: acc = 0.428  -  acc (over) = 0.450\n",
      "\tH-GCNN-True: acc = 0.384  -  acc (over) = 0.393\n",
      "\tW-GCN-A-False: acc = 0.441  -  acc (over) = 0.461\n",
      "\tW-GCN-A-True: acc = 0.182  -  acc (over) = 0.182\n",
      "- RUN: 7\n",
      "\tKipf-none: acc = 0.382  -  acc (over) = 0.386\n",
      "\tKipf-both: acc = 0.322  -  acc (over) = 0.338\n",
      "\tA-GCNN-False: acc = 0.421  -  acc (over) = 0.465\n",
      "\tA-GCNN-True: acc = 0.202  -  acc (over) = 0.202\n",
      "\tH-GCNN-False: acc = 0.401  -  acc (over) = 0.436\n",
      "\tH-GCNN-True: acc = 0.399  -  acc (over) = 0.412\n",
      "\tW-GCN-A-False: acc = 0.423  -  acc (over) = 0.467\n",
      "\tW-GCN-A-True: acc = 0.202  -  acc (over) = 0.202\n",
      "- RUN: 8\n",
      "\tKipf-none: acc = 0.344  -  acc (over) = 0.366\n",
      "\tKipf-both: acc = 0.336  -  acc (over) = 0.342\n",
      "\tA-GCNN-False: acc = 0.450  -  acc (over) = 0.450\n",
      "\tA-GCNN-True: acc = 0.186  -  acc (over) = 0.186\n",
      "\tH-GCNN-False: acc = 0.364  -  acc (over) = 0.428\n",
      "\tH-GCNN-True: acc = 0.384  -  acc (over) = 0.412\n",
      "\tW-GCN-A-False: acc = 0.452  -  acc (over) = 0.476\n",
      "\tW-GCN-A-True: acc = 0.186  -  acc (over) = 0.186\n",
      "- RUN: 9\n",
      "\tKipf-none: acc = 0.368  -  acc (over) = 0.375\n",
      "\tKipf-both: acc = 0.309  -  acc (over) = 0.314\n",
      "\tA-GCNN-False: acc = 0.439  -  acc (over) = 0.465\n",
      "\tA-GCNN-True: acc = 0.206  -  acc (over) = 0.206\n",
      "\tH-GCNN-False: acc = 0.443  -  acc (over) = 0.452\n",
      "\tH-GCNN-True: acc = 0.439  -  acc (over) = 0.461\n",
      "\tW-GCN-A-False: acc = 0.432  -  acc (over) = 0.456\n",
      "\tW-GCN-A-True: acc = 0.206  -  acc (over) = 0.206\n",
      "- RUN: 10\n",
      "\tKipf-none: acc = 0.377  -  acc (over) = 0.377\n",
      "\tKipf-both: acc = 0.368  -  acc (over) = 0.404\n",
      "\tA-GCNN-False: acc = 0.439  -  acc (over) = 0.456\n",
      "\tA-GCNN-True: acc = 0.171  -  acc (over) = 0.171\n",
      "\tH-GCNN-False: acc = 0.432  -  acc (over) = 0.450\n",
      "\tH-GCNN-True: acc = 0.401  -  acc (over) = 0.412\n",
      "\tW-GCN-A-False: acc = 0.443  -  acc (over) = 0.447\n",
      "\tW-GCN-A-True: acc = 0.171  -  acc (over) = 0.171\n",
      "- RUN: 11\n",
      "\tKipf-none: acc = 0.377  -  acc (over) = 0.388\n",
      "\tKipf-both: acc = 0.357  -  acc (over) = 0.362\n",
      "\tA-GCNN-False: acc = 0.454  -  acc (over) = 0.456\n",
      "\tA-GCNN-True: acc = 0.182  -  acc (over) = 0.182\n",
      "\tH-GCNN-False: acc = 0.458  -  acc (over) = 0.471\n",
      "\tH-GCNN-True: acc = 0.434  -  acc (over) = 0.447\n",
      "\tW-GCN-A-False: acc = 0.443  -  acc (over) = 0.458\n",
      "\tW-GCN-A-True: acc = 0.182  -  acc (over) = 0.182\n",
      "- RUN: 12\n",
      "\tKipf-none: acc = 0.360  -  acc (over) = 0.368\n",
      "\tKipf-both: acc = 0.377  -  acc (over) = 0.382\n",
      "\tA-GCNN-False: acc = 0.432  -  acc (over) = 0.474\n",
      "\tA-GCNN-True: acc = 0.189  -  acc (over) = 0.189\n",
      "\tH-GCNN-False: acc = 0.404  -  acc (over) = 0.441\n",
      "\tH-GCNN-True: acc = 0.395  -  acc (over) = 0.414\n",
      "\tW-GCN-A-False: acc = 0.399  -  acc (over) = 0.399\n",
      "\tW-GCN-A-True: acc = 0.189  -  acc (over) = 0.189\n",
      "- RUN: 13\n",
      "\tKipf-none: acc = 0.316  -  acc (over) = 0.336\n",
      "\tKipf-both: acc = 0.393  -  acc (over) = 0.395\n",
      "\tA-GCNN-False: acc = 0.388  -  acc (over) = 0.406\n",
      "\tA-GCNN-True: acc = 0.195  -  acc (over) = 0.195\n",
      "\tH-GCNN-False: acc = 0.428  -  acc (over) = 0.445\n",
      "\tH-GCNN-True: acc = 0.368  -  acc (over) = 0.395\n",
      "\tW-GCN-A-False: acc = 0.373  -  acc (over) = 0.447\n",
      "\tW-GCN-A-True: acc = 0.195  -  acc (over) = 0.195\n",
      "- RUN: 14\n",
      "\tKipf-none: acc = 0.331  -  acc (over) = 0.364\n",
      "\tKipf-both: acc = 0.360  -  acc (over) = 0.406\n",
      "\tA-GCNN-False: acc = 0.454  -  acc (over) = 0.465\n",
      "\tA-GCNN-True: acc = 0.211  -  acc (over) = 0.211\n",
      "\tH-GCNN-False: acc = 0.452  -  acc (over) = 0.458\n",
      "\tH-GCNN-True: acc = 0.441  -  acc (over) = 0.463\n",
      "\tW-GCN-A-False: acc = 0.461  -  acc (over) = 0.485\n",
      "\tW-GCN-A-True: acc = 0.211  -  acc (over) = 0.211\n",
      "- RUN: 15\n",
      "\tKipf-none: acc = 0.379  -  acc (over) = 0.384\n",
      "\tKipf-both: acc = 0.344  -  acc (over) = 0.357\n",
      "\tA-GCNN-False: acc = 0.406  -  acc (over) = 0.423\n",
      "\tA-GCNN-True: acc = 0.235  -  acc (over) = 0.235\n",
      "\tH-GCNN-False: acc = 0.423  -  acc (over) = 0.428\n",
      "\tH-GCNN-True: acc = 0.408  -  acc (over) = 0.421\n",
      "\tW-GCN-A-False: acc = 0.434  -  acc (over) = 0.456\n",
      "\tW-GCN-A-True: acc = 0.235  -  acc (over) = 0.235\n",
      "- RUN: 16\n",
      "\tKipf-none: acc = 0.397  -  acc (over) = 0.397\n",
      "\tKipf-both: acc = 0.360  -  acc (over) = 0.375\n",
      "\tA-GCNN-False: acc = 0.423  -  acc (over) = 0.428\n",
      "\tA-GCNN-True: acc = 0.182  -  acc (over) = 0.182\n",
      "\tH-GCNN-False: acc = 0.441  -  acc (over) = 0.445\n",
      "\tH-GCNN-True: acc = 0.393  -  acc (over) = 0.408\n",
      "\tW-GCN-A-False: acc = 0.434  -  acc (over) = 0.467\n",
      "\tW-GCN-A-True: acc = 0.182  -  acc (over) = 0.182\n",
      "- RUN: 17\n",
      "\tKipf-none: acc = 0.386  -  acc (over) = 0.406\n",
      "\tKipf-both: acc = 0.329  -  acc (over) = 0.338\n",
      "\tA-GCNN-False: acc = 0.421  -  acc (over) = 0.434\n",
      "\tA-GCNN-True: acc = 0.202  -  acc (over) = 0.202\n",
      "\tH-GCNN-False: acc = 0.439  -  acc (over) = 0.452\n",
      "\tH-GCNN-True: acc = 0.410  -  acc (over) = 0.430\n",
      "\tW-GCN-A-False: acc = 0.428  -  acc (over) = 0.452\n",
      "\tW-GCN-A-True: acc = 0.202  -  acc (over) = 0.202\n",
      "- RUN: 18\n",
      "\tKipf-none: acc = 0.373  -  acc (over) = 0.382\n",
      "\tKipf-both: acc = 0.336  -  acc (over) = 0.338\n",
      "\tA-GCNN-False: acc = 0.397  -  acc (over) = 0.408\n",
      "\tA-GCNN-True: acc = 0.186  -  acc (over) = 0.186\n",
      "\tH-GCNN-False: acc = 0.443  -  acc (over) = 0.456\n",
      "\tH-GCNN-True: acc = 0.368  -  acc (over) = 0.395\n",
      "\tW-GCN-A-False: acc = 0.465  -  acc (over) = 0.465\n",
      "\tW-GCN-A-True: acc = 0.186  -  acc (over) = 0.186\n",
      "- RUN: 19\n",
      "\tKipf-none: acc = 0.338  -  acc (over) = 0.344\n",
      "\tKipf-both: acc = 0.314  -  acc (over) = 0.395\n",
      "\tA-GCNN-False: acc = 0.465  -  acc (over) = 0.487\n",
      "\tA-GCNN-True: acc = 0.206  -  acc (over) = 0.206\n",
      "\tH-GCNN-False: acc = 0.463  -  acc (over) = 0.463\n",
      "\tH-GCNN-True: acc = 0.417  -  acc (over) = 0.441\n",
      "\tW-GCN-A-False: acc = 0.417  -  acc (over) = 0.445\n",
      "\tW-GCN-A-True: acc = 0.206  -  acc (over) = 0.206\n",
      "- RUN: 20\n",
      "\tKipf-none: acc = 0.296  -  acc (over) = 0.314\n",
      "\tKipf-both: acc = 0.373  -  acc (over) = 0.393\n",
      "\tA-GCNN-False: acc = 0.436  -  acc (over) = 0.454\n",
      "\tA-GCNN-True: acc = 0.171  -  acc (over) = 0.171\n",
      "\tH-GCNN-False: acc = 0.436  -  acc (over) = 0.463\n",
      "\tH-GCNN-True: acc = 0.425  -  acc (over) = 0.436\n",
      "\tW-GCN-A-False: acc = 0.428  -  acc (over) = 0.454\n",
      "\tW-GCN-A-True: acc = 0.171  -  acc (over) = 0.171\n"
     ]
    }
   ],
   "source": [
    "N_RUNS = 20\n",
    "\n",
    "best_accs = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs2 = np.zeros((len(EXPS), N_RUNS))\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    print(f'- RUN: {i+1}')\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        # t_i = time.time()\n",
    "        if exp['name'] == 'Kipf':\n",
    "            arch = GCNN_2L(IN_DIM, HID_DIM, OUT_DIM, act=ACT, last_act=LAST_ACT,\n",
    "                           dropout=DROPOUT, norm=exp['norm'])\n",
    "            S = dgl.from_networkx(nx.from_numpy_array(A)).add_self_loop().to(device)\n",
    "            \n",
    "        elif exp['name'] == 'A-GCNN':\n",
    "            arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "            dropout=DROPOUT, diff_layer=GFGCNLayer, init_h0=h0)\n",
    "            if exp['norm']:\n",
    "                S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "            else:\n",
    "                S = torch.Tensor(A).to(device)\n",
    "\n",
    "        elif exp['name'] == 'H-GCNN':\n",
    "            arch = GFGCN_Spows(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                               dropout=DROPOUT, norm=exp['norm'], dev=device)\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        elif exp['name'] == 'W-GCN-A':\n",
    "            arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                         dropout=DROPOUT, diff_layer=GFGCN_noh_Layer)\n",
    "            if exp['norm']:\n",
    "                S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "            else:\n",
    "                S = torch.Tensor(A).to(device)  \n",
    "            \n",
    "        else:\n",
    "            raise Exception(f'ERROR: Unknown architecture: {exp[\"name\"]}')\n",
    "\n",
    "        if exp['name'] in ['Kipf', 'W-GCN-A']:\n",
    "            model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "        else:\n",
    "            model = GF_NodeClassModel(arch, S, K, masks, LOSS_FN, device=device)\n",
    "\n",
    "        loss, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "\n",
    "        best_accs[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs[j,i] = model.test(feat, model.S, labels, masks['test'])\n",
    "        best_val_accs2[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'\\t{exp[\"name\"]}-{exp[\"norm\"]}: acc = {best_val_accs[j,i]:.3f}  -  acc (over) = {best_accs[j,i]:.3f}')\n",
    "\n",
    "\n",
    "# Print results\n",
    "index_name = [f'{exp[\"name\"]}-{exp[\"norm\"]}' for exp in EXPS]\n",
    "table_comp_over = summary_table(best_accs, index_name)\n",
    "table_comp = summary_table(best_val_accs, index_name)\n",
    "table_comp2 = summary_table(best_val_accs2, index_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kipf-none</th>\n",
       "      <td>0.359320</td>\n",
       "      <td>0.367325</td>\n",
       "      <td>0.027880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kipf-both</th>\n",
       "      <td>0.353180</td>\n",
       "      <td>0.358553</td>\n",
       "      <td>0.023461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-False</th>\n",
       "      <td>0.432566</td>\n",
       "      <td>0.435307</td>\n",
       "      <td>0.021306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-True</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-False</th>\n",
       "      <td>0.430921</td>\n",
       "      <td>0.436404</td>\n",
       "      <td>0.021492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-True</th>\n",
       "      <td>0.405373</td>\n",
       "      <td>0.404605</td>\n",
       "      <td>0.021557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-False</th>\n",
       "      <td>0.426864</td>\n",
       "      <td>0.429825</td>\n",
       "      <td>0.027463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-True</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean accs       med       std\n",
       "Kipf-none       0.359320  0.367325  0.027880\n",
       "Kipf-both       0.353180  0.358553  0.023461\n",
       "A-GCNN-False    0.432566  0.435307  0.021306\n",
       "A-GCNN-True     0.195833  0.191886  0.017325\n",
       "H-GCNN-False    0.430921  0.436404  0.021492\n",
       "H-GCNN-True     0.405373  0.404605  0.021557\n",
       "W-GCN-A-False   0.426864  0.429825  0.027463\n",
       "W-GCN-A-True    0.195833  0.191886  0.017325"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kipf-none</th>\n",
       "      <td>0.359320</td>\n",
       "      <td>0.365132</td>\n",
       "      <td>0.028026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kipf-both</th>\n",
       "      <td>0.360307</td>\n",
       "      <td>0.358553</td>\n",
       "      <td>0.022160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-False</th>\n",
       "      <td>0.423136</td>\n",
       "      <td>0.425439</td>\n",
       "      <td>0.021369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-True</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-False</th>\n",
       "      <td>0.426425</td>\n",
       "      <td>0.424342</td>\n",
       "      <td>0.020212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-True</th>\n",
       "      <td>0.410746</td>\n",
       "      <td>0.406798</td>\n",
       "      <td>0.028357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-False</th>\n",
       "      <td>0.443531</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.021101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-True</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean accs       med       std\n",
       "Kipf-none       0.359320  0.365132  0.028026\n",
       "Kipf-both       0.360307  0.358553  0.022160\n",
       "A-GCNN-False    0.423136  0.425439  0.021369\n",
       "A-GCNN-True     0.195833  0.191886  0.017325\n",
       "H-GCNN-False    0.426425  0.424342  0.020212\n",
       "H-GCNN-True     0.410746  0.406798  0.028357\n",
       "W-GCN-A-False   0.443531  0.447368  0.021101\n",
       "W-GCN-A-True    0.195833  0.191886  0.017325"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_comp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kipf-none</th>\n",
       "      <td>0.372149</td>\n",
       "      <td>0.376096</td>\n",
       "      <td>0.023964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kipf-both</th>\n",
       "      <td>0.370395</td>\n",
       "      <td>0.372807</td>\n",
       "      <td>0.026866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-False</th>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.455044</td>\n",
       "      <td>0.021497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-True</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-False</th>\n",
       "      <td>0.447478</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.012434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-True</th>\n",
       "      <td>0.425439</td>\n",
       "      <td>0.425439</td>\n",
       "      <td>0.022642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-False</th>\n",
       "      <td>0.456689</td>\n",
       "      <td>0.457237</td>\n",
       "      <td>0.020086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-True</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean accs       med       std\n",
       "Kipf-none       0.372149  0.376096  0.023964\n",
       "Kipf-both       0.370395  0.372807  0.026866\n",
       "A-GCNN-False    0.447917  0.455044  0.021497\n",
       "A-GCNN-True     0.195833  0.191886  0.017325\n",
       "H-GCNN-False    0.447478  0.447368  0.012434\n",
       "H-GCNN-True     0.425439  0.425439  0.022642\n",
       "W-GCN-A-False   0.456689  0.457237  0.020086\n",
       "W-GCN-A-True    0.195833  0.191886  0.017325"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_comp_over"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
