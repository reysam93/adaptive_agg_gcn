{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f08d56127d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import sys\n",
    "import os\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from src import utils\n",
    "from src.baselines_models import NodeClassModel, GF_NodeClassModel\n",
    "from src.data import normalize_gso\n",
    "from src.arch import GFGCN, GFGCNLayer, GFGCN_noh_Layer, GFGCN_Spows\n",
    "\n",
    "SEED = 15\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "def summary_table(acc, index_name):\n",
    "    mean_accs = acc.mean(axis=1)\n",
    "    med_accs = np.median(acc, axis=1)\n",
    "    std_accs = acc.std(axis=1)\n",
    "    return DataFrame(np.vstack((mean_accs, med_accs, std_accs)).T, columns=['mean accs', 'med', 'std'], index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: ChameleonDataset\n",
      "Number of nodes: 2277\n",
      "Number of features: 2325\n",
      "Shape of signals: torch.Size([2277, 2325])\n",
      "Number of classes: 5\n",
      "Norm of A: 190.00262451171875\n",
      "Max value of A: 1.0\n",
      "Proportion of validation data: 0.32\n",
      "Proportion of test data: 0.20\n",
      "Node homophily: 0.25\n",
      "Edge homophily: 0.24\n"
     ]
    }
   ],
   "source": [
    "# Dataset must be from DGL\n",
    "dataset_name = 'ChameleonDataset'\n",
    "\n",
    "A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device,\n",
    "                                                     verb=True)\n",
    "N = A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without normalizing the GSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PARAMETERS  - 0.61\n",
    "## Reaining params\n",
    "N_RUNS = 20\n",
    "N_EPOCHS = 6000  # 500\n",
    "LR = .001  # .01\n",
    "WD = .01  # .005\n",
    "DROPOUT = .5\n",
    "\n",
    "# BEST PARAMETERS\n",
    "## Architecture params\n",
    "N_LAYERS = 2\n",
    "K = 2  # 2\n",
    "HID_DIM = 64 # 8\n",
    "\n",
    "## Model params\n",
    "h0 = 1  # 1\n",
    "NORM = False\n",
    "\n",
    "IN_DIM = feat.shape[1]\n",
    "OUT_DIM = n_class\n",
    "\n",
    "ACT = nn.ReLU()\n",
    "LAST_ACT = nn.Identity(dim=1)\n",
    "LOSS_FN = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting eval/test acc/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc at best val: 0.465  -  Best test acc: 0.531\n",
      "Test acc (based on loss): 0.474\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = N_EPOCHS\n",
    "lr = LR\n",
    "wd = WD\n",
    "drop = DROPOUT\n",
    "L = N_LAYERS\n",
    "K_aux = K\n",
    "hid_dim = HID_DIM\n",
    "norm = False\n",
    "act = ACT\n",
    "lact = LAST_ACT\n",
    "loss = LOSS_FN\n",
    "patience = 6000 #300\n",
    "\n",
    "iters_aux = 1\n",
    "\n",
    "err1 = np.zeros(iters_aux)\n",
    "err2 = np.zeros(iters_aux)\n",
    "for i in range(iters_aux):\n",
    "    # Create model\n",
    "    arch = GFGCN(IN_DIM, hid_dim, OUT_DIM, L, K_aux, act=act, l_act=lact,\n",
    "                dropout=drop, diff_layer=GFGCN_noh_Layer)\n",
    "\n",
    "    S = torch.Tensor(A).to(device)\n",
    "    model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "    loss, acc = model.train(feat, labels, epochs, lr, wd, patience=patience)\n",
    "\n",
    "    idx_max_acc = np.argmax(acc[\"val\"])\n",
    "    print(f'Test acc at best val: {acc[\"test\"][idx_max_acc]:.3f}  -  Best test acc: {np.max(acc[\"test\"]):.3f}')\n",
    "    acc_val = model.test(feat, model.S, labels, masks['val'])\n",
    "    acc_test = model.test(feat, model.S, labels, masks['test'])\n",
    "    print(f'Test acc (based on loss): {acc_test:.3f}')\n",
    "\n",
    "    err1[i] = acc[\"test\"][idx_max_acc]\n",
    "    err2[i] = acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc at best val acc: 0.465 +- 0.000\n",
      "Acc at test: 0.474 +- 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f0782546380>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApsAAAH5CAYAAADORvWoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABx40lEQVR4nO3dd3gUVcMF8LPphRQIgQQIEJESIPQeUYRAAGkWVMRPQMRCE6MIKN0CoiACKpZXUGlWEBWQGAid0KWHAKEn9HSSbLL3++OSLcmGJGRmJ+X8nmef7M5OZu7enZ05c2fmjk4IIUBEREREpAI7rQtAREREROUXwyYRERERqYZhk4iIiIhUw7BJRERERKph2CQiIiIi1TBsEhEREZFqGDaJiIiISDUOWhcgL4PBgCtXrsDDwwM6nU7r4hARERFRHkIIpKSkoEaNGrCzu3fbZakLm1euXEFAQIDWxSAiIiKiQly8eBG1atW65zilLmx6eHgAkIX39PS0yTz1ej02btyIHj16wNHR0SbzrOhY59pgvWuD9a4N1rs2WO/asHW9JycnIyAgwJjb7qXUhc3cQ+eenp42DZtubm7w9PTkD8NGWOfaYL1rg/WuDda7Nljv2tCq3otyyiMvECIiIiIi1TBsEhEREZFqGDaJiIiISDWl7pxNIiIiKtt0Oh0yMzORk5OjdVEqDL1eDwcHB2RkZChW705OToV2a1QUDJtERESkCCEErl69Cn9/f1y4cIH9ZduQEAJ+fn64ePGiYvVuZ2eHwMBAODk5lWg6DJtERESkiISEBCQnJ8PPzw9VqlSBvb291kWqMAwGA1JTU1GpUiVFWiNzb7ITHx+P2rVrlyjAMmwSERFRieXk5CAxMRG+vr5wdHSEq6urIqGHisZgMCArKwsuLi6K1buvry+uXLmC7OzsEnWnxKWAiIiISkyv1wMA3NzcNC4JKSX38HlJzwFl2CQiIiLF8DzN8kOxcz8VmQoRERERkRUMm0RERESkGoZNIiIionIoKioKOp0OiYmJmpaDYZOIiIgqtKFDh2LAgAFaFwPnzp2DTqfDoUOHFJlep06dEB8fDy8vL0Wmd78YNomIiIjKkKysrCKN5+TkBD8/P80v2mLYJCIiIlUIAaSlafMQQrnPsWXLFrRr1w7Ozs7w9/fHxIkTkZ2dbXz/119/RXBwMFxdXeHj44PQ0FCkpaUBkIey27VrB3d3d3h7eyMkJATnz5+3Op/AwEAAQMuWLaHT6dClSxcAppbXDz74ADVq1EDDhg0BAD/++CPatGkDDw8P1KhRAy+99BKuXbtmnF7ew+hLly6Ft7c3/vnnHwQFBaFSpUro2bMn4uPjlassK9ipOxEREakiPR2oVEmbeaemAu7uJZ/O5cuX0bt3bwwdOhQ//PADTp48iREjRsDFxQXTp09HfHw8Bg0ahDlz5uDxxx9HSkoKtm3bBiEEsrOzMWDAAIwYMQIrV65EVlYW9uzZU2BL4549e9CuXTv8+++/aNKkicVtIiMjI+Hp6YmIiAjjML1ej/feew8NGzZEQkICxo0bh2HDhmH9+vUFfp709HR88skn+PHHH2FnZ4fnn38eb731FpYvX17yyioAwyYRERFRAb744gsEBARg0aJF0Ol0aNSoEa5cuYIJEyZg6tSpiI+PR3Z2Np544gnUqVMHABAcHAwAuHXrFpKSktCnTx/Uq1cPABAUFFTgvHx9fQEAPj4+8PPzs3jP3d0d3377rUUAffHFF43P69ati48++ghdu3Y13rbSGr1ej8WLFxvLM3r0aMycObO41VIsDJtXrsB+3DhUadEC6N1b69IQERGVG25usoVRq3kr4cSJE+jYsaNFa2RISAhSU1Nx6dIlNG/eHN26dUNwcDDCwsLQo0cPPPXUU6hcuTKqVKmCoUOHIiwsDN27d0doaCiefvpp+Pv7F7scwcHBFkETAPbv34/p06fjv//+w+3bt2EwGAAAFy5cQOPGja1Ox83NzRg0AcDf39/i0LsaeM7miBGw++UXdH73Xa1LQkREVK7odPJQthYPW10TY29vj4iICKxfvx6NGzfGwoUL0bBhQ8TFxQEAlixZgl27dqFTp0746aef0KBBA+zevbvY83HPc05AWloawsLC4OnpieXLlyM6Oho//vgjgHtfQJT3Huc6nQ5CyRNcrWDYPHtW6xIQERFRKRUUFIRdu3ZZBLIdO3bAw8MDtWrVAiADW0hICGbMmIGDBw/CyckJq1evNo7fsmVLTJo0CTt37kTTpk2xYsUKq/Mqzr3IT548iZs3b2L27Nno3LkzGjVqhOvXr5fko6qGh9GJiIiowktKSsrXv6WPjw9GjhyJ+fPnY8yYMRg9ejRiYmIwbdo0hIeHw87ODtHR0YiMjESPHj1QrVo1REdH4/r16wgKCkJcXBy+/vpr9OvXDzVq1EBMTAxiY2PxwgsvWC1DtWrV4Orqig0bNqBWrVpwcXEpsI/M2rVrw8nJCQsXLsSrr76Kw4cP45NPPlG6WhTBlk0iIiKq8KKiotCyZUuLx4wZM1CzZk2sW7cOe/bsQfPmzfHqq69i+PDhmDx5MgDA09MTW7duRe/evdGgQQNMnjwZc+fORa9eveDm5oaTJ0/iySefRIMGDfDyyy9j1KhReOWVV6yWwcHBAQsWLMBXX32FGjVqoH///gWW19fXF0uXLsUvv/yCxo0bY86cOapf6HO/dELtA/XFlJycDC8vLyQlJcHT01P9GQYFASdPAgD0WVn5zmUgdej1eqxbtw69e/dmndsQ610brHdtsN5tKyMjA3FxcahTpw6ysrLg6ekJOzu2admKwWBAcnKyovWe+50GBgbCxcXF4r3i5DUuBURERESkGoZNIiIiIlINwyYRERERqYZhk4iIiIhUw7BJRERERKph2CQiIiIi1TBsEhEREZFqGDaJiIiISDUMm0RERETlRFRUFHQ6HRITE7UuihHDJhEREVVoQ4cOxYABA7QuRrnFsElEREREqmHYJCIiIrqHLVu2oF27dnB2doa/vz8mTpyI7Oxs4/u//vorgoOD4erqCh8fH4SGhiItLQ2APKzdrl07uLu7w9vbGyEhITh//rzV+XTq1AkTJkywGHb9+nU4Ojpi69atAIAff/wRbdq0gYeHB/z8/PDcc8/h2rVrKn1yZThoXQAiIiIqn4QQSNenazJvN0c36HS6Ek/n8uXL6N27N4YOHYoffvgBJ0+exIgRI+Di4oLp06cjPj4egwYNwpw5c/D4448jJSUF27ZtgxAC2dnZGDBgAEaMGIGVK1ciKysLe/bsKbBcgwcPxpw5czB79mzjOD/99BNq1KiBzp07AwD0ej3ee+89NGzYENeuXUN4eDiGDh2Kv/76q8SfVS0Mm0RERKSKdH06Ks2qpMm8Uyelwt3JvcTT+eKLLxAQEIBFixZBp9OhUaNGuHLlCiZMmICpU6ciPj4e2dnZeOKJJ1CnTh0AQHBwMADg1q1bSEpKQp8+fVCvXj0AQFBQUIHzevrppzFu3Dhs377dGC5XrFiBQYMGGcPniy++aBz/gQcewIIFC9C2bVukpqaW+LOqhYfRiYiIiApw4sQJdOzY0aI1MiQkBKmpqbh06RKaN2+Obt26ITg4GAMHDsQ333yD27dvAwCqVKmCoUOHIiwsDH379sVnn32G+Pj4Aufl6+uLHj16YPny5QCAuLg47Nq1C4MHDzaOs3//fvTt2xe1a9eGh4cHHnnkEQDAhQsX1Pj4imDLJhEREanCzdENqZO0aXFzc3SzyXzs7e0RERGBnTt3YuPGjVi4cCHeffddREdHIzAwEEuWLMHYsWOxYcMG/PTTT5g8eTIiIiLQoUMHq9MbPHgwxo4di4ULF2LFihUIDg42tpSmpaUhLCwMYWFhWL58OXx9fXHhwgWEhYUhKyvLJp/3frBlk4iIiFSh0+ng7uSuyUOJ8zUBedh7165dEEIYh+3YsQMeHh6oVauW8XOGhIRgxowZOHjwIJycnLB69Wrj+C1btsSkSZOwc+dONG3aFCtWrChwfv3790dGRgY2bNiAFStWWLRqnjx5Ejdv3sTs2bPRuXNnNGrUqNRfHASwZZOIiIgISUlJOHTokMUwHx8fjBw5EvPnz8eYMWMwevRoxMTEYNq0aQgPD4ednR2io6MRGRmJHj16oFq1aoiOjsb169cRFBSEuLg4fP311+jXrx9q1KiBmJgYxMbG4oUXXiiwHO7u7hgwYACmTJmCEydOYNCgQcb3ateuDScnJyxcuBCvvvoqjh49ivfee0+tKlEMwyYRERFVeFFRUWjZsqXFsOHDh+Pbb7/FunXrMH78eDRv3hxVqlTB8OHDMXnyZACAp6cntm7divnz5yM5ORl16tTB3Llz0atXL1y9ehUnT57E999/j5s3b8Lf3x+jRo3CK6+8cs+yDB48GL1798bDDz+M2rVrG4f7+vpi6dKleOedd7BgwQK0atUKn3zyCfr166d8hSio2GFz69at+Pjjj7F//37Ex8dj9erVxl739Xo9Jk+ejHXr1uHs2bPw8vJCaGgoZs+ejRo1aihddiIiIqISW7p0KZYuXVrg+4888gj27Nlj9b2goCBs2LDB6nvVq1e3OJxeVL169bI4bG9u0KBBFq2dgOxiymAwIDk5GV26dCnwf7VS7HM209LS0Lx5c3z++ef53ktPT8eBAwcwZcoUHDhwAL///jtiYmJKfeImIiIiInUUu2WzV69e6NWrl9X3vLy8EBERYTFs0aJFaNeuHS5cuGDRFExERERE5Z/q52wmJSVBp9PB29vb6vuZmZnIzMw0vk5OTgYgD8nr9Xq1iwcHIZB7vZot5kdSbl2zzm2L9a4N1rs2WO+2pdfrIYQwHsLNPbRLtqFGvRsMBgghoNfrYW9vb/FecX5XqobNjIwMTJgwAYMGDYKnp6fVcWbNmoUZM2bkG75x40a4uanfR1bXtDR43H2et1WW1Mc61wbrXRusd22w3m3DwcEBfn5+SEtLg5OTE1JSUrQuUoWkZL1nZWXhzp072Lp1q8W94AF56mRR6UQJziLV6XQWFwiZ0+v1ePLJJ3Hp0iVERUUVGDattWwGBATgxo0bBf6PkhyCg6GLiQEApKelwdHRUfV5klw+IiIi0L17d9a5DbHetcF61wbr3bYyMjJw8eJF1KlTB3q9Hh4eHor1dUmFE0IgJSVF0XrPyMjAuXPnEBAQABcXF4v3kpOTUbVqVSQlJRWa11Rp2dTr9Xj66adx/vx5bNq06Z6FcHZ2hrOzc77hjo6Otlk5mH0hNpsnGbHOtcF61wbrXRusd9vIycmBTqczBh2dTgc7O947xlZyD50rWe92dnbQ6XRWf0PF+U0pHjZzg2ZsbCw2b94MHx8fpWdBRERERGVEscNmamoqTp8+bXwdFxeHQ4cOoUqVKvD398dTTz2FAwcO4K+//kJOTg4SEhIAyJvROzk5KVdyIiIiIir1ih029+3bh0cffdT4Ojw8HAAwZMgQTJ8+HWvXrgUAtGjRwuL/Nm/ejC5dutx/SYmIiIiozCl22CysZ/rS1ms9EREREWmHZ+4SERFRhTZ06FCrPevY2rlz56DT6XDo0CFFp6vT6bBmzRpFp1kcDJtEREREpBqGTSIiIqJ72LJlC9q1awdnZ2f4+/tj4sSJFp2c//rrrwgODoarqyt8fHwQGhqKtLQ0AEBUVBTatWsHd3d3eHt7IyQkBOfPn7c6n8DAQABAy5YtodPpLK51+fbbbxEUFAQXFxc0atQIX3zxhfG9rKwsjBkzBo0aNYKbmxvq1KmDWbNmAQDq1q0LAHj88ceh0+mMr21J9dtVEhERUQUlBFCMO80oys3Noi/t+3X58mX07t0bQ4cOxQ8//ICTJ09ixIgRcHFxwfTp0xEfH49BgwZhzpw5ePzxx5GSkoJt27ZBCIHs7GwMGDAAI0aMwMqVK5GVlYU9e/YU2On6nj170K5dO/z7779o0qSJsRef5cuXY+rUqVi0aBFatmyJgwcPYsSIEXB3d8eQIUOwYMEC/Pnnn/juu+8QFBSEy5cv4+LFiwCAvXv3olq1aliyZAl69uyZ77aTtsCwybsbEBERqSM9HahUSZt5p6YC7u4lnswXX3yBgIAALFq0CDqdDo0aNcKVK1cwYcIETJ06FfHx8cjOzsYTTzyBOnXqAACCg4MBALdu3UJSUhL69OmDevXqAQCCgoIKnJevry8AwMfHB35+fsbh06ZNw9y5c/HEE08AkC2gx48fx1dffYUhQ4bgwoULqF+/Pjp27AgvLy9jC6n5NL29vS2maUs8jM6r54mIiKgAJ06cQMeOHS1aI0NCQpCamopLly6hefPm6NatG4KDgzFw4EB88803uH37NgDZx/jQoUMRFhaGvn374rPPPkN8fHyx5p+WloYzZ85g+PDhqFSpkvHx/vvv48yZMwDkBU6HDh1C27Zt8frrr2Pjxo3KVYACGDaJiIhIHW5usoVRi4ebm00+or29PSIiIrB+/Xo0btwYCxcuRMOGDREXFwcAWLJkCXbt2oVOnTrhp59+QoMGDbB79+4iTz81NRUA8M033+DQoUPGx9GjR43TadWqFc6cOYN33nkHd+7cwdNPP42nnnpK+Q97nyr8YfTMLCD/ndmJiIioxHQ6RQ5laykoKAi//fYbhBDG1s0dO3bAw8MDtWrVAiC7FgoJCUFISAimTp2KOnXqYPXq1cYb37Rs2RItW7bEpEmT0LFjR6xYsQIdOnTIN6/cczRzcnKMw6pXr44aNWrg7NmzGDx4cIHl9PT0xBNPPIGhQ4di4MCB6NmzJ27duoUqVarA0dHRYpq2VuHDpl7PsElERFTRJSUl5evf0sfHByNHjsT8+fMxZswYjB49GjExMZg2bRrCw8NhZ2eH6OhoREZGokePHqhWrRqio6Nx/fp1BAUFIS4uDl9//TX69euHGjVqICYmBrGxsXjhhReslqFatWpwdXXFhg0bUKtWLbi4uMDLywszZszA2LFj4eXlhZ49eyIzMxP79u3D7du3ER4ejnnz5qF69eqoX78+PD098csvv8DPzw/e3t4A5BXpkZGRCAkJgbOzMypXrqxybVqq8GGTiIiIKCoqCi1btrQYNnz4cHz77bdYt24dxo8fj+bNm6NKlSoYPnw4Jk+eDEC2KG7duhXz589HcnIy6tSpg7lz56JXr164evUqTp48ie+//x43b96Ev78/Ro0ahVdeecVqGRwcHLBgwQLMnDkTU6dORefOnREVFYWXXnoJbm5u+PjjjzF+/Hi4u7sjODgY48aNAwB4eHjgk08+QWxsLOzt7dG2bVusW7cOdnbybMm5c+ciPDwc33zzDWrWrIlz586pVo/W6EQpu79kcnIyvLy8kJSUBE9PT9Xnl1o7CJUungQA6LOy4OjoqPo8CdDr9Vi3bh169+7NOrch1rs2WO/aYL3bVkZGBuLi4lCnTh1kZWXB09PTGHZIfQaDAcnJyYrWe+53GhgYCBcXF4v3ipPXuBQQERERkWoYNtnPJhEREZFqGDZL11kEREREROUKwyYRERERqabCh83UFK1LQEREVH6UsuuOqQSU+i4rfNi8nah1CYiIiMq+3Cv+09PTNS4JKSUrKwuAvEtSSbCfTSIiIioxe3t7eHt74/r16/Dw8ICjo2OJQwoVncFgQFZWFjIyMhTp+shgMOD69etwc3ODg0PJ4iLDJhERESnCz88POTk5iI+PR0pKivH2jqQ+IQTu3LkDV1dXxerdzs4OtWvXLvH0GDaJiIhIETqdDtWrV8eBAwfQtWvXEreIUdHp9Xps3boVDz/8sGI3MXByclKklbTCLwUC3OsiIiJSkhACzs7OvHOTDdnb2yM7OxsuLi6lrt4r/AVCOvCqOSIiIiK1VPiwSURERETqYdgkIiIiItUwbBIRERGRahg2iYiIiEg1DJtEREREpJoKHzbZ9RERERGReip82CQiIiIi9VT4sMl+NomIiIjUU+HDJhERERGph2GTiIiIiFTDsElEREREqmHYJCIiIiLVMGwSERERkWoqfNhkP5tERERE6qnwYRO6bK1LQERERFRuVfiwqXPI0LoIREREROVWhQ+bRERERKQehk0iIiIiUg3DJhERERGphmGTiIiIiFTDsElEREREqqnwYZP9bBIRERGpp8KHTSIiIiJST4UPmzoIrYtAREREVG5V+LBJREREROph2CQiIiIi1TBs8gIhIiIiItUwbPKcTSIiIiLVMGy6JGldAiIiIqJyq8KHTeGcrHURiIiIiMqtCh82iYiIiEg9FT5s8vIgIiIiIvVU+LBJREREROph2DQjeGE6ERERkaIYNomIiIhINcUOm1u3bkXfvn1Ro0YN6HQ6rFmzxuJ9IQSmTp0Kf39/uLq6IjQ0FLGxsUqVl4iIiIjKkGKHzbS0NDRv3hyff/651ffnzJmDBQsWYPHixYiOjoa7uzvCwsKQkZFR4sKqwfzIucHA4+hERERESnIo7j/06tULvXr1svqeEALz58/H5MmT0b9/fwDADz/8gOrVq2PNmjV49tln8/1PZmYmMjMzja+Tk2W/l3q9Hnq9vrjFKxG9Xg97e16fbgu5362tv+OKjvWuDda7Nljv2mC9a8PW9V6c+RQ7bN5LXFwcEhISEBoaahzm5eWF9u3bY9euXVbD5qxZszBjxox8wzdu3Ag3Nzcli2dVffN5RmyEgz1PY7WliIgIrYtQIbHetcF61wbrXRusd23Yqt7T09OLPK6iYTMhIQEAUL16dYvh1atXN76X16RJkxAeHm58nZycjICAAPTo0QOenp5KFs+q02bPu3ULhbubi+rzJLlHFBERge7du8PR0VHr4lQYrHdtsN61wXrXButdG7au99wj0UWhaNi8H87OznB2ds433NHR0eYLqRbzrOhY59pgvWuD9a4N1rs2WO/asFW9F2ceih4z9vPzAwBcvXrVYvjVq1eN75VmArxAiIiIiEhJiobNwMBA+Pn5ITIy0jgsOTkZ0dHR6Nixo5KzUgWvRiciIiJSVrEPo6empuL0adOZjnFxcTh06BCqVKmC2rVrY9y4cXj//fdRv359BAYGYsqUKahRowYGDBigZLmJiIiIqAwodtjct28fHn30UePr3It7hgwZgqVLl+Ltt99GWloaXn75ZSQmJuKhhx7Chg0b4OJSOi+8ERbP2bJJREREpKRih80uXbpA3OMm4jqdDjNnzsTMmTNLVDAt8DA6ERERkbLYqSQRERERqabCh83KZnfR5GF0IiIiImVV+LDpn2p6zsPoRERERMqq8GGTiIiIiNTDsGmGh9GJiIiIlMWwaeZeV9kTERERUfExbJph1CQiIiJSFsOmGbZsEhERESmLYdMMz9kkIiIiUhbDphk2bBIREREpi2HTDA+jExERESmLYdMMD6MTERERKYth0wwbNomIiIiUxbBphofRiYiIiJTFsGnmQvI5rYtAREREVK4wbJr5I/Z3rYtAREREVK4wbBIRERGRahg2zfCcTSIiIiJlMWyaYdQkIiIiUhbDphm2bBIREREpi2HTDDt1JyIiIlIWw6YZtmwSERERKYthk4iIiIhUw7BJRERERKph2DTDczaJiIiIlMWwaYanbBIREREpi2GTiIiIiFTDsGmBTZtERERESmLYNCMMDJtERERESmLYNKNj1iQiIiJSFMOmGWZNIiIiImUxbJrjYXQiIiIiRTFsEhEREZFqGDYtsGWTiIiISEkMm2bYqTsRERGRshg2zQhh0LoIREREROUKwyYRERERqYZh0wz72SQiIiJSFsOmOR5GJyIiIlIUw6YZHxdfrYtAREREVK4wbJppkV1N6yIQERERlSsMm2bqbtyudRGIiIiIyhWGTXPsaJOIiIhIUQybFhg2iYiIiJTEsGnOwLBJREREpCSGTXM8jE5ERESkKIZNIiIiIlINw6Y5dupOREREpCiGTTOCh9GJiIiIFMWwaY5hk4iIiEhRDJvmGDaJiIiIFMWwSURERESqYdg0Z+AFQkRERERKYtg0o+NhdCIiIiJFMWyaYdQkIiIiUhbDpjm2bBIREREpSvGwmZOTgylTpiAwMBCurq6oV68e3nvvvTLRh6XgvdGJiIiIFOWg9AQ/+ugjfPnll/j+++/RpEkT7Nu3D8OGDYOXlxfGjh2r9OyIiIiIqBRTPGzu3LkT/fv3x2OPPQYAqFu3LlauXIk9e/YoPSsVsGWTiIiISEmKh81OnTrh66+/xqlTp9CgQQP8999/2L59O+bNm2d1/MzMTGRmZhpfJycnAwD0ej30er3SxcvH0ey5yMmxyTwJxnpmfdsW610brHdtsN61wXrXhq3rvTjz0QmFT6Y0GAx45513MGfOHNjb2yMnJwcffPABJk2aZHX86dOnY8aMGfmGr1ixAm5ubkoWzar+AwYYn2/s1hZ3xryr+jyJiIiIyrL09HQ899xzSEpKgqen5z3HVTxsrlq1CuPHj8fHH3+MJk2a4NChQxg3bhzmzZuHIUOG5BvfWstmQEAAbty4UWjhleDo5GR8vu+pHmi+4i/V50lyjygiIgLdu3eHo6Nj4f9AimC9a4P1rg3WuzZY79qwdb0nJyejatWqRQqbih9GHz9+PCZOnIhnn30WABAcHIzz589j1qxZVsOms7MznJ2d8w13dHS0+UJqp9Pxh2FjWnzPxHrXCutdG6x3bbDetWGrei/OPBTv+ig9PR12dpaTtbe3h6EM3AqyLHTPRERERFSWKN6y2bdvX3zwwQeoXbs2mjRpgoMHD2LevHl48cUXlZ6V4nTMmkRERESKUjxsLly4EFOmTMHIkSNx7do11KhRA6+88gqmTp2q9KxUwLRJREREpCTFw6aHhwfmz5+P+fPnKz1p1fEwOhEREZGyeG90MzqGTSIiIiJFMWyaYdYkIiIiUhbDpjmmTSIiIiJFMWxaYNgkIiIiUhLDphmes0lERESkLIZNM8LAsElERESkJIZNM9evaV0CIiIiovKFYdNMZpbWJSAiIiIqXxg2zdjxnE0iIiIiRTFsWmDYJCIiIlISwyYRERERqYZh0wy7PiIiIiJSFsOmBYZNIiIiIiUxbJrRCYPWRSAiIiIqVxg2zeh0WpeAiIiIqHyp8GFzZy2zFzxnk4iIiEhRFT5sRj5geq7jOZtEREREiqrwYdOcjlmTiIiISFEMmxaYNomIiIiUxLBp5qyvv9ZFICIiIipXGDbNJLm5a10EIiIionKFYdMMLxAiIiIiUlaFD5trG5q9YNdHRERERIqq8GFzX01g3YO5rxg2iYiIiJRU4cOmORdnrUtAREREVL4wbAIQd29T6eXFe6MTERERKYlhE2YHzw08jE5ERESkJIZNmFo2iYiIiEhZDJswa9nk1ehEREREimLYtMCwSURERKQkhk2YHUZnyyYRERGRohg2YX4YXctSEBEREZU/DJtgyyYRERGRWhg2LTBsEhERESmJYRPsZ5OIiIhILQybMB1G17Flk4iIiEhRDJswtWzylE0iIiIiZTFsmhFMm0RERESKYtiE2WF0hk0iIiIiRVX4sNkQfXmmJhEREZFKKnzY9ENL0wth0K4gREREROVQhQ+bgOkwOls4iYiIiJTFsGkWMXnOJhEREZGyGDZh1vWRpqUgIiIiKn8YNs2xZZOIiIhIUQybMJ2zybBJREREpCyGTfDwOREREZFaGDbNsWWTiIiISFEVPmwKCLOujxg2iYiIiJRU4cOmOR2zJhEREZGiGDZhfs4m0yYRERGRkhg2wavRiYiIiNTCsElEREREqqnwYdMTAaaD52zZJCIiIlJUhQ+bbfAycKORfMGwSURERKSoCh82AUBAd/cvwyYRERGRklQJm5cvX8bzzz8PHx8fuLq6Ijg4GPv27VNjVopi10dEREREynJQeoK3b99GSEgIHn30Uaxfvx6+vr6IjY1F5cqVlZ6VIoQwtWyy6yMiIiIiZSkeNj/66CMEBARgyZIlxmGBgYFKz0ZRvECIiIiISB2Kh821a9ciLCwMAwcOxJYtW1CzZk2MHDkSI0aMsDp+ZmYmMjMzja+Tk5MBAHq9Hnq9Xuni5ZOTYwfknrMphE3mSTDWM+vbtljv2mC9a4P1rg3WuzZsXe/FmY9OCGWb81xcXAAA4eHhGDhwIPbu3YvXX38dixcvxpAhQ/KNP336dMyYMSPf8BUrVsDNzU3Jolm1cmVD9Nj2FF68cgwLH2mJ2m9MU32eRERERGVZeno6nnvuOSQlJcHT0/Oe4yoeNp2cnNCmTRvs3LnTOGzs2LHYu3cvdu3alW98ay2bAQEBuHHjRqGFV8LMmXYI/K45XrxyDP/rGYIX1m5WfZ4k94giIiLQvXt3ODo6al2cCoP1rg3WuzZY79pgvWvD1vWenJyMqlWrFilsKn4Y3d/fH40bN7YYFhQUhN9++83q+M7OznB2ds433NHR0SaVZW9vukBId3e+ZDu2+p7JEutdG6x3bbDetcF614at6r0481C866OQkBDExMRYDDt16hTq1Kmj9KwUY7oanYiIiIiUpHjYfOONN7B79258+OGHOH36NFasWIGvv/4ao0aNUnpWitPxanQiIiIiRSkeNtu2bYvVq1dj5cqVaNq0Kd577z3Mnz8fgwcPVnpWiuEdhIiIiIjUofg5mwDQp08f9OnTR41JK86iMZNZk4iIiEhRvDc6eAchIiIiIrUwbJpj1iQiIiJSFMMmzLo+4gVCRERERIpi2IT5BUJEREREpCSGTTM6xk0iIiIiRTFsAhC6uy2bPIxOREREpCiGTcB4/Jz3ESIiIiJSFsMmzLo+YssmERERkaIqfNi0zJcMm0RERERKqvBhEzBv2dS2HERERETlDcMmzO8gRERERERKYtg0w07diYiIiJTFsAnzTt0ZNomIiIiUxLBpjlmTiIiISFEMmzC7NzrTJhEREZGiGDZhfoEQwyYRERGRkhg2zTFrEhERESmqwodNIcwvECIiIiIiJVX4sGmOXR8RERERKYthEzxnk4iIiEgtDJsAkHs1OrMmERERkaIYNmHensm0SURERKQkhk3wAiEiIiIitTBsmuEFQkRERETKYtiE+QVCRERERKQkhk1zbNkkIiIiUlSFD5sjRwKVKrHrIyIiIiI1VPiw6e8PuHncDZvMmkRERESKqvBh05yOaZOIiIhIUQybAMALhIiIiIhUwbBpjhcIERERESmKYRMAdGzZJCIiIlIDw6YZdupOREREpCyGTQCmczYZNomIiIiUxLAJQNw9jN4l5gpw9qzGpSEiIiIqPxg2zQQkpgH16mldDCIiIqJyg2ET4AVCRERERCph2ASgYz+bRERERKpg2ITpnE0iIiIiUhbDJngNOhEREZFaGDYB6NiySURERKQKhk0AvDc6ERERkToYNomIiIhINQyb4AVCRERERGph2AQPohMRERGphWETYKfuRERERCph2AQg2LZJREREpAqGTQA5ugyti0BERERULjFsArjkuF3rIhARERGVSwybAOwMmVoXgYiIiKhcYtgEMHoPq4GIiIhIDUxZACpnGLQuAhEREVG5xLBJRERERKph2CQiIiIi1TBsEhEREZFqGDaJiIiISDUMm1YIIbQuAhEREVG5oHrYnD17NnQ6HcaNG6f2rBQjwLBJREREpARVw+bevXvx1VdfoVmzZmrORnGGnGyti0BUag1dMxSPfv8oDIJdhhERUeFUC5upqakYPHgwvvnmG1SuXFmt2ShiZqXhFq/F/v0alYSo9Pv+v+8RdS4K+6/wd0JERIVzUGvCo0aNwmOPPYbQ0FC8//77BY6XmZmJzEzT7SKTk5MBAHq9Hnq9Xq3iWVjj8iimpv7P+FqfmgLYaN4VVe53a6vvmCQl6z1Ln8Xvr4i4vGuD9a4N1rs2bF3vxZmPKmFz1apVOHDgAPbu3VvouLNmzcKMGTPyDd+4cSPc3NzUKF4+Qlg28Ebv3IHk1Ds2mXdFFxERoXURKiQl6n3nzp244X5DgdJUHFzetcF61wbrXRu2qvf09PQij6t42Lx48SJef/11REREwMXFpdDxJ02ahPDwcOPr5ORkBAQEoEePHvD09FS6eFZNE79bvG7Tti1cuoXZZN4VlV6vR0REBLp37w5HR0eti1NhKFLvh+SfTp06oV3NdoqVrTzj8q4N1rs2WO/asHW95x6JLgrFw+b+/ftx7do1tGrVyjgsJycHW7duxaJFi5CZmQl7e3vje87OznB2ds43HUdHR5stpIY81WDn7MQfiI3Y8nsmEyXq3cHBgd9dMXF51wbrXRusd23Yqt6LMw/Fw2a3bt1w5MgRi2HDhg1Do0aNMGHCBIugWVrkPYxuMORoVBIiIiKi8kXxsOnh4YGmTZtaDHN3d4ePj0++4aWFEJYBmF0fERERESmDdxACYMgTNkU2wyYRERGRElTr+shcVFSULWZz30SezC14GJ2oQCP3AP4pAIYXOioREZFtwmZpZxCW1WDIZt9gRAX5fJ38e/hUHFCrvbaFISKiUo+H0ZH/AiHBczaJCmWXlqZ1EYiIqAxg2ET+C4Ry2LJJREREpAiGTQAG5Amb+iyNSkJERERUvjBsAqha1bIacrIZNomIiIiUwLAJYOpUy9c8jE5ERESkDIZNAD6+ec/ZZMsmERERkRIYNgE42DNsEhEREamBYROAvQNvV0lERESkBoZNAPZ5WjbZqTsRERGRMhg2kb9lM0fPsElERESkBIZNWDuMznM2iYiIiJTAsAnAIU/YFDkGjUpCREREVL4wbAKwc3CweG0QORqVhIiIiKh8YdgE4OBgWQ2Zdxg2iYiIiJTAsAnAwc7yMPrp0wybRNYIIbQuAhERlTEMmwDsHS0Po//fb0sAblSJiIiISoxhE4Cjo33+gVev2r4gREREROUMwyYAO/v81cC7CBEVgo3/RERUBAybAOwc8lfDjdRrGpSEqHQTZgnz/AUNC0JERGUGwyYAO3tdvmGGbLZsEt1LDq+jIyKiImDYBGDvyMPoRERERGpg2ASgs7PSssmO3YnyYddHRERUXAybKCBs6nl/dKJ8GDaJiKiYGDYB6Kxejc6WTSIiIqKSYthEAS2bBp6zSURERFRSDJsAYGelZZNXoxPlI4TB7IV25SAiorKDYRMAdPlbNnNy9BoUhIiISDlpWWnI5pE60hjDJmA1bLLrI6JC5P/ZEFEpkpSRhEqzKiHo8yCti0IVHMMmwLBJVETCYCh8JCIqFbZf2A4AOH3rtMYloYqOYbMADJtEheA5m0Slmk6ng0MO+FslzTFsFoBhk4iIyjL79Axc/RiIWqp1SaiiY9gsAMMmUSF4ziZRqeYTfQRVMoBHzmtdEqroGDYLIAzs1J0oL4uuj4iodLNyPQKRFhg2C8B+NokKwfPAiEo3Zk0qJRg2C8A7CBERUZnGlk0qJRg2C8BzNonys+j6iNsxolJNxx8plRIMmwVIS+E5m0REVIaxZZNKCYbNAuizGDaJ7onnbBIRUREwbBYgW8/D6ET5CCZMojKDLZtUSjBsFsCQzZZNoryEeXMmt2NERFQEDJsFyOEFQkREVJaxZZNKCYbNArBlk4iIiKjkGDYLwK6PiPKz6PqIiEo1nR038VQ6cEksgCGHLZtEREREJcWwWQAeRiciojLN7JxNwZ4kSEMMmwXgYXSi/ISBGyyissL8DkIGwVNgSDsMmwUQPIxOlI9gT+5EZYZ52ORvl7TEsFkAnrNJRERlmvlhdF7cRxpi2LzrSocOFq8ZNlWWnAzfgweBbJ6uUKbwvC+issOsm03Bw+ikIYbNu/ZOmICDDz5pfM3D6OoSYaHoNGMGDB9+oHVRqBgMPGeTVLDl3BYM+m0QElITtC5K+cKWTSolGDZz6XTQ27saXzJsqstp/yEAQMq3i7QtCBFprsv3XbDqyCq89vdrWhelXLE4Z5Mtm6Qhhk0z5m02hpwszcpRkaTp07QuAt0vNnKSQkbuAW7MARwPH9O6KOULWzaplGDYNKMzOx9tx43fNSxJxSF4694yha0jpIbP1wE+d4AZP1zUuijlFn+7pCWGTTPbGw4zPrdjq41N6FjPZYrF9UHcUSAq3diySaUEw6aZGP8uOFxNPmfYJCKi8oItm6QlxcPmrFmz0LZtW3h4eKBatWoYMGAAYmJilJ6NKgwG4EwV+Zxhk8gKdn1EVGbodKZNPC96JS0pHja3bNmCUaNGYffu3YiIiIBer0ePHj2Qllb6LwQRQgfD3aMOAUnalqXiYHgpS3h/ZVIXly8liVLYz2ZiRiJO3TyldTHIxhyUnuCGDRssXi9duhTVqlXD/v378fDDDys9O0UZDMBjd38Dk7dpWxai0ohZk9TExasIUlOBw4eBDh0Au3u3F+nM3i8t52z6fuyLbEM2To46iYZVG2pdHLIRxcNmXklJsomwSpUqVt/PzMxEZmam8XVycjIAQK/XQ6/Xq10847wAIDvbAJec/MPp/sSnxuPzvYvwUssRqOtd1+I9x7t/DWA921JuXd9vnWdnmboEy8nJ4XdXRCWt9/Iud30AoWwdlcd6d+jcGbpDh5C9eDHEiy/ec9wcs0Pn+qwsm29Trc3v8cPZaHcZiOwRgQe8HrBJeSoKWy/vxZmPqmHTYDBg3LhxCAkJQdOmTa2OM2vWLMyYMSPf8I0bN8LNzU3N4uVz+fIVi9fr1q2z6fzLm9Xrx2DW8ouY8PjXePLJ7y3e63/3rxCC9ayBiIiI+/q/O8m38ezd52dOn+F3V0z3W+/lXe76wGDIUWWZKk/13v/QIQBA0mefYbuf3z3HTTl6DC3vPo+M/BcuntXULVwe1ur951/l36+W/YV11wJsWp6KwlbLe3p6epHH1QkVT8J67bXXsH79emzfvh21atWyOo61ls2AgADcuHEDnp6eahXNgl6vR0REBH7++TEsX+FoGp7Fjt1LwtHJyfg8b13mvhfnY49a8XdsWq6KLHdZ7969OxwdHQv/hzxuX72CagF1AQCr536FPmOG3fsfCEDJ6728y10fHK/phPpxqYpNtzzWe25dGTp0QM7Wrfcc9+jqxWj5zFgAwLULp1DZr67axQNw73rPLf+W8c+g0wc/2qQ8FYWtl/fk5GRUrVoVSUlJheY11Vo2R48ejb/++gtbt24tMGgCgLOzM5ydnfMNd3R0tPnKoUMHHbDCsgykjHvVJevZUrYhG5/u+hSdAjohpHaIKvO439+Xvb2D2XN7fnfFpMV6raxRo37KY73bAbAr5DM5mP1eHTT4vd6r3u10unL3nZQWtlreizMPxa9GF0Jg9OjRWL16NTZt2oTAwEClZ6Gal16yPIH6jp4tbmR7Sw4uwdv/vo2nfnlK66IQUSmVfKcIXaboSu+90XlDj4pF8bA5atQoLFu2DCtWrICHhwcSEhKQkJCAO3dKf3BzyNPOu+PiDm0KQhVWZnYmXv7rZQBAQmoC0rJKWZdhvBydVMSlq+guJRd+a0+d2W2+DIZS1s9mGVmXjF0/Fu9vfV/rYpR5iofNL7/8EklJSejSpQv8/f2Nj59++knpWaliToclxufX0q5pWBIq7bJysvDLsV9wPe26YtOcunmqxevzSecVmzYRlR86Q+FhzaKfzVLS9VEuXRkImydvnMTCPQsxZfMUrYtS5qlyGN3aY+jQoUrPShUX3DsYnw/+fbDxeXxKPDu0Jgvzds3D078+jcdWPKbYNL858I3F6yspVwoYUxuiCBs4IlJfccNaaTuMXhZaNs1PpeP2v2R4b/Q8Nm0znfDa9CoQfSkac3bMQY15NfDRjo9sWpZ3It/BiLUjyvlCXnY/29JDSwEAe6/sVWyaLg4uFq/jU+IVm7YSRBn+vtSy8shK9F7eG7su7tK6KOUAl6+iKkrYtBijlG1Hyto5m1z3lQzDZh4ZWabjDi0SgA7/64AJ/04AAEyKnIRvD3wLANgUtwlvbHgDiRmJAACDwnuNQgjM2j4L3x78FidvnFR02lRymdmZiEuMU3y6NTxqAAD8K/kDAN6KeAvZhmzF50PKOHPrDJ77/TmsP70e3X/sXqJprYtdh6d+fgo30m8oVDoqz4ob1kpdy2YZCG8CAg45gM4A5JS2c17LGIbNPHRmPwAnK8vWiD9HIPCzQHT7oRvmR89H5Y8qw2OWB+xn2kM3Q4efjprOTY25EYPvD31faMuktYVYbzD1zJ+RnXEfn6RsUHt1k2PIwW/Hf0NShrI3u3/t79eQlWPqO/SXY78U+X+vpl7F7yd+t7pc3M64DQDoGtgVgDxv2PE9R/xx8o/7Lmvc7TiMWDsCMTdi7nsaRlbKfDP9JqZHTcfZ22eLNakcQw6qzqmKSh9Wwo30GzgYfxD6HNvc+SItKw0nrp/IN3z3pd14a+NbhV6YtTZmLTr+ryPWnFxjmqY+Dfuv7L/vMj224jH8duI3TI+aft/TAIDVJ1Yj4NMAbDm3pUTT0YLa64PCehjZcm4Lfj72c9k4mlSEMl68ZAqY9zpnMzkzGQuiF9j2tJ0yUMW6LD0ufAoc+Er5BqWKhmEzj7OoZ3zeIjF//58AcC7xnMXr1CxTJ8TP/vYs9l3ZBwDoubwnhv4xFM0WN8O289vwzK/PYMWRFRiwaoDxopKIMxHwmOWB7w5+Z5xGtiEbvZf3Nr4uqPleCIEf//sRF5MKvyrRFlIyU7BozyJcTr6c773cFmBros5F5atTpczbNQ9P/fIUvD/yxoH4A/neN9+o7Lm8B+M2jENCagI2xW2y+F7Nx5+9fTaWHFpiMfzpX5+GQRgsWiFv37mNNSfX4PcTvyP8n3B8f0jeRemJn5/Akz8/CbuZdvkC2q07twCYwmau7w59h+I4eu0oBv02CLE3YzHizxH49uC36PRdJwBypfnJrk8Q+FkgwpaFFWuP3dr27Z3IdzBjywzUW1APZ2+fxYnrJ4oUGiPjInHzzk2k6dMQ+FkgWn3dCqPWjbIYZ1PcJuy7sg+xN2MReTYSp26eKnJZ72X42uFo/EVj/HXqLzyy9BE8/tPjMAgDuv/YHXN3zUWlWZWshtFco9aNksE04i2L4WM3jEWOIQeRZyMxI2oGfj3+a4HT+P7Q91Z3UmJuxmDAqgEYsXaEcdjQNUMR8l1IkXoneOLnJ3Ap+RK6fN8F6fqi3+GjKNKy0hB323qL/o30Gxi9bjT2Xr7/00r0OXrVWpDe3/o+PGd74oOtH6Dzks756vJ84nl0+b4Lnvn1GeNvRWmJGYlFvvA07/okJTMFb/7zpvG1XRHOn7592/T8Xi2bE/+diNc3vF7s889vpN9AtY+rYfb22cX6P6BsXCDkGnMW/qlAi6vAw0sfxp8xfyp6QWhFovq90cuiCwhAbVzEmK2ZuDT5bczZOQfPN3seq0+sRpq+8JV922/a4kr4FWOAOnrtKB5e+jAA4OdjPwOQ3SqNbjsa07dMByA3fkkZSUjTp8HT2RORcZHQGQA7kX+P6vj14/Cv5I91sevwwpoX4OHkgeRJyfjn9D8AgG4PdEPflX3RxLcJhrUYhs/3fo7JD082HqK1xiAMuJl+E77uvgWO88fJPxBzMwbjO42H7m7/bTmGHNjp7KDT6TA+Yjy+2v8VPt39Kc6MPWPxvwWFjxxhwKPfPwoAuD7+Orac24KdF3di56WdmB82H+ti12F8yHhUcqoEQK7cUrNSjfdaj0+JR/jGcIQGhmJ8xHh4u3jjzNgz0Ol0eGPDG5gfPd84r47/64jMyaa7VR2+ehihP4RiVNtRGNxsMJ799VnEJcbhs+jPjONsHrIZXep2wbLDy+Bg54AcQw4mRU6y+lnsZ9oDAFY9uQqNqjZCm2/a5DsEHlg5EDsv7jS+rrdA7txMemgSztw+Ywzl3R+wPCQbcyMGJ2+cxMwtM1HXuy5mbZ+FwcGDseyJZVbLEvxlsCzL0VXGYbfu3MLGsxsReSsSn//3OQC547T+9Hr0adDH6nRyRV+KxulbpxHq/ZBx2O0sudGMOh+V7/N0qdsFm17YZFxOrDEvW26w/+bAN2jo0xBvdnoT/Vf1x9qYtfn+75XWr+DNjm+iXpV6sNPl31/W5+hx7PoxjPx7JAQEPg37FB1qmS78+3zP5/jpmDwC8fKfLyM+VZ4X+8RPT1jsYEzePBm/Pf2b8fXC6IUYu2EsfhjwAy4lX7L6mXZe3AmH9yxXqwdfOYicnBzMPDMTHhc80LVeV/x87GcM/WMoAOAn/ITm1Zsbx//37L/G5291eguPfv+osYyb4jbh2PVj2HZhG35+6me4O7kbx427HYeZW2dazHvUulFY0t9yxyjXHf0dXEq+hAerPJjve9Ln6OFon7/D5uaLm+PM7TM4NvIYGvs2tnjv6/1f4/O9n+PzvZ9j4/Mb0b2eaRkWQuSbx5qTa7D/yn70rt8bHc2G91vVD38/97fVMpdE7hXFkzdPBgBMj5qOj3t8bHz/3U3vGp/vvrQb8Snx8Kvkd89lOPZmLLIN2QjyDTIOy/2sV1OvIiM7A3W86yAlMwVp+jT4z5Wnx6x5Zg36N+qPKylXkJKZglqetXDyxkm08m+FFl+1wOGrhwEAPq4+OD7qOH747wdEnYvC37F/Y65xRkX40GaB7l4tm7nbpUMJhxB9KRpBvkH47fhvGNBoACq7VgYgj7IsO7wMQ1sMRRXXKgAA34/l9mJS5CSM7zQez/72LG6m38T8HvMLLZqWYTPHkINsQzacHaw3KFmz59Ie9FvVDwDwx7N/oF/DfoX+T9S5KFR1q4rq7tXh4+ZjdX11L6lZqbDT2cHN0ba37laDqrervB/Jycnw8vIq0u2PlKLX67Fu3Tr07t0bEyY44ulPO6ADogEAYkB/HHp7CILa9oKzvTPm7ZqXrzVDLTu/BaqlAbcO7EDbunJP+9i1Y2j6Zf77zMeOiUXDRQ1hEAYse3wZnl/9vMX7fRr0wZ+D/kRGdga2nt+KAM8AixXkpH8nYfaO2fhl4C94qnH+zsTPJ55H3c/qAgDm9piLIc2HwMvFC22/aYszt85gfs/5eHfTu0hITQAA5EzNgZ2dvfH/n/3lGax6ahVu37mNzks64+ioY7LcVYAG8m5qeKH5C/jhvx/yzbt/w/5Y8+wapOvTUWd+HdxIv2EMgSPWylY7c10Du+KPZ/+AxyyPfNP69//+ReiPoQgJCClyP6p+lfyMnysvx2xAX4xdts61O2PbhW2Fjpc5ORP2OnvM3j7buHG0ZnTb0Rjbfizq+9RHYkYi3t/6Pn4+9jMuFqEPPnNL+y/FkBZD8g3/9+y/iDoXhQ+2fQAA+LDlJEzqPwsA0P4lYMyYH/FO5DsFzq9+lfp4t/O76Nuwr3EDBQDLDi/D/63+v2KV0ZoHKj+AbEM2+jfsjz4N+qB9zfbw/sg733jvdn4XSw8txbNNn8XcXXPzT8iKB6s8iH0j9iE+NR5V3aoaN6zWPBf8HFYcWVHg++ZSJqVYXTaLy9neGZk5mYWOd/S1o2jg08AYHv85/Q/S9el45a9XcD39OjydPbFvxD5k5WShSbUmmLp5Kt7b+h4AIHliMjycZVkNwmDcoXoy6EmsfHIlHO0dseTgEmy/sB3/Xf0P++NNpxHsHbEXHk4eaPR5IwBAUNUgRL4QCX8Pf/zw3w8Ysubu8iYAMUM+PegHtHpVlrlJtSYWn2PnxZ0QQljcVev0rdP4dNenCO8YjnpV6uGTnZ/g8NXD+K7/d3Cwc0B6RjoW/r4QVRtWxUt/vWQxvbY12mLPiD34/cTv+PbAt4g4G5Fv5/DFFi9iyiNTcCD+AAY0GmARFnIMOcYdiz+e/QMf7fjIuCM5ss1I/BX7Fy4kXSjwe3m26bPGHa6nmzyNn4/9DAc7h0LP0RbT5d8TVYH3Fg9Ch1od0KNeD5y4fgJrT63F/zX7P1R3r47EjER89tGL+PkLeTRg/961aPO3DEjnx51Hba/aOHL1CJotbnbP+R165RCa+zXHS2tfwv8O/g/dH+iOjf+3EQCgm2EK4mPbjcWCPQuMrz3sPTD10alYf2Y9Pg37FM2q353P3fC+e8zj6LDg93zz23FhB577/Tk08W2COd3noFHVRkjLSoOXi5fFeFvPb4WXsxea+zXPN40/Y/7E+tPrMbfHXDg7OOOO/g5yRA48nT0hhEDrr1vjdsZtHH3tKNwc3Sx2KIQQ+GjHRwiqGgT3Y6cQ+tTbAACHKUCOaZOG7/p9h1M3T+HPU3/i237forV/ayzetxgL9yzE+sHrkW3INi77APDeo+9hVNtRxvBekGxDNrad34bWNVqj6RdNUdm1Mg6+ctBYN6E/huKvQX9BQODMrTMY2GQgqrpVBQAs2r0IMcdjMO//5tnsdpVFzWsMm7AMm7GxjmjfJAUpMJu3uzvwv/8BffsCbm5YG7MWq46uwryweZjxzfPwXxOJr9oAVxQsrs4AGO42Uvy8bBLmZ0Shf8P+0Bv099XnVzX3arj61lW0/aat8TD/+4++D71BjxlbZhjH61y7M+Z0n4Ozt8/iQPwBNPFtghZ+LdDq61b5ptnEtwmOXT9W4DxzV4oA4PYO8GY3GZre3/a+8T3zsHkv83rMQ/jGcIthlZwqWT3UDQAvtXwpXwgtLqdsIOseQbL/CWD1746Y+Igecx4qeLzi6hkLrP/FCejRA1izBi6z3AsNFZuHbDa2EOcjgPo3gdNVAFHAjrWLgwtmdZuFN/55A94u3gjvEI6pUVPzjed1B0i82ylD+5eAPQXfiVYxdW8DrnrgRDX153U/KqcDnw74EkdSThc5yKpKAA/eAhrcBDbWA7LvbiDb1WyHPZf3FHtyj9R5BK38W6FZ9WYY9scw4/ChLYbKnQmzFsH78dJ+4Js/5fMEd8D/LSC0Xig2Pr8ROp0OKZkpuJJyxbjhvj3hNrycvfDWxrcwb/c8AECngE7YPGQznN+XLVVPN3kaw1oMw3cHvsMvJ6yfT+3i4IKv+nxlCr1FNLPLTEx+eDLe3PgmPt396X1+6vuXu+6M8QEajbn3uCHnge13G7Zrhpu2UU18myBNn1bk05fy7ti82fHNfMu6k72TxXns5gK9AxEzOkbu8NwNdt8Paoyk119Bl7pdcOzaMQwKHgTAMsACQC3PWriWdg27h+9GS/+WEEJg2eFleGHNCwCApIlJ8HQ2bXzNGyBa+beyOH0q8oVIxN6Mxat/vwoAsNfZI0fk4MSoEzh54yTGrh9rsePcIh44+NXdOph87+1BUb3e/nX8duI3vNL6FTT0aYgBjQag1/JeiIyLtBiv+wPdEXE2okjTHNV2FBIzErH8yHIAwOlRp1Gvar1C/qvkGDaLyTxsnjnjiKAgwA45SPKui0qJeQ6Xbd0K7N8PVKoE9O6NnNatYJ9wFQBg+OZr/N3Zz9jUDsgfWaFXLQsAeY7UBCQCF+bL561eBg5aOwIugPaXgOO+QIqLlffzeKX1K/hq/1eFj3i/BNDsqtzj1jtYhk3/N4FHzgG7awHnK5veO10ZqP96yWbbLAE4UxlIy3NEpPE1oGU88GxiTUx/8DL21yz6NJteBfZ+DczvAEzKc5Gx1x3AwQDcMB2Bw8yHgWl3T7N0ywKy7QD/FEBvL1fwra4AD9wGfm0idyTeC5mMybss70phnwP88gvwuHnnA1OmQP/h+3juCYFfmwCVXSobLyIqqjd2AvM2Au93BqZ0Mw0P8mmEmOsnYSjGkZ28YfOYLxB2BugbAximvIu/f/4AB/2AuCr3nk6uUdHALF03bKtlgEdyBsIe2AX/VFl/F7yAV/YBi+8eUa09DrjoDbhmAXccAb9U2WPEhgeR7/fT8QKweKMTXuuRhZ2188934Ak7TNlswMCBQEyeBsuwWCDspjfC2yfmm645tyzghf+AL3OP+P7+Ox648Kb137sAWl8B/vOT4c8zA/jfH8CyZsAfQflHr5YKZNkDia7ydYMbgFcGsLcI4X7sbuCzDfL5u12BDx8u/H9KJHcLogN8U4EbbgXv1ORlvlOda1h/YGlLGY53D9+NDv/rUKSQ7OHkgZSsFFTKBFJz1wUCqJRl9hoygHWLAz7pBKQ75Z/O732XY+CawRatWHmpvi69h9x156kqQMNCdtRDzwARP8rntd4ALntZH88zA3htL7C8GXDp7jjOeqDdZWBXgFxmayUBA48BX7YFMkrQaJZb/ondgI86F/3/fFx98FDth/BHTP6LJXvU6wEA2Hhm4/0XzIrm8cChu1/zY88B6xooOnnV/P3s3+jdsHfhI5YQw2YxmYfNEycc0fxuq/x3E2Iw7KNG9/7nvNNa/SvevrESy29GYfYON7z450XE1vVA5ydT0DsW2FEb+J/TU3jU/Vdk2wPra01C2zGzsKQF8N4jQLKL/OFf/9h0NXzrl4EDZmHTRQ9k2gP9YoA1PwE7awEhlkeI0O8kEF9JhqJpW4CRjwFnq8iNVsMbwJ+NAAjg4fPAc0eA6V2A626Whwn6nwCuuQMH/IFMR/m60Q3go4dkS9OdPCvqUdHAovXA8mDg+Sctw+Y7XYEPN8nnuumWYbP1K0CaI9DkOjBxO7C2IfDiQWDJ43Xx/JZEXBSJeLUPjBv+5w4DVzzkKQYZDsAfq4DdNYFOwwGdvZ3xHNfLnwA1zBo+HxibJwQJoHkCcLi63DgG3gIePQf8XR9IMNtprzpetrzccgXGdwduzbH+3fvOqoxHb3jg57kXkOQMeN1tCNjnD7S5211mu5eAd3faof9xAwZNfBCrnE9jVrtJGH+6Guxff8P6hO86/3QY6thXAWrUAObOxdLmwLDHLccZsQ/4YBOwqikQdF2e89v1nOn9f3s1QkrcSbzcF4hf3wR3LpzB6K4Z+D3IcoMMyPD70gHg/w7Lz53VvjVOn91vDJsTQuXGp00BXYG+8ZwP9nmmIDwqC7WSgRceB3J0wGOxwKf/3POjWrV3ynC8H/s//LEKGBcGzL87jdWN5E7B1rpA35MysDubXWMy82FgVmfLDWTu8nfSBwgaDfwZ0wppRw7grwbAj6tNn8+8xXqKz5MY/WEEPh7dCjMqPw63kfn3krJeHYHDjzRCy/HzYH9JXih3ZdIYHNqyCr13XsdvQcCbPYBzptOCMao3kOoEbAoETjb7GrGdGqJZizDY3cmA6wwnVLGvhDMLdXC5ehPr+jTE5SsxOFlVfmadkD+L7/4A/Nt1Rfdamyx+d/GVHfB/fbIRkAz8Vx3wuQM8dVwG2cl3d46S8+yoNksA/lsMXKkEPDoU6BUrQ/FNN2DmZiDOG1hidqDj72VA79PAwIGy7pOdgDd6At+1hPE366KXv1XoADsD4J0BNL4OPHMUGG3leqJqbwHX5WnacMsCnLOB225Ao+tyvXjYL///AMC1OYBvOvBiP1nGWRHAxB1AeA+5Uz5qL9DmCuCfCnwUArzTDXh9N/DeZsBdDyxpAQyNccW+ynfw2HPyMz93RK6Xnn8COFrN1FJcEMdsYP1yGWgPN/DCqJAkbK8j33PWA9+2mIK5UR/icf2D2J8cg1rJwI/NgK/+kstGvCcQ3iEcMx6dgahzUei7oi8mbgcSKgFPHwN6nZbTOl0ZeHogEFfZtFNirvtpYKPZKd0BbwDJzvKRdydq35ZGaL1Z7uV2HA7sDgB+XO+K56Pl1fs+bwMrfwV6nAV+agI8O1D+X89YuVO9pBXgfQd4/ATwa2MgxRmofEd+Z3nlLp/vdAVmFbAjVD0FqJWMwhsIzHZ0zPmlyPXf5kDTe8EJcr2wz2yaTa8C9gbgP//8kx5wAlht5caHCU0D8ZVPHN57GPl2SAIS5fryo5D876kib0OVANp7tcO20dtL3WF0iFImKSlJABBJSUk2m2dWVpZYs2aNyMrKElevCiHPqhbitdeEEKdOCdGzp2mggo8zx3YIUaOG8XVUHYgeP/YQDw+1HK/dSxCYLh/tXrI+rdz3MR2iwej87+/1h3j1MdPr287Wp3PBE6LnYIhNdU3DNtSDOOeVf9zRvSAGPQHxfwPkfM97mt5zfrfgz91wlOl5iqN8HK6Wf7xsnyrG51+1ghjVq+h1m2NlWHzTQPHTnCFCAGJsT9PwrG5dxJAXPMXBmvaqfM/mD0O1ahavsyq5lWh6WZ8vFLO2zRK6qRC6qfc/nWwdxPvhbcWPk/qIzkMhjle1Pl5G166q15Hajw8eKuK43bsLw/r1mpf3Xo/USs4iy8GuZNN4qL1I3LNN/LLsXbFs5TsFjme+/vCYKH/z9lMKnu64MAj3SRDr65mGba3nIG646YpUrg8fgujxvHye6Gw5HWuPQU9Yvn5hQMnr98dgy9d37CEwzfRbi61suTxdruJY4LRuNqgt0qtVsfpeqqPla/2yH4RYtEiI998XqZ3bF1rOuq+bng9+vPDP5T0Bot+z+T9fUR/th0P8b+CDxtcDnoFY2tz0/pHXnhICEItbQ2wLgGj6mmn7lDvOO13NtlvT7j6mQwzrZxpn5iNymMu7cp6YJrctrzxmWkcdqg7hONk0rdYjTP8/pifEf20CLMre9DWIHbUg3utsGtbjeYjqb8rh+/2KVgc59qbf3f5gX/HQGHfj698b24mHhkGcqaITD46x3B76jJfbsnFhEFXHW267r/34lTA4O4sVTzYUTpMhRj1fRRwKHyzOtKknYitD7POX06g/2lTOLbUhjvqaph/fuKHIysy0SXYqTl6DDcpTLFqHTSEslykLzs4lXnkV9tA/+0yB773bw6HA9/7au0JETnpW9fIV9FjTUJv5ltuHY8EbLT744IOPEj1GjLD5PM94y4YL82Gv/TZcxLduZHy964t38/9vnz7G53M7lIK6U/jxbUuItQ0g0kYq951kHTtmk+xUnLzGw+iwPIzu6CgPox+WPU/AYDCezyzdugXcuQPExgLbtgFT819EQRVIcDBw5Iiy01y7FvD3B9q2VXa696tnT2DDhsLHS0kBGjcGLhbjSnh7e/l7ysoCdu6Uv6lKlYAJ8q5dGD4cSEgA/i5mVzg9egAblT1/656OHweSk4EOHQofl/Jr3hyYOxcIDdW6JERlXnZkJBy6di18xBLiYfRiytuyGRdn2knYv7+Qf46PF2LjRiESEoRIS5PDNm4UIjBQTuCZglsq+Sjg8X//d+/3N20S4scfTa8HDRLi2QJaddetE+LyZSEOHzYN8/YuWfmmTxciOFiIYcPk926+wDz1lBA7dgjx119CXLokxJgxQlSuLFvFnZxM482fL0StWvL52LFCZGYKkZMjREaGXIYMBvm/gBBr1woxYIBcpg4eFCIxUYgNG+6r7DntzQ7JRUUJ8cEH9/6fCxdMy/r06fnfb9BAnmYyc6Yc5/ZtIW7cEGL2bHkeSna2rJ9Ro+Rnzv2/X38VQq+Xn7Oohg41fd8nTphaZ774QoizZ4X47DMhOncW4r//5PixsUKsXi1ETIwQp08L0bixZdkjI2WrSXi4LK/5e337Wr4ePVqIrCwh1qwRYvx4IZ54Qg6vXVuIXbtMZVy3Tgg7OyEee0y+Tk4WYt8+kXX7ttg+c6bIXrRICFdXIZo0ESIiQv5/ly6W86pcWa5Ldu8Wok4dIQIsDwOKtm3zfw+9egmRkmI5bOhQIW7dEmLq1Pzjf/yxEN27CzF3bsHffdeuQjRqJESzZsVbzhYvFuLVV/MPf+YZId56S4h+/eTyMnKkPFTcvr0QZ86Y6rBVK9P/uLoK8e23ptfffy9f+/rK34VcgcvvuYDybPjuO5ETElJweYcPF2LvXnnKVO5vb//++1s3NGokxOefCxEdLcSePXJaK1ZY/w6++kqIn38u2nQfe0yIKVNM65MFC0zvdetm/X/q1St6ua9fl3Uwbty9x7tzR4h//rm/utHyMWaMMkeLHn5YrgPMl+mDB+VvRamyWlvPFuXx669C/++/FllGbTyMXkx5w6YQpvVrnz73OdE7d+QKMCNDiCtX5Ioid6HYvFlugDMy5MZoxw7rC0/ejQwgxIMPypXYxYtyPt27W//frl1lEE5JEeLPP4UICxM5veS5p4Y+fSx/MFu2yBXIq68K0bSpEO+8I8RPP8m/uWGkWzchnntOCAcHIUJDZSCsWtV6yPvlFyF++EEYQkNFzNYEkRN7Rojt24V4+WVTYLD2WLJEfqbMTDmvevVkULx4UW5Q8rp9W4gjR0yvL1yQG+isLBk4cnIsx1+0SIh335Vh7b335PgGg5zOmTMysCQmyjBrMMjpJCfLDUVYmKyzW7esf98+PvIzHDxY8DKRkyM3Llu2yNfZ2XIFn7ecRZWaKkPPsGFCzJolQxcghIuLEEOGyGXM/NydtDSRlZYmoidOFNnz55uC3vLlcgO5aZN8xMYK8dFHMqDlkZ6SLZbhOTEN08Tu3fdXbE1duCDE1avW3/vwQ1l/4eEyCC9aJOuiuOLi5Hdjxto6xsK778oQtn27/P+8EhPl95W7XrxzRy4/V67I30iuyEgh3n5biPR0y//PyZFlGjdO/j6Ly2CQv4+cHPmbr1RJiEOHhPj0U1lPQghx7pwQR4/K53q9DHG5v21nZyFu3iz+PHP//vOP/KyF0evl32PHhDh6VGRlZMh6T0sT4oUXZEibOlWIkydlWL19u/AyLFliWuc2bCh3aE6dktNISpLLSFZWwctVrv37hVi50vKzCSHEtWtyvf7OO6bvbfduIXbuLLhM1mRnyzA/cKDc0RVynxcQoiYuytBrMMh12vffC9G6tdxpNt9Zyp3OzZvyu87MlI+4OPn9mktNlfXXoUO+dXnOI4+IyIULix6UzK5dMD5++EEI3T3O7x0+XNZ5kyZC9O8vP1/t2qYdwUqVhKhbV+5I5vrzT7ltiY+X246IiPzf2969Qnz9tWm9vHOn/L4NBlk3QliuV82lpMjt+k8/CbFqlWn4qVOyznPLHhkphLt7/s9093sTKSnyd/Xdd7Ke9+yROzKAEJ06yWktWSK3N+3aGRsFCl3PKIxhs5isfUHHj8tcBcgfrOr0ernSyZ3psmVyAzN3rmyF+O47+ePKu6Ixb1UbM0b+gAqqu8xMIf7+W7Y8CSEX6OK0LAlhffyMDBlO9u2zGJy7gzZmTJ4ynDghAIMAhJjqs1CIyZPvP3CVJqXhMxTSWljSlVF6umlxy7uNKhcuXSr+b6IIbL0RUFV2tlx3FIXBYAqAGihX9X4fcsMm1N7SnztnOkIjilDvBoM8OjNqlPzf9HR55OOVV2Rrfu4OTGys3Bh//LEQ27bJkFbo4cZSLCnJ1IpvMMjP9O+/cgdAAaU5bPJ2lQUICgLGjQM++QQYPRp49FHATc07Rjk4AL6+wMGDQHQ08Nxz8mTRcLOOzIcNy/9/devKdUlRODkBvc363qpUqfjltHbrNmdn4O238w2ePl3+XbgQWJB7YwknJ6CRqTupZV6jMOO9e3RmWJbYFe9WZKpwUPcnXdRFrcyqWYzOWCsqe/uirzt0OtWXSSoF6tQBPvus8PFy6XRAWJh85Hr99fzjPfig/BtkpTPassjTUz4AWQcPKXg3kFKuFGwdS69p04CAAODcOWDmzEJHV0bTpvKiiHvcj5eoNOAiSkRERcGweQ+VKgGLFsnnc+cCR49qWx4iIiKisoZhsxD9+gEDBgDZ2bLB0WDQukREpUO5P6RORESKYNgsggUL5GkWe/YAy5YVPj5RecWASURExcWwWQQBAcCkSfJ5eDhw9aq25SEqDXjOJhERFQXDZhG9+SbQogVw8ybw2mts4SEiIirv/vtP3jCQSoZhs4gcHYElS2QvHqtXy/M4iSoy7nARUXl286ZsZGrQQOuSlH0Mm8XQogUwcaJ8vnYt8NdfmhaHyOYYMImoorhwQesSlB8Mm8U0ZYrpeb9+wI4d2pWFSEs8Z5OUcvQo8O67QFKS1iUhIjUwbBaTkxOQnAzUri1beR56SN4EgS0+RET3JzgY+PBDeW48EZU/DJv3wcMDiIoyvd64EejaVbPiEGmCO1iktP37tS4BkXVc35UMw+Z9CgwEVq0yvY6KAv78U7PiENkEV7hEVBLbtwOLF5e9dUlZK29pw7BZAs88A5w8aXrdrx+QkKBdeYhsiedsElFxde4suw/ctEnrkhQPw2bJMGyWUMOG8sr0XF27AsePa1ceIiKi0u70aa1LUDgGTOUwbCqgb1/g4EGgenXgxAmgSRNg0CCtS1U2sHWs7OKKmJTGZYpKKy6bJcOwqZAWLYDdu2Xn74A8n/ODDzQtUpnAH3DZwu+LiCoirvtKhmFTQXXrAqmppteTJ8uWu6NHNSsSkWrYKk1K4zJVNAw+tsc6LxmGTYU5OcmF8umnTcOCg4ENGwCDQbtylVbcuBARFd0XXwABAZYXpxYFw1LJsP5KhmFTJT/9JO+lnqtXL8DZueK2ciYmWh/OH3DZxe+OyPZGjQIuX5ZXdJPtcH1XMgybKho6FIiMNL3OzpatnDodcPOmZsXSREaG1iUgJXCFS1Q6ZGcXb3z+dkuG9VcyDJsq69oV0OuBt96yHF61qrxyvaLgKQTlD0+BIKKKgmGzZBg2bcDBAfj44/x7oo0bAyNGADt3AseOATduyI5ucxfqyZOBV14pHwt5To7WJSAiqrjKw3ZES6y/kmHYtCF7e7nALlxoGvbtt0BICNC0KeDrC3TrBowcKTu8/eAD4OuvgVOntCuzUgpq2WTrWNnFlS8R3a+ytv4oa+UtbRg2NTB6NHDnDjB3LuDnl//9xYuB+vVNr2NjbVc2tRTUsskfcNnC74uobOJvt2RYfyXDsKkRFxcgPFxeVXju3L3H7dsX6N4duHBBHoqPj7dJERVV3JPZqfRjqzSRdsp6+Clr64+yXt9ac9C6ABWdnR1Qp47lgnzgANCnj2Wo/PdfOZ659u2BLl2ATp1kmHviCdN7q1cD/v5Ahw6qFr/ICgqbZW2FQ0RUFjEslQzrr2QYNkuhVq2AK1dk35SnTslQaU10tHzk6t0bePxxGTJzg+f164CXlwx1Dhp+23o9kJQEeHpaBkz+gMsufndE2inrO+plbf2hZXlv3ZJHNlu00K4MJcXD6KWYtzfQrp1cyA0GYP16oF69gsdft05e3d6nj2mYr6+8q5Gjo1w51a8vD8l//bW8A8Xff8u/WVmFl+fOnXv3D7pvHxAYCPzyS/739u+Xn+eFFyyHF2W+VHqUtQ0ElS1lafmaPx9Ys0brUhRdWarb0kjL+qtXD2jZEtizR7sylBRbNssInQ7o2VNepZ4rMRFISQE+/FBeVFSlitwDupfTp+Xj33/zv+foKFsgi2LSJBlsAwLk69RU4Jln5PmnTz8NpKVZjj98uPy7bJkMpLkuXdLh+HEgKEi+jo+X3UBt3iy7fnJzK1p5yPZ4Hm7Fk3uOeUiI1iXR1oEDwBtvyOdahZCizNcWR5Fu3AA8POQd8oqjrLXMahk2c+/A99dfsgGqLGLLZhnm7S3D3pdfyh/CzZvyTj3x8cDevXKv+/vvgdDQok2vqEETAGbNAh54QAZUR0egcmXg7FnT++7uBf/ve+9Zvm7SRJ67amcH1KwJ9Oghp+/uDrRpI1dKOh0wdSrw4ovAwIHAnDnAmDFyJVetmmmc6dNll1G5r3U6YOlSYMcOYNEiICZGzjMrS/ZpGhcnD+9nZckr5vftk+fAdugg/9fNTdbvmTOm8q5bJ1to9++XK4Hr14HkZCAqSrYSb98uu7Lav1+OLwSwdSswfjzw33/A1avAypXWw9q+ffKCsN9+s7yCPyVFTifvCi+31fvqVeD8eflZ58yRYT81VY6TkyPLlp0tx9frC/7ZW1uhGgzWh3fuDCxYIDe8uWWNjpYbn/uRmqruCv3Gjfw7QVpJSAAee0zuWJUldesCDz0E7N6tzvRLGkBOnACef970O7fmzh1g27aS9f1bFi/STE9Xfppz5sijZzVrKj/t0qY0tAxnZpaOctwPnRClq+jJycnw8vJCUlISPD09bTJPvV6PdevWoXfv3nB0dLTJPLVkMMhHYqJs5TxwQO6V7tkjD6+Tdb6+MnyeP1/0//HwkEHxXnIXuaKG/cBAGZKLystLBuq8nnxShlpABseUFODQIdP7XbvKQJ6rRQvZsnX9euHzrFZNLmMPPSTn7eUFXLxoCuAA8M47suuvLVtM5QDkaSB//SWf9+4tw31ew4bJVnwh5DyuXpUB4IEHZKj08pKfJzERaN1a1lnueczvvy93ZK5ckfPPzJTl2rFDhsBcb7wBPPqo/E3o9fJe1JmZsk4aNJCfZ/580/iLFgFt28og4+QkT1m5elXOw9MzG2PH3kDXrr44ccIef/5p+j9fX8s6dXaW8xkzBggLkyHh0CG5Q7Rqldw5Cw+XG/grV+SOTPPmQO3aMkitXy93avz85OcODpblrVpVLjf16sl5nD0LXLsmx8nMlNM9dw44fNi0HHh6Ag8+KOtr2TJgyRJTOYODZRnT04ENG+Qj159/yv89cQL44Qe589W1q5y+i4vsX9jBAXj2WWDiRMvv9uWX5fc3cKAcN3cnadgwufP5yityp+H99+X3PXiw/LwdOgCPPCI/CyA/X0JCNo4c2YS2bR/Fv/864sYNGZAA+f1UqyaXm44d5Y7iU0/J72PnTrmDVq8ecOSIDNm5AfvLL4Fdu+TnAmQZvvtO7izXqCF/85cvyzBbubI81+6ff4BeveS4BoPcee3YUS4nI0YAy5cD06bJMv3vf4Crq/x9urjILvLeeksux5mZQKNGcr6VK8syPvig7MP5779lGerXl+XOzpZHoBYtMtXtxIlyGi+8IIN9kyZyuMEgl42UFDktd3c5nk4nP9unn8oWtU8+kd87IL9385A5b56sf1dX4Pr1bKxadRSPP94UX33lgN9+A95+W/YhXbeu6X+++krWwcmTchm+fVt+vq1b5Q6DgwMwaJAs27Zt8vfQoYNcfrOz5TLu5CRf5+6o3Lwplz93d/l9AvK91FS5/jbfobl1S9ajnZX9byGA3383TePoUblc+vnJup4xQy7HjRrJssTGAg8/LBtLpk0DmjWTy2D16rIs16/LI3atW8vv7OxZ4PhxuY46ckSun1q3lvNNTJRHCJ99Vl6HkbeLxN27ZX0tXiyn37Sp/N5v3NBj5cpdGDeuo02yTHHyGsMmKl7YVEJyslzZxMbK1jg7O/lD9vGRC//583Jl5ewsfzR2dnIl4OsrV77btmUjIWE7xo4NwcWLjti9G6hVS4YMZ2e5AbS3BzZuBFasMLWiJifLQ+6VK1sGlFx+fpaBgdTj4sJ73hNR6WBnJ8Npca8DcHSUO2JubjLopaTI9VpWlmnHRWmVKpmOPKnh1Ck96tdn2Lwnhs2KwZZ1nruE5+TI5wkJ8q+dnQy2bm5yL1OnkyHWx0cOz8mRgReQrViXL8u/Xl6yJXD1arkXn5kp9yyrV5d75s7O8nHtmnydmgr88YfsVaBWLRm6Y2NlMNbr5coudy/dwUGGcU9PucK7elUG8KpV5XyrVZMhf/NmoGFDuTd85YqcbmamfLRpIz/b5ctyWlWryr1qNzcgOTkHy5bdRrt2VRAUZIfNm+Vhe51O9nzw6afyfLz+/WVr3y+/yFaPWrVki9ORI/LzDBsm9/ijomRLmru73MPfsEHWraurbP1p2lTueGRkyJ2SXAMGyBaJO3dg0crn7i5P+/jjD/n6gQcsT8/I1b+//N9Dh2RLy5Ejsl7t7Ex1mttSXLu2LE/uodVOnWRr67Fj8jSUpCS5E2NN9eryOwBka0TuOdM6Xf7DWS4usq5yN3a5GxQ3t/s/hOnvb/2QrbOznH9RN6xOTra9GK9mTbn8mc/f21v+JoqqRg3TRt98w+zgYP0UFHd3ZU6TKCwIVK2a/1SRvGUqqIwlYW9fum/76+0tl/N7LWd5P4Paoasi8vDIxM8/26NnT/UvyWHYLCaGTdtjnWuD9a4NNeo99xxe80OAQpiCsPnhwpwcuSPi6ioDWW4oz13FZmXJcFi1qgyyjo6m6WZmyrBY0PmUBoPp/Git5Z4iZG8vw15Wlh7r1m1Av3494eTkWKIy5q3TgsbJyTF1M2e+dc39XtLSZP0mJspyurkV/ULInBzTzpTBIHew7O1lYMvOljsn9vZy59LOTl406uBg6o3EvJx37shwqNPJ6Xp5yWmnpcn35c6pXGZ8fCznb29vOs/efLq59W8w6LF+ff7lPSdHztPNzbQjX1AdGwxy+tnZlt323bkjy1ililw209Nzz0WX9eDiInfKhZCHyTMy5A65t7ecp8Egl+fbt+VfLy+585CVJeeVe+pD9epyZzc9XTYEZGXJ8iQlyfnExckd7Nw6tLOT4bl6ddk6evmyrLfcWzXfvCl3oFxcgEuX5PRq1ZLzB+Rnun3btGzUqmU6emRvL5+b1+PNm3JH2NtbHjHMzgbs7PT455916NPHNuv34uQ1Xo1ORFQGWQt4ua/zDs8NNYDcIObl7Cw3rNYUdpWxtfPdtJIbgADTudBOToZ8oeh+FOX/8/ZnbO37ya3/6tWLXwZ7e1NIs7c3ncvn62s5Xu3ahZfTWsh1drZcPvLmB/P5W5Nb/wWdf25vL89pvVe5zKcF5O8f2tVVPvI+t6ZGDfm3oGU7l3kPKYA83zJXbtDOXZ5y5/fggwVPz9Mzf92Zn3dpfjvqXO7upgtrzZcNaxfb2tvLQF2tmuVwvb50/R7NldJiEREREVF5wLBJRERERKph2CQiIiIi1TBsEhEREZFqGDaJiIiISDUMm0RERESkGtXC5ueff466devCxcUF7du3x549e9SaFRERERGVUqqEzZ9++gnh4eGYNm0aDhw4gObNmyMsLAzXinP7CCIiIiIq81QJm/PmzcOIESMwbNgwNG7cGIsXL4abmxu+++47NWZHRERERKWU4ncQysrKwv79+zFp0iTjMDs7O4SGhmLXrl35xs/MzESm2d3uk+/epFiv10Nf0G0IFJY7H1vNj1jnWmG9a4P1rg3WuzZY79qwdb0XZz6K3xv9ypUrqFmzJnbu3ImOHTsah7/99tvYsmULoqOjLcafPn06ZsyYkW86K1asgFtRbxhLRERERDaTnp6O5557rmzcG33SpEkIDw83vk5OTkZAQAB69OhRaOGVotfrERERge7du9vk5vXEOtcK610brHdtsN61wXrXhq3rPfdIdFEoHjarVq0Ke3t7XL161WL41atX4Wd+J/q7nJ2d4ezsnG+4o6OjzRdSLeZZ0bHOtcF61wbrXRusd22w3rVhq3ovzjwUv0DIyckJrVu3RmRkpHGYwWBAZGSkxWF1IiIiIir/VDmMHh4ejiFDhqBNmzZo164d5s+fj7S0NAwbNkyN2RERERFRKaVK2HzmmWdw/fp1TJ06FQkJCWjRogU2bNiA6tWrqzE7IiIiIiqlVLtAaPTo0Rg9erRakyciIiKiMkDzq9Hzyu2JqThXOZWUXq9Heno6kpOTeTKzjbDOtcF61wbrXRusd22w3rVh63rPzWlF6UGz1IXNlJQUAEBAQIDGJSEiIiKie0lJSYGXl9c9x1G8U/eSMhgMuHLlCjw8PKDT6Wwyz9y+PS9evGizvj0rOta5Nljv2mC9a4P1rg3WuzZsXe9CCKSkpKBGjRqws7t350alrmXTzs4OtWrV0mTenp6e/GHYGOtcG6x3bbDetcF61wbrXRu2rPfCWjRzKd7PJhERERFRLoZNIiIiIlINwybkLTOnTZtm9baZpA7WuTZY79pgvWuD9a4N1rs2SnO9l7oLhIiIiIio/GDLJhERERGphmGTiIiIiFTDsElEREREqmHYJCIiIiLVMGwSERERkWoqfNj8/PPPUbduXbi4uKB9+/bYs2eP1kUqM7Zu3Yq+ffuiRo0a0Ol0WLNmjcX7QghMnToV/v7+cHV1RWhoKGJjYy3GuXXrFgYPHgxPT094e3tj+PDhSE1NtRjn8OHD6Ny5M1xcXBAQEIA5c+ao/dFKtVmzZqFt27bw8PBAtWrVMGDAAMTExFiMk5GRgVGjRsHHxweVKlXCk08+iatXr1qMc+HCBTz22GNwc3NDtWrVMH78eGRnZ1uMExUVhVatWsHZ2RkPPvggli5dqvbHK7W+/PJLNGvWzHh3jo4dO2L9+vXG91nntjF79mzodDqMGzfOOIx1r7zp06dDp9NZPBo1amR8n3WunsuXL+P555+Hj48PXF1dERwcjH379hnfL5PbVlGBrVq1Sjg5OYnvvvtOHDt2TIwYMUJ4e3uLq1eval20MmHdunXi3XffFb///rsAIFavXm3x/uzZs4WXl5dYs2aN+O+//0S/fv1EYGCguHPnjnGcnj17iubNm4vdu3eLbdu2iQcffFAMGjTI+H5SUpKoXr26GDx4sDh69KhYuXKlcHV1FV999ZWtPmapExYWJpYsWSKOHj0qDh06JHr37i1q164tUlNTjeO8+uqrIiAgQERGRop9+/aJDh06iE6dOhnfz87OFk2bNhWhoaHi4MGDYt26daJq1api0qRJxnHOnj0r3NzcRHh4uDh+/LhYuHChsLe3Fxs2bLDp5y0t1q5dK/7++29x6tQpERMTI9555x3h6Ogojh49KoRgndvCnj17RN26dUWzZs3E66+/bhzOulfetGnTRJMmTUR8fLzxcf36deP7rHN13Lp1S9SpU0cMHTpUREdHi7Nnz4p//vlHnD592jhOWdy2Vuiw2a5dOzFq1Cjj65ycHFGjRg0xa9YsDUtVNuUNmwaDQfj5+YmPP/7YOCwxMVE4OzuLlStXCiGEOH78uAAg9u7daxxn/fr1QqfTicuXLwshhPjiiy9E5cqVRWZmpnGcCRMmiIYNG6r8icqOa9euCQBiy5YtQghZz46OjuKXX34xjnPixAkBQOzatUsIIXcU7OzsREJCgnGcL7/8Unh6ehrr+u233xZNmjSxmNczzzwjwsLC1P5IZUblypXFt99+yzq3gZSUFFG/fn0REREhHnnkEWPYZN2rY9q0aaJ58+ZW32Odq2fChAnioYceKvD9srptrbCH0bOysrB//36EhoYah9nZ2SE0NBS7du3SsGTlQ1xcHBISEizq18vLC+3btzfW765du+Dt7Y02bdoYxwkNDYWdnR2io6ON4zz88MNwcnIyjhMWFoaYmBjcvn3bRp+mdEtKSgIAVKlSBQCwf/9+6PV6i7pv1KgRateubVH3wcHBqF69unGcsLAwJCcn49ixY8ZxzKeROw5/H0BOTg5WrVqFtLQ0dOzYkXVuA6NGjcJjjz2Wr35Y9+qJjY1FjRo18MADD2Dw4MG4cOECANa5mtauXYs2bdpg4MCBqFatGlq2bIlvvvnG+H5Z3bZW2LB548YN5OTkWPwQAKB69epISEjQqFTlR24d3qt+ExISUK1aNYv3HRwcUKVKFYtxrE3DfB4VmcFgwLhx4xASEoKmTZsCkPXi5OQEb29vi3Hz1n1h9VrQOMnJybhz544aH6fUO3LkCCpVqgRnZ2e8+uqrWL16NRo3bsw6V9mqVatw4MABzJo1K997rHt1tG/fHkuXLsWGDRvw5ZdfIi4uDp07d0ZKSgrrXEVnz57Fl19+ifr16+Off/7Ba6+9hrFjx+L7778HUHa3rQ6KT5GIbGbUqFE4evQotm/frnVRKoSGDRvi0KFDSEpKwq+//oohQ4Zgy5YtWherXLt48SJef/11REREwMXFReviVBi9evUyPm/WrBnat2+POnXq4Oeff4arq6uGJSvfDAYD2rRpgw8//BAA0LJlSxw9ehSLFy/GkCFDNC7d/auwLZtVq1aFvb19vqvnrl69Cj8/P41KVX7k1uG96tfPzw/Xrl2zeD87Oxu3bt2yGMfaNMznUVGNHj0af/31FzZv3oxatWoZh/v5+SErKwuJiYkW4+et+8LqtaBxPD09K+zGxsnJCQ8++CBat26NWbNmoXnz5vjss89Y5yrav38/rl27hlatWsHBwQEODg7YsmULFixYAAcHB1SvXp11bwPe3t5o0KABTp8+zeVdRf7+/mjcuLHFsKCgIOMpDGV121phw6aTkxNat26NyMhI4zCDwYDIyEh07NhRw5KVD4GBgfDz87Oo3+TkZERHRxvrt2PHjkhMTMT+/fuN42zatAkGgwHt27c3jrN161bo9XrjOBEREWjYsCEqV65so09TugghMHr0aKxevRqbNm1CYGCgxfutW7eGo6OjRd3HxMTgwoULFnV/5MgRixVSREQEPD09jSu6jh07Wkwjdxz+PkwMBgMyMzNZ5yrq1q0bjhw5gkOHDhkfbdq0weDBg43PWffqS01NxZkzZ+Dv78/lXUUhISH5urI7deoU6tSpA6AMb1tVueyojFi1apVwdnYWS5cuFcePHxcvv/yy8Pb2trh6jgqWkpIiDh48KA4ePCgAiHnz5omDBw+K8+fPCyFk9wze3t7ijz/+EIcPHxb9+/e32j1Dy5YtRXR0tNi+fbuoX7++RfcMiYmJonr16uL//u//xNGjR8WqVauEm5tbhe766LXXXhNeXl4iKirKoluS9PR04zivvvqqqF27tti0aZPYt2+f6Nixo+jYsaPx/dxuSXr06CEOHTokNmzYIHx9fa12SzJ+/Hhx4sQJ8fnnn1fobkkmTpwotmzZIuLi4sThw4fFxIkThU6nExs3bhRCsM5tyfxqdCFY92p48803RVRUlIiLixM7duwQoaGhomrVquLatWtCCNa5Wvbs2SMcHBzEBx98IGJjY8Xy5cuFm5ubWLZsmXGcsrhtrdBhUwghFi5cKGrXri2cnJxEu3btxO7du7UuUpmxefNmASDfY8iQIUII2UXDlClTRPXq1YWzs7Po1q2biImJsZjGzZs3xaBBg0SlSpWEp6enGDZsmEhJSbEY57///hMPPfSQcHZ2FjVr1hSzZ8+21UcslazVOQCxZMkS4zh37twRI0eOFJUrVxZubm7i8ccfF/Hx8RbTOXfunOjVq5dwdXUVVatWFW+++abQ6/UW42zevFm0aNFCODk5iQceeMBiHhXNiy++KOrUqSOcnJyEr6+v6NatmzFoCsE6t6W8YZN1r7xnnnlG+Pv7CycnJ1GzZk3xzDPPWPT1yDpXz59//imaNm0qnJ2dRaNGjcTXX39t8X5Z3LbqhBBC+fZSIiIiIqIKfM4mEREREamPYZOIiIiIVMOwSURERESqYdgkIiIiItUwbBIRERGRahg2iYiIiEg1DJtEREREpBqGTSIiIiJSDcMmEREREamGYZOIiIiIVMOwSURERESq+X8Xc2ICs1nfwgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'Acc at best val acc: {err1.mean():.3f} +- {err1.std():.3f}')\n",
    "print(f'Acc at test: {err2.mean():.3f} +- {err1.std():.3f}')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(acc['train'], 'b-', label='Acc train')\n",
    "plt.plot(acc['val'], 'g-', label='Acc val')\n",
    "plt.plot(acc['test'], 'r-', label='Acc test')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(loss['train'], 'b-', label='Loss train')\n",
    "plt.plot(loss['val'], 'g-', label='Loss val')\n",
    "plt.plot(loss['test'], 'r-', label='Loss test')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training paramerters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 200-0.01-0.0005-0.5: 0.434 (0.458)\n",
      "-1: 200-0.005-0.0005-0.5: 0.445 (0.489)\n",
      "-1: 200-0.001-0.0005-0.5: 0.480 (0.507)\n",
      "-1: 300-0.0005-0.0005-0.5: 0.489 (0.511)\n",
      "-1: 200-0.01-0.001-0.5: 0.417 (0.474)\n",
      "-1: 200-0.005-0.001-0.5: 0.436 (0.489)\n",
      "-1: 200-0.001-0.001-0.5: 0.504 (0.535)\n",
      "-1: 300-0.0005-0.001-0.5: 0.428 (0.485)\n",
      "-1: 200-0.01-0.005-0.5: 0.465 (0.504)\n",
      "-1: 200-0.005-0.005-0.5: 0.482 (0.489)\n",
      "-1: 200-0.001-0.005-0.5: 0.476 (0.504)\n",
      "-1: 200-0.05-0.01-0.5: 0.452 (0.452)\n",
      "-1: 200-0.01-0.01-0.5: 0.469 (0.500)\n",
      "-1: 200-0.005-0.01-0.5: 0.482 (0.502)\n",
      "-1: 200-0.001-0.01-0.5: 0.500 (0.502)\n",
      "-1: 200-0.01-0.001-0: 0.399 (0.463)\n",
      "-1: 200-0.01-0.001-0.25: 0.465 (0.491)\n",
      "-1: 200-0.01-0.001-0.75: 0.452 (0.480)\n",
      "-1: 200-0.001-0.005-0: 0.441 (0.491)\n",
      "-1: 200-0.001-0.005-0.25: 0.439 (0.502)\n",
      "-1: 200-0.001-0.005-0.75: 0.467 (0.500)\n",
      "-1: 100-0.001-0.005-0.5: 0.480 (0.491)\n",
      "-1: 500-0.001-0.005-0.5: 0.355 (0.509)\n",
      "-1: 1000-0.001-0.005-0.5: 0.351 (0.485)\n",
      "-2: 200-0.01-0.0005-0.5: 0.478 (0.500)\n",
      "-2: 200-0.005-0.0005-0.5: 0.390 (0.498)\n",
      "-2: 200-0.001-0.0005-0.5: 0.491 (0.524)\n",
      "-2: 300-0.0005-0.0005-0.5: 0.443 (0.509)\n",
      "-2: 200-0.01-0.001-0.5: 0.465 (0.509)\n",
      "-2: 200-0.005-0.001-0.5: 0.463 (0.487)\n",
      "-2: 200-0.001-0.001-0.5: 0.322 (0.480)\n",
      "-2: 300-0.0005-0.001-0.5: 0.351 (0.461)\n",
      "-2: 200-0.01-0.005-0.5: 0.474 (0.509)\n",
      "-2: 200-0.005-0.005-0.5: 0.480 (0.496)\n",
      "-2: 200-0.001-0.005-0.5: 0.476 (0.496)\n",
      "-2: 200-0.05-0.01-0.5: 0.417 (0.489)\n",
      "-2: 200-0.01-0.01-0.5: 0.487 (0.493)\n",
      "-2: 200-0.005-0.01-0.5: 0.480 (0.511)\n",
      "-2: 200-0.001-0.01-0.5: 0.461 (0.480)\n",
      "-2: 200-0.01-0.001-0: 0.406 (0.471)\n",
      "-2: 200-0.01-0.001-0.25: 0.465 (0.489)\n",
      "-2: 200-0.01-0.001-0.75: 0.478 (0.502)\n",
      "-2: 200-0.001-0.005-0: 0.298 (0.456)\n",
      "-2: 200-0.001-0.005-0.25: 0.498 (0.513)\n",
      "-2: 200-0.001-0.005-0.75: 0.456 (0.487)\n",
      "-2: 100-0.001-0.005-0.5: 0.368 (0.421)\n",
      "-2: 500-0.001-0.005-0.5: 0.482 (0.509)\n",
      "-2: 1000-0.001-0.005-0.5: 0.502 (0.520)\n",
      "-3: 200-0.01-0.0005-0.5: 0.366 (0.414)\n",
      "-3: 200-0.005-0.0005-0.5: 0.406 (0.474)\n",
      "-3: 200-0.001-0.0005-0.5: 0.458 (0.465)\n",
      "-3: 300-0.0005-0.0005-0.5: 0.436 (0.452)\n",
      "-3: 200-0.01-0.001-0.5: 0.408 (0.458)\n",
      "-3: 200-0.005-0.001-0.5: 0.397 (0.471)\n",
      "-3: 200-0.001-0.001-0.5: 0.406 (0.461)\n",
      "-3: 300-0.0005-0.001-0.5: 0.463 (0.463)\n",
      "-3: 200-0.01-0.005-0.5: 0.430 (0.487)\n",
      "-3: 200-0.005-0.005-0.5: 0.441 (0.467)\n",
      "-3: 200-0.001-0.005-0.5: 0.439 (0.452)\n",
      "-3: 200-0.05-0.01-0.5: 0.364 (0.425)\n",
      "-3: 200-0.01-0.01-0.5: 0.423 (0.450)\n",
      "-3: 200-0.005-0.01-0.5: 0.430 (0.452)\n",
      "-3: 200-0.001-0.01-0.5: 0.434 (0.434)\n",
      "-3: 200-0.01-0.001-0: 0.364 (0.439)\n",
      "-3: 200-0.01-0.001-0.25: 0.382 (0.452)\n",
      "-3: 200-0.01-0.001-0.75: 0.410 (0.461)\n",
      "-3: 200-0.001-0.005-0: 0.430 (0.458)\n",
      "-3: 200-0.001-0.005-0.25: 0.443 (0.456)\n",
      "-3: 200-0.001-0.005-0.75: 0.414 (0.428)\n",
      "-3: 100-0.001-0.005-0.5: 0.406 (0.406)\n",
      "-3: 500-0.001-0.005-0.5: 0.454 (0.461)\n",
      "-3: 1000-0.001-0.005-0.5: 0.452 (0.480)\n",
      "-4: 200-0.01-0.0005-0.5: 0.471 (0.502)\n",
      "-4: 200-0.005-0.0005-0.5: 0.482 (0.502)\n",
      "-4: 200-0.001-0.0005-0.5: 0.450 (0.507)\n",
      "-4: 300-0.0005-0.0005-0.5: 0.498 (0.533)\n",
      "-4: 200-0.01-0.001-0.5: 0.441 (0.500)\n",
      "-4: 200-0.005-0.001-0.5: 0.463 (0.500)\n",
      "-4: 200-0.001-0.001-0.5: 0.491 (0.515)\n",
      "-4: 300-0.0005-0.001-0.5: 0.518 (0.531)\n",
      "-4: 200-0.01-0.005-0.5: 0.489 (0.502)\n",
      "-4: 200-0.005-0.005-0.5: 0.482 (0.491)\n",
      "-4: 200-0.001-0.005-0.5: 0.504 (0.513)\n",
      "-4: 200-0.05-0.01-0.5: 0.454 (0.467)\n",
      "-4: 200-0.01-0.01-0.5: 0.476 (0.485)\n",
      "-4: 200-0.005-0.01-0.5: 0.458 (0.482)\n",
      "-4: 200-0.001-0.01-0.5: 0.489 (0.491)\n",
      "-4: 200-0.01-0.001-0: 0.463 (0.491)\n",
      "-4: 200-0.01-0.001-0.25: 0.423 (0.500)\n",
      "-4: 200-0.01-0.001-0.75: 0.465 (0.498)\n",
      "-4: 200-0.001-0.005-0: 0.482 (0.496)\n",
      "-4: 200-0.001-0.005-0.25: 0.507 (0.509)\n",
      "-4: 200-0.001-0.005-0.75: 0.489 (0.504)\n",
      "-4: 100-0.001-0.005-0.5: 0.430 (0.454)\n",
      "-4: 500-0.001-0.005-0.5: 0.493 (0.509)\n",
      "-4: 1000-0.001-0.005-0.5: 0.482 (0.515)\n",
      "-5: 200-0.01-0.0005-0.5: 0.404 (0.467)\n",
      "-5: 200-0.005-0.0005-0.5: 0.404 (0.500)\n",
      "-5: 200-0.001-0.0005-0.5: 0.467 (0.491)\n",
      "-5: 300-0.0005-0.0005-0.5: 0.489 (0.502)\n",
      "-5: 200-0.01-0.001-0.5: 0.461 (0.509)\n",
      "-5: 200-0.005-0.001-0.5: 0.441 (0.489)\n",
      "-5: 200-0.001-0.001-0.5: 0.469 (0.491)\n",
      "-5: 300-0.0005-0.001-0.5: 0.502 (0.509)\n",
      "-5: 200-0.01-0.005-0.5: 0.465 (0.491)\n",
      "-5: 200-0.005-0.005-0.5: 0.465 (0.489)\n",
      "-5: 200-0.001-0.005-0.5: 0.454 (0.476)\n",
      "-5: 200-0.05-0.01-0.5: 0.399 (0.458)\n",
      "-5: 200-0.01-0.01-0.5: 0.480 (0.498)\n",
      "-5: 200-0.005-0.01-0.5: 0.467 (0.489)\n",
      "-5: 200-0.001-0.01-0.5: 0.467 (0.478)\n",
      "-5: 200-0.01-0.001-0: 0.395 (0.461)\n",
      "-5: 200-0.01-0.001-0.25: 0.439 (0.491)\n",
      "-5: 200-0.01-0.001-0.75: 0.478 (0.500)\n",
      "-5: 200-0.001-0.005-0: 0.469 (0.474)\n",
      "-5: 200-0.001-0.005-0.25: 0.474 (0.480)\n",
      "-5: 200-0.001-0.005-0.75: 0.487 (0.500)\n",
      "-5: 100-0.001-0.005-0.5: 0.458 (0.461)\n",
      "-5: 500-0.001-0.005-0.5: 0.471 (0.487)\n",
      "-5: 1000-0.001-0.005-0.5: 0.491 (0.504)\n",
      "-6: 200-0.01-0.0005-0.5: 0.480 (0.509)\n",
      "-6: 200-0.005-0.0005-0.5: 0.423 (0.465)\n",
      "-6: 200-0.001-0.0005-0.5: 0.456 (0.485)\n",
      "-6: 300-0.0005-0.0005-0.5: 0.469 (0.493)\n",
      "-6: 200-0.01-0.001-0.5: 0.390 (0.436)\n",
      "-6: 200-0.005-0.001-0.5: 0.428 (0.465)\n",
      "-6: 200-0.001-0.001-0.5: 0.476 (0.491)\n",
      "-6: 300-0.0005-0.001-0.5: 0.469 (0.487)\n",
      "-6: 200-0.01-0.005-0.5: 0.465 (0.502)\n",
      "-6: 200-0.005-0.005-0.5: 0.461 (0.478)\n",
      "-6: 200-0.001-0.005-0.5: 0.476 (0.485)\n",
      "-6: 200-0.05-0.01-0.5: 0.445 (0.461)\n",
      "-6: 200-0.01-0.01-0.5: 0.474 (0.489)\n",
      "-6: 200-0.005-0.01-0.5: 0.425 (0.478)\n",
      "-6: 200-0.001-0.01-0.5: 0.480 (0.487)\n",
      "-6: 200-0.01-0.001-0: 0.408 (0.458)\n",
      "-6: 200-0.01-0.001-0.25: 0.412 (0.487)\n",
      "-6: 200-0.01-0.001-0.75: 0.454 (0.504)\n",
      "-6: 200-0.001-0.005-0: 0.458 (0.480)\n",
      "-6: 200-0.001-0.005-0.25: 0.476 (0.487)\n",
      "-6: 200-0.001-0.005-0.75: 0.463 (0.469)\n",
      "-6: 100-0.001-0.005-0.5: 0.425 (0.428)\n",
      "-6: 500-0.001-0.005-0.5: 0.469 (0.480)\n",
      "-6: 1000-0.001-0.005-0.5: 0.493 (0.502)\n",
      "-7: 200-0.01-0.0005-0.5: 0.428 (0.476)\n",
      "-7: 200-0.005-0.0005-0.5: 0.428 (0.465)\n",
      "-7: 200-0.001-0.0005-0.5: 0.454 (0.469)\n",
      "-7: 300-0.0005-0.0005-0.5: 0.410 (0.447)\n",
      "-7: 200-0.01-0.001-0.5: 0.414 (0.461)\n",
      "-7: 200-0.005-0.001-0.5: 0.417 (0.461)\n",
      "-7: 200-0.001-0.001-0.5: 0.452 (0.463)\n",
      "-7: 300-0.0005-0.001-0.5: 0.401 (0.465)\n",
      "-7: 200-0.01-0.005-0.5: 0.434 (0.487)\n",
      "-7: 200-0.005-0.005-0.5: 0.474 (0.491)\n",
      "-7: 200-0.001-0.005-0.5: 0.436 (0.456)\n",
      "-7: 200-0.05-0.01-0.5: 0.419 (0.461)\n",
      "-7: 200-0.01-0.01-0.5: 0.432 (0.498)\n",
      "-7: 200-0.005-0.01-0.5: 0.463 (0.478)\n",
      "-7: 200-0.001-0.01-0.5: 0.445 (0.480)\n",
      "-7: 200-0.01-0.001-0: 0.338 (0.452)\n",
      "-7: 200-0.01-0.001-0.25: 0.425 (0.456)\n",
      "-7: 200-0.01-0.001-0.75: 0.436 (0.454)\n",
      "-7: 200-0.001-0.005-0: 0.452 (0.485)\n",
      "-7: 200-0.001-0.005-0.25: 0.300 (0.441)\n",
      "-7: 200-0.001-0.005-0.75: 0.447 (0.458)\n",
      "-7: 100-0.001-0.005-0.5: 0.404 (0.436)\n",
      "-7: 500-0.001-0.005-0.5: 0.432 (0.463)\n",
      "-7: 1000-0.001-0.005-0.5: 0.445 (0.458)\n",
      "-8: 200-0.01-0.0005-0.5: 0.456 (0.474)\n",
      "-8: 200-0.005-0.0005-0.5: 0.454 (0.476)\n",
      "-8: 200-0.001-0.0005-0.5: 0.439 (0.465)\n",
      "-8: 300-0.0005-0.0005-0.5: 0.434 (0.469)\n",
      "-8: 200-0.01-0.001-0.5: 0.465 (0.467)\n",
      "-8: 200-0.005-0.001-0.5: 0.480 (0.509)\n",
      "-8: 200-0.001-0.001-0.5: 0.414 (0.507)\n",
      "-8: 300-0.0005-0.001-0.5: 0.482 (0.496)\n",
      "-8: 200-0.01-0.005-0.5: 0.441 (0.487)\n",
      "-8: 200-0.005-0.005-0.5: 0.458 (0.487)\n",
      "-8: 200-0.001-0.005-0.5: 0.496 (0.515)\n",
      "-8: 200-0.05-0.01-0.5: 0.493 (0.502)\n",
      "-8: 200-0.01-0.01-0.5: 0.476 (0.511)\n",
      "-8: 200-0.005-0.01-0.5: 0.491 (0.518)\n",
      "-8: 200-0.001-0.01-0.5: 0.471 (0.487)\n",
      "-8: 200-0.01-0.001-0: 0.364 (0.432)\n",
      "-8: 200-0.01-0.001-0.25: 0.439 (0.491)\n",
      "-8: 200-0.01-0.001-0.75: 0.443 (0.476)\n",
      "-8: 200-0.001-0.005-0: 0.465 (0.478)\n",
      "-8: 200-0.001-0.005-0.25: 0.461 (0.474)\n",
      "-8: 200-0.001-0.005-0.75: 0.493 (0.511)\n",
      "-8: 100-0.001-0.005-0.5: 0.493 (0.504)\n",
      "-8: 500-0.001-0.005-0.5: 0.467 (0.531)\n",
      "-8: 1000-0.001-0.005-0.5: 0.485 (0.509)\n",
      "-9: 200-0.01-0.0005-0.5: 0.410 (0.458)\n",
      "-9: 200-0.005-0.0005-0.5: 0.395 (0.461)\n",
      "-9: 200-0.001-0.0005-0.5: 0.458 (0.485)\n",
      "-9: 300-0.0005-0.0005-0.5: 0.463 (0.482)\n",
      "-9: 200-0.01-0.001-0.5: 0.423 (0.478)\n",
      "-9: 200-0.005-0.001-0.5: 0.456 (0.474)\n",
      "-9: 200-0.001-0.001-0.5: 0.478 (0.482)\n",
      "-9: 300-0.0005-0.001-0.5: 0.430 (0.461)\n",
      "-9: 200-0.01-0.005-0.5: 0.445 (0.482)\n",
      "-9: 200-0.005-0.005-0.5: 0.447 (0.480)\n",
      "-9: 200-0.001-0.005-0.5: 0.463 (0.471)\n",
      "-9: 200-0.05-0.01-0.5: 0.417 (0.456)\n",
      "-9: 200-0.01-0.01-0.5: 0.491 (0.509)\n",
      "-9: 200-0.005-0.01-0.5: 0.487 (0.502)\n",
      "-9: 200-0.001-0.01-0.5: 0.434 (0.439)\n",
      "-9: 200-0.01-0.001-0: 0.397 (0.425)\n",
      "-9: 200-0.01-0.001-0.25: 0.443 (0.458)\n",
      "-9: 200-0.01-0.001-0.75: 0.450 (0.482)\n",
      "-9: 200-0.001-0.005-0: 0.445 (0.471)\n",
      "-9: 200-0.001-0.005-0.25: 0.454 (0.471)\n",
      "-9: 200-0.001-0.005-0.75: 0.439 (0.474)\n",
      "-9: 100-0.001-0.005-0.5: 0.428 (0.432)\n",
      "-9: 500-0.001-0.005-0.5: 0.471 (0.480)\n",
      "-9: 1000-0.001-0.005-0.5: 0.454 (0.469)\n",
      "-10: 200-0.01-0.0005-0.5: 0.458 (0.471)\n",
      "-10: 200-0.005-0.0005-0.5: 0.467 (0.485)\n",
      "-10: 200-0.001-0.0005-0.5: 0.465 (0.489)\n",
      "-10: 300-0.0005-0.0005-0.5: 0.485 (0.507)\n",
      "-10: 200-0.01-0.001-0.5: 0.458 (0.491)\n",
      "-10: 200-0.005-0.001-0.5: 0.458 (0.489)\n",
      "-10: 200-0.001-0.001-0.5: 0.371 (0.507)\n",
      "-10: 300-0.0005-0.001-0.5: 0.344 (0.496)\n",
      "-10: 200-0.01-0.005-0.5: 0.463 (0.500)\n",
      "-10: 200-0.005-0.005-0.5: 0.469 (0.491)\n",
      "-10: 200-0.001-0.005-0.5: 0.465 (0.496)\n",
      "-10: 200-0.05-0.01-0.5: 0.436 (0.471)\n",
      "-10: 200-0.01-0.01-0.5: 0.480 (0.493)\n",
      "-10: 200-0.005-0.01-0.5: 0.447 (0.493)\n",
      "-10: 200-0.001-0.01-0.5: 0.476 (0.493)\n",
      "-10: 200-0.01-0.001-0: 0.327 (0.445)\n",
      "-10: 200-0.01-0.001-0.25: 0.397 (0.456)\n",
      "-10: 200-0.01-0.001-0.75: 0.491 (0.531)\n",
      "-10: 200-0.001-0.005-0: 0.346 (0.493)\n",
      "-10: 200-0.001-0.005-0.25: 0.482 (0.509)\n",
      "-10: 200-0.001-0.005-0.75: 0.450 (0.463)\n",
      "-10: 100-0.001-0.005-0.5: 0.421 (0.436)\n",
      "-10: 500-0.001-0.005-0.5: 0.480 (0.498)\n",
      "-10: 1000-0.001-0.005-0.5: 0.463 (0.504)\n",
      "-11: 200-0.01-0.0005-0.5: 0.406 (0.452)\n",
      "-11: 200-0.005-0.0005-0.5: 0.454 (0.496)\n",
      "-11: 200-0.001-0.0005-0.5: 0.384 (0.496)\n",
      "-11: 300-0.0005-0.0005-0.5: 0.436 (0.504)\n",
      "-11: 200-0.01-0.001-0.5: 0.450 (0.474)\n",
      "-11: 200-0.005-0.001-0.5: 0.390 (0.489)\n",
      "-11: 200-0.001-0.001-0.5: 0.351 (0.491)\n",
      "-11: 300-0.0005-0.001-0.5: 0.441 (0.500)\n",
      "-11: 200-0.01-0.005-0.5: 0.461 (0.487)\n",
      "-11: 200-0.005-0.005-0.5: 0.456 (0.496)\n",
      "-11: 200-0.001-0.005-0.5: 0.489 (0.496)\n",
      "-11: 200-0.05-0.01-0.5: 0.410 (0.474)\n",
      "-11: 200-0.01-0.01-0.5: 0.502 (0.502)\n",
      "-11: 200-0.005-0.01-0.5: 0.465 (0.496)\n",
      "-11: 200-0.001-0.01-0.5: 0.502 (0.515)\n",
      "-11: 200-0.01-0.001-0: 0.327 (0.439)\n",
      "-11: 200-0.01-0.001-0.25: 0.465 (0.489)\n",
      "-11: 200-0.01-0.001-0.75: 0.458 (0.478)\n",
      "-11: 200-0.001-0.005-0: 0.428 (0.504)\n",
      "-11: 200-0.001-0.005-0.25: 0.307 (0.482)\n",
      "-11: 200-0.001-0.005-0.75: 0.485 (0.493)\n",
      "-11: 100-0.001-0.005-0.5: 0.485 (0.491)\n",
      "-11: 500-0.001-0.005-0.5: 0.487 (0.511)\n",
      "-11: 1000-0.001-0.005-0.5: 0.498 (0.515)\n",
      "-12: 200-0.01-0.0005-0.5: 0.452 (0.487)\n",
      "-12: 200-0.005-0.0005-0.5: 0.432 (0.489)\n",
      "-12: 200-0.001-0.0005-0.5: 0.412 (0.498)\n",
      "-12: 300-0.0005-0.0005-0.5: 0.441 (0.522)\n",
      "-12: 200-0.01-0.001-0.5: 0.496 (0.513)\n",
      "-12: 200-0.005-0.001-0.5: 0.454 (0.471)\n",
      "-12: 200-0.001-0.001-0.5: 0.500 (0.524)\n",
      "-12: 300-0.0005-0.001-0.5: 0.360 (0.454)\n",
      "-12: 200-0.01-0.005-0.5: 0.480 (0.496)\n",
      "-12: 200-0.005-0.005-0.5: 0.489 (0.507)\n",
      "-12: 200-0.001-0.005-0.5: 0.478 (0.518)\n",
      "-12: 200-0.05-0.01-0.5: 0.445 (0.474)\n",
      "-12: 200-0.01-0.01-0.5: 0.478 (0.496)\n",
      "-12: 200-0.005-0.01-0.5: 0.482 (0.493)\n",
      "-12: 200-0.001-0.01-0.5: 0.476 (0.487)\n",
      "-12: 200-0.01-0.001-0: 0.445 (0.452)\n",
      "-12: 200-0.01-0.001-0.25: 0.524 (0.524)\n",
      "-12: 200-0.01-0.001-0.75: 0.498 (0.513)\n",
      "-12: 200-0.001-0.005-0: 0.294 (0.485)\n",
      "-12: 200-0.001-0.005-0.25: 0.498 (0.507)\n",
      "-12: 200-0.001-0.005-0.75: 0.480 (0.498)\n",
      "-12: 100-0.001-0.005-0.5: 0.377 (0.458)\n",
      "-12: 500-0.001-0.005-0.5: 0.412 (0.489)\n",
      "-12: 1000-0.001-0.005-0.5: 0.463 (0.498)\n",
      "-13: 200-0.01-0.0005-0.5: 0.395 (0.441)\n",
      "-13: 200-0.005-0.0005-0.5: 0.368 (0.434)\n",
      "-13: 200-0.001-0.0005-0.5: 0.395 (0.425)\n",
      "-13: 300-0.0005-0.0005-0.5: 0.414 (0.441)\n",
      "-13: 200-0.01-0.001-0.5: 0.357 (0.423)\n",
      "-13: 200-0.005-0.001-0.5: 0.408 (0.471)\n",
      "-13: 200-0.001-0.001-0.5: 0.430 (0.436)\n",
      "-13: 300-0.0005-0.001-0.5: 0.452 (0.469)\n",
      "-13: 200-0.01-0.005-0.5: 0.439 (0.480)\n",
      "-13: 200-0.005-0.005-0.5: 0.404 (0.458)\n",
      "-13: 200-0.001-0.005-0.5: 0.461 (0.469)\n",
      "-13: 200-0.05-0.01-0.5: 0.360 (0.423)\n",
      "-13: 200-0.01-0.01-0.5: 0.417 (0.439)\n",
      "-13: 200-0.005-0.01-0.5: 0.421 (0.441)\n",
      "-13: 200-0.001-0.01-0.5: 0.452 (0.458)\n",
      "-13: 200-0.01-0.001-0: 0.388 (0.439)\n",
      "-13: 200-0.01-0.001-0.25: 0.399 (0.447)\n",
      "-13: 200-0.01-0.001-0.75: 0.452 (0.458)\n",
      "-13: 200-0.001-0.005-0: 0.434 (0.456)\n",
      "-13: 200-0.001-0.005-0.25: 0.436 (0.452)\n",
      "-13: 200-0.001-0.005-0.75: 0.439 (0.454)\n",
      "-13: 100-0.001-0.005-0.5: 0.371 (0.373)\n",
      "-13: 500-0.001-0.005-0.5: 0.434 (0.467)\n",
      "-13: 1000-0.001-0.005-0.5: 0.456 (0.471)\n",
      "-14: 200-0.01-0.0005-0.5: 0.450 (0.500)\n",
      "-14: 200-0.005-0.0005-0.5: 0.489 (0.502)\n",
      "-14: 200-0.001-0.0005-0.5: 0.496 (0.513)\n",
      "-14: 300-0.0005-0.0005-0.5: 0.478 (0.502)\n",
      "-14: 200-0.01-0.001-0.5: 0.476 (0.513)\n",
      "-14: 200-0.005-0.001-0.5: 0.482 (0.498)\n",
      "-14: 200-0.001-0.001-0.5: 0.511 (0.531)\n",
      "-14: 300-0.0005-0.001-0.5: 0.504 (0.511)\n",
      "-14: 200-0.01-0.005-0.5: 0.478 (0.491)\n",
      "-14: 200-0.005-0.005-0.5: 0.458 (0.487)\n",
      "-14: 200-0.001-0.005-0.5: 0.493 (0.515)\n",
      "-14: 200-0.05-0.01-0.5: 0.436 (0.456)\n",
      "-14: 200-0.01-0.01-0.5: 0.487 (0.498)\n",
      "-14: 200-0.005-0.01-0.5: 0.480 (0.489)\n",
      "-14: 200-0.001-0.01-0.5: 0.487 (0.500)\n",
      "-14: 200-0.01-0.001-0: 0.447 (0.500)\n",
      "-14: 200-0.01-0.001-0.25: 0.450 (0.469)\n",
      "-14: 200-0.01-0.001-0.75: 0.478 (0.507)\n",
      "-14: 200-0.001-0.005-0: 0.467 (0.496)\n",
      "-14: 200-0.001-0.005-0.25: 0.498 (0.504)\n",
      "-14: 200-0.001-0.005-0.75: 0.456 (0.474)\n",
      "-14: 100-0.001-0.005-0.5: 0.469 (0.469)\n",
      "-14: 500-0.001-0.005-0.5: 0.471 (0.496)\n",
      "-14: 1000-0.001-0.005-0.5: 0.491 (0.504)\n",
      "-15: 200-0.01-0.0005-0.5: 0.434 (0.489)\n",
      "-15: 200-0.005-0.0005-0.5: 0.428 (0.489)\n",
      "-15: 200-0.001-0.0005-0.5: 0.463 (0.502)\n",
      "-15: 300-0.0005-0.0005-0.5: 0.474 (0.513)\n",
      "-15: 200-0.01-0.001-0.5: 0.445 (0.513)\n",
      "-15: 200-0.005-0.001-0.5: 0.447 (0.496)\n",
      "-15: 200-0.001-0.001-0.5: 0.461 (0.504)\n",
      "-15: 300-0.0005-0.001-0.5: 0.487 (0.491)\n",
      "-15: 200-0.01-0.005-0.5: 0.474 (0.504)\n",
      "-15: 200-0.005-0.005-0.5: 0.476 (0.498)\n",
      "-15: 200-0.001-0.005-0.5: 0.463 (0.476)\n",
      "-15: 200-0.05-0.01-0.5: 0.379 (0.425)\n",
      "-15: 200-0.01-0.01-0.5: 0.480 (0.489)\n",
      "-15: 200-0.005-0.01-0.5: 0.467 (0.491)\n",
      "-15: 200-0.001-0.01-0.5: 0.474 (0.485)\n",
      "-15: 200-0.01-0.001-0: 0.357 (0.456)\n",
      "-15: 200-0.01-0.001-0.25: 0.443 (0.480)\n",
      "-15: 200-0.01-0.001-0.75: 0.496 (0.502)\n",
      "-15: 200-0.001-0.005-0: 0.480 (0.485)\n",
      "-15: 200-0.001-0.005-0.25: 0.489 (0.500)\n",
      "-15: 200-0.001-0.005-0.75: 0.476 (0.478)\n",
      "-15: 100-0.001-0.005-0.5: 0.463 (0.465)\n",
      "-15: 500-0.001-0.005-0.5: 0.467 (0.485)\n",
      "-15: 1000-0.001-0.005-0.5: 0.463 (0.482)\n",
      "-16: 200-0.01-0.0005-0.5: 0.410 (0.476)\n",
      "-16: 200-0.005-0.0005-0.5: 0.445 (0.500)\n",
      "-16: 200-0.001-0.0005-0.5: 0.467 (0.478)\n",
      "-16: 300-0.0005-0.0005-0.5: 0.436 (0.467)\n",
      "-16: 200-0.01-0.001-0.5: 0.412 (0.467)\n",
      "-16: 200-0.005-0.001-0.5: 0.447 (0.476)\n",
      "-16: 200-0.001-0.001-0.5: 0.471 (0.485)\n",
      "-16: 300-0.0005-0.001-0.5: 0.476 (0.487)\n",
      "-16: 200-0.01-0.005-0.5: 0.485 (0.498)\n",
      "-16: 200-0.005-0.005-0.5: 0.474 (0.489)\n",
      "-16: 200-0.001-0.005-0.5: 0.487 (0.498)\n",
      "-16: 200-0.05-0.01-0.5: 0.410 (0.458)\n",
      "-16: 200-0.01-0.01-0.5: 0.469 (0.489)\n",
      "-16: 200-0.005-0.01-0.5: 0.463 (0.500)\n",
      "-16: 200-0.001-0.01-0.5: 0.480 (0.491)\n",
      "-16: 200-0.01-0.001-0: 0.342 (0.447)\n",
      "-16: 200-0.01-0.001-0.25: 0.414 (0.480)\n",
      "-16: 200-0.01-0.001-0.75: 0.428 (0.467)\n",
      "-16: 200-0.001-0.005-0: 0.471 (0.489)\n",
      "-16: 200-0.001-0.005-0.25: 0.461 (0.476)\n",
      "-16: 200-0.001-0.005-0.75: 0.463 (0.471)\n",
      "-16: 100-0.001-0.005-0.5: 0.443 (0.450)\n",
      "-16: 500-0.001-0.005-0.5: 0.456 (0.485)\n",
      "-16: 1000-0.001-0.005-0.5: 0.465 (0.489)\n",
      "-17: 200-0.01-0.0005-0.5: 0.346 (0.443)\n",
      "-17: 200-0.005-0.0005-0.5: 0.375 (0.463)\n",
      "-17: 200-0.001-0.0005-0.5: 0.447 (0.454)\n",
      "-17: 300-0.0005-0.0005-0.5: 0.439 (0.474)\n",
      "-17: 200-0.01-0.001-0.5: 0.465 (0.498)\n",
      "-17: 200-0.005-0.001-0.5: 0.439 (0.485)\n",
      "-17: 200-0.001-0.001-0.5: 0.375 (0.465)\n",
      "-17: 300-0.0005-0.001-0.5: 0.336 (0.428)\n",
      "-17: 200-0.01-0.005-0.5: 0.461 (0.491)\n",
      "-17: 200-0.005-0.005-0.5: 0.450 (0.482)\n",
      "-17: 200-0.001-0.005-0.5: 0.463 (0.485)\n",
      "-17: 200-0.05-0.01-0.5: 0.421 (0.454)\n",
      "-17: 200-0.01-0.01-0.5: 0.469 (0.496)\n",
      "-17: 200-0.005-0.01-0.5: 0.458 (0.491)\n",
      "-17: 200-0.001-0.01-0.5: 0.469 (0.469)\n",
      "-17: 200-0.01-0.001-0: 0.454 (0.471)\n",
      "-17: 200-0.01-0.001-0.25: 0.375 (0.452)\n",
      "-17: 200-0.01-0.001-0.75: 0.432 (0.471)\n",
      "-17: 200-0.001-0.005-0: 0.362 (0.471)\n",
      "-17: 200-0.001-0.005-0.25: 0.456 (0.476)\n",
      "-17: 200-0.001-0.005-0.75: 0.456 (0.461)\n",
      "-17: 100-0.001-0.005-0.5: 0.366 (0.450)\n",
      "-17: 500-0.001-0.005-0.5: 0.445 (0.474)\n",
      "-17: 1000-0.001-0.005-0.5: 0.360 (0.430)\n",
      "-18: 200-0.01-0.0005-0.5: 0.478 (0.482)\n",
      "-18: 200-0.005-0.0005-0.5: 0.443 (0.458)\n",
      "-18: 200-0.001-0.0005-0.5: 0.480 (0.507)\n",
      "-18: 300-0.0005-0.0005-0.5: 0.493 (0.518)\n",
      "-18: 200-0.01-0.001-0.5: 0.463 (0.493)\n",
      "-18: 200-0.005-0.001-0.5: 0.441 (0.467)\n",
      "-18: 200-0.001-0.001-0.5: 0.513 (0.518)\n",
      "-18: 300-0.0005-0.001-0.5: 0.465 (0.493)\n",
      "-18: 200-0.01-0.005-0.5: 0.461 (0.493)\n",
      "-18: 200-0.005-0.005-0.5: 0.480 (0.489)\n",
      "-18: 200-0.001-0.005-0.5: 0.461 (0.474)\n",
      "-18: 200-0.05-0.01-0.5: 0.417 (0.456)\n",
      "-18: 200-0.01-0.01-0.5: 0.487 (0.513)\n",
      "-18: 200-0.005-0.01-0.5: 0.482 (0.515)\n",
      "-18: 200-0.001-0.01-0.5: 0.498 (0.504)\n",
      "-18: 200-0.01-0.001-0: 0.447 (0.491)\n",
      "-18: 200-0.01-0.001-0.25: 0.456 (0.487)\n",
      "-18: 200-0.01-0.001-0.75: 0.454 (0.487)\n",
      "-18: 200-0.001-0.005-0: 0.480 (0.504)\n",
      "-18: 200-0.001-0.005-0.25: 0.487 (0.507)\n",
      "-18: 200-0.001-0.005-0.75: 0.465 (0.478)\n",
      "-18: 100-0.001-0.005-0.5: 0.478 (0.485)\n",
      "-18: 500-0.001-0.005-0.5: 0.458 (0.507)\n",
      "-18: 1000-0.001-0.005-0.5: 0.482 (0.502)\n",
      "-19: 200-0.01-0.0005-0.5: 0.445 (0.467)\n",
      "-19: 200-0.005-0.0005-0.5: 0.436 (0.482)\n",
      "-19: 200-0.001-0.0005-0.5: 0.447 (0.454)\n",
      "-19: 300-0.0005-0.0005-0.5: 0.456 (0.471)\n",
      "-19: 200-0.01-0.001-0.5: 0.447 (0.493)\n",
      "-19: 200-0.005-0.001-0.5: 0.432 (0.480)\n",
      "-19: 200-0.001-0.001-0.5: 0.493 (0.496)\n",
      "-19: 300-0.0005-0.001-0.5: 0.454 (0.502)\n",
      "-19: 200-0.01-0.005-0.5: 0.465 (0.487)\n",
      "-19: 200-0.005-0.005-0.5: 0.469 (0.482)\n",
      "-19: 200-0.001-0.005-0.5: 0.463 (0.489)\n",
      "-19: 200-0.05-0.01-0.5: 0.428 (0.456)\n",
      "-19: 200-0.01-0.01-0.5: 0.489 (0.496)\n",
      "-19: 200-0.005-0.01-0.5: 0.461 (0.478)\n",
      "-19: 200-0.001-0.01-0.5: 0.474 (0.491)\n",
      "-19: 200-0.01-0.001-0: 0.382 (0.441)\n",
      "-19: 200-0.01-0.001-0.25: 0.441 (0.443)\n",
      "-19: 200-0.01-0.001-0.75: 0.419 (0.465)\n",
      "-19: 200-0.001-0.005-0: 0.450 (0.474)\n",
      "-19: 200-0.001-0.005-0.25: 0.441 (0.461)\n",
      "-19: 200-0.001-0.005-0.75: 0.443 (0.445)\n",
      "-19: 100-0.001-0.005-0.5: 0.377 (0.397)\n",
      "-19: 500-0.001-0.005-0.5: 0.471 (0.491)\n",
      "-19: 1000-0.001-0.005-0.5: 0.443 (0.467)\n",
      "-20: 200-0.01-0.0005-0.5: 0.491 (0.500)\n",
      "-20: 200-0.005-0.0005-0.5: 0.491 (0.513)\n",
      "-20: 200-0.001-0.0005-0.5: 0.474 (0.502)\n",
      "-20: 300-0.0005-0.0005-0.5: 0.485 (0.502)\n",
      "-20: 200-0.01-0.001-0.5: 0.467 (0.502)\n",
      "-20: 200-0.005-0.001-0.5: 0.471 (0.507)\n",
      "-20: 200-0.001-0.001-0.5: 0.496 (0.509)\n",
      "-20: 300-0.0005-0.001-0.5: 0.461 (0.502)\n",
      "-20: 200-0.01-0.005-0.5: 0.461 (0.491)\n",
      "-20: 200-0.005-0.005-0.5: 0.463 (0.491)\n",
      "-20: 200-0.001-0.005-0.5: 0.474 (0.491)\n",
      "-20: 200-0.05-0.01-0.5: 0.432 (0.480)\n",
      "-20: 200-0.01-0.01-0.5: 0.487 (0.487)\n",
      "-20: 200-0.005-0.01-0.5: 0.463 (0.482)\n",
      "-20: 200-0.001-0.01-0.5: 0.487 (0.502)\n",
      "-20: 200-0.01-0.001-0: 0.351 (0.489)\n",
      "-20: 200-0.01-0.001-0.25: 0.441 (0.500)\n",
      "-20: 200-0.01-0.001-0.75: 0.458 (0.502)\n",
      "-20: 200-0.001-0.005-0: 0.478 (0.491)\n",
      "-20: 200-0.001-0.005-0.25: 0.511 (0.515)\n",
      "-20: 200-0.001-0.005-0.75: 0.485 (0.496)\n",
      "-20: 100-0.001-0.005-0.5: 0.357 (0.452)\n",
      "-20: 500-0.001-0.005-0.5: 0.454 (0.498)\n",
      "-20: 1000-0.001-0.005-0.5: 0.471 (0.502)\n",
      "----- 25.93 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-4, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-4, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 5e-4, 'drop': .5},\n",
    "        {'epochs': 300, 'lr': .0005, 'wd': 5e-4, 'drop': .5},\n",
    "        \n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 300, 'lr': .0005, 'wd': 1e-3, 'drop': .5},\n",
    "\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 5e-3, 'drop': .5},\n",
    "\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-2, 'drop': .5},\n",
    "\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-2, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-2, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-2, 'drop': .75},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-2, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-2, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-2, 'drop': .75},\n",
    "\n",
    "        {'epochs': 100, 'lr': .001, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 500, 'lr': .001, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 1000, 'lr': .001, 'wd': 1e-2, 'drop': .5},\n",
    "        ]\n",
    "\n",
    "best_accs1 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs1 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, l_act=LAST_ACT,\n",
    "                     dropout=exp['drop'], init_h0=h0, diff_layer=GFGCN_noh_Layer)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, exp['epochs'], exp['lr'], exp['wd'])\n",
    "\n",
    "        best_accs1[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs1[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}: {best_val_accs1[j,i]:.3f} ({best_accs1[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}' for exp in EXPS]\n",
    "table_over1 = summary_table(best_accs1, index_name)\n",
    "table1 = summary_table(best_val_accs1, index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.0005-0.5</th>\n",
       "      <td>0.434649</td>\n",
       "      <td>0.439693</td>\n",
       "      <td>0.038189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0005-0.5</th>\n",
       "      <td>0.432785</td>\n",
       "      <td>0.434211</td>\n",
       "      <td>0.034648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.0005-0.5</th>\n",
       "      <td>0.454167</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.028279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300-0.0005-0.0005-0.5</th>\n",
       "      <td>0.458443</td>\n",
       "      <td>0.459430</td>\n",
       "      <td>0.026493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.5</th>\n",
       "      <td>0.441009</td>\n",
       "      <td>0.448465</td>\n",
       "      <td>0.032371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.5</th>\n",
       "      <td>0.442544</td>\n",
       "      <td>0.444079</td>\n",
       "      <td>0.024846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.001-0.5</th>\n",
       "      <td>0.449232</td>\n",
       "      <td>0.470395</td>\n",
       "      <td>0.056079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300-0.0005-0.001-0.5</th>\n",
       "      <td>0.441118</td>\n",
       "      <td>0.457237</td>\n",
       "      <td>0.053973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.5</th>\n",
       "      <td>0.461623</td>\n",
       "      <td>0.463816</td>\n",
       "      <td>0.016182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.005-0.5</th>\n",
       "      <td>0.463925</td>\n",
       "      <td>0.467105</td>\n",
       "      <td>0.018769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.5</th>\n",
       "      <td>0.470724</td>\n",
       "      <td>0.469298</td>\n",
       "      <td>0.017313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.5</th>\n",
       "      <td>0.421711</td>\n",
       "      <td>0.419956</td>\n",
       "      <td>0.030490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.01-0.5</th>\n",
       "      <td>0.472697</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.022110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.01-0.5</th>\n",
       "      <td>0.463706</td>\n",
       "      <td>0.463816</td>\n",
       "      <td>0.019608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.01-0.5</th>\n",
       "      <td>0.472807</td>\n",
       "      <td>0.474781</td>\n",
       "      <td>0.019236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0</th>\n",
       "      <td>0.390022</td>\n",
       "      <td>0.391447</td>\n",
       "      <td>0.042874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.25</th>\n",
       "      <td>0.434868</td>\n",
       "      <td>0.439693</td>\n",
       "      <td>0.033063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.75</th>\n",
       "      <td>0.456469</td>\n",
       "      <td>0.453947</td>\n",
       "      <td>0.024096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0</th>\n",
       "      <td>0.431579</td>\n",
       "      <td>0.450658</td>\n",
       "      <td>0.057214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.25</th>\n",
       "      <td>0.455811</td>\n",
       "      <td>0.467105</td>\n",
       "      <td>0.055588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.75</th>\n",
       "      <td>0.462610</td>\n",
       "      <td>0.462719</td>\n",
       "      <td>0.020129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100-0.001-0.005-0.5</th>\n",
       "      <td>0.425000</td>\n",
       "      <td>0.426535</td>\n",
       "      <td>0.043907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.005-0.5</th>\n",
       "      <td>0.456689</td>\n",
       "      <td>0.467105</td>\n",
       "      <td>0.030211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000-0.001-0.005-0.5</th>\n",
       "      <td>0.460526</td>\n",
       "      <td>0.463816</td>\n",
       "      <td>0.039162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean accs       med       std\n",
       "200-0.01-0.0005-0.5     0.434649  0.439693  0.038189\n",
       "200-0.005-0.0005-0.5    0.432785  0.434211  0.034648\n",
       "200-0.001-0.0005-0.5    0.454167  0.458333  0.028279\n",
       "300-0.0005-0.0005-0.5   0.458443  0.459430  0.026493\n",
       "200-0.01-0.001-0.5      0.441009  0.448465  0.032371\n",
       "200-0.005-0.001-0.5     0.442544  0.444079  0.024846\n",
       "200-0.001-0.001-0.5     0.449232  0.470395  0.056079\n",
       "300-0.0005-0.001-0.5    0.441118  0.457237  0.053973\n",
       "200-0.01-0.005-0.5      0.461623  0.463816  0.016182\n",
       "200-0.005-0.005-0.5     0.463925  0.467105  0.018769\n",
       "200-0.001-0.005-0.5     0.470724  0.469298  0.017313\n",
       "200-0.05-0.01-0.5       0.421711  0.419956  0.030490\n",
       "200-0.01-0.01-0.5       0.472697  0.479167  0.022110\n",
       "200-0.005-0.01-0.5      0.463706  0.463816  0.019608\n",
       "200-0.001-0.01-0.5      0.472807  0.474781  0.019236\n",
       "200-0.01-0.001-0        0.390022  0.391447  0.042874\n",
       "200-0.01-0.001-0.25     0.434868  0.439693  0.033063\n",
       "200-0.01-0.001-0.75     0.456469  0.453947  0.024096\n",
       "200-0.001-0.005-0       0.431579  0.450658  0.057214\n",
       "200-0.001-0.005-0.25    0.455811  0.467105  0.055588\n",
       "200-0.001-0.005-0.75    0.462610  0.462719  0.020129\n",
       "100-0.001-0.005-0.5     0.425000  0.426535  0.043907\n",
       "500-0.001-0.005-0.5     0.456689  0.467105  0.030211\n",
       "1000-0.001-0.005-0.5    0.460526  0.463816  0.039162"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.0005-0.5</th>\n",
       "      <td>0.473355</td>\n",
       "      <td>0.474781</td>\n",
       "      <td>0.023652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0005-0.5</th>\n",
       "      <td>0.482018</td>\n",
       "      <td>0.486842</td>\n",
       "      <td>0.019226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.0005-0.5</th>\n",
       "      <td>0.485746</td>\n",
       "      <td>0.490132</td>\n",
       "      <td>0.023675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300-0.0005-0.0005-0.5</th>\n",
       "      <td>0.491009</td>\n",
       "      <td>0.502193</td>\n",
       "      <td>0.025677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.5</th>\n",
       "      <td>0.483662</td>\n",
       "      <td>0.492325</td>\n",
       "      <td>0.025267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.5</th>\n",
       "      <td>0.483662</td>\n",
       "      <td>0.485746</td>\n",
       "      <td>0.013545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.001-0.5</th>\n",
       "      <td>0.494518</td>\n",
       "      <td>0.493421</td>\n",
       "      <td>0.024787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300-0.0005-0.001-0.5</th>\n",
       "      <td>0.484430</td>\n",
       "      <td>0.489035</td>\n",
       "      <td>0.023537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.5</th>\n",
       "      <td>0.493531</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.007830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.005-0.5</th>\n",
       "      <td>0.486952</td>\n",
       "      <td>0.489035</td>\n",
       "      <td>0.010297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.5</th>\n",
       "      <td>0.488706</td>\n",
       "      <td>0.490132</td>\n",
       "      <td>0.018728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.5</th>\n",
       "      <td>0.459978</td>\n",
       "      <td>0.458333</td>\n",
       "      <td>0.019366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.01-0.5</th>\n",
       "      <td>0.491447</td>\n",
       "      <td>0.495614</td>\n",
       "      <td>0.017542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.01-0.5</th>\n",
       "      <td>0.489145</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.018315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.01-0.5</th>\n",
       "      <td>0.483772</td>\n",
       "      <td>0.486842</td>\n",
       "      <td>0.020080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0</th>\n",
       "      <td>0.458114</td>\n",
       "      <td>0.453947</td>\n",
       "      <td>0.021033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.25</th>\n",
       "      <td>0.477193</td>\n",
       "      <td>0.483553</td>\n",
       "      <td>0.021187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.75</th>\n",
       "      <td>0.486952</td>\n",
       "      <td>0.484649</td>\n",
       "      <td>0.020331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0</th>\n",
       "      <td>0.481908</td>\n",
       "      <td>0.484649</td>\n",
       "      <td>0.014176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.25</th>\n",
       "      <td>0.486075</td>\n",
       "      <td>0.484649</td>\n",
       "      <td>0.021966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.75</th>\n",
       "      <td>0.477083</td>\n",
       "      <td>0.475877</td>\n",
       "      <td>0.021244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100-0.001-0.005-0.5</th>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.450658</td>\n",
       "      <td>0.032430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.005-0.5</th>\n",
       "      <td>0.491338</td>\n",
       "      <td>0.490132</td>\n",
       "      <td>0.017619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000-0.001-0.005-0.5</th>\n",
       "      <td>0.490461</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.022075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean accs       med       std\n",
       "200-0.01-0.0005-0.5     0.473355  0.474781  0.023652\n",
       "200-0.005-0.0005-0.5    0.482018  0.486842  0.019226\n",
       "200-0.001-0.0005-0.5    0.485746  0.490132  0.023675\n",
       "300-0.0005-0.0005-0.5   0.491009  0.502193  0.025677\n",
       "200-0.01-0.001-0.5      0.483662  0.492325  0.025267\n",
       "200-0.005-0.001-0.5     0.483662  0.485746  0.013545\n",
       "200-0.001-0.001-0.5     0.494518  0.493421  0.024787\n",
       "300-0.0005-0.001-0.5    0.484430  0.489035  0.023537\n",
       "200-0.01-0.005-0.5      0.493531  0.491228  0.007830\n",
       "200-0.005-0.005-0.5     0.486952  0.489035  0.010297\n",
       "200-0.001-0.005-0.5     0.488706  0.490132  0.018728\n",
       "200-0.05-0.01-0.5       0.459978  0.458333  0.019366\n",
       "200-0.01-0.01-0.5       0.491447  0.495614  0.017542\n",
       "200-0.005-0.01-0.5      0.489145  0.491228  0.018315\n",
       "200-0.001-0.01-0.5      0.483772  0.486842  0.020080\n",
       "200-0.01-0.001-0        0.458114  0.453947  0.021033\n",
       "200-0.01-0.001-0.25     0.477193  0.483553  0.021187\n",
       "200-0.01-0.001-0.75     0.486952  0.484649  0.020331\n",
       "200-0.001-0.005-0       0.481908  0.484649  0.014176\n",
       "200-0.001-0.005-0.25    0.486075  0.484649  0.021966\n",
       "200-0.001-0.005-0.75    0.477083  0.475877  0.021244\n",
       "100-0.001-0.005-0.5     0.447917  0.450658  0.032430\n",
       "500-0.001-0.005-0.5     0.491338  0.490132  0.017619\n",
       "1000-0.001-0.005-0.5    0.490461  0.500000  0.022075"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_over1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design - Bias True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 2-2-16: 0.248 (0.303)\n",
      "-1: 2-3-16: 0.283 (0.320)\n",
      "-1: 2-4-16: 0.272 (0.322)\n",
      "-1: 3-2-16: 0.410 (0.423)\n",
      "-1: 3-3-16: 0.202 (0.261)\n",
      "-1: 2-2-32: 0.318 (0.487)\n",
      "-1: 2-3-32: 0.279 (0.292)\n",
      "-1: 2-4-32: 0.285 (0.298)\n",
      "-1: 3-2-32: 0.292 (0.327)\n",
      "-1: 3-3-32: 0.248 (0.287)\n",
      "-1: 2-2-64: 0.469 (0.518)\n",
      "-1: 2-3-64: 0.340 (0.349)\n",
      "-1: 3-2-64: 0.274 (0.294)\n",
      "-1: 3-3-64: 0.217 (0.254)\n",
      "-1: 2-2-128: 0.489 (0.522)\n",
      "-1: 2-3-128: 0.300 (0.322)\n",
      "-1: 3-2-128: 0.294 (0.386)\n",
      "-1: 3-3-128: 0.263 (0.268)\n",
      "-1: 2-2-256: 0.471 (0.498)\n",
      "-2: 2-2-16: 0.270 (0.371)\n",
      "-2: 2-3-16: 0.204 (0.272)\n",
      "-2: 2-4-16: 0.268 (0.285)\n",
      "-2: 3-2-16: 0.265 (0.292)\n",
      "-2: 3-3-16: 0.228 (0.298)\n",
      "-2: 2-2-32: 0.498 (0.513)\n",
      "-2: 2-3-32: 0.265 (0.333)\n",
      "-2: 2-4-32: 0.217 (0.265)\n",
      "-2: 3-2-32: 0.353 (0.371)\n",
      "-2: 3-3-32: 0.200 (0.333)\n",
      "-2: 2-2-64: 0.474 (0.502)\n",
      "-2: 2-3-64: 0.287 (0.325)\n",
      "-2: 3-2-64: 0.467 (0.480)\n",
      "-2: 3-3-64: 0.246 (0.279)\n",
      "-2: 2-2-128: 0.480 (0.502)\n",
      "-2: 2-3-128: 0.252 (0.311)\n",
      "-2: 3-2-128: 0.265 (0.344)\n",
      "-2: 3-3-128: 0.228 (0.344)\n",
      "-2: 2-2-256: 0.493 (0.509)\n",
      "-3: 2-2-16: 0.404 (0.436)\n",
      "-3: 2-3-16: 0.268 (0.311)\n",
      "-3: 2-4-16: 0.206 (0.296)\n",
      "-3: 3-2-16: 0.285 (0.311)\n",
      "-3: 3-3-16: 0.241 (0.248)\n",
      "-3: 2-2-32: 0.434 (0.458)\n",
      "-3: 2-3-32: 0.268 (0.287)\n",
      "-3: 2-4-32: 0.184 (0.268)\n",
      "-3: 3-2-32: 0.320 (0.349)\n",
      "-3: 3-3-32: 0.219 (0.259)\n",
      "-3: 2-2-64: 0.445 (0.456)\n",
      "-3: 2-3-64: 0.289 (0.322)\n",
      "-3: 3-2-64: 0.371 (0.388)\n",
      "-3: 3-3-64: 0.226 (0.303)\n",
      "-3: 2-2-128: 0.441 (0.458)\n",
      "-3: 2-3-128: 0.294 (0.331)\n",
      "-3: 3-2-128: 0.342 (0.382)\n",
      "-3: 3-3-128: 0.221 (0.318)\n",
      "-3: 2-2-256: 0.447 (0.471)\n",
      "-4: 2-2-16: 0.432 (0.456)\n",
      "-4: 2-3-16: 0.276 (0.289)\n",
      "-4: 2-4-16: 0.298 (0.305)\n",
      "-4: 3-2-16: 0.298 (0.298)\n",
      "-4: 3-3-16: 0.164 (0.281)\n",
      "-4: 2-2-32: 0.487 (0.502)\n",
      "-4: 2-3-32: 0.294 (0.320)\n",
      "-4: 2-4-32: 0.270 (0.292)\n",
      "-4: 3-2-32: 0.399 (0.404)\n",
      "-4: 3-3-32: 0.243 (0.281)\n",
      "-4: 2-2-64: 0.496 (0.509)\n",
      "-4: 2-3-64: 0.294 (0.327)\n",
      "-4: 3-2-64: 0.428 (0.463)\n",
      "-4: 3-3-64: 0.283 (0.300)\n",
      "-4: 2-2-128: 0.511 (0.511)\n",
      "-4: 2-3-128: 0.393 (0.428)\n",
      "-4: 3-2-128: 0.436 (0.465)\n",
      "-4: 3-3-128: 0.248 (0.294)\n",
      "-4: 2-2-256: 0.509 (0.518)\n",
      "-5: 2-2-16: 0.461 (0.471)\n",
      "-5: 2-3-16: 0.261 (0.272)\n",
      "-5: 2-4-16: 0.193 (0.292)\n",
      "-5: 3-2-16: 0.261 (0.316)\n",
      "-5: 3-3-16: 0.191 (0.287)\n",
      "-5: 2-2-32: 0.476 (0.487)\n",
      "-5: 2-3-32: 0.279 (0.287)\n",
      "-5: 2-4-32: 0.232 (0.254)\n",
      "-5: 3-2-32: 0.353 (0.366)\n",
      "-5: 3-3-32: 0.195 (0.261)\n",
      "-5: 2-2-64: 0.480 (0.502)\n",
      "-5: 2-3-64: 0.287 (0.311)\n",
      "-5: 3-2-64: 0.474 (0.478)\n",
      "-5: 3-3-64: 0.224 (0.272)\n",
      "-5: 2-2-128: 0.502 (0.502)\n",
      "-5: 2-3-128: 0.204 (0.320)\n",
      "-5: 3-2-128: 0.421 (0.447)\n",
      "-5: 3-3-128: 0.213 (0.294)\n",
      "-5: 2-2-256: 0.476 (0.493)\n",
      "-6: 2-2-16: 0.432 (0.447)\n",
      "-6: 2-3-16: 0.292 (0.298)\n",
      "-6: 2-4-16: 0.261 (0.283)\n",
      "-6: 3-2-16: 0.349 (0.353)\n",
      "-6: 3-3-16: 0.182 (0.289)\n",
      "-6: 2-2-32: 0.465 (0.476)\n",
      "-6: 2-3-32: 0.261 (0.292)\n",
      "-6: 2-4-32: 0.171 (0.257)\n",
      "-6: 3-2-32: 0.283 (0.362)\n",
      "-6: 3-3-32: 0.215 (0.274)\n",
      "-6: 2-2-64: 0.480 (0.485)\n",
      "-6: 2-3-64: 0.279 (0.342)\n",
      "-6: 3-2-64: 0.386 (0.404)\n",
      "-6: 3-3-64: 0.250 (0.307)\n",
      "-6: 2-2-128: 0.454 (0.498)\n",
      "-6: 2-3-128: 0.377 (0.386)\n",
      "-6: 3-2-128: 0.436 (0.447)\n",
      "-6: 3-3-128: 0.268 (0.285)\n",
      "-6: 2-2-256: 0.469 (0.487)\n",
      "-7: 2-2-16: 0.228 (0.432)\n",
      "-7: 2-3-16: 0.246 (0.296)\n",
      "-7: 2-4-16: 0.219 (0.289)\n",
      "-7: 3-2-16: 0.237 (0.287)\n",
      "-7: 3-3-16: 0.200 (0.329)\n",
      "-7: 2-2-32: 0.349 (0.458)\n",
      "-7: 2-3-32: 0.232 (0.296)\n",
      "-7: 2-4-32: 0.228 (0.300)\n",
      "-7: 3-2-32: 0.283 (0.386)\n",
      "-7: 3-3-32: 0.235 (0.272)\n",
      "-7: 2-2-64: 0.441 (0.474)\n",
      "-7: 2-3-64: 0.235 (0.292)\n",
      "-7: 3-2-64: 0.241 (0.344)\n",
      "-7: 3-3-64: 0.230 (0.309)\n",
      "-7: 2-2-128: 0.441 (0.474)\n",
      "-7: 2-3-128: 0.208 (0.298)\n",
      "-7: 3-2-128: 0.430 (0.447)\n",
      "-7: 3-3-128: 0.221 (0.263)\n",
      "-7: 2-2-256: 0.454 (0.485)\n",
      "-8: 2-2-16: 0.478 (0.487)\n",
      "-8: 2-3-16: 0.250 (0.281)\n",
      "-8: 2-4-16: 0.268 (0.300)\n",
      "-8: 3-2-16: 0.364 (0.368)\n",
      "-8: 3-3-16: 0.226 (0.289)\n",
      "-8: 2-2-32: 0.465 (0.493)\n",
      "-8: 2-3-32: 0.254 (0.263)\n",
      "-8: 2-4-32: 0.202 (0.285)\n",
      "-8: 3-2-32: 0.434 (0.445)\n",
      "-8: 3-3-32: 0.197 (0.292)\n",
      "-8: 2-2-64: 0.474 (0.502)\n",
      "-8: 2-3-64: 0.252 (0.274)\n",
      "-8: 3-2-64: 0.425 (0.469)\n",
      "-8: 3-3-64: 0.237 (0.318)\n",
      "-8: 2-2-128: 0.487 (0.502)\n",
      "-8: 2-3-128: 0.211 (0.314)\n",
      "-8: 3-2-128: 0.441 (0.487)\n",
      "-8: 3-3-128: 0.204 (0.318)\n",
      "-8: 2-2-256: 0.463 (0.511)\n",
      "-9: 2-2-16: 0.281 (0.366)\n",
      "-9: 2-3-16: 0.270 (0.292)\n",
      "-9: 2-4-16: 0.259 (0.265)\n",
      "-9: 3-2-16: 0.281 (0.303)\n",
      "-9: 3-3-16: 0.230 (0.259)\n",
      "-9: 2-2-32: 0.476 (0.496)\n",
      "-9: 2-3-32: 0.276 (0.285)\n",
      "-9: 2-4-32: 0.272 (0.279)\n",
      "-9: 3-2-32: 0.283 (0.303)\n",
      "-9: 3-3-32: 0.237 (0.276)\n",
      "-9: 2-2-64: 0.461 (0.469)\n",
      "-9: 2-3-64: 0.204 (0.305)\n",
      "-9: 3-2-64: 0.276 (0.300)\n",
      "-9: 3-3-64: 0.254 (0.272)\n",
      "-9: 2-2-128: 0.471 (0.502)\n",
      "-9: 2-3-128: 0.377 (0.388)\n",
      "-9: 3-2-128: 0.274 (0.311)\n",
      "-9: 3-3-128: 0.261 (0.279)\n",
      "-9: 2-2-256: 0.469 (0.502)\n",
      "-10: 2-2-16: 0.289 (0.428)\n",
      "-10: 2-3-16: 0.300 (0.322)\n",
      "-10: 2-4-16: 0.217 (0.320)\n",
      "-10: 3-2-16: 0.294 (0.331)\n",
      "-10: 3-3-16: 0.219 (0.283)\n",
      "-10: 2-2-32: 0.386 (0.461)\n",
      "-10: 2-3-32: 0.235 (0.325)\n",
      "-10: 2-4-32: 0.235 (0.316)\n",
      "-10: 3-2-32: 0.241 (0.331)\n",
      "-10: 3-3-32: 0.254 (0.292)\n",
      "-10: 2-2-64: 0.476 (0.507)\n",
      "-10: 2-3-64: 0.279 (0.320)\n",
      "-10: 3-2-64: 0.333 (0.388)\n",
      "-10: 3-3-64: 0.261 (0.281)\n",
      "-10: 2-2-128: 0.489 (0.500)\n",
      "-10: 2-3-128: 0.355 (0.406)\n",
      "-10: 3-2-128: 0.325 (0.417)\n",
      "-10: 3-3-128: 0.246 (0.270)\n",
      "-10: 2-2-256: 0.467 (0.498)\n",
      "-11: 2-2-16: 0.357 (0.465)\n",
      "-11: 2-3-16: 0.283 (0.316)\n",
      "-11: 2-4-16: 0.265 (0.287)\n",
      "-11: 3-2-16: 0.320 (0.382)\n",
      "-11: 3-3-16: 0.259 (0.272)\n",
      "-11: 2-2-32: 0.338 (0.474)\n",
      "-11: 2-3-32: 0.287 (0.307)\n",
      "-11: 2-4-32: 0.270 (0.283)\n",
      "-11: 3-2-32: 0.285 (0.340)\n",
      "-11: 3-3-32: 0.226 (0.296)\n",
      "-11: 2-2-64: 0.349 (0.515)\n",
      "-11: 2-3-64: 0.279 (0.331)\n",
      "-11: 3-2-64: 0.272 (0.292)\n",
      "-11: 3-3-64: 0.224 (0.252)\n",
      "-11: 2-2-128: 0.469 (0.511)\n",
      "-11: 2-3-128: 0.322 (0.362)\n",
      "-11: 3-2-128: 0.270 (0.311)\n",
      "-11: 3-3-128: 0.250 (0.289)\n",
      "-11: 2-2-256: 0.485 (0.515)\n",
      "-12: 2-2-16: 0.265 (0.349)\n",
      "-12: 2-3-16: 0.213 (0.248)\n",
      "-12: 2-4-16: 0.213 (0.230)\n",
      "-12: 3-2-16: 0.228 (0.272)\n",
      "-12: 3-3-16: 0.320 (0.338)\n",
      "-12: 2-2-32: 0.406 (0.489)\n",
      "-12: 2-3-32: 0.206 (0.285)\n",
      "-12: 2-4-32: 0.204 (0.296)\n",
      "-12: 3-2-32: 0.331 (0.351)\n",
      "-12: 3-3-32: 0.248 (0.276)\n",
      "-12: 2-2-64: 0.493 (0.507)\n",
      "-12: 2-3-64: 0.289 (0.355)\n",
      "-12: 3-2-64: 0.465 (0.480)\n",
      "-12: 3-3-64: 0.200 (0.289)\n",
      "-12: 2-2-128: 0.491 (0.511)\n",
      "-12: 2-3-128: 0.232 (0.254)\n",
      "-12: 3-2-128: 0.263 (0.342)\n",
      "-12: 3-3-128: 0.246 (0.322)\n",
      "-12: 2-2-256: 0.485 (0.509)\n",
      "-13: 2-2-16: 0.399 (0.423)\n",
      "-13: 2-3-16: 0.246 (0.270)\n",
      "-13: 2-4-16: 0.221 (0.289)\n",
      "-13: 3-2-16: 0.294 (0.329)\n",
      "-13: 3-3-16: 0.217 (0.261)\n",
      "-13: 2-2-32: 0.447 (0.463)\n",
      "-13: 2-3-32: 0.211 (0.265)\n",
      "-13: 2-4-32: 0.283 (0.285)\n",
      "-13: 3-2-32: 0.289 (0.316)\n",
      "-13: 3-3-32: 0.169 (0.246)\n",
      "-13: 2-2-64: 0.443 (0.467)\n",
      "-13: 2-3-64: 0.281 (0.349)\n",
      "-13: 3-2-64: 0.351 (0.382)\n",
      "-13: 3-3-64: 0.241 (0.272)\n",
      "-13: 2-2-128: 0.434 (0.465)\n",
      "-13: 2-3-128: 0.338 (0.357)\n",
      "-13: 3-2-128: 0.412 (0.443)\n",
      "-13: 3-3-128: 0.224 (0.265)\n",
      "-13: 2-2-256: 0.441 (0.456)\n",
      "-14: 2-2-16: 0.476 (0.496)\n",
      "-14: 2-3-16: 0.300 (0.316)\n",
      "-14: 2-4-16: 0.285 (0.318)\n",
      "-14: 3-2-16: 0.292 (0.300)\n",
      "-14: 3-3-16: 0.226 (0.300)\n",
      "-14: 2-2-32: 0.491 (0.498)\n",
      "-14: 2-3-32: 0.289 (0.316)\n",
      "-14: 2-4-32: 0.300 (0.307)\n",
      "-14: 3-2-32: 0.434 (0.436)\n",
      "-14: 3-3-32: 0.265 (0.309)\n",
      "-14: 2-2-64: 0.493 (0.504)\n",
      "-14: 2-3-64: 0.215 (0.325)\n",
      "-14: 3-2-64: 0.454 (0.469)\n",
      "-14: 3-3-64: 0.294 (0.336)\n",
      "-14: 2-2-128: 0.482 (0.511)\n",
      "-14: 2-3-128: 0.226 (0.325)\n",
      "-14: 3-2-128: 0.456 (0.474)\n",
      "-14: 3-3-128: 0.250 (0.294)\n",
      "-14: 2-2-256: 0.504 (0.511)\n",
      "-15: 2-2-16: 0.412 (0.445)\n",
      "-15: 2-3-16: 0.246 (0.289)\n",
      "-15: 2-4-16: 0.228 (0.270)\n",
      "-15: 3-2-16: 0.303 (0.307)\n",
      "-15: 3-3-16: 0.226 (0.307)\n",
      "-15: 2-2-32: 0.482 (0.496)\n",
      "-15: 2-3-32: 0.261 (0.292)\n",
      "-15: 2-4-32: 0.228 (0.327)\n",
      "-15: 3-2-32: 0.272 (0.274)\n",
      "-15: 3-3-32: 0.202 (0.272)\n",
      "-15: 2-2-64: 0.485 (0.511)\n",
      "-15: 2-3-64: 0.276 (0.322)\n",
      "-15: 3-2-64: 0.456 (0.458)\n",
      "-15: 3-3-64: 0.221 (0.276)\n",
      "-15: 2-2-128: 0.465 (0.502)\n",
      "-15: 2-3-128: 0.303 (0.382)\n",
      "-15: 3-2-128: 0.450 (0.489)\n",
      "-15: 3-3-128: 0.182 (0.322)\n",
      "-15: 2-2-256: 0.463 (0.489)\n",
      "-16: 2-2-16: 0.443 (0.465)\n",
      "-16: 2-3-16: 0.202 (0.294)\n",
      "-16: 2-4-16: 0.252 (0.274)\n",
      "-16: 3-2-16: 0.270 (0.303)\n",
      "-16: 3-3-16: 0.272 (0.276)\n",
      "-16: 2-2-32: 0.480 (0.487)\n",
      "-16: 2-3-32: 0.298 (0.307)\n",
      "-16: 2-4-32: 0.268 (0.283)\n",
      "-16: 3-2-32: 0.397 (0.408)\n",
      "-16: 3-3-32: 0.235 (0.351)\n",
      "-16: 2-2-64: 0.463 (0.489)\n",
      "-16: 2-3-64: 0.327 (0.333)\n",
      "-16: 3-2-64: 0.397 (0.397)\n",
      "-16: 3-3-64: 0.217 (0.281)\n",
      "-16: 2-2-128: 0.474 (0.482)\n",
      "-16: 2-3-128: 0.298 (0.311)\n",
      "-16: 3-2-128: 0.441 (0.461)\n",
      "-16: 3-3-128: 0.184 (0.254)\n",
      "-16: 2-2-256: 0.480 (0.498)\n",
      "-17: 2-2-16: 0.338 (0.428)\n",
      "-17: 2-3-16: 0.215 (0.270)\n",
      "-17: 2-4-16: 0.221 (0.292)\n",
      "-17: 3-2-16: 0.314 (0.364)\n",
      "-17: 3-3-16: 0.226 (0.259)\n",
      "-17: 2-2-32: 0.458 (0.469)\n",
      "-17: 2-3-32: 0.248 (0.283)\n",
      "-17: 2-4-32: 0.186 (0.289)\n",
      "-17: 3-2-32: 0.276 (0.375)\n",
      "-17: 3-3-32: 0.232 (0.263)\n",
      "-17: 2-2-64: 0.456 (0.474)\n",
      "-17: 2-3-64: 0.259 (0.327)\n",
      "-17: 3-2-64: 0.281 (0.384)\n",
      "-17: 3-3-64: 0.237 (0.281)\n",
      "-17: 2-2-128: 0.452 (0.478)\n",
      "-17: 2-3-128: 0.204 (0.285)\n",
      "-17: 3-2-128: 0.362 (0.445)\n",
      "-17: 3-3-128: 0.221 (0.283)\n",
      "-17: 2-2-256: 0.467 (0.485)\n",
      "-18: 2-2-16: 0.465 (0.487)\n",
      "-18: 2-3-16: 0.252 (0.270)\n",
      "-18: 2-4-16: 0.241 (0.268)\n",
      "-18: 3-2-16: 0.384 (0.399)\n",
      "-18: 3-3-16: 0.167 (0.211)\n",
      "-18: 2-2-32: 0.469 (0.498)\n",
      "-18: 2-3-32: 0.261 (0.272)\n",
      "-18: 2-4-32: 0.263 (0.296)\n",
      "-18: 3-2-32: 0.375 (0.419)\n",
      "-18: 3-3-32: 0.186 (0.283)\n",
      "-18: 2-2-64: 0.456 (0.485)\n",
      "-18: 2-3-64: 0.314 (0.342)\n",
      "-18: 3-2-64: 0.463 (0.498)\n",
      "-18: 3-3-64: 0.208 (0.298)\n",
      "-18: 2-2-128: 0.471 (0.502)\n",
      "-18: 2-3-128: 0.164 (0.274)\n",
      "-18: 3-2-128: 0.443 (0.507)\n",
      "-18: 3-3-128: 0.252 (0.257)\n",
      "-18: 2-2-256: 0.491 (0.498)\n",
      "-19: 2-2-16: 0.458 (0.458)\n",
      "-19: 2-3-16: 0.265 (0.289)\n",
      "-19: 2-4-16: 0.259 (0.318)\n",
      "-19: 3-2-16: 0.270 (0.336)\n",
      "-19: 3-3-16: 0.237 (0.281)\n",
      "-19: 2-2-32: 0.471 (0.500)\n",
      "-19: 2-3-32: 0.296 (0.311)\n",
      "-19: 2-4-32: 0.279 (0.283)\n",
      "-19: 3-2-32: 0.289 (0.305)\n",
      "-19: 3-3-32: 0.246 (0.257)\n",
      "-19: 2-2-64: 0.447 (0.471)\n",
      "-19: 2-3-64: 0.285 (0.316)\n",
      "-19: 3-2-64: 0.401 (0.410)\n",
      "-19: 3-3-64: 0.259 (0.316)\n",
      "-19: 2-2-128: 0.469 (0.489)\n",
      "-19: 2-3-128: 0.371 (0.393)\n",
      "-19: 3-2-128: 0.279 (0.333)\n",
      "-19: 3-3-128: 0.272 (0.272)\n",
      "-19: 2-2-256: 0.467 (0.500)\n",
      "-20: 2-2-16: 0.450 (0.478)\n",
      "-20: 2-3-16: 0.250 (0.263)\n",
      "-20: 2-4-16: 0.268 (0.292)\n",
      "-20: 3-2-16: 0.235 (0.283)\n",
      "-20: 3-3-16: 0.241 (0.265)\n",
      "-20: 2-2-32: 0.331 (0.458)\n",
      "-20: 2-3-32: 0.239 (0.303)\n",
      "-20: 2-4-32: 0.261 (0.309)\n",
      "-20: 3-2-32: 0.237 (0.309)\n",
      "-20: 3-3-32: 0.250 (0.305)\n",
      "-20: 2-2-64: 0.498 (0.507)\n",
      "-20: 2-3-64: 0.285 (0.320)\n",
      "-20: 3-2-64: 0.309 (0.368)\n",
      "-20: 3-3-64: 0.237 (0.373)\n",
      "-20: 2-2-128: 0.480 (0.513)\n",
      "-20: 2-3-128: 0.327 (0.375)\n",
      "-20: 3-2-128: 0.303 (0.408)\n",
      "-20: 3-3-128: 0.248 (0.276)\n",
      "-20: 2-2-256: 0.480 (0.502)\n",
      "----- 57.13 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [\n",
    "        # {'L': 2, 'K': 2, 'hid_dim': 8},\n",
    "        # {'L': 2, 'K': 3, 'hid_dim': 8},\n",
    "        # {'L': 2, 'K': 4, 'hid_dim': 8},\n",
    "\n",
    "        # {'L': 3, 'K': 2, 'hid_dim': 8},\n",
    "        # {'L': 4, 'K': 2, 'hid_dim': 8},\n",
    "        # {'L': 3, 'K': 3, 'hid_dim': 8},\n",
    "        # {'L': 4, 'K': 3, 'hid_dim': 8},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 16},\n",
    "        # {'L': 4, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 16},\n",
    "        # {'L': 4, 'K': 3, 'hid_dim': 16},\n",
    "    \n",
    "        {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 32},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 32},\n",
    "        # {'L': 4, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 32},\n",
    "        # {'L': 4, 'K': 3, 'hid_dim': 32},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 64},\n",
    "        # {'L': 2, 'K': 4, 'hid_dim': 64},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 64},\n",
    "        # {'L': 4, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 64},\n",
    "        # {'L': 4, 'K': 3, 'hid_dim': 64},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 128},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 128},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 128},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 128},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 256},\n",
    "        ]\n",
    "\n",
    "best_accs3 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs3 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, exp['hid_dim'], OUT_DIM, exp['L'], exp['K'], act=ACT, l_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0, diff_layer=GFGCN_noh_Layer)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "\n",
    "        best_accs3[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs3[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}: {best_val_accs3[j,i]:.3f} ({best_accs3[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}' for exp in EXPS]\n",
    "table_over3 = summary_table(best_accs3, index_name)\n",
    "table3 = summary_table(best_val_accs3, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.379276</td>\n",
       "      <td>0.407895</td>\n",
       "      <td>0.083970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.256031</td>\n",
       "      <td>0.256579</td>\n",
       "      <td>0.029273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-16</th>\n",
       "      <td>0.245724</td>\n",
       "      <td>0.255482</td>\n",
       "      <td>0.028182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.297588</td>\n",
       "      <td>0.292763</td>\n",
       "      <td>0.047340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-16</th>\n",
       "      <td>0.223684</td>\n",
       "      <td>0.225877</td>\n",
       "      <td>0.034957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.436404</td>\n",
       "      <td>0.464912</td>\n",
       "      <td>0.057901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.261952</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.026320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-32</th>\n",
       "      <td>0.241886</td>\n",
       "      <td>0.247807</td>\n",
       "      <td>0.037320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-32</th>\n",
       "      <td>0.321382</td>\n",
       "      <td>0.290570</td>\n",
       "      <td>0.058703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-32</th>\n",
       "      <td>0.225110</td>\n",
       "      <td>0.233553</td>\n",
       "      <td>0.025417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-64</th>\n",
       "      <td>0.463925</td>\n",
       "      <td>0.471491</td>\n",
       "      <td>0.031806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-64</th>\n",
       "      <td>0.277741</td>\n",
       "      <td>0.282895</td>\n",
       "      <td>0.032135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-64</th>\n",
       "      <td>0.376206</td>\n",
       "      <td>0.391447</td>\n",
       "      <td>0.076612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-64</th>\n",
       "      <td>0.238268</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.023128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-128</th>\n",
       "      <td>0.472697</td>\n",
       "      <td>0.472588</td>\n",
       "      <td>0.020021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-128</th>\n",
       "      <td>0.287829</td>\n",
       "      <td>0.299342</td>\n",
       "      <td>0.068489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-128</th>\n",
       "      <td>0.367105</td>\n",
       "      <td>0.387061</td>\n",
       "      <td>0.073807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-128</th>\n",
       "      <td>0.235088</td>\n",
       "      <td>0.245614</td>\n",
       "      <td>0.025258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-256</th>\n",
       "      <td>0.474123</td>\n",
       "      <td>0.470395</td>\n",
       "      <td>0.016995</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean accs       med       std\n",
       "2-2-16    0.379276  0.407895  0.083970\n",
       "2-3-16    0.256031  0.256579  0.029273\n",
       "2-4-16    0.245724  0.255482  0.028182\n",
       "3-2-16    0.297588  0.292763  0.047340\n",
       "3-3-16    0.223684  0.225877  0.034957\n",
       "2-2-32    0.436404  0.464912  0.057901\n",
       "2-3-32    0.261952  0.263158  0.026320\n",
       "2-4-32    0.241886  0.247807  0.037320\n",
       "3-2-32    0.321382  0.290570  0.058703\n",
       "3-3-32    0.225110  0.233553  0.025417\n",
       "2-2-64    0.463925  0.471491  0.031806\n",
       "2-3-64    0.277741  0.282895  0.032135\n",
       "3-2-64    0.376206  0.391447  0.076612\n",
       "3-3-64    0.238268  0.236842  0.023128\n",
       "2-2-128   0.472697  0.472588  0.020021\n",
       "2-3-128   0.287829  0.299342  0.068489\n",
       "3-2-128   0.367105  0.387061  0.073807\n",
       "3-3-128   0.235088  0.245614  0.025258\n",
       "2-2-256   0.474123  0.470395  0.016995"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.434539</td>\n",
       "      <td>0.446272</td>\n",
       "      <td>0.049686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.288925</td>\n",
       "      <td>0.289474</td>\n",
       "      <td>0.020394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-16</th>\n",
       "      <td>0.289803</td>\n",
       "      <td>0.290570</td>\n",
       "      <td>0.021590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.327851</td>\n",
       "      <td>0.313596</td>\n",
       "      <td>0.040362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-16</th>\n",
       "      <td>0.279715</td>\n",
       "      <td>0.280702</td>\n",
       "      <td>0.027596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.483114</td>\n",
       "      <td>0.486842</td>\n",
       "      <td>0.016703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.296053</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.018608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-32</th>\n",
       "      <td>0.288596</td>\n",
       "      <td>0.287281</td>\n",
       "      <td>0.018327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-32</th>\n",
       "      <td>0.358772</td>\n",
       "      <td>0.356360</td>\n",
       "      <td>0.046133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-32</th>\n",
       "      <td>0.284211</td>\n",
       "      <td>0.278509</td>\n",
       "      <td>0.025094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-64</th>\n",
       "      <td>0.492654</td>\n",
       "      <td>0.502193</td>\n",
       "      <td>0.018114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-64</th>\n",
       "      <td>0.324342</td>\n",
       "      <td>0.324561</td>\n",
       "      <td>0.018927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-64</th>\n",
       "      <td>0.407346</td>\n",
       "      <td>0.400219</td>\n",
       "      <td>0.064198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-64</th>\n",
       "      <td>0.293421</td>\n",
       "      <td>0.285088</td>\n",
       "      <td>0.027744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-128</th>\n",
       "      <td>0.496820</td>\n",
       "      <td>0.502193</td>\n",
       "      <td>0.016636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-128</th>\n",
       "      <td>0.341118</td>\n",
       "      <td>0.327851</td>\n",
       "      <td>0.046196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-128</th>\n",
       "      <td>0.417325</td>\n",
       "      <td>0.444079</td>\n",
       "      <td>0.060094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-128</th>\n",
       "      <td>0.288377</td>\n",
       "      <td>0.283991</td>\n",
       "      <td>0.024445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-256</th>\n",
       "      <td>0.496711</td>\n",
       "      <td>0.497807</td>\n",
       "      <td>0.014654</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean accs       med       std\n",
       "2-2-16    0.434539  0.446272  0.049686\n",
       "2-3-16    0.288925  0.289474  0.020394\n",
       "2-4-16    0.289803  0.290570  0.021590\n",
       "3-2-16    0.327851  0.313596  0.040362\n",
       "3-3-16    0.279715  0.280702  0.027596\n",
       "2-2-32    0.483114  0.486842  0.016703\n",
       "2-3-32    0.296053  0.291667  0.018608\n",
       "2-4-32    0.288596  0.287281  0.018327\n",
       "3-2-32    0.358772  0.356360  0.046133\n",
       "3-3-32    0.284211  0.278509  0.025094\n",
       "2-2-64    0.492654  0.502193  0.018114\n",
       "2-3-64    0.324342  0.324561  0.018927\n",
       "3-2-64    0.407346  0.400219  0.064198\n",
       "3-3-64    0.293421  0.285088  0.027744\n",
       "2-2-128   0.496820  0.502193  0.016636\n",
       "2-3-128   0.341118  0.327851  0.046196\n",
       "3-2-128   0.417325  0.444079  0.060094\n",
       "3-3-128   0.288377  0.283991  0.024445\n",
       "2-2-256   0.496711  0.497807  0.014654"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_over3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design - Bias False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 2-2-16: 0.248 (0.303)\n",
      "-1: 2-3-16: 0.283 (0.320)\n",
      "-1: 2-4-16: 0.272 (0.322)\n",
      "-1: 3-2-16: 0.410 (0.423)\n",
      "-1: 3-3-16: 0.202 (0.261)\n",
      "-1: 2-2-32: 0.318 (0.487)\n",
      "-1: 2-3-32: 0.279 (0.292)\n",
      "-1: 2-4-32: 0.285 (0.298)\n",
      "-2: 2-2-16: 0.270 (0.371)\n",
      "-2: 2-3-16: 0.204 (0.272)\n",
      "-2: 2-4-16: 0.268 (0.285)\n",
      "-2: 3-2-16: 0.265 (0.292)\n",
      "-2: 3-3-16: 0.228 (0.298)\n",
      "-2: 2-2-32: 0.498 (0.513)\n",
      "-2: 2-3-32: 0.265 (0.333)\n",
      "-2: 2-4-32: 0.217 (0.265)\n",
      "-3: 2-2-16: 0.404 (0.436)\n",
      "-3: 2-3-16: 0.268 (0.311)\n",
      "-3: 2-4-16: 0.206 (0.296)\n",
      "-3: 3-2-16: 0.285 (0.311)\n",
      "-3: 3-3-16: 0.241 (0.248)\n",
      "-3: 2-2-32: 0.434 (0.458)\n",
      "-3: 2-3-32: 0.268 (0.287)\n",
      "-3: 2-4-32: 0.184 (0.268)\n",
      "-4: 2-2-16: 0.432 (0.456)\n",
      "-4: 2-3-16: 0.276 (0.289)\n",
      "-4: 2-4-16: 0.298 (0.305)\n",
      "-4: 3-2-16: 0.298 (0.298)\n",
      "-4: 3-3-16: 0.164 (0.281)\n",
      "-4: 2-2-32: 0.487 (0.502)\n",
      "-4: 2-3-32: 0.294 (0.320)\n",
      "-4: 2-4-32: 0.270 (0.292)\n",
      "-5: 2-2-16: 0.461 (0.471)\n",
      "-5: 2-3-16: 0.261 (0.272)\n",
      "-5: 2-4-16: 0.193 (0.292)\n",
      "-5: 3-2-16: 0.261 (0.316)\n",
      "-5: 3-3-16: 0.191 (0.287)\n",
      "-5: 2-2-32: 0.476 (0.487)\n",
      "-5: 2-3-32: 0.279 (0.287)\n",
      "-5: 2-4-32: 0.232 (0.254)\n",
      "-6: 2-2-16: 0.432 (0.447)\n",
      "-6: 2-3-16: 0.292 (0.298)\n",
      "-6: 2-4-16: 0.261 (0.283)\n",
      "-6: 3-2-16: 0.349 (0.353)\n",
      "-6: 3-3-16: 0.182 (0.289)\n",
      "-6: 2-2-32: 0.465 (0.476)\n",
      "-6: 2-3-32: 0.261 (0.292)\n",
      "-6: 2-4-32: 0.171 (0.257)\n",
      "-7: 2-2-16: 0.228 (0.432)\n",
      "-7: 2-3-16: 0.246 (0.296)\n",
      "-7: 2-4-16: 0.219 (0.289)\n",
      "-7: 3-2-16: 0.237 (0.287)\n",
      "-7: 3-3-16: 0.200 (0.329)\n",
      "-7: 2-2-32: 0.349 (0.458)\n",
      "-7: 2-3-32: 0.232 (0.296)\n",
      "-7: 2-4-32: 0.228 (0.300)\n",
      "-8: 2-2-16: 0.478 (0.487)\n",
      "-8: 2-3-16: 0.250 (0.281)\n",
      "-8: 2-4-16: 0.268 (0.300)\n",
      "-8: 3-2-16: 0.364 (0.368)\n",
      "-8: 3-3-16: 0.226 (0.289)\n",
      "-8: 2-2-32: 0.465 (0.493)\n",
      "-8: 2-3-32: 0.254 (0.263)\n",
      "-8: 2-4-32: 0.202 (0.285)\n",
      "-9: 2-2-16: 0.281 (0.366)\n",
      "-9: 2-3-16: 0.270 (0.292)\n",
      "-9: 2-4-16: 0.259 (0.265)\n",
      "-9: 3-2-16: 0.281 (0.303)\n",
      "-9: 3-3-16: 0.230 (0.259)\n",
      "-9: 2-2-32: 0.476 (0.496)\n",
      "-9: 2-3-32: 0.276 (0.285)\n",
      "-9: 2-4-32: 0.272 (0.279)\n",
      "-10: 2-2-16: 0.289 (0.428)\n",
      "-10: 2-3-16: 0.300 (0.322)\n",
      "-10: 2-4-16: 0.217 (0.320)\n",
      "-10: 3-2-16: 0.294 (0.331)\n",
      "-10: 3-3-16: 0.219 (0.283)\n",
      "-10: 2-2-32: 0.386 (0.461)\n",
      "-10: 2-3-32: 0.235 (0.325)\n",
      "-10: 2-4-32: 0.235 (0.316)\n",
      "-11: 2-2-16: 0.357 (0.465)\n",
      "-11: 2-3-16: 0.283 (0.316)\n",
      "-11: 2-4-16: 0.265 (0.287)\n",
      "-11: 3-2-16: 0.320 (0.382)\n",
      "-11: 3-3-16: 0.259 (0.272)\n",
      "-11: 2-2-32: 0.338 (0.474)\n",
      "-11: 2-3-32: 0.287 (0.307)\n",
      "-11: 2-4-32: 0.270 (0.283)\n",
      "-12: 2-2-16: 0.265 (0.349)\n",
      "-12: 2-3-16: 0.213 (0.248)\n",
      "-12: 2-4-16: 0.213 (0.230)\n",
      "-12: 3-2-16: 0.228 (0.272)\n",
      "-12: 3-3-16: 0.320 (0.338)\n",
      "-12: 2-2-32: 0.406 (0.489)\n",
      "-12: 2-3-32: 0.206 (0.285)\n",
      "-12: 2-4-32: 0.204 (0.296)\n",
      "-13: 2-2-16: 0.399 (0.423)\n",
      "-13: 2-3-16: 0.246 (0.270)\n",
      "-13: 2-4-16: 0.221 (0.289)\n",
      "-13: 3-2-16: 0.294 (0.329)\n",
      "-13: 3-3-16: 0.217 (0.261)\n",
      "-13: 2-2-32: 0.447 (0.463)\n",
      "-13: 2-3-32: 0.211 (0.265)\n",
      "-13: 2-4-32: 0.283 (0.285)\n",
      "-14: 2-2-16: 0.476 (0.496)\n",
      "-14: 2-3-16: 0.300 (0.316)\n",
      "-14: 2-4-16: 0.285 (0.318)\n",
      "-14: 3-2-16: 0.292 (0.300)\n",
      "-14: 3-3-16: 0.226 (0.300)\n",
      "-14: 2-2-32: 0.491 (0.498)\n",
      "-14: 2-3-32: 0.289 (0.316)\n",
      "-14: 2-4-32: 0.300 (0.307)\n",
      "-15: 2-2-16: 0.412 (0.445)\n",
      "-15: 2-3-16: 0.246 (0.289)\n",
      "-15: 2-4-16: 0.228 (0.270)\n",
      "-15: 3-2-16: 0.303 (0.307)\n",
      "-15: 3-3-16: 0.226 (0.307)\n",
      "-15: 2-2-32: 0.482 (0.496)\n",
      "-15: 2-3-32: 0.261 (0.292)\n",
      "-15: 2-4-32: 0.228 (0.327)\n",
      "-16: 2-2-16: 0.443 (0.465)\n",
      "-16: 2-3-16: 0.202 (0.294)\n",
      "-16: 2-4-16: 0.252 (0.274)\n",
      "-16: 3-2-16: 0.270 (0.303)\n",
      "-16: 3-3-16: 0.272 (0.276)\n",
      "-16: 2-2-32: 0.480 (0.487)\n",
      "-16: 2-3-32: 0.298 (0.307)\n",
      "-16: 2-4-32: 0.268 (0.283)\n",
      "-17: 2-2-16: 0.338 (0.428)\n",
      "-17: 2-3-16: 0.215 (0.270)\n",
      "-17: 2-4-16: 0.221 (0.292)\n",
      "-17: 3-2-16: 0.314 (0.364)\n",
      "-17: 3-3-16: 0.226 (0.259)\n",
      "-17: 2-2-32: 0.458 (0.469)\n",
      "-17: 2-3-32: 0.248 (0.283)\n",
      "-17: 2-4-32: 0.186 (0.289)\n",
      "-18: 2-2-16: 0.465 (0.487)\n",
      "-18: 2-3-16: 0.252 (0.270)\n",
      "-18: 2-4-16: 0.241 (0.268)\n",
      "-18: 3-2-16: 0.384 (0.399)\n",
      "-18: 3-3-16: 0.167 (0.211)\n",
      "-18: 2-2-32: 0.469 (0.498)\n",
      "-18: 2-3-32: 0.261 (0.272)\n",
      "-18: 2-4-32: 0.263 (0.296)\n",
      "-19: 2-2-16: 0.458 (0.458)\n",
      "-19: 2-3-16: 0.265 (0.289)\n",
      "-19: 2-4-16: 0.259 (0.318)\n",
      "-19: 3-2-16: 0.270 (0.336)\n",
      "-19: 3-3-16: 0.237 (0.281)\n",
      "-19: 2-2-32: 0.471 (0.500)\n",
      "-19: 2-3-32: 0.296 (0.311)\n",
      "-19: 2-4-32: 0.279 (0.283)\n",
      "-20: 2-2-16: 0.450 (0.478)\n",
      "-20: 2-3-16: 0.250 (0.263)\n",
      "-20: 2-4-16: 0.268 (0.292)\n",
      "-20: 3-2-16: 0.235 (0.283)\n",
      "-20: 3-3-16: 0.241 (0.265)\n",
      "-20: 2-2-32: 0.331 (0.458)\n",
      "-20: 2-3-32: 0.239 (0.303)\n",
      "-20: 2-4-32: 0.261 (0.309)\n",
      "----- 27.62 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 16},\n",
    "        # {'L': 4, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 16},\n",
    "        # {'L': 4, 'K': 3, 'hid_dim': 16},\n",
    "    \n",
    "        {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 32},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 32},\n",
    "        # {'L': 4, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 32},\n",
    "        # {'L': 4, 'K': 3, 'hid_dim': 32},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 64},\n",
    "        # {'L': 2, 'K': 4, 'hid_dim': 64},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 64},\n",
    "        # {'L': 4, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 64},\n",
    "        # {'L': 4, 'K': 3, 'hid_dim': 64},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 128},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 128},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 128},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 128},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 256},\n",
    "        ]\n",
    "\n",
    "best_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, exp['hid_dim'], OUT_DIM, exp['L'], exp['K'], act=ACT, l_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0, diff_layer=GFGCN_noh_Layer, bias=False)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "\n",
    "        best_accs3b[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs3b[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}: {best_val_accs3[j,i]:.3f} ({best_accs3[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}' for exp in EXPS]\n",
    "table_over3b = summary_table(best_accs3b, index_name)\n",
    "table3b = summary_table(best_val_accs3b, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.365461</td>\n",
       "      <td>0.404605</td>\n",
       "      <td>0.075214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.230592</td>\n",
       "      <td>0.228070</td>\n",
       "      <td>0.027454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-16</th>\n",
       "      <td>0.199123</td>\n",
       "      <td>0.195175</td>\n",
       "      <td>0.023969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.285965</td>\n",
       "      <td>0.292763</td>\n",
       "      <td>0.048788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-16</th>\n",
       "      <td>0.206360</td>\n",
       "      <td>0.211623</td>\n",
       "      <td>0.025356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.428728</td>\n",
       "      <td>0.430921</td>\n",
       "      <td>0.030224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.262390</td>\n",
       "      <td>0.256579</td>\n",
       "      <td>0.034600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-32</th>\n",
       "      <td>0.202632</td>\n",
       "      <td>0.205044</td>\n",
       "      <td>0.022720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean accs       med       std\n",
       "2-2-16   0.365461  0.404605  0.075214\n",
       "2-3-16   0.230592  0.228070  0.027454\n",
       "2-4-16   0.199123  0.195175  0.023969\n",
       "3-2-16   0.285965  0.292763  0.048788\n",
       "3-3-16   0.206360  0.211623  0.025356\n",
       "2-2-32   0.428728  0.430921  0.030224\n",
       "2-3-32   0.262390  0.256579  0.034600\n",
       "2-4-32   0.202632  0.205044  0.022720"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_over3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearities and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.450 (0.487)\n",
      "-1: ReLU()-Softmax(dim=1)-NLLLoss(): 0.471 (0.485)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.480 (0.504)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.485 (0.507)\n",
      "-1: ReLU()-Identity()-CrossEntropyLoss(): 0.498 (0.511)\n",
      "-1: ReLU()-Identity()-NLLLoss(): 0.211 (0.248)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.458 (0.480)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.445 (0.485)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.456 (0.496)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.461 (0.491)\n",
      "-1: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.452 (0.485)\n",
      "-1: Identity()-Softmax(dim=1)-NLLLoss(): 0.439 (0.476)\n",
      "-1: Identity()-Identity()-CrossEntropyLoss(): 0.450 (0.491)\n",
      "-2: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.465 (0.482)\n",
      "-2: ReLU()-Softmax(dim=1)-NLLLoss(): 0.461 (0.478)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.485 (0.504)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.436 (0.487)\n",
      "-2: ReLU()-Identity()-CrossEntropyLoss(): 0.476 (0.500)\n",
      "-2: ReLU()-Identity()-NLLLoss(): 0.217 (0.252)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.458 (0.467)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.452 (0.478)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.452 (0.469)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.458 (0.471)\n",
      "-2: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.452 (0.480)\n",
      "-2: Identity()-Softmax(dim=1)-NLLLoss(): 0.450 (0.482)\n",
      "-2: Identity()-Identity()-CrossEntropyLoss(): 0.518 (0.537)\n",
      "-3: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.408 (0.421)\n",
      "-3: ReLU()-Softmax(dim=1)-NLLLoss(): 0.384 (0.417)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.445 (0.458)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.441 (0.445)\n",
      "-3: ReLU()-Identity()-CrossEntropyLoss(): 0.439 (0.467)\n",
      "-3: ReLU()-Identity()-NLLLoss(): 0.195 (0.268)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.408 (0.428)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.404 (0.434)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.430 (0.445)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.428 (0.439)\n",
      "-3: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.417 (0.458)\n",
      "-3: Identity()-Softmax(dim=1)-NLLLoss(): 0.421 (0.452)\n",
      "-3: Identity()-Identity()-CrossEntropyLoss(): 0.414 (0.456)\n",
      "-4: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.476 (0.491)\n",
      "-4: ReLU()-Softmax(dim=1)-NLLLoss(): 0.452 (0.458)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.502 (0.511)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.474 (0.491)\n",
      "-4: ReLU()-Identity()-CrossEntropyLoss(): 0.493 (0.511)\n",
      "-4: ReLU()-Identity()-NLLLoss(): 0.178 (0.257)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.441 (0.482)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.450 (0.476)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.452 (0.469)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.458 (0.478)\n",
      "-4: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.469 (0.485)\n",
      "-4: Identity()-Softmax(dim=1)-NLLLoss(): 0.474 (0.491)\n",
      "-4: Identity()-Identity()-CrossEntropyLoss(): 0.498 (0.507)\n",
      "-5: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.423 (0.456)\n",
      "-5: ReLU()-Softmax(dim=1)-NLLLoss(): 0.414 (0.454)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.471 (0.491)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.461 (0.485)\n",
      "-5: ReLU()-Identity()-CrossEntropyLoss(): 0.476 (0.496)\n",
      "-5: ReLU()-Identity()-NLLLoss(): 0.232 (0.314)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.436 (0.461)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.414 (0.465)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.432 (0.454)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.423 (0.461)\n",
      "-5: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.408 (0.458)\n",
      "-5: Identity()-Softmax(dim=1)-NLLLoss(): 0.406 (0.456)\n",
      "-5: Identity()-Identity()-CrossEntropyLoss(): 0.480 (0.487)\n",
      "-6: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.456 (0.480)\n",
      "-6: ReLU()-Softmax(dim=1)-NLLLoss(): 0.445 (0.474)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.456 (0.480)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.452 (0.471)\n",
      "-6: ReLU()-Identity()-CrossEntropyLoss(): 0.480 (0.485)\n",
      "-6: ReLU()-Identity()-NLLLoss(): 0.237 (0.270)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.467 (0.485)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.447 (0.476)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.452 (0.485)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.452 (0.489)\n",
      "-6: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.452 (0.482)\n",
      "-6: Identity()-Softmax(dim=1)-NLLLoss(): 0.456 (0.478)\n",
      "-6: Identity()-Identity()-CrossEntropyLoss(): 0.454 (0.474)\n",
      "-7: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.441 (0.471)\n",
      "-7: ReLU()-Softmax(dim=1)-NLLLoss(): 0.452 (0.465)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.461 (0.485)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.450 (0.485)\n",
      "-7: ReLU()-Identity()-CrossEntropyLoss(): 0.443 (0.476)\n",
      "-7: ReLU()-Identity()-NLLLoss(): 0.219 (0.261)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.445 (0.467)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.450 (0.476)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.447 (0.476)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.465 (0.471)\n",
      "-7: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.454 (0.467)\n",
      "-7: Identity()-Softmax(dim=1)-NLLLoss(): 0.447 (0.467)\n",
      "-7: Identity()-Identity()-CrossEntropyLoss(): 0.412 (0.478)\n",
      "-8: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.478 (0.511)\n",
      "-8: ReLU()-Softmax(dim=1)-NLLLoss(): 0.474 (0.496)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.482 (0.507)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.485 (0.502)\n",
      "-8: ReLU()-Identity()-CrossEntropyLoss(): 0.471 (0.489)\n",
      "-8: ReLU()-Identity()-NLLLoss(): 0.213 (0.215)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.493 (0.504)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.471 (0.509)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.487 (0.502)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.478 (0.500)\n",
      "-8: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.496 (0.513)\n",
      "-8: Identity()-Softmax(dim=1)-NLLLoss(): 0.489 (0.509)\n",
      "-8: Identity()-Identity()-CrossEntropyLoss(): 0.436 (0.480)\n",
      "-9: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.465 (0.478)\n",
      "-9: ReLU()-Softmax(dim=1)-NLLLoss(): 0.458 (0.463)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.480 (0.496)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.456 (0.478)\n",
      "-9: ReLU()-Identity()-CrossEntropyLoss(): 0.487 (0.502)\n",
      "-9: ReLU()-Identity()-NLLLoss(): 0.226 (0.230)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.458 (0.474)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.432 (0.463)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.474 (0.485)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.456 (0.476)\n",
      "-9: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.463 (0.476)\n",
      "-9: Identity()-Softmax(dim=1)-NLLLoss(): 0.458 (0.463)\n",
      "-9: Identity()-Identity()-CrossEntropyLoss(): 0.423 (0.458)\n",
      "-10: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.439 (0.461)\n",
      "-10: ReLU()-Softmax(dim=1)-NLLLoss(): 0.430 (0.461)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.461 (0.504)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.482 (0.515)\n",
      "-10: ReLU()-Identity()-CrossEntropyLoss(): 0.485 (0.513)\n",
      "-10: ReLU()-Identity()-NLLLoss(): 0.254 (0.254)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.454 (0.463)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.432 (0.454)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.439 (0.461)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.441 (0.447)\n",
      "-10: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.445 (0.454)\n",
      "-10: Identity()-Softmax(dim=1)-NLLLoss(): 0.450 (0.456)\n",
      "-10: Identity()-Identity()-CrossEntropyLoss(): 0.511 (0.518)\n",
      "-11: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.461 (0.478)\n",
      "-11: ReLU()-Softmax(dim=1)-NLLLoss(): 0.447 (0.471)\n",
      "-11: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.480 (0.507)\n",
      "-11: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.408 (0.507)\n",
      "-11: ReLU()-Identity()-CrossEntropyLoss(): 0.485 (0.513)\n",
      "-11: ReLU()-Identity()-NLLLoss(): 0.213 (0.257)\n",
      "-11: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.450 (0.487)\n",
      "-11: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.434 (0.458)\n",
      "-11: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.456 (0.487)\n",
      "-11: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.456 (0.485)\n",
      "-11: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.465 (0.482)\n",
      "-11: Identity()-Softmax(dim=1)-NLLLoss(): 0.456 (0.487)\n",
      "-11: Identity()-Identity()-CrossEntropyLoss(): 0.485 (0.504)\n",
      "-12: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.456 (0.485)\n",
      "-12: ReLU()-Softmax(dim=1)-NLLLoss(): 0.454 (0.469)\n",
      "-12: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.327 (0.502)\n",
      "-12: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.480 (0.498)\n",
      "-12: ReLU()-Identity()-CrossEntropyLoss(): 0.491 (0.515)\n",
      "-12: ReLU()-Identity()-NLLLoss(): 0.213 (0.300)\n",
      "-12: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.458 (0.482)\n",
      "-12: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.450 (0.480)\n",
      "-12: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.467 (0.482)\n",
      "-12: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.454 (0.476)\n",
      "-12: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.463 (0.474)\n",
      "-12: Identity()-Softmax(dim=1)-NLLLoss(): 0.478 (0.480)\n",
      "-12: Identity()-Identity()-CrossEntropyLoss(): 0.482 (0.504)\n",
      "-13: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.417 (0.425)\n",
      "-13: ReLU()-Softmax(dim=1)-NLLLoss(): 0.412 (0.423)\n",
      "-13: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.439 (0.450)\n",
      "-13: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.443 (0.458)\n",
      "-13: ReLU()-Identity()-CrossEntropyLoss(): 0.423 (0.439)\n",
      "-13: ReLU()-Identity()-NLLLoss(): 0.195 (0.228)\n",
      "-13: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.423 (0.436)\n",
      "-13: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.417 (0.430)\n",
      "-13: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.417 (0.458)\n",
      "-13: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.423 (0.425)\n",
      "-13: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.408 (0.434)\n",
      "-13: Identity()-Softmax(dim=1)-NLLLoss(): 0.417 (0.443)\n",
      "-13: Identity()-Identity()-CrossEntropyLoss(): 0.465 (0.480)\n",
      "-14: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.454 (0.465)\n",
      "-14: ReLU()-Softmax(dim=1)-NLLLoss(): 0.447 (0.465)\n",
      "-14: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.463 (0.482)\n",
      "-14: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.493 (0.504)\n",
      "-14: ReLU()-Identity()-CrossEntropyLoss(): 0.487 (0.504)\n",
      "-14: ReLU()-Identity()-NLLLoss(): 0.221 (0.261)\n",
      "-14: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.450 (0.509)\n",
      "-14: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.458 (0.478)\n",
      "-14: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.456 (0.471)\n",
      "-14: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.452 (0.454)\n",
      "-14: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.480 (0.493)\n",
      "-14: Identity()-Softmax(dim=1)-NLLLoss(): 0.458 (0.474)\n",
      "-14: Identity()-Identity()-CrossEntropyLoss(): 0.502 (0.515)\n",
      "-15: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.421 (0.450)\n",
      "-15: ReLU()-Softmax(dim=1)-NLLLoss(): 0.428 (0.439)\n",
      "-15: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.467 (0.489)\n",
      "-15: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.485 (0.493)\n",
      "-15: ReLU()-Identity()-CrossEntropyLoss(): 0.485 (0.496)\n",
      "-15: ReLU()-Identity()-NLLLoss(): 0.215 (0.303)\n",
      "-15: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.412 (0.461)\n",
      "-15: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.404 (0.465)\n",
      "-15: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.423 (0.465)\n",
      "-15: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.412 (0.463)\n",
      "-15: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.421 (0.467)\n",
      "-15: Identity()-Softmax(dim=1)-NLLLoss(): 0.421 (0.447)\n",
      "-15: Identity()-Identity()-CrossEntropyLoss(): 0.489 (0.489)\n",
      "-16: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.454 (0.476)\n",
      "-16: ReLU()-Softmax(dim=1)-NLLLoss(): 0.456 (0.467)\n",
      "-16: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.463 (0.476)\n",
      "-16: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.467 (0.485)\n",
      "-16: ReLU()-Identity()-CrossEntropyLoss(): 0.443 (0.463)\n",
      "-16: ReLU()-Identity()-NLLLoss(): 0.169 (0.200)\n",
      "-16: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.463 (0.480)\n",
      "-16: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.447 (0.480)\n",
      "-16: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.461 (0.487)\n",
      "-16: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.447 (0.476)\n",
      "-16: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.463 (0.480)\n",
      "-16: Identity()-Softmax(dim=1)-NLLLoss(): 0.441 (0.469)\n",
      "-16: Identity()-Identity()-CrossEntropyLoss(): 0.463 (0.476)\n",
      "-17: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.469 (0.480)\n",
      "-17: ReLU()-Softmax(dim=1)-NLLLoss(): 0.432 (0.458)\n",
      "-17: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.285 (0.445)\n",
      "-17: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.452 (0.482)\n",
      "-17: ReLU()-Identity()-CrossEntropyLoss(): 0.425 (0.461)\n",
      "-17: ReLU()-Identity()-NLLLoss(): 0.154 (0.230)\n",
      "-17: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.452 (0.465)\n",
      "-17: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.476 (0.485)\n",
      "-17: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.434 (0.469)\n",
      "-17: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.482 (0.489)\n",
      "-17: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.461 (0.476)\n",
      "-17: Identity()-Softmax(dim=1)-NLLLoss(): 0.456 (0.467)\n",
      "-17: Identity()-Identity()-CrossEntropyLoss(): 0.480 (0.507)\n",
      "-18: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.487 (0.507)\n",
      "-18: ReLU()-Softmax(dim=1)-NLLLoss(): 0.476 (0.487)\n",
      "-18: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.476 (0.504)\n",
      "-18: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.493 (0.513)\n",
      "-18: ReLU()-Identity()-CrossEntropyLoss(): 0.458 (0.482)\n",
      "-18: ReLU()-Identity()-NLLLoss(): 0.193 (0.239)\n",
      "-18: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.480 (0.496)\n",
      "-18: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.491 (0.502)\n",
      "-18: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.513 (0.513)\n",
      "-18: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.487 (0.504)\n",
      "-18: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.500 (0.502)\n",
      "-18: Identity()-Softmax(dim=1)-NLLLoss(): 0.485 (0.500)\n",
      "-18: Identity()-Identity()-CrossEntropyLoss(): 0.443 (0.498)\n",
      "-19: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.456 (0.467)\n",
      "-19: ReLU()-Softmax(dim=1)-NLLLoss(): 0.458 (0.463)\n",
      "-19: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.469 (0.482)\n",
      "-19: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.463 (0.487)\n",
      "-19: ReLU()-Identity()-CrossEntropyLoss(): 0.454 (0.478)\n",
      "-19: ReLU()-Identity()-NLLLoss(): 0.169 (0.189)\n",
      "-19: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.467 (0.485)\n",
      "-19: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.452 (0.469)\n",
      "-19: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.463 (0.489)\n",
      "-19: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.461 (0.487)\n",
      "-19: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.454 (0.478)\n",
      "-19: Identity()-Softmax(dim=1)-NLLLoss(): 0.454 (0.463)\n",
      "-19: Identity()-Identity()-CrossEntropyLoss(): 0.436 (0.478)\n",
      "-20: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.445 (0.465)\n",
      "-20: ReLU()-Softmax(dim=1)-NLLLoss(): 0.419 (0.461)\n",
      "-20: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.487 (0.507)\n",
      "-20: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.471 (0.493)\n",
      "-20: ReLU()-Identity()-CrossEntropyLoss(): 0.480 (0.504)\n",
      "-20: ReLU()-Identity()-NLLLoss(): 0.182 (0.211)\n",
      "-20: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.436 (0.461)\n",
      "-20: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.445 (0.463)\n",
      "-20: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.443 (0.458)\n",
      "-20: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.441 (0.454)\n",
      "-20: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.452 (0.465)\n",
      "-20: Identity()-Softmax(dim=1)-NLLLoss(): 0.443 (0.452)\n",
      "-20: Identity()-Identity()-CrossEntropyLoss(): 0.493 (0.511)\n",
      "----- 30.24 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},]\n",
    "\n",
    "best_accs4 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs4 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=exp['act'], l_act=exp['lact'],\n",
    "                     dropout=DROPOUT, init_h0=h0, diff_layer=GFGCN_noh_Layer)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = NodeClassModel(arch, S, masks, exp['loss'], device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "\n",
    "        best_accs4[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs4[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}: {best_val_accs4[j,i]:.3f} ({best_accs4[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}' for exp in EXPS]\n",
    "table_over4 = summary_table(best_accs4, index_name)\n",
    "table4 = summary_table(best_val_accs4, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.450987</td>\n",
       "      <td>0.455044</td>\n",
       "      <td>0.020668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.443531</td>\n",
       "      <td>0.449561</td>\n",
       "      <td>0.022884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.453947</td>\n",
       "      <td>0.468202</td>\n",
       "      <td>0.051812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.463816</td>\n",
       "      <td>0.464912</td>\n",
       "      <td>0.021504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.468969</td>\n",
       "      <td>0.478070</td>\n",
       "      <td>0.022645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Identity()-NLLLoss()</th>\n",
       "      <td>0.205263</td>\n",
       "      <td>0.212719</td>\n",
       "      <td>0.024950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.450548</td>\n",
       "      <td>0.452851</td>\n",
       "      <td>0.020283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.443531</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.022049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.452632</td>\n",
       "      <td>0.451754</td>\n",
       "      <td>0.021736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.451754</td>\n",
       "      <td>0.455044</td>\n",
       "      <td>0.019168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.453618</td>\n",
       "      <td>0.453947</td>\n",
       "      <td>0.024482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.449890</td>\n",
       "      <td>0.451754</td>\n",
       "      <td>0.021635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.466776</td>\n",
       "      <td>0.472588</td>\n",
       "      <td>0.031124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean accs       med  \\\n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()             0.450987  0.455044   \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                      0.443531  0.449561   \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()          0.453947  0.468202   \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                   0.463816  0.464912   \n",
       "ReLU()-Identity()-CrossEntropyLoss()                 0.468969  0.478070   \n",
       "ReLU()-Identity()-NLLLoss()                          0.205263  0.212719   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()     0.450548  0.452851   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()              0.443531  0.447368   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...   0.452632  0.451754   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...   0.451754  0.455044   \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()         0.453618  0.453947   \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                  0.449890  0.451754   \n",
       "Identity()-Identity()-CrossEntropyLoss()             0.466776  0.472588   \n",
       "\n",
       "                                                         std  \n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()            0.020668  \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                     0.022884  \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()         0.051812  \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                  0.021504  \n",
       "ReLU()-Identity()-CrossEntropyLoss()                0.022645  \n",
       "ReLU()-Identity()-NLLLoss()                         0.024950  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()    0.020283  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()             0.022049  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...  0.021736  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...  0.019168  \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()        0.024482  \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                 0.021635  \n",
       "Identity()-Identity()-CrossEntropyLoss()            0.031124  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.471820</td>\n",
       "      <td>0.476974</td>\n",
       "      <td>0.021878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.462610</td>\n",
       "      <td>0.463816</td>\n",
       "      <td>0.018756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.489254</td>\n",
       "      <td>0.493421</td>\n",
       "      <td>0.019167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.489364</td>\n",
       "      <td>0.489035</td>\n",
       "      <td>0.017061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.490241</td>\n",
       "      <td>0.495614</td>\n",
       "      <td>0.020648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Identity()-NLLLoss()</th>\n",
       "      <td>0.249232</td>\n",
       "      <td>0.253289</td>\n",
       "      <td>0.032314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.473575</td>\n",
       "      <td>0.476974</td>\n",
       "      <td>0.019522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.471272</td>\n",
       "      <td>0.475877</td>\n",
       "      <td>0.018477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.476096</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.016772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.471820</td>\n",
       "      <td>0.475877</td>\n",
       "      <td>0.020006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.475548</td>\n",
       "      <td>0.476974</td>\n",
       "      <td>0.017005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.470614</td>\n",
       "      <td>0.468202</td>\n",
       "      <td>0.017203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.492434</td>\n",
       "      <td>0.490132</td>\n",
       "      <td>0.020105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean accs       med  \\\n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()             0.471820  0.476974   \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                      0.462610  0.463816   \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()          0.489254  0.493421   \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                   0.489364  0.489035   \n",
       "ReLU()-Identity()-CrossEntropyLoss()                 0.490241  0.495614   \n",
       "ReLU()-Identity()-NLLLoss()                          0.249232  0.253289   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()     0.473575  0.476974   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()              0.471272  0.475877   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...   0.476096  0.473684   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...   0.471820  0.475877   \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()         0.475548  0.476974   \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                  0.470614  0.468202   \n",
       "Identity()-Identity()-CrossEntropyLoss()             0.492434  0.490132   \n",
       "\n",
       "                                                         std  \n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()            0.021878  \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                     0.018756  \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()         0.019167  \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                  0.017061  \n",
       "ReLU()-Identity()-CrossEntropyLoss()                0.020648  \n",
       "ReLU()-Identity()-NLLLoss()                         0.032314  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()    0.019522  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()             0.018477  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...  0.016772  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...  0.020006  \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()        0.017005  \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                 0.017203  \n",
       "Identity()-Identity()-CrossEntropyLoss()            0.020105  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_over4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the GSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PARAMETERS  - 0.61\n",
    "## Reaining params\n",
    "N_RUNS = 10\n",
    "N_EPOCHS = 200  # 500\n",
    "LR = .005  # .01\n",
    "WD = .01  # .005\n",
    "DROPOUT = .5\n",
    "\n",
    "# BEST PARAMETERS\n",
    "## Architecture params\n",
    "N_LAYERS = 2\n",
    "K = 2  # 2\n",
    "HID_DIM = 32 # 8\n",
    "\n",
    "## Model params\n",
    "h0 = 1  # 1\n",
    "NORM = False\n",
    "\n",
    "IN_DIM = feat.shape[1]\n",
    "OUT_DIM = n_class\n",
    "\n",
    "ACT = nn.LeakyReLU()\n",
    "LAST_ACT = nn.Softmax(dim=1)\n",
    "LOSS_FN = nn.CrossEntropyLoss()\n",
    "\n",
    "NORM = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training paramerters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:43: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(1/S.sum(1))\n",
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:51: RuntimeWarning: invalid value encountered in matmul\n",
      "  return D_inv_sqr @ S @ D_inv_sqr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 200-0.05-0.0005-0.5: 0.182 (0.182)\n",
      "-1: 200-0.01-0.0005-0.5: 0.182 (0.182)\n",
      "-1: 200-0.005-0.0005-0.5: 0.182 (0.182)\n",
      "-1: 200-0.001-0.0005-0.5: 0.182 (0.182)\n",
      "-1: 200-0.05-0.001-0.5: 0.182 (0.182)\n",
      "-1: 200-0.01-0.001-0.5: 0.182 (0.182)\n",
      "-1: 200-0.005-0.001-0.5: 0.182 (0.182)\n",
      "-1: 200-0.001-0.001-0.5: 0.182 (0.182)\n",
      "-1: 200-0.05-0.005-0.5: 0.182 (0.182)\n",
      "-1: 200-0.01-0.005-0.5: 0.182 (0.182)\n",
      "-1: 200-0.005-0.005-0.5: 0.182 (0.182)\n",
      "-1: 200-0.001-0.005-0.5: 0.182 (0.182)\n",
      "-1: 200-0.05-0.01-0.5: 0.182 (0.182)\n",
      "-1: 200-0.01-0.01-0.5: 0.182 (0.182)\n",
      "-1: 200-0.005-0.01-0.5: 0.182 (0.182)\n",
      "-1: 200-0.001-0.01-0.5: 0.182 (0.182)\n",
      "-1: 200-0.005-0.001-0: 0.182 (0.182)\n",
      "-1: 200-0.005-0.001-0.25: 0.182 (0.182)\n",
      "-1: 200-0.001-0.001-0: 0.182 (0.182)\n",
      "-1: 200-0.001-0.001-0.25: 0.182 (0.182)\n",
      "-1: 100-0.005-0.001-0.5: 0.182 (0.182)\n",
      "-1: 500-0.005-0.001-0.5: 0.182 (0.182)\n",
      "-1: 1000-0.005-0.001-0.5: 0.182 (0.182)\n",
      "-2: 200-0.05-0.0005-0.5: 0.189 (0.189)\n",
      "-2: 200-0.01-0.0005-0.5: 0.189 (0.189)\n",
      "-2: 200-0.005-0.0005-0.5: 0.189 (0.189)\n",
      "-2: 200-0.001-0.0005-0.5: 0.189 (0.189)\n",
      "-2: 200-0.05-0.001-0.5: 0.189 (0.189)\n",
      "-2: 200-0.01-0.001-0.5: 0.189 (0.189)\n",
      "-2: 200-0.005-0.001-0.5: 0.189 (0.189)\n",
      "-2: 200-0.001-0.001-0.5: 0.189 (0.189)\n",
      "-2: 200-0.05-0.005-0.5: 0.189 (0.189)\n",
      "-2: 200-0.01-0.005-0.5: 0.189 (0.189)\n",
      "-2: 200-0.005-0.005-0.5: 0.189 (0.189)\n",
      "-2: 200-0.001-0.005-0.5: 0.189 (0.189)\n",
      "-2: 200-0.05-0.01-0.5: 0.189 (0.189)\n",
      "-2: 200-0.01-0.01-0.5: 0.189 (0.189)\n",
      "-2: 200-0.005-0.01-0.5: 0.189 (0.189)\n",
      "-2: 200-0.001-0.01-0.5: 0.189 (0.189)\n",
      "-2: 200-0.005-0.001-0: 0.189 (0.189)\n",
      "-2: 200-0.005-0.001-0.25: 0.189 (0.189)\n",
      "-2: 200-0.001-0.001-0: 0.189 (0.189)\n",
      "-2: 200-0.001-0.001-0.25: 0.189 (0.189)\n",
      "-2: 100-0.005-0.001-0.5: 0.189 (0.189)\n",
      "-2: 500-0.005-0.001-0.5: 0.189 (0.189)\n",
      "-2: 1000-0.005-0.001-0.5: 0.189 (0.189)\n",
      "-3: 200-0.05-0.0005-0.5: 0.195 (0.195)\n",
      "-3: 200-0.01-0.0005-0.5: 0.195 (0.195)\n",
      "-3: 200-0.005-0.0005-0.5: 0.195 (0.195)\n",
      "-3: 200-0.001-0.0005-0.5: 0.195 (0.195)\n",
      "-3: 200-0.05-0.001-0.5: 0.195 (0.195)\n",
      "-3: 200-0.01-0.001-0.5: 0.195 (0.195)\n",
      "-3: 200-0.005-0.001-0.5: 0.195 (0.195)\n",
      "-3: 200-0.001-0.001-0.5: 0.195 (0.195)\n",
      "-3: 200-0.05-0.005-0.5: 0.195 (0.195)\n",
      "-3: 200-0.01-0.005-0.5: 0.195 (0.195)\n",
      "-3: 200-0.005-0.005-0.5: 0.195 (0.195)\n",
      "-3: 200-0.001-0.005-0.5: 0.195 (0.195)\n",
      "-3: 200-0.05-0.01-0.5: 0.195 (0.195)\n",
      "-3: 200-0.01-0.01-0.5: 0.195 (0.195)\n",
      "-3: 200-0.005-0.01-0.5: 0.195 (0.195)\n",
      "-3: 200-0.001-0.01-0.5: 0.195 (0.195)\n",
      "-3: 200-0.005-0.001-0: 0.195 (0.195)\n",
      "-3: 200-0.005-0.001-0.25: 0.195 (0.195)\n",
      "-3: 200-0.001-0.001-0: 0.195 (0.195)\n",
      "-3: 200-0.001-0.001-0.25: 0.195 (0.195)\n",
      "-3: 100-0.005-0.001-0.5: 0.195 (0.195)\n",
      "-3: 500-0.005-0.001-0.5: 0.195 (0.195)\n",
      "-3: 1000-0.005-0.001-0.5: 0.195 (0.195)\n",
      "-4: 200-0.05-0.0005-0.5: 0.211 (0.211)\n",
      "-4: 200-0.01-0.0005-0.5: 0.211 (0.211)\n",
      "-4: 200-0.005-0.0005-0.5: 0.211 (0.211)\n",
      "-4: 200-0.001-0.0005-0.5: 0.211 (0.211)\n",
      "-4: 200-0.05-0.001-0.5: 0.211 (0.211)\n",
      "-4: 200-0.01-0.001-0.5: 0.211 (0.211)\n",
      "-4: 200-0.005-0.001-0.5: 0.211 (0.211)\n",
      "-4: 200-0.001-0.001-0.5: 0.211 (0.211)\n",
      "-4: 200-0.05-0.005-0.5: 0.211 (0.211)\n",
      "-4: 200-0.01-0.005-0.5: 0.211 (0.211)\n",
      "-4: 200-0.005-0.005-0.5: 0.211 (0.211)\n",
      "-4: 200-0.001-0.005-0.5: 0.211 (0.211)\n",
      "-4: 200-0.05-0.01-0.5: 0.211 (0.211)\n",
      "-4: 200-0.01-0.01-0.5: 0.211 (0.211)\n",
      "-4: 200-0.005-0.01-0.5: 0.211 (0.211)\n",
      "-4: 200-0.001-0.01-0.5: 0.211 (0.211)\n",
      "-4: 200-0.005-0.001-0: 0.211 (0.211)\n",
      "-4: 200-0.005-0.001-0.25: 0.211 (0.211)\n",
      "-4: 200-0.001-0.001-0: 0.211 (0.211)\n",
      "-4: 200-0.001-0.001-0.25: 0.211 (0.211)\n",
      "-4: 100-0.005-0.001-0.5: 0.211 (0.211)\n",
      "-4: 500-0.005-0.001-0.5: 0.211 (0.211)\n",
      "-4: 1000-0.005-0.001-0.5: 0.211 (0.211)\n",
      "-5: 200-0.05-0.0005-0.5: 0.235 (0.235)\n",
      "-5: 200-0.01-0.0005-0.5: 0.235 (0.235)\n",
      "-5: 200-0.005-0.0005-0.5: 0.235 (0.235)\n",
      "-5: 200-0.001-0.0005-0.5: 0.235 (0.235)\n",
      "-5: 200-0.05-0.001-0.5: 0.235 (0.235)\n",
      "-5: 200-0.01-0.001-0.5: 0.235 (0.235)\n",
      "-5: 200-0.005-0.001-0.5: 0.235 (0.235)\n",
      "-5: 200-0.001-0.001-0.5: 0.235 (0.235)\n",
      "-5: 200-0.05-0.005-0.5: 0.235 (0.235)\n",
      "-5: 200-0.01-0.005-0.5: 0.235 (0.235)\n",
      "-5: 200-0.005-0.005-0.5: 0.235 (0.235)\n",
      "-5: 200-0.001-0.005-0.5: 0.235 (0.235)\n",
      "-5: 200-0.05-0.01-0.5: 0.235 (0.235)\n",
      "-5: 200-0.01-0.01-0.5: 0.235 (0.235)\n",
      "-5: 200-0.005-0.01-0.5: 0.235 (0.235)\n",
      "-5: 200-0.001-0.01-0.5: 0.235 (0.235)\n",
      "-5: 200-0.005-0.001-0: 0.235 (0.235)\n",
      "-5: 200-0.005-0.001-0.25: 0.235 (0.235)\n",
      "-5: 200-0.001-0.001-0: 0.235 (0.235)\n",
      "-5: 200-0.001-0.001-0.25: 0.235 (0.235)\n",
      "-5: 100-0.005-0.001-0.5: 0.235 (0.235)\n",
      "-5: 500-0.005-0.001-0.5: 0.235 (0.235)\n",
      "-5: 1000-0.005-0.001-0.5: 0.235 (0.235)\n",
      "-6: 200-0.05-0.0005-0.5: 0.182 (0.182)\n",
      "-6: 200-0.01-0.0005-0.5: 0.182 (0.182)\n",
      "-6: 200-0.005-0.0005-0.5: 0.182 (0.182)\n",
      "-6: 200-0.001-0.0005-0.5: 0.182 (0.182)\n",
      "-6: 200-0.05-0.001-0.5: 0.182 (0.182)\n",
      "-6: 200-0.01-0.001-0.5: 0.182 (0.182)\n",
      "-6: 200-0.005-0.001-0.5: 0.182 (0.182)\n",
      "-6: 200-0.001-0.001-0.5: 0.182 (0.182)\n",
      "-6: 200-0.05-0.005-0.5: 0.182 (0.182)\n",
      "-6: 200-0.01-0.005-0.5: 0.182 (0.182)\n",
      "-6: 200-0.005-0.005-0.5: 0.182 (0.182)\n",
      "-6: 200-0.001-0.005-0.5: 0.182 (0.182)\n",
      "-6: 200-0.05-0.01-0.5: 0.182 (0.182)\n",
      "-6: 200-0.01-0.01-0.5: 0.182 (0.182)\n",
      "-6: 200-0.005-0.01-0.5: 0.182 (0.182)\n",
      "-6: 200-0.001-0.01-0.5: 0.182 (0.182)\n",
      "-6: 200-0.005-0.001-0: 0.182 (0.182)\n",
      "-6: 200-0.005-0.001-0.25: 0.182 (0.182)\n",
      "-6: 200-0.001-0.001-0: 0.182 (0.182)\n",
      "-6: 200-0.001-0.001-0.25: 0.182 (0.182)\n",
      "-6: 100-0.005-0.001-0.5: 0.182 (0.182)\n",
      "-6: 500-0.005-0.001-0.5: 0.182 (0.182)\n",
      "-6: 1000-0.005-0.001-0.5: 0.182 (0.182)\n",
      "-7: 200-0.05-0.0005-0.5: 0.202 (0.202)\n",
      "-7: 200-0.01-0.0005-0.5: 0.202 (0.202)\n",
      "-7: 200-0.005-0.0005-0.5: 0.202 (0.202)\n",
      "-7: 200-0.001-0.0005-0.5: 0.202 (0.202)\n",
      "-7: 200-0.05-0.001-0.5: 0.202 (0.202)\n",
      "-7: 200-0.01-0.001-0.5: 0.202 (0.202)\n",
      "-7: 200-0.005-0.001-0.5: 0.202 (0.202)\n",
      "-7: 200-0.001-0.001-0.5: 0.202 (0.202)\n",
      "-7: 200-0.05-0.005-0.5: 0.202 (0.202)\n",
      "-7: 200-0.01-0.005-0.5: 0.202 (0.202)\n",
      "-7: 200-0.005-0.005-0.5: 0.202 (0.202)\n",
      "-7: 200-0.001-0.005-0.5: 0.202 (0.202)\n",
      "-7: 200-0.05-0.01-0.5: 0.202 (0.202)\n",
      "-7: 200-0.01-0.01-0.5: 0.202 (0.202)\n",
      "-7: 200-0.005-0.01-0.5: 0.202 (0.202)\n",
      "-7: 200-0.001-0.01-0.5: 0.202 (0.202)\n",
      "-7: 200-0.005-0.001-0: 0.202 (0.202)\n",
      "-7: 200-0.005-0.001-0.25: 0.202 (0.202)\n",
      "-7: 200-0.001-0.001-0: 0.202 (0.202)\n",
      "-7: 200-0.001-0.001-0.25: 0.202 (0.202)\n",
      "-7: 100-0.005-0.001-0.5: 0.202 (0.202)\n",
      "-7: 500-0.005-0.001-0.5: 0.202 (0.202)\n",
      "-7: 1000-0.005-0.001-0.5: 0.202 (0.202)\n",
      "-8: 200-0.05-0.0005-0.5: 0.186 (0.186)\n",
      "-8: 200-0.01-0.0005-0.5: 0.186 (0.186)\n",
      "-8: 200-0.005-0.0005-0.5: 0.186 (0.186)\n",
      "-8: 200-0.001-0.0005-0.5: 0.186 (0.186)\n",
      "-8: 200-0.05-0.001-0.5: 0.186 (0.186)\n",
      "-8: 200-0.01-0.001-0.5: 0.186 (0.186)\n",
      "-8: 200-0.005-0.001-0.5: 0.186 (0.186)\n",
      "-8: 200-0.001-0.001-0.5: 0.186 (0.186)\n",
      "-8: 200-0.05-0.005-0.5: 0.186 (0.186)\n",
      "-8: 200-0.01-0.005-0.5: 0.186 (0.186)\n",
      "-8: 200-0.005-0.005-0.5: 0.186 (0.186)\n",
      "-8: 200-0.001-0.005-0.5: 0.186 (0.186)\n",
      "-8: 200-0.05-0.01-0.5: 0.186 (0.186)\n",
      "-8: 200-0.01-0.01-0.5: 0.186 (0.186)\n",
      "-8: 200-0.005-0.01-0.5: 0.186 (0.186)\n",
      "-8: 200-0.001-0.01-0.5: 0.186 (0.186)\n",
      "-8: 200-0.005-0.001-0: 0.186 (0.186)\n",
      "-8: 200-0.005-0.001-0.25: 0.186 (0.186)\n",
      "-8: 200-0.001-0.001-0: 0.186 (0.186)\n",
      "-8: 200-0.001-0.001-0.25: 0.186 (0.186)\n",
      "-8: 100-0.005-0.001-0.5: 0.186 (0.186)\n",
      "-8: 500-0.005-0.001-0.5: 0.186 (0.186)\n",
      "-8: 1000-0.005-0.001-0.5: 0.186 (0.186)\n",
      "-9: 200-0.05-0.0005-0.5: 0.206 (0.206)\n",
      "-9: 200-0.01-0.0005-0.5: 0.206 (0.206)\n",
      "-9: 200-0.005-0.0005-0.5: 0.206 (0.206)\n",
      "-9: 200-0.001-0.0005-0.5: 0.206 (0.206)\n",
      "-9: 200-0.05-0.001-0.5: 0.206 (0.206)\n",
      "-9: 200-0.01-0.001-0.5: 0.206 (0.206)\n",
      "-9: 200-0.005-0.001-0.5: 0.206 (0.206)\n",
      "-9: 200-0.001-0.001-0.5: 0.206 (0.206)\n",
      "-9: 200-0.05-0.005-0.5: 0.206 (0.206)\n",
      "-9: 200-0.01-0.005-0.5: 0.206 (0.206)\n",
      "-9: 200-0.005-0.005-0.5: 0.206 (0.206)\n",
      "-9: 200-0.001-0.005-0.5: 0.206 (0.206)\n",
      "-9: 200-0.05-0.01-0.5: 0.206 (0.206)\n",
      "-9: 200-0.01-0.01-0.5: 0.206 (0.206)\n",
      "-9: 200-0.005-0.01-0.5: 0.206 (0.206)\n",
      "-9: 200-0.001-0.01-0.5: 0.206 (0.206)\n",
      "-9: 200-0.005-0.001-0: 0.206 (0.206)\n",
      "-9: 200-0.005-0.001-0.25: 0.206 (0.206)\n",
      "-9: 200-0.001-0.001-0: 0.206 (0.206)\n",
      "-9: 200-0.001-0.001-0.25: 0.206 (0.206)\n",
      "-9: 100-0.005-0.001-0.5: 0.206 (0.206)\n",
      "-9: 500-0.005-0.001-0.5: 0.206 (0.206)\n",
      "-9: 1000-0.005-0.001-0.5: 0.206 (0.206)\n",
      "-10: 200-0.05-0.0005-0.5: 0.171 (0.171)\n",
      "-10: 200-0.01-0.0005-0.5: 0.171 (0.171)\n",
      "-10: 200-0.005-0.0005-0.5: 0.171 (0.171)\n",
      "-10: 200-0.001-0.0005-0.5: 0.171 (0.171)\n",
      "-10: 200-0.05-0.001-0.5: 0.171 (0.171)\n",
      "-10: 200-0.01-0.001-0.5: 0.171 (0.171)\n",
      "-10: 200-0.005-0.001-0.5: 0.171 (0.171)\n",
      "-10: 200-0.001-0.001-0.5: 0.171 (0.171)\n",
      "-10: 200-0.05-0.005-0.5: 0.171 (0.171)\n",
      "-10: 200-0.01-0.005-0.5: 0.171 (0.171)\n",
      "-10: 200-0.005-0.005-0.5: 0.171 (0.171)\n",
      "-10: 200-0.001-0.005-0.5: 0.171 (0.171)\n",
      "-10: 200-0.05-0.01-0.5: 0.171 (0.171)\n",
      "-10: 200-0.01-0.01-0.5: 0.171 (0.171)\n",
      "-10: 200-0.005-0.01-0.5: 0.171 (0.171)\n",
      "-10: 200-0.001-0.01-0.5: 0.171 (0.171)\n",
      "-10: 200-0.005-0.001-0: 0.171 (0.171)\n",
      "-10: 200-0.005-0.001-0.25: 0.171 (0.171)\n",
      "-10: 200-0.001-0.001-0: 0.171 (0.171)\n",
      "-10: 200-0.001-0.001-0.25: 0.171 (0.171)\n",
      "-10: 100-0.005-0.001-0.5: 0.171 (0.171)\n",
      "-10: 500-0.005-0.001-0.5: 0.171 (0.171)\n",
      "-10: 1000-0.005-0.001-0.5: 0.171 (0.171)\n",
      "----- 2.14 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-4, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-4, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-4, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 5e-4, 'drop': .5},\n",
    "        \n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-3, 'drop': .5},\n",
    "\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 5e-3, 'drop': .5},\n",
    "\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-2, 'drop': .5},\n",
    "\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-3, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "\n",
    "        {'epochs': 100, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 1000, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        ]\n",
    "\n",
    "best_accs5 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs5 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, l_act=LAST_ACT,\n",
    "                     dropout=exp['drop'], init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, exp['epochs'], exp['lr'], exp['wd'])\n",
    "\n",
    "        best_accs5[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs5[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}: {best_val_accs5[j,i]:.3f} ({best_accs5[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}' for exp in EXPS]\n",
    "table_over5 = summary_table(best_accs5, index_name)\n",
    "table5 = summary_table(best_val_accs5, index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.0005-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.0005-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0005-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.0005-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.001-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.001-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.005-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.01-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.01-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.01-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.25</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.001-0</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.001-0.25</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100-0.005-0.001-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000-0.005-0.001-0.5</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      mean accs       med       std\n",
       "200-0.05-0.0005-0.5    0.195833  0.191886  0.017325\n",
       "200-0.01-0.0005-0.5    0.195833  0.191886  0.017325\n",
       "200-0.005-0.0005-0.5   0.195833  0.191886  0.017325\n",
       "200-0.001-0.0005-0.5   0.195833  0.191886  0.017325\n",
       "200-0.05-0.001-0.5     0.195833  0.191886  0.017325\n",
       "200-0.01-0.001-0.5     0.195833  0.191886  0.017325\n",
       "200-0.005-0.001-0.5    0.195833  0.191886  0.017325\n",
       "200-0.001-0.001-0.5    0.195833  0.191886  0.017325\n",
       "200-0.05-0.005-0.5     0.195833  0.191886  0.017325\n",
       "200-0.01-0.005-0.5     0.195833  0.191886  0.017325\n",
       "200-0.005-0.005-0.5    0.195833  0.191886  0.017325\n",
       "200-0.001-0.005-0.5    0.195833  0.191886  0.017325\n",
       "200-0.05-0.01-0.5      0.195833  0.191886  0.017325\n",
       "200-0.01-0.01-0.5      0.195833  0.191886  0.017325\n",
       "200-0.005-0.01-0.5     0.195833  0.191886  0.017325\n",
       "200-0.001-0.01-0.5     0.195833  0.191886  0.017325\n",
       "200-0.005-0.001-0      0.195833  0.191886  0.017325\n",
       "200-0.005-0.001-0.25   0.195833  0.191886  0.017325\n",
       "200-0.001-0.001-0      0.195833  0.191886  0.017325\n",
       "200-0.001-0.001-0.25   0.195833  0.191886  0.017325\n",
       "100-0.005-0.001-0.5    0.195833  0.191886  0.017325\n",
       "500-0.005-0.001-0.5    0.195833  0.191886  0.017325\n",
       "1000-0.005-0.001-0.5   0.195833  0.191886  0.017325"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:43: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(1/S.sum(1))\n",
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:51: RuntimeWarning: invalid value encountered in matmul\n",
      "  return D_inv_sqr @ S @ D_inv_sqr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 2-2-8: 0.182 (0.182)\n",
      "-1: 2-3-8: 0.182 (0.182)\n",
      "-1: 2-4-8: 0.182 (0.182)\n",
      "-1: 3-2-8: 0.182 (0.182)\n",
      "-1: 4-2-8: 0.182 (0.182)\n",
      "-1: 3-3-8: 0.182 (0.182)\n",
      "-1: 4-3-8: 0.182 (0.182)\n",
      "-1: 2-2-16: 0.182 (0.182)\n",
      "-1: 2-3-16: 0.182 (0.182)\n",
      "-1: 2-4-16: 0.182 (0.182)\n",
      "-1: 3-2-16: 0.182 (0.182)\n",
      "-1: 4-2-16: 0.182 (0.182)\n",
      "-1: 3-3-16: 0.182 (0.182)\n",
      "-1: 4-3-16: 0.182 (0.182)\n",
      "-1: 2-2-32: 0.182 (0.182)\n",
      "-1: 2-3-32: 0.182 (0.182)\n",
      "-1: 2-4-32: 0.182 (0.182)\n",
      "-1: 3-2-32: 0.182 (0.182)\n",
      "-1: 4-2-32: 0.182 (0.182)\n",
      "-1: 3-3-32: 0.182 (0.182)\n",
      "-1: 4-3-32: 0.182 (0.182)\n",
      "-1: 2-2-64: 0.182 (0.182)\n",
      "-1: 2-3-64: 0.182 (0.182)\n",
      "-1: 2-4-64: 0.182 (0.182)\n",
      "-1: 3-2-64: 0.182 (0.182)\n",
      "-1: 4-2-64: 0.182 (0.182)\n",
      "-1: 3-3-64: 0.182 (0.182)\n",
      "-1: 4-3-64: 0.182 (0.182)\n",
      "-1: 2-2-100: 0.182 (0.182)\n",
      "-1: 2-3-100: 0.182 (0.182)\n",
      "-1: 2-4-100: 0.182 (0.182)\n",
      "-1: 3-2-100: 0.182 (0.182)\n",
      "-1: 4-2-100: 0.182 (0.182)\n",
      "-1: 3-3-100: 0.182 (0.182)\n",
      "-1: 4-3-100: 0.182 (0.182)\n",
      "-2: 2-2-8: 0.189 (0.189)\n",
      "-2: 2-3-8: 0.189 (0.189)\n",
      "-2: 2-4-8: 0.189 (0.189)\n",
      "-2: 3-2-8: 0.189 (0.189)\n",
      "-2: 4-2-8: 0.189 (0.189)\n",
      "-2: 3-3-8: 0.189 (0.189)\n",
      "-2: 4-3-8: 0.189 (0.189)\n",
      "-2: 2-2-16: 0.189 (0.189)\n",
      "-2: 2-3-16: 0.189 (0.189)\n",
      "-2: 2-4-16: 0.189 (0.189)\n",
      "-2: 3-2-16: 0.189 (0.189)\n",
      "-2: 4-2-16: 0.189 (0.189)\n",
      "-2: 3-3-16: 0.189 (0.189)\n",
      "-2: 4-3-16: 0.189 (0.189)\n",
      "-2: 2-2-32: 0.189 (0.189)\n",
      "-2: 2-3-32: 0.189 (0.189)\n",
      "-2: 2-4-32: 0.189 (0.189)\n",
      "-2: 3-2-32: 0.189 (0.189)\n",
      "-2: 4-2-32: 0.189 (0.189)\n",
      "-2: 3-3-32: 0.189 (0.189)\n",
      "-2: 4-3-32: 0.189 (0.189)\n",
      "-2: 2-2-64: 0.189 (0.189)\n",
      "-2: 2-3-64: 0.189 (0.189)\n",
      "-2: 2-4-64: 0.189 (0.189)\n",
      "-2: 3-2-64: 0.189 (0.189)\n",
      "-2: 4-2-64: 0.189 (0.189)\n",
      "-2: 3-3-64: 0.189 (0.189)\n",
      "-2: 4-3-64: 0.189 (0.189)\n",
      "-2: 2-2-100: 0.189 (0.189)\n",
      "-2: 2-3-100: 0.189 (0.189)\n",
      "-2: 2-4-100: 0.189 (0.189)\n",
      "-2: 3-2-100: 0.189 (0.189)\n",
      "-2: 4-2-100: 0.189 (0.189)\n",
      "-2: 3-3-100: 0.189 (0.189)\n",
      "-2: 4-3-100: 0.189 (0.189)\n",
      "-3: 2-2-8: 0.195 (0.195)\n",
      "-3: 2-3-8: 0.195 (0.195)\n",
      "-3: 2-4-8: 0.195 (0.195)\n",
      "-3: 3-2-8: 0.195 (0.195)\n",
      "-3: 4-2-8: 0.195 (0.195)\n",
      "-3: 3-3-8: 0.195 (0.195)\n",
      "-3: 4-3-8: 0.195 (0.195)\n",
      "-3: 2-2-16: 0.195 (0.195)\n",
      "-3: 2-3-16: 0.195 (0.195)\n",
      "-3: 2-4-16: 0.195 (0.195)\n",
      "-3: 3-2-16: 0.195 (0.195)\n",
      "-3: 4-2-16: 0.195 (0.195)\n",
      "-3: 3-3-16: 0.195 (0.195)\n",
      "-3: 4-3-16: 0.195 (0.195)\n",
      "-3: 2-2-32: 0.195 (0.195)\n",
      "-3: 2-3-32: 0.195 (0.195)\n",
      "-3: 2-4-32: 0.195 (0.195)\n",
      "-3: 3-2-32: 0.195 (0.195)\n",
      "-3: 4-2-32: 0.195 (0.195)\n",
      "-3: 3-3-32: 0.195 (0.195)\n",
      "-3: 4-3-32: 0.195 (0.195)\n",
      "-3: 2-2-64: 0.195 (0.195)\n",
      "-3: 2-3-64: 0.195 (0.195)\n",
      "-3: 2-4-64: 0.195 (0.195)\n",
      "-3: 3-2-64: 0.195 (0.195)\n",
      "-3: 4-2-64: 0.195 (0.195)\n",
      "-3: 3-3-64: 0.195 (0.195)\n",
      "-3: 4-3-64: 0.195 (0.195)\n",
      "-3: 2-2-100: 0.195 (0.195)\n",
      "-3: 2-3-100: 0.195 (0.195)\n",
      "-3: 2-4-100: 0.195 (0.195)\n",
      "-3: 3-2-100: 0.195 (0.195)\n",
      "-3: 4-2-100: 0.195 (0.195)\n",
      "-3: 3-3-100: 0.195 (0.195)\n",
      "-3: 4-3-100: 0.195 (0.195)\n",
      "-4: 2-2-8: 0.211 (0.211)\n",
      "-4: 2-3-8: 0.211 (0.211)\n",
      "-4: 2-4-8: 0.211 (0.211)\n",
      "-4: 3-2-8: 0.211 (0.211)\n",
      "-4: 4-2-8: 0.211 (0.211)\n",
      "-4: 3-3-8: 0.211 (0.211)\n",
      "-4: 4-3-8: 0.211 (0.211)\n",
      "-4: 2-2-16: 0.211 (0.211)\n",
      "-4: 2-3-16: 0.211 (0.211)\n",
      "-4: 2-4-16: 0.211 (0.211)\n",
      "-4: 3-2-16: 0.211 (0.211)\n",
      "-4: 4-2-16: 0.211 (0.211)\n",
      "-4: 3-3-16: 0.211 (0.211)\n",
      "-4: 4-3-16: 0.211 (0.211)\n",
      "-4: 2-2-32: 0.211 (0.211)\n",
      "-4: 2-3-32: 0.211 (0.211)\n",
      "-4: 2-4-32: 0.211 (0.211)\n",
      "-4: 3-2-32: 0.211 (0.211)\n",
      "-4: 4-2-32: 0.211 (0.211)\n",
      "-4: 3-3-32: 0.211 (0.211)\n",
      "-4: 4-3-32: 0.211 (0.211)\n",
      "-4: 2-2-64: 0.211 (0.211)\n",
      "-4: 2-3-64: 0.211 (0.211)\n",
      "-4: 2-4-64: 0.211 (0.211)\n",
      "-4: 3-2-64: 0.211 (0.211)\n",
      "-4: 4-2-64: 0.211 (0.211)\n",
      "-4: 3-3-64: 0.211 (0.211)\n",
      "-4: 4-3-64: 0.211 (0.211)\n",
      "-4: 2-2-100: 0.211 (0.211)\n",
      "-4: 2-3-100: 0.211 (0.211)\n",
      "-4: 2-4-100: 0.211 (0.211)\n",
      "-4: 3-2-100: 0.211 (0.211)\n",
      "-4: 4-2-100: 0.211 (0.211)\n",
      "-4: 3-3-100: 0.211 (0.211)\n",
      "-4: 4-3-100: 0.211 (0.211)\n",
      "-5: 2-2-8: 0.235 (0.235)\n",
      "-5: 2-3-8: 0.235 (0.235)\n",
      "-5: 2-4-8: 0.235 (0.235)\n",
      "-5: 3-2-8: 0.235 (0.235)\n",
      "-5: 4-2-8: 0.235 (0.235)\n",
      "-5: 3-3-8: 0.235 (0.235)\n",
      "-5: 4-3-8: 0.235 (0.235)\n",
      "-5: 2-2-16: 0.235 (0.235)\n",
      "-5: 2-3-16: 0.235 (0.235)\n",
      "-5: 2-4-16: 0.235 (0.235)\n",
      "-5: 3-2-16: 0.235 (0.235)\n",
      "-5: 4-2-16: 0.235 (0.235)\n",
      "-5: 3-3-16: 0.235 (0.235)\n",
      "-5: 4-3-16: 0.235 (0.235)\n",
      "-5: 2-2-32: 0.235 (0.235)\n",
      "-5: 2-3-32: 0.235 (0.235)\n",
      "-5: 2-4-32: 0.235 (0.235)\n",
      "-5: 3-2-32: 0.235 (0.235)\n",
      "-5: 4-2-32: 0.235 (0.235)\n",
      "-5: 3-3-32: 0.235 (0.235)\n",
      "-5: 4-3-32: 0.235 (0.235)\n",
      "-5: 2-2-64: 0.235 (0.235)\n",
      "-5: 2-3-64: 0.235 (0.235)\n",
      "-5: 2-4-64: 0.235 (0.235)\n",
      "-5: 3-2-64: 0.235 (0.235)\n",
      "-5: 4-2-64: 0.235 (0.235)\n",
      "-5: 3-3-64: 0.235 (0.235)\n",
      "-5: 4-3-64: 0.235 (0.235)\n",
      "-5: 2-2-100: 0.235 (0.235)\n",
      "-5: 2-3-100: 0.235 (0.235)\n",
      "-5: 2-4-100: 0.235 (0.235)\n",
      "-5: 3-2-100: 0.235 (0.235)\n",
      "-5: 4-2-100: 0.235 (0.235)\n",
      "-5: 3-3-100: 0.235 (0.235)\n",
      "-5: 4-3-100: 0.235 (0.235)\n",
      "-6: 2-2-8: 0.182 (0.182)\n",
      "-6: 2-3-8: 0.182 (0.182)\n",
      "-6: 2-4-8: 0.182 (0.182)\n",
      "-6: 3-2-8: 0.182 (0.182)\n",
      "-6: 4-2-8: 0.182 (0.182)\n",
      "-6: 3-3-8: 0.182 (0.182)\n",
      "-6: 4-3-8: 0.182 (0.182)\n",
      "-6: 2-2-16: 0.182 (0.182)\n",
      "-6: 2-3-16: 0.182 (0.182)\n",
      "-6: 2-4-16: 0.182 (0.182)\n",
      "-6: 3-2-16: 0.182 (0.182)\n",
      "-6: 4-2-16: 0.182 (0.182)\n",
      "-6: 3-3-16: 0.182 (0.182)\n",
      "-6: 4-3-16: 0.182 (0.182)\n",
      "-6: 2-2-32: 0.182 (0.182)\n",
      "-6: 2-3-32: 0.182 (0.182)\n",
      "-6: 2-4-32: 0.182 (0.182)\n",
      "-6: 3-2-32: 0.182 (0.182)\n",
      "-6: 4-2-32: 0.182 (0.182)\n",
      "-6: 3-3-32: 0.182 (0.182)\n",
      "-6: 4-3-32: 0.182 (0.182)\n",
      "-6: 2-2-64: 0.182 (0.182)\n",
      "-6: 2-3-64: 0.182 (0.182)\n",
      "-6: 2-4-64: 0.182 (0.182)\n",
      "-6: 3-2-64: 0.182 (0.182)\n",
      "-6: 4-2-64: 0.182 (0.182)\n",
      "-6: 3-3-64: 0.182 (0.182)\n",
      "-6: 4-3-64: 0.182 (0.182)\n",
      "-6: 2-2-100: 0.182 (0.182)\n",
      "-6: 2-3-100: 0.182 (0.182)\n",
      "-6: 2-4-100: 0.182 (0.182)\n",
      "-6: 3-2-100: 0.182 (0.182)\n",
      "-6: 4-2-100: 0.182 (0.182)\n",
      "-6: 3-3-100: 0.182 (0.182)\n",
      "-6: 4-3-100: 0.182 (0.182)\n",
      "-7: 2-2-8: 0.202 (0.202)\n",
      "-7: 2-3-8: 0.202 (0.202)\n",
      "-7: 2-4-8: 0.202 (0.202)\n",
      "-7: 3-2-8: 0.202 (0.202)\n",
      "-7: 4-2-8: 0.202 (0.202)\n",
      "-7: 3-3-8: 0.202 (0.202)\n",
      "-7: 4-3-8: 0.202 (0.202)\n",
      "-7: 2-2-16: 0.202 (0.202)\n",
      "-7: 2-3-16: 0.202 (0.202)\n",
      "-7: 2-4-16: 0.202 (0.202)\n",
      "-7: 3-2-16: 0.202 (0.202)\n",
      "-7: 4-2-16: 0.202 (0.202)\n",
      "-7: 3-3-16: 0.202 (0.202)\n",
      "-7: 4-3-16: 0.202 (0.202)\n",
      "-7: 2-2-32: 0.202 (0.202)\n",
      "-7: 2-3-32: 0.202 (0.202)\n",
      "-7: 2-4-32: 0.202 (0.202)\n",
      "-7: 3-2-32: 0.202 (0.202)\n",
      "-7: 4-2-32: 0.202 (0.202)\n",
      "-7: 3-3-32: 0.202 (0.202)\n",
      "-7: 4-3-32: 0.202 (0.202)\n",
      "-7: 2-2-64: 0.202 (0.202)\n",
      "-7: 2-3-64: 0.202 (0.202)\n",
      "-7: 2-4-64: 0.202 (0.202)\n",
      "-7: 3-2-64: 0.202 (0.202)\n",
      "-7: 4-2-64: 0.202 (0.202)\n",
      "-7: 3-3-64: 0.202 (0.202)\n",
      "-7: 4-3-64: 0.202 (0.202)\n",
      "-7: 2-2-100: 0.202 (0.202)\n",
      "-7: 2-3-100: 0.202 (0.202)\n",
      "-7: 2-4-100: 0.202 (0.202)\n",
      "-7: 3-2-100: 0.202 (0.202)\n",
      "-7: 4-2-100: 0.202 (0.202)\n",
      "-7: 3-3-100: 0.202 (0.202)\n",
      "-7: 4-3-100: 0.202 (0.202)\n",
      "-8: 2-2-8: 0.186 (0.186)\n",
      "-8: 2-3-8: 0.186 (0.186)\n",
      "-8: 2-4-8: 0.186 (0.186)\n",
      "-8: 3-2-8: 0.186 (0.186)\n",
      "-8: 4-2-8: 0.186 (0.186)\n",
      "-8: 3-3-8: 0.186 (0.186)\n",
      "-8: 4-3-8: 0.186 (0.186)\n",
      "-8: 2-2-16: 0.186 (0.186)\n",
      "-8: 2-3-16: 0.186 (0.186)\n",
      "-8: 2-4-16: 0.186 (0.186)\n",
      "-8: 3-2-16: 0.186 (0.186)\n",
      "-8: 4-2-16: 0.186 (0.186)\n",
      "-8: 3-3-16: 0.186 (0.186)\n",
      "-8: 4-3-16: 0.186 (0.186)\n",
      "-8: 2-2-32: 0.186 (0.186)\n",
      "-8: 2-3-32: 0.186 (0.186)\n",
      "-8: 2-4-32: 0.186 (0.186)\n",
      "-8: 3-2-32: 0.186 (0.186)\n",
      "-8: 4-2-32: 0.186 (0.186)\n",
      "-8: 3-3-32: 0.186 (0.186)\n",
      "-8: 4-3-32: 0.186 (0.186)\n",
      "-8: 2-2-64: 0.186 (0.186)\n",
      "-8: 2-3-64: 0.186 (0.186)\n",
      "-8: 2-4-64: 0.186 (0.186)\n",
      "-8: 3-2-64: 0.186 (0.186)\n",
      "-8: 4-2-64: 0.186 (0.186)\n",
      "-8: 3-3-64: 0.186 (0.186)\n",
      "-8: 4-3-64: 0.186 (0.186)\n",
      "-8: 2-2-100: 0.186 (0.186)\n",
      "-8: 2-3-100: 0.186 (0.186)\n",
      "-8: 2-4-100: 0.186 (0.186)\n",
      "-8: 3-2-100: 0.186 (0.186)\n",
      "-8: 4-2-100: 0.186 (0.186)\n",
      "-8: 3-3-100: 0.186 (0.186)\n",
      "-8: 4-3-100: 0.186 (0.186)\n",
      "-9: 2-2-8: 0.206 (0.206)\n",
      "-9: 2-3-8: 0.206 (0.206)\n",
      "-9: 2-4-8: 0.206 (0.206)\n",
      "-9: 3-2-8: 0.206 (0.206)\n",
      "-9: 4-2-8: 0.206 (0.206)\n",
      "-9: 3-3-8: 0.206 (0.206)\n",
      "-9: 4-3-8: 0.206 (0.206)\n",
      "-9: 2-2-16: 0.206 (0.206)\n",
      "-9: 2-3-16: 0.206 (0.206)\n",
      "-9: 2-4-16: 0.206 (0.206)\n",
      "-9: 3-2-16: 0.206 (0.206)\n",
      "-9: 4-2-16: 0.206 (0.206)\n",
      "-9: 3-3-16: 0.206 (0.206)\n",
      "-9: 4-3-16: 0.206 (0.206)\n",
      "-9: 2-2-32: 0.206 (0.206)\n",
      "-9: 2-3-32: 0.206 (0.206)\n",
      "-9: 2-4-32: 0.206 (0.206)\n",
      "-9: 3-2-32: 0.206 (0.206)\n",
      "-9: 4-2-32: 0.206 (0.206)\n",
      "-9: 3-3-32: 0.206 (0.206)\n",
      "-9: 4-3-32: 0.206 (0.206)\n",
      "-9: 2-2-64: 0.206 (0.206)\n",
      "-9: 2-3-64: 0.206 (0.206)\n",
      "-9: 2-4-64: 0.206 (0.206)\n",
      "-9: 3-2-64: 0.206 (0.206)\n",
      "-9: 4-2-64: 0.206 (0.206)\n",
      "-9: 3-3-64: 0.206 (0.206)\n",
      "-9: 4-3-64: 0.206 (0.206)\n",
      "-9: 2-2-100: 0.206 (0.206)\n",
      "-9: 2-3-100: 0.206 (0.206)\n",
      "-9: 2-4-100: 0.206 (0.206)\n",
      "-9: 3-2-100: 0.206 (0.206)\n",
      "-9: 4-2-100: 0.206 (0.206)\n",
      "-9: 3-3-100: 0.206 (0.206)\n",
      "-9: 4-3-100: 0.206 (0.206)\n",
      "-10: 2-2-8: 0.171 (0.171)\n",
      "-10: 2-3-8: 0.171 (0.171)\n",
      "-10: 2-4-8: 0.171 (0.171)\n",
      "-10: 3-2-8: 0.171 (0.171)\n",
      "-10: 4-2-8: 0.171 (0.171)\n",
      "-10: 3-3-8: 0.171 (0.171)\n",
      "-10: 4-3-8: 0.171 (0.171)\n",
      "-10: 2-2-16: 0.171 (0.171)\n",
      "-10: 2-3-16: 0.171 (0.171)\n",
      "-10: 2-4-16: 0.171 (0.171)\n",
      "-10: 3-2-16: 0.171 (0.171)\n",
      "-10: 4-2-16: 0.171 (0.171)\n",
      "-10: 3-3-16: 0.171 (0.171)\n",
      "-10: 4-3-16: 0.171 (0.171)\n",
      "-10: 2-2-32: 0.171 (0.171)\n",
      "-10: 2-3-32: 0.171 (0.171)\n",
      "-10: 2-4-32: 0.171 (0.171)\n",
      "-10: 3-2-32: 0.171 (0.171)\n",
      "-10: 4-2-32: 0.171 (0.171)\n",
      "-10: 3-3-32: 0.171 (0.171)\n",
      "-10: 4-3-32: 0.171 (0.171)\n",
      "-10: 2-2-64: 0.171 (0.171)\n",
      "-10: 2-3-64: 0.171 (0.171)\n",
      "-10: 2-4-64: 0.171 (0.171)\n",
      "-10: 3-2-64: 0.171 (0.171)\n",
      "-10: 4-2-64: 0.171 (0.171)\n",
      "-10: 3-3-64: 0.171 (0.171)\n",
      "-10: 4-3-64: 0.171 (0.171)\n",
      "-10: 2-2-100: 0.171 (0.171)\n",
      "-10: 2-3-100: 0.171 (0.171)\n",
      "-10: 2-4-100: 0.171 (0.171)\n",
      "-10: 3-2-100: 0.171 (0.171)\n",
      "-10: 4-2-100: 0.171 (0.171)\n",
      "-10: 3-3-100: 0.171 (0.171)\n",
      "-10: 4-3-100: 0.171 (0.171)\n",
      "----- 4.62 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'L': 2, 'K': 2, 'hid_dim': 8},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 8},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 8},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 8},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 8},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 8},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 8},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 16},\n",
    "    \n",
    "        {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 32},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 32},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 64},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 64},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 64},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 64},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 100},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 100},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 100},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 100},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 100},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 100},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 100},\n",
    "        ]\n",
    "\n",
    "best_accs7 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs7 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, exp['hid_dim'], OUT_DIM, exp['L'], exp['K'], act=ACT, l_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "\n",
    "        best_accs7[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs7[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}: {best_val_accs7[j,i]:.3f} ({best_accs7[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}' for exp in EXPS]\n",
    "table_over7 = summary_table(best_accs7, index_name)\n",
    "table7 = summary_table(best_val_accs7, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-8</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-8</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-8</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-8</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-8</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-8</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-8</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-16</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-16</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-16</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-16</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-32</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-32</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-32</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-32</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-32</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-64</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-64</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-64</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-64</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-64</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-64</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-64</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-100</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-100</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-100</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-100</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-100</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-100</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-100</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean accs       med       std\n",
       "2-2-8     0.195833  0.191886  0.017325\n",
       "2-3-8     0.195833  0.191886  0.017325\n",
       "2-4-8     0.195833  0.191886  0.017325\n",
       "3-2-8     0.195833  0.191886  0.017325\n",
       "4-2-8     0.195833  0.191886  0.017325\n",
       "3-3-8     0.195833  0.191886  0.017325\n",
       "4-3-8     0.195833  0.191886  0.017325\n",
       "2-2-16    0.195833  0.191886  0.017325\n",
       "2-3-16    0.195833  0.191886  0.017325\n",
       "2-4-16    0.195833  0.191886  0.017325\n",
       "3-2-16    0.195833  0.191886  0.017325\n",
       "4-2-16    0.195833  0.191886  0.017325\n",
       "3-3-16    0.195833  0.191886  0.017325\n",
       "4-3-16    0.195833  0.191886  0.017325\n",
       "2-2-32    0.195833  0.191886  0.017325\n",
       "2-3-32    0.195833  0.191886  0.017325\n",
       "2-4-32    0.195833  0.191886  0.017325\n",
       "3-2-32    0.195833  0.191886  0.017325\n",
       "4-2-32    0.195833  0.191886  0.017325\n",
       "3-3-32    0.195833  0.191886  0.017325\n",
       "4-3-32    0.195833  0.191886  0.017325\n",
       "2-2-64    0.195833  0.191886  0.017325\n",
       "2-3-64    0.195833  0.191886  0.017325\n",
       "2-4-64    0.195833  0.191886  0.017325\n",
       "3-2-64    0.195833  0.191886  0.017325\n",
       "4-2-64    0.195833  0.191886  0.017325\n",
       "3-3-64    0.195833  0.191886  0.017325\n",
       "4-3-64    0.195833  0.191886  0.017325\n",
       "2-2-100   0.195833  0.191886  0.017325\n",
       "2-3-100   0.195833  0.191886  0.017325\n",
       "2-4-100   0.195833  0.191886  0.017325\n",
       "3-2-100   0.195833  0.191886  0.017325\n",
       "4-2-100   0.195833  0.191886  0.017325\n",
       "3-3-100   0.195833  0.191886  0.017325\n",
       "4-3-100   0.195833  0.191886  0.017325"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearities and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:43: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(1/S.sum(1))\n",
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:51: RuntimeWarning: invalid value encountered in matmul\n",
      "  return D_inv_sqr @ S @ D_inv_sqr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-1: ReLU()-Softmax(dim=1)-NLLLoss(): 0.182 (0.182)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.182 (0.182)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.182 (0.182)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.182 (0.182)\n",
      "-1: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-1: Identity()-Softmax(dim=1)-NLLLoss(): 0.182 (0.182)\n",
      "-1: Identity()-Identity()-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-2: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.189 (0.189)\n",
      "-2: ReLU()-Softmax(dim=1)-NLLLoss(): 0.189 (0.189)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.189 (0.189)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.189 (0.189)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.189 (0.189)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.189 (0.189)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.189 (0.189)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.189 (0.189)\n",
      "-2: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.189 (0.189)\n",
      "-2: Identity()-Softmax(dim=1)-NLLLoss(): 0.189 (0.189)\n",
      "-2: Identity()-Identity()-CrossEntropyLoss(): 0.189 (0.189)\n",
      "-3: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.195 (0.195)\n",
      "-3: ReLU()-Softmax(dim=1)-NLLLoss(): 0.195 (0.195)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.195 (0.195)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.195 (0.195)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.195 (0.195)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.195 (0.195)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.195 (0.195)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.195 (0.195)\n",
      "-3: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.195 (0.195)\n",
      "-3: Identity()-Softmax(dim=1)-NLLLoss(): 0.195 (0.195)\n",
      "-3: Identity()-Identity()-CrossEntropyLoss(): 0.195 (0.195)\n",
      "-4: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.211 (0.211)\n",
      "-4: ReLU()-Softmax(dim=1)-NLLLoss(): 0.211 (0.211)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.211 (0.211)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.211 (0.211)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.211 (0.211)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.211 (0.211)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.211 (0.211)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.211 (0.211)\n",
      "-4: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.211 (0.211)\n",
      "-4: Identity()-Softmax(dim=1)-NLLLoss(): 0.211 (0.211)\n",
      "-4: Identity()-Identity()-CrossEntropyLoss(): 0.211 (0.211)\n",
      "-5: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.235 (0.235)\n",
      "-5: ReLU()-Softmax(dim=1)-NLLLoss(): 0.235 (0.235)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.235 (0.235)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.235 (0.235)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.235 (0.235)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.235 (0.235)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.235 (0.235)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.235 (0.235)\n",
      "-5: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.235 (0.235)\n",
      "-5: Identity()-Softmax(dim=1)-NLLLoss(): 0.235 (0.235)\n",
      "-5: Identity()-Identity()-CrossEntropyLoss(): 0.235 (0.235)\n",
      "-6: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-6: ReLU()-Softmax(dim=1)-NLLLoss(): 0.182 (0.182)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.182 (0.182)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.182 (0.182)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.182 (0.182)\n",
      "-6: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-6: Identity()-Softmax(dim=1)-NLLLoss(): 0.182 (0.182)\n",
      "-6: Identity()-Identity()-CrossEntropyLoss(): 0.182 (0.182)\n",
      "-7: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.202 (0.202)\n",
      "-7: ReLU()-Softmax(dim=1)-NLLLoss(): 0.202 (0.202)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.202 (0.202)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.202 (0.202)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.202 (0.202)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.202 (0.202)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.202 (0.202)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.202 (0.202)\n",
      "-7: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.202 (0.202)\n",
      "-7: Identity()-Softmax(dim=1)-NLLLoss(): 0.202 (0.202)\n",
      "-7: Identity()-Identity()-CrossEntropyLoss(): 0.202 (0.202)\n",
      "-8: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.186 (0.186)\n",
      "-8: ReLU()-Softmax(dim=1)-NLLLoss(): 0.186 (0.186)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.186 (0.186)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.186 (0.186)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.186 (0.186)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.186 (0.186)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.186 (0.186)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.186 (0.186)\n",
      "-8: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.186 (0.186)\n",
      "-8: Identity()-Softmax(dim=1)-NLLLoss(): 0.186 (0.186)\n",
      "-8: Identity()-Identity()-CrossEntropyLoss(): 0.186 (0.186)\n",
      "-9: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.206 (0.206)\n",
      "-9: ReLU()-Softmax(dim=1)-NLLLoss(): 0.206 (0.206)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.206 (0.206)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.206 (0.206)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.206 (0.206)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.206 (0.206)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.206 (0.206)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.206 (0.206)\n",
      "-9: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.206 (0.206)\n",
      "-9: Identity()-Softmax(dim=1)-NLLLoss(): 0.206 (0.206)\n",
      "-9: Identity()-Identity()-CrossEntropyLoss(): 0.206 (0.206)\n",
      "-10: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.171 (0.171)\n",
      "-10: ReLU()-Softmax(dim=1)-NLLLoss(): 0.171 (0.171)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.171 (0.171)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.171 (0.171)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.171 (0.171)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.171 (0.171)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.171 (0.171)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.171 (0.171)\n",
      "-10: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.171 (0.171)\n",
      "-10: Identity()-Softmax(dim=1)-NLLLoss(): 0.171 (0.171)\n",
      "-10: Identity()-Identity()-CrossEntropyLoss(): 0.171 (0.171)\n",
      "----- 1.10 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},]\n",
    "\n",
    "best_accs8 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs8 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=exp['act'], l_act=exp['lact'],\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = NodeClassModel(arch, S, masks, exp['loss'], device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "\n",
    "        best_accs8[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs8[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}: {best_val_accs8[j,i]:.3f} ({best_accs8[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}' for exp in EXPS]\n",
    "table_over8 = summary_table(best_accs8, index_name)\n",
    "table8 = summary_table(best_val_accs8, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean accs       med  \\\n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()             0.195833  0.191886   \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                      0.195833  0.191886   \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()          0.195833  0.191886   \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                   0.195833  0.191886   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()     0.195833  0.191886   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()              0.195833  0.191886   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...   0.195833  0.191886   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...   0.195833  0.191886   \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()         0.195833  0.191886   \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                  0.195833  0.191886   \n",
       "Identity()-Identity()-CrossEntropyLoss()             0.195833  0.191886   \n",
       "\n",
       "                                                         std  \n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()            0.017325  \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                     0.017325  \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()         0.017325  \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                  0.017325  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()    0.017325  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()             0.017325  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...  0.017325  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...  0.017325  \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()        0.017325  \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                 0.017325  \n",
       "Identity()-Identity()-CrossEntropyLoss()            0.017325  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPS = [\n",
    "        {'name': 'Kipf', 'norm': 'none'},\n",
    "        {'name': 'Kipf', 'norm': 'both'},\n",
    "\n",
    "        {'name': 'A-GCNN', 'norm': False},\n",
    "        {'name': 'A-GCNN', 'norm': True},\n",
    "\n",
    "        {'name': 'H-GCNN', 'norm': False}, # This should be the same as A-GCNN not norm\n",
    "        {'name': 'H-GCNN', 'norm': True},\n",
    "\n",
    "        {'name': 'W-GCN-A', 'norm': False},\n",
    "        {'name': 'W-GCN-A', 'norm': True},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- RUN: 1\n",
      "\tKipf-none: acc = 0.325  -  acc (over) = 0.353\n",
      "\tKipf-both: acc = 0.360  -  acc (over) = 0.410\n",
      "\tA-GCNN-False: acc = 0.454  -  acc (over) = 0.467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:43: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(1/S.sum(1))\n",
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:51: RuntimeWarning: invalid value encountered in matmul\n",
      "  return D_inv_sqr @ S @ D_inv_sqr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tA-GCNN-True: acc = 0.182  -  acc (over) = 0.182\n",
      "\tH-GCNN-False: acc = 0.441  -  acc (over) = 0.463\n",
      "\tH-GCNN-True: acc = 0.425  -  acc (over) = 0.454\n",
      "\tW-GCN-A-False: acc = 0.421  -  acc (over) = 0.450\n",
      "\tW-GCN-A-True: acc = 0.182  -  acc (over) = 0.182\n",
      "- RUN: 2\n",
      "\tKipf-none: acc = 0.384  -  acc (over) = 0.384\n",
      "\tKipf-both: acc = 0.377  -  acc (over) = 0.384\n",
      "\tA-GCNN-False: acc = 0.465  -  acc (over) = 0.465\n",
      "\tA-GCNN-True: acc = 0.189  -  acc (over) = 0.189\n",
      "\tH-GCNN-False: acc = 0.436  -  acc (over) = 0.439\n",
      "\tH-GCNN-True: acc = 0.390  -  acc (over) = 0.390\n",
      "\tW-GCN-A-False: acc = 0.373  -  acc (over) = 0.482\n",
      "\tW-GCN-A-True: acc = 0.189  -  acc (over) = 0.189\n",
      "- RUN: 3\n",
      "\tKipf-none: acc = 0.366  -  acc (over) = 0.384\n",
      "\tKipf-both: acc = 0.393  -  acc (over) = 0.395\n",
      "\tA-GCNN-False: acc = 0.404  -  acc (over) = 0.430\n",
      "\tA-GCNN-True: acc = 0.195  -  acc (over) = 0.195\n",
      "\tH-GCNN-False: acc = 0.428  -  acc (over) = 0.441\n",
      "\tH-GCNN-True: acc = 0.386  -  acc (over) = 0.445\n",
      "\tW-GCN-A-False: acc = 0.377  -  acc (over) = 0.419\n",
      "\tW-GCN-A-True: acc = 0.195  -  acc (over) = 0.195\n",
      "- RUN: 4\n",
      "\tKipf-none: acc = 0.357  -  acc (over) = 0.368\n",
      "\tKipf-both: acc = 0.355  -  acc (over) = 0.362\n",
      "\tA-GCNN-False: acc = 0.434  -  acc (over) = 0.456\n",
      "\tA-GCNN-True: acc = 0.211  -  acc (over) = 0.211\n",
      "\tH-GCNN-False: acc = 0.441  -  acc (over) = 0.445\n",
      "\tH-GCNN-True: acc = 0.428  -  acc (over) = 0.441\n",
      "\tW-GCN-A-False: acc = 0.469  -  acc (over) = 0.485\n",
      "\tW-GCN-A-True: acc = 0.211  -  acc (over) = 0.211\n",
      "- RUN: 5\n",
      "\tKipf-none: acc = 0.399  -  acc (over) = 0.419\n",
      "\tKipf-both: acc = 0.340  -  acc (over) = 0.351\n",
      "\tA-GCNN-False: acc = 0.430  -  acc (over) = 0.430\n",
      "\tA-GCNN-True: acc = 0.235  -  acc (over) = 0.235\n",
      "\tH-GCNN-False: acc = 0.419  -  acc (over) = 0.425\n",
      "\tH-GCNN-True: acc = 0.412  -  acc (over) = 0.439\n",
      "\tW-GCN-A-False: acc = 0.425  -  acc (over) = 0.463\n",
      "\tW-GCN-A-True: acc = 0.235  -  acc (over) = 0.235\n",
      "- RUN: 6\n",
      "\tKipf-none: acc = 0.331  -  acc (over) = 0.349\n",
      "\tKipf-both: acc = 0.362  -  acc (over) = 0.371\n",
      "\tA-GCNN-False: acc = 0.441  -  acc (over) = 0.441\n",
      "\tA-GCNN-True: acc = 0.182  -  acc (over) = 0.182\n",
      "\tH-GCNN-False: acc = 0.428  -  acc (over) = 0.450\n",
      "\tH-GCNN-True: acc = 0.384  -  acc (over) = 0.393\n",
      "\tW-GCN-A-False: acc = 0.441  -  acc (over) = 0.461\n",
      "\tW-GCN-A-True: acc = 0.182  -  acc (over) = 0.182\n",
      "- RUN: 7\n",
      "\tKipf-none: acc = 0.382  -  acc (over) = 0.386\n",
      "\tKipf-both: acc = 0.322  -  acc (over) = 0.338\n",
      "\tA-GCNN-False: acc = 0.421  -  acc (over) = 0.465\n",
      "\tA-GCNN-True: acc = 0.202  -  acc (over) = 0.202\n",
      "\tH-GCNN-False: acc = 0.401  -  acc (over) = 0.436\n",
      "\tH-GCNN-True: acc = 0.399  -  acc (over) = 0.412\n",
      "\tW-GCN-A-False: acc = 0.423  -  acc (over) = 0.467\n",
      "\tW-GCN-A-True: acc = 0.202  -  acc (over) = 0.202\n",
      "- RUN: 8\n",
      "\tKipf-none: acc = 0.344  -  acc (over) = 0.366\n",
      "\tKipf-both: acc = 0.336  -  acc (over) = 0.342\n",
      "\tA-GCNN-False: acc = 0.450  -  acc (over) = 0.450\n",
      "\tA-GCNN-True: acc = 0.186  -  acc (over) = 0.186\n",
      "\tH-GCNN-False: acc = 0.364  -  acc (over) = 0.428\n",
      "\tH-GCNN-True: acc = 0.384  -  acc (over) = 0.412\n",
      "\tW-GCN-A-False: acc = 0.452  -  acc (over) = 0.476\n",
      "\tW-GCN-A-True: acc = 0.186  -  acc (over) = 0.186\n",
      "- RUN: 9\n",
      "\tKipf-none: acc = 0.368  -  acc (over) = 0.375\n",
      "\tKipf-both: acc = 0.309  -  acc (over) = 0.314\n",
      "\tA-GCNN-False: acc = 0.439  -  acc (over) = 0.465\n",
      "\tA-GCNN-True: acc = 0.206  -  acc (over) = 0.206\n",
      "\tH-GCNN-False: acc = 0.443  -  acc (over) = 0.452\n",
      "\tH-GCNN-True: acc = 0.439  -  acc (over) = 0.461\n",
      "\tW-GCN-A-False: acc = 0.432  -  acc (over) = 0.456\n",
      "\tW-GCN-A-True: acc = 0.206  -  acc (over) = 0.206\n",
      "- RUN: 10\n",
      "\tKipf-none: acc = 0.377  -  acc (over) = 0.377\n",
      "\tKipf-both: acc = 0.368  -  acc (over) = 0.404\n",
      "\tA-GCNN-False: acc = 0.439  -  acc (over) = 0.456\n",
      "\tA-GCNN-True: acc = 0.171  -  acc (over) = 0.171\n",
      "\tH-GCNN-False: acc = 0.432  -  acc (over) = 0.450\n",
      "\tH-GCNN-True: acc = 0.401  -  acc (over) = 0.412\n",
      "\tW-GCN-A-False: acc = 0.443  -  acc (over) = 0.447\n",
      "\tW-GCN-A-True: acc = 0.171  -  acc (over) = 0.171\n",
      "- RUN: 11\n",
      "\tKipf-none: acc = 0.377  -  acc (over) = 0.388\n",
      "\tKipf-both: acc = 0.357  -  acc (over) = 0.362\n",
      "\tA-GCNN-False: acc = 0.454  -  acc (over) = 0.456\n",
      "\tA-GCNN-True: acc = 0.182  -  acc (over) = 0.182\n",
      "\tH-GCNN-False: acc = 0.458  -  acc (over) = 0.471\n",
      "\tH-GCNN-True: acc = 0.434  -  acc (over) = 0.447\n",
      "\tW-GCN-A-False: acc = 0.443  -  acc (over) = 0.458\n",
      "\tW-GCN-A-True: acc = 0.182  -  acc (over) = 0.182\n",
      "- RUN: 12\n",
      "\tKipf-none: acc = 0.360  -  acc (over) = 0.368\n",
      "\tKipf-both: acc = 0.377  -  acc (over) = 0.382\n",
      "\tA-GCNN-False: acc = 0.432  -  acc (over) = 0.474\n",
      "\tA-GCNN-True: acc = 0.189  -  acc (over) = 0.189\n",
      "\tH-GCNN-False: acc = 0.404  -  acc (over) = 0.441\n",
      "\tH-GCNN-True: acc = 0.395  -  acc (over) = 0.414\n",
      "\tW-GCN-A-False: acc = 0.399  -  acc (over) = 0.399\n",
      "\tW-GCN-A-True: acc = 0.189  -  acc (over) = 0.189\n",
      "- RUN: 13\n",
      "\tKipf-none: acc = 0.316  -  acc (over) = 0.336\n",
      "\tKipf-both: acc = 0.393  -  acc (over) = 0.395\n",
      "\tA-GCNN-False: acc = 0.388  -  acc (over) = 0.406\n",
      "\tA-GCNN-True: acc = 0.195  -  acc (over) = 0.195\n",
      "\tH-GCNN-False: acc = 0.428  -  acc (over) = 0.445\n",
      "\tH-GCNN-True: acc = 0.368  -  acc (over) = 0.395\n",
      "\tW-GCN-A-False: acc = 0.373  -  acc (over) = 0.447\n",
      "\tW-GCN-A-True: acc = 0.195  -  acc (over) = 0.195\n",
      "- RUN: 14\n",
      "\tKipf-none: acc = 0.331  -  acc (over) = 0.364\n",
      "\tKipf-both: acc = 0.360  -  acc (over) = 0.406\n",
      "\tA-GCNN-False: acc = 0.454  -  acc (over) = 0.465\n",
      "\tA-GCNN-True: acc = 0.211  -  acc (over) = 0.211\n",
      "\tH-GCNN-False: acc = 0.452  -  acc (over) = 0.458\n",
      "\tH-GCNN-True: acc = 0.441  -  acc (over) = 0.463\n",
      "\tW-GCN-A-False: acc = 0.461  -  acc (over) = 0.485\n",
      "\tW-GCN-A-True: acc = 0.211  -  acc (over) = 0.211\n",
      "- RUN: 15\n",
      "\tKipf-none: acc = 0.379  -  acc (over) = 0.384\n",
      "\tKipf-both: acc = 0.344  -  acc (over) = 0.357\n",
      "\tA-GCNN-False: acc = 0.406  -  acc (over) = 0.423\n",
      "\tA-GCNN-True: acc = 0.235  -  acc (over) = 0.235\n",
      "\tH-GCNN-False: acc = 0.423  -  acc (over) = 0.428\n",
      "\tH-GCNN-True: acc = 0.408  -  acc (over) = 0.421\n",
      "\tW-GCN-A-False: acc = 0.434  -  acc (over) = 0.456\n",
      "\tW-GCN-A-True: acc = 0.235  -  acc (over) = 0.235\n",
      "- RUN: 16\n",
      "\tKipf-none: acc = 0.397  -  acc (over) = 0.397\n",
      "\tKipf-both: acc = 0.360  -  acc (over) = 0.375\n",
      "\tA-GCNN-False: acc = 0.423  -  acc (over) = 0.428\n",
      "\tA-GCNN-True: acc = 0.182  -  acc (over) = 0.182\n",
      "\tH-GCNN-False: acc = 0.441  -  acc (over) = 0.445\n",
      "\tH-GCNN-True: acc = 0.393  -  acc (over) = 0.408\n",
      "\tW-GCN-A-False: acc = 0.434  -  acc (over) = 0.467\n",
      "\tW-GCN-A-True: acc = 0.182  -  acc (over) = 0.182\n",
      "- RUN: 17\n",
      "\tKipf-none: acc = 0.386  -  acc (over) = 0.406\n",
      "\tKipf-both: acc = 0.329  -  acc (over) = 0.338\n",
      "\tA-GCNN-False: acc = 0.421  -  acc (over) = 0.434\n",
      "\tA-GCNN-True: acc = 0.202  -  acc (over) = 0.202\n",
      "\tH-GCNN-False: acc = 0.439  -  acc (over) = 0.452\n",
      "\tH-GCNN-True: acc = 0.410  -  acc (over) = 0.430\n",
      "\tW-GCN-A-False: acc = 0.428  -  acc (over) = 0.452\n",
      "\tW-GCN-A-True: acc = 0.202  -  acc (over) = 0.202\n",
      "- RUN: 18\n",
      "\tKipf-none: acc = 0.373  -  acc (over) = 0.382\n",
      "\tKipf-both: acc = 0.336  -  acc (over) = 0.338\n",
      "\tA-GCNN-False: acc = 0.397  -  acc (over) = 0.408\n",
      "\tA-GCNN-True: acc = 0.186  -  acc (over) = 0.186\n",
      "\tH-GCNN-False: acc = 0.443  -  acc (over) = 0.456\n",
      "\tH-GCNN-True: acc = 0.368  -  acc (over) = 0.395\n",
      "\tW-GCN-A-False: acc = 0.465  -  acc (over) = 0.465\n",
      "\tW-GCN-A-True: acc = 0.186  -  acc (over) = 0.186\n",
      "- RUN: 19\n",
      "\tKipf-none: acc = 0.338  -  acc (over) = 0.344\n",
      "\tKipf-both: acc = 0.314  -  acc (over) = 0.395\n",
      "\tA-GCNN-False: acc = 0.465  -  acc (over) = 0.487\n",
      "\tA-GCNN-True: acc = 0.206  -  acc (over) = 0.206\n",
      "\tH-GCNN-False: acc = 0.463  -  acc (over) = 0.463\n",
      "\tH-GCNN-True: acc = 0.417  -  acc (over) = 0.441\n",
      "\tW-GCN-A-False: acc = 0.417  -  acc (over) = 0.445\n",
      "\tW-GCN-A-True: acc = 0.206  -  acc (over) = 0.206\n",
      "- RUN: 20\n",
      "\tKipf-none: acc = 0.296  -  acc (over) = 0.314\n",
      "\tKipf-both: acc = 0.373  -  acc (over) = 0.393\n",
      "\tA-GCNN-False: acc = 0.436  -  acc (over) = 0.454\n",
      "\tA-GCNN-True: acc = 0.171  -  acc (over) = 0.171\n",
      "\tH-GCNN-False: acc = 0.436  -  acc (over) = 0.463\n",
      "\tH-GCNN-True: acc = 0.425  -  acc (over) = 0.436\n",
      "\tW-GCN-A-False: acc = 0.428  -  acc (over) = 0.454\n",
      "\tW-GCN-A-True: acc = 0.171  -  acc (over) = 0.171\n"
     ]
    }
   ],
   "source": [
    "N_RUNS = 20\n",
    "\n",
    "best_accs = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs2 = np.zeros((len(EXPS), N_RUNS))\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    print(f'- RUN: {i+1}')\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        # t_i = time.time()\n",
    "        if exp['name'] == 'Kipf':\n",
    "            arch = GCNN_2L(IN_DIM, HID_DIM, OUT_DIM, act=ACT, l_act=LAST_ACT,\n",
    "                           dropout=DROPOUT, norm=exp['norm'])\n",
    "            S = dgl.from_networkx(nx.from_numpy_array(A)).add_self_loop().to(device)\n",
    "            \n",
    "        elif exp['name'] == 'A-GCNN':\n",
    "            arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, l_act=LAST_ACT,\n",
    "            dropout=DROPOUT, diff_layer=GFGCNLayer, init_h0=h0)\n",
    "            if exp['norm']:\n",
    "                S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "            else:\n",
    "                S = torch.Tensor(A).to(device)\n",
    "\n",
    "        elif exp['name'] == 'H-GCNN':\n",
    "            arch = GFGCN_Spows(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, l_act=LAST_ACT,\n",
    "                               dropout=DROPOUT, norm=exp['norm'], dev=device)\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        elif exp['name'] == 'W-GCN-A':\n",
    "            arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, l_act=LAST_ACT,\n",
    "                         dropout=DROPOUT, diff_layer=GFGCN_noh_Layer)\n",
    "            if exp['norm']:\n",
    "                S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "            else:\n",
    "                S = torch.Tensor(A).to(device)  \n",
    "            \n",
    "        else:\n",
    "            raise Exception(f'ERROR: Unknown architecture: {exp[\"name\"]}')\n",
    "\n",
    "        if exp['name'] in ['Kipf', 'W-GCN-A']:\n",
    "            model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "        else:\n",
    "            model = GF_NodeClassModel(arch, S, K, masks, LOSS_FN, device=device)\n",
    "\n",
    "        loss, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "\n",
    "        best_accs[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs[j,i] = model.test(feat, model.S, labels, masks['test'])\n",
    "        best_val_accs2[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'\\t{exp[\"name\"]}-{exp[\"norm\"]}: acc = {best_val_accs[j,i]:.3f}  -  acc (over) = {best_accs[j,i]:.3f}')\n",
    "\n",
    "\n",
    "# Print results\n",
    "index_name = [f'{exp[\"name\"]}-{exp[\"norm\"]}' for exp in EXPS]\n",
    "table_comp_over = summary_table(best_accs, index_name)\n",
    "table_comp = summary_table(best_val_accs, index_name)\n",
    "table_comp2 = summary_table(best_val_accs2, index_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kipf-none</th>\n",
       "      <td>0.359320</td>\n",
       "      <td>0.367325</td>\n",
       "      <td>0.027880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kipf-both</th>\n",
       "      <td>0.353180</td>\n",
       "      <td>0.358553</td>\n",
       "      <td>0.023461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-False</th>\n",
       "      <td>0.432566</td>\n",
       "      <td>0.435307</td>\n",
       "      <td>0.021306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-True</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-False</th>\n",
       "      <td>0.430921</td>\n",
       "      <td>0.436404</td>\n",
       "      <td>0.021492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-True</th>\n",
       "      <td>0.405373</td>\n",
       "      <td>0.404605</td>\n",
       "      <td>0.021557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-False</th>\n",
       "      <td>0.426864</td>\n",
       "      <td>0.429825</td>\n",
       "      <td>0.027463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-True</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean accs       med       std\n",
       "Kipf-none       0.359320  0.367325  0.027880\n",
       "Kipf-both       0.353180  0.358553  0.023461\n",
       "A-GCNN-False    0.432566  0.435307  0.021306\n",
       "A-GCNN-True     0.195833  0.191886  0.017325\n",
       "H-GCNN-False    0.430921  0.436404  0.021492\n",
       "H-GCNN-True     0.405373  0.404605  0.021557\n",
       "W-GCN-A-False   0.426864  0.429825  0.027463\n",
       "W-GCN-A-True    0.195833  0.191886  0.017325"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kipf-none</th>\n",
       "      <td>0.359320</td>\n",
       "      <td>0.365132</td>\n",
       "      <td>0.028026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kipf-both</th>\n",
       "      <td>0.360307</td>\n",
       "      <td>0.358553</td>\n",
       "      <td>0.022160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-False</th>\n",
       "      <td>0.423136</td>\n",
       "      <td>0.425439</td>\n",
       "      <td>0.021369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-True</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-False</th>\n",
       "      <td>0.426425</td>\n",
       "      <td>0.424342</td>\n",
       "      <td>0.020212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-True</th>\n",
       "      <td>0.410746</td>\n",
       "      <td>0.406798</td>\n",
       "      <td>0.028357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-False</th>\n",
       "      <td>0.443531</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.021101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-True</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean accs       med       std\n",
       "Kipf-none       0.359320  0.365132  0.028026\n",
       "Kipf-both       0.360307  0.358553  0.022160\n",
       "A-GCNN-False    0.423136  0.425439  0.021369\n",
       "A-GCNN-True     0.195833  0.191886  0.017325\n",
       "H-GCNN-False    0.426425  0.424342  0.020212\n",
       "H-GCNN-True     0.410746  0.406798  0.028357\n",
       "W-GCN-A-False   0.443531  0.447368  0.021101\n",
       "W-GCN-A-True    0.195833  0.191886  0.017325"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_comp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kipf-none</th>\n",
       "      <td>0.372149</td>\n",
       "      <td>0.376096</td>\n",
       "      <td>0.023964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kipf-both</th>\n",
       "      <td>0.370395</td>\n",
       "      <td>0.372807</td>\n",
       "      <td>0.026866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-False</th>\n",
       "      <td>0.447917</td>\n",
       "      <td>0.455044</td>\n",
       "      <td>0.021497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-True</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-False</th>\n",
       "      <td>0.447478</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.012434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-True</th>\n",
       "      <td>0.425439</td>\n",
       "      <td>0.425439</td>\n",
       "      <td>0.022642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-False</th>\n",
       "      <td>0.456689</td>\n",
       "      <td>0.457237</td>\n",
       "      <td>0.020086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-True</th>\n",
       "      <td>0.195833</td>\n",
       "      <td>0.191886</td>\n",
       "      <td>0.017325</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean accs       med       std\n",
       "Kipf-none       0.372149  0.376096  0.023964\n",
       "Kipf-both       0.370395  0.372807  0.026866\n",
       "A-GCNN-False    0.447917  0.455044  0.021497\n",
       "A-GCNN-True     0.195833  0.191886  0.017325\n",
       "H-GCNN-False    0.447478  0.447368  0.012434\n",
       "H-GCNN-True     0.425439  0.425439  0.022642\n",
       "W-GCN-A-False   0.456689  0.457237  0.020086\n",
       "W-GCN-A-True    0.195833  0.191886  0.017325"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_comp_over"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
