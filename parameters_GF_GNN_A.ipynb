{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5e095a0870>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import utils\n",
    "from gsp_utils.baselines_archs import GCNN_2L\n",
    "from gsp_utils.baselines_models import NodeClassModel, GF_NodeClassModel\n",
    "from gsp_utils.data import normalize_gso\n",
    "from src.arch import GFGCN, GFGCNLayer, GFGCN_noh_Layer, GFGCN_Spows\n",
    "\n",
    "SEED = 15\n",
    "# PATH = 'results/diff_filters/'\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTAS:\n",
    "- Se consiguen resultado prácticamente iguales con y sin bias\n",
    "- Clara diferencia usando optimización alterna vs única pasada (la única iba mejor con LR=0.05 y WD=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "def summary_table(acc, index_name):\n",
    "    mean_accs = acc.mean(axis=1)\n",
    "    med_accs = np.median(acc, axis=1)\n",
    "    std_accs = acc.std(axis=1)\n",
    "    return DataFrame(np.vstack((mean_accs, med_accs, std_accs)).T, columns=['mean accs', 'med', 'std'], index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: WisconsinDataset\n",
      "Number of nodes: 251\n",
      "Number of features: 1703\n",
      "Shape of signals: torch.Size([251, 1703])\n",
      "Number of classes: 5\n",
      "Norm of A: 22.69361114501953\n",
      "Max value of A: 1.0\n",
      "Proportion of validation data: 0.32\n",
      "Proportion of test data: 0.20\n",
      "Node homophily: 0.13\n",
      "Edge homophily: 0.20\n"
     ]
    }
   ],
   "source": [
    "# Dataset must be from DGL\n",
    "dataset_name = 'WisconsinDataset'\n",
    "\n",
    "A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device,\n",
    "                                                     verb=True)\n",
    "N = A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without normalizing the GSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PARAMETERS  -  0.874\n",
    "## Training params\n",
    "N_RUNS = 20\n",
    "N_EPOCHS = 200  # 100\n",
    "EPOCHS_h = 25\n",
    "EPOCHS_W = 25\n",
    "LR = .005\n",
    "WD = .001\n",
    "DROPOUT = .25\n",
    "\n",
    "# BEST PARAMETERS\n",
    "## Architecture params\n",
    "N_LAYERS = 2\n",
    "K = 3  # 2\n",
    "HID_DIM = 32  # 32\n",
    "\n",
    "## Model params\n",
    "h0 = 1  # 1\n",
    "NORM = False\n",
    "\n",
    "IN_DIM = feat.shape[1]\n",
    "OUT_DIM = n_class\n",
    "\n",
    "ACT = nn.LeakyReLU() \n",
    "LAST_ACT = nn.Softmax(dim=1)\n",
    "LOSS_FN = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting eval/test acc/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val acc: 0.850  -  Best test acc: 0.843\n",
      "Test acc at best val: 0.765\n",
      "Val acc: 0.850  -  Test acc: 0.784\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = N_EPOCHS\n",
    "epochs_h = EPOCHS_h\n",
    "epochs_W = EPOCHS_W\n",
    "lr =  .005 #LR\n",
    "wd = .005  #WD\n",
    "drop = DROPOUT\n",
    "L = N_LAYERS\n",
    "K_aux = K\n",
    "hid_dim = HID_DIM\n",
    "h0_aux = 1\n",
    "norm = False\n",
    "act = ACT\n",
    "lact = LAST_ACT\n",
    "loss_fn = LOSS_FN\n",
    "patience = 200\n",
    "bias = True\n",
    "\n",
    "iters_aux = 1\n",
    "\n",
    "err1 = np.zeros(iters_aux)\n",
    "err2 = np.zeros(iters_aux)\n",
    "for i in range(iters_aux):\n",
    "\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "\n",
    "    # Create model\n",
    "    arch = GFGCN(IN_DIM, hid_dim, OUT_DIM, L, K_aux, act=act, last_act=lact,\n",
    "                dropout=drop, init_h0=h0_aux, bias=bias)\n",
    "    S = torch.Tensor(A).to(device)\n",
    "    model = GF_NodeClassModel(arch, S, K, masks, loss_fn, device=device)\n",
    "    loss, acc = model.train(feat, labels, epochs, lr, wd, epochs_h=epochs_h, epochs_W=epochs_W,\n",
    "                            patience=patience)\n",
    "\n",
    "    idx_max_acc = np.argmax(acc[\"val\"])\n",
    "    print(f'Best val acc: {acc[\"val\"][idx_max_acc]:.3f}  -  Best test acc: {np.max(acc[\"test\"]):.3f}')\n",
    "    print(f'Test acc at best val: {acc[\"test\"][idx_max_acc]:.3f}')\n",
    "\n",
    "    acc_val = model.test(feat, S, labels, masks['val'])\n",
    "    acc_test = model.test(feat, S, labels, masks['test'])\n",
    "    print(f'Val acc: {acc_val:.3f}  -  Test acc: {acc_test:.3f}')\n",
    "\n",
    "    err1[i] = acc[\"test\"][idx_max_acc]\n",
    "    err2[i] = acc_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc at best val acc: 0.765 +- 0.000\n",
      "Acc at test: 0.784 +- 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fc4303e3490>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAFfCAYAAAAxo9Q/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAACZN0lEQVR4nOydd3hUVfrHPzOTSSMNEpJQUuidgCiIiKCiIPaKioKssurKWnAtqCvqb1121xXLrqvuLiz2riiCBVGaoNTQe0mAkArpZSYz9/fHmXtnJo0kTDIJeT/PM8+dufWce++c8z3vec97TJqmaQiCIAiC0KYx+zsBgiAIgiD4HxEEgiAIgiCIIBAEQRAEQQSBIAiCIAiIIBAEQRAEAREEgiAIgiAggkAQBEEQBCDA3wmoD06nk4yMDMLDwzGZTP5OjiAIgiC0GjRNo6ioiM6dO2M2124HaBWCICMjg4SEBH8nQxAEQRBaLUeOHKFr1661bm8VgiA8PBxQmYmIiPDJOe12O99//z2XXnopVqvVJ+f0N5Kn1oHkqXUgeWo9nIn58mWeCgsLSUhIMOrS2mgVgkDvJoiIiPCpIAgNDSUiIuKMeoEkTy0fyVPrQPLUejgT89UUeTpVl7s4FQqCIAiCIIJAEARBEAQRBIIgCIIgIIJAEARBEAREEAiCIAiCgAgCQRAEQRAQQSAIgiAIAo0QBCtXruTKK6+kc+fOmEwmFi5ceMpjli9fzllnnUVQUBA9e/ZkwYIFjUiqIAiCIAhNRYMFQUlJCSkpKbz22mv12v/QoUNcfvnlXHjhhaSmpvLggw9y11138d133zU4sYIgCIIgNA0NjlR42WWXcdlll9V7/zfeeINu3brx4osvAtCvXz9Wr17NSy+9xPjx4xt6eUEQBEEQmoAmD128du1axo0b57Vu/PjxPPjgg7UeU1FRQUVFhfG7sLAQUKEc7Xa7T9Kln8dX52sJtMU87d4NDz9soaSkOVN1emiamfz88/nrX82YTE5/J8cnSJ5aB2dinuDMy9ell2o88ojvyvP6nqPJBUFmZiZxcXFe6+Li4igsLKSsrIyQkJBqx8yZM4dnn3222vrvv/+e0NBQn6Zv6dKlPj1fS6Ct5EnT4KmnRrFjR4wfUnS6RPs7AU2A5Kl1cCbmCc6kfAUFpTNkyGbAN+V5aWlpvfZrkZMbzZo1i5kzZxq/9ZmaLr30Up9ObrR06VIuueSSM2oyjLaUp6++MrFjRwDBwRpvvukgONhPiWwgDoeDLVu2kJKSgsVi8XdyfILkqXVwJuYJzrx8JSZ2ZvDgGJ+V57qV/VQ0uSCIj48nKyvLa11WVhYRERE1WgcAgoKCCAoKqrbearX6vKJrinP6m7aQJ7sdnnhCfX/oIRNTprRIbVsjdrtGaOhxJk4citXaetJdF5Kn1sGZmCc4M/Nlt2uAb8rz+h7f5Hdu5MiRLFmyxGvd0qVLGTlyZFNf+oxmzx4YPx5ycz3XBuBwXH5GKGQ3NefJ6YSyMoiNhccf91PSBEEQziAaLAiKi4vZv3+/8fvQoUOkpqbSoUMHEhMTmTVrFseOHePtt98G4J577uGf//wnjz76KL/5zW/48ccf+fjjj1m8eLHvctEGeeghSEurutZEC+0FOg3qztNf/wo+6kUSBEFo0zS49tiwYQMXXnih8Vvv6586dSoLFizg+PHjpKenG9u7devG4sWLeeihh3jllVfo2rUr//3vf2XI4WmwdCl88w0EBMCKFdCpk1pvt9tZvnw5Y8eOPWO6DOrKU2goVPFXFQRBEBpJgwXB2LFj0TSt1u01RSEcO3YsmzdvbuilBJRpPC0NKivVb02DP/xBfb/vPjjvPPe+djvs2lVKt25whuiBMzJPgiAILZEzzb58xnHvvfDvf1dfHxUFf/xjsydHEARBOEMRQdCC2bTJLQYiI93rrVZ44QWIPnOG3QqCIAh+RgRBC0XT4OGH1fdbb4X33vNvegRBEIQzG5n+uIWyaBEsXw5BQfDnP/s7NYIgCMKZjgiCFsr//Z9aPvQQJCX5Ny2CIAjCmY8IghZIZiZs2KC+1zEHlCAIgiD4DBEELZDvv1fLs86ScfaCIAhC8yCCoAXy3XdqKbGbBEEQhOZCBEELw+l0WwgmTPBvWgRBEIS2gwiCFsbmzWrCovBwkPmfBEEQhOZCBEEL49tv1fLiiyVUryAIgtB8iCBoYYj/gCAIguAPRBC0IDTNPdzQY0JJQRAEQWhyRBC0IEpLoaxMfe/a1b9pEQRBENoWIghaEDk5ahkcDKGh/k2LIAiC0LYQQdCCyM1Vy5gYMJn8mxZBEAShbSGCoAWhWwg6dvRvOgRBEIS2hwiCFoSnhUAQBEEQmhMRBC0IsRAIgiAI/kIEQQtCLASCIAiCvxBB0IIQQSAIgiD4CxEELQjpMhAEQRD8hQiCFoRYCARBEAR/IYKgBSEWAkEQBMFfiCBoQYiFQBAEQfAXIghaCA4HnDihvosgEARBEJobEQQthBMn1GyHANHR/k2LIAiC0PYQQdBC0LsL2reHgAD/pkUQBEFoe4ggaCGIQ6EgCILgT0QQtBDEoVAQBEHwJyIIWgi6hUAEgSAIguAPRBC0EHQLgXQZCIIgCP5ABEELQboMBEEQBH8igqCFIE6FgiAIgj8RQdBCEAuBIAiC4E8aJQhee+01kpOTCQ4OZsSIEaxbt67Wfe12O8899xw9evQgODiYlJQUvv3220Yn+ExFnAoFQRAEf9JgQfDRRx8xc+ZMZs+ezaZNm0hJSWH8+PFkZ2fXuP9TTz3Fm2++yT/+8Q927tzJPffcw7XXXsvmzZtPO/FnEuJUKAiCIPiTBguCuXPnMn36dKZNm0b//v154403CA0NZf78+TXu/8477/DEE08wceJEunfvzr333svEiRN58cUXTzvxZwIOBxQViYVAEARB8C8NCpJrs9nYuHEjs2bNMtaZzWbGjRvH2rVrazymoqKC4OBgr3UhISGsXr261utUVFRQUVFh/C4sLARU94Pdbm9IkmtFP4+vztcYcnPh7LMDyMgwGeuiouw0NkktIU++RvLUOpA8tQ7OxDzBmZkvX+apvucwaZo+pc6pycjIoEuXLqxZs4aRI0ca6x999FFWrFjBr7/+Wu2YW2+9lS1btrBw4UJ69OjBsmXLuPrqq3E4HF6VvifPPPMMzz77bLX177//PqGhofVNbotn2bJE/vGPocbvQYNyeO65NZhMdRwkCIIgCA2gtLSUW2+9lYKCAiIiImrdr8mn0XnllVeYPn06ffv2xWQy0aNHD6ZNm1ZrFwPArFmzmDlzpvG7sLCQhIQELr300joz0xDsdjtLly7lkksuwWq1+uScDeW99ywAPPaYgyeecBIcHIXJNLHR52sJefI1kqfWgeSpdXAm5gnOzHz5Mk+6lf1UNEgQxMTEYLFYyMrK8lqflZVFfHx8jcd07NiRhQsXUl5eTl5eHp07d+bxxx+ne/futV4nKCiIoKCgauutVqvPH3ZTnLM+OBzwww/q+1VXWYiIsPjs3P7KU1MieWodSJ5aB2dinuDMzJcv8lTf4xvkVBgYGMiwYcNYtmyZsc7pdLJs2TKvLoSaCA4OpkuXLlRWVvLZZ59x9dVXN+TSZxwbNsDJkxAZCcOH+zs1giAIQlunwV0GM2fOZOrUqZx99tkMHz6cl19+mZKSEqZNmwbAlClT6NKlC3PmzAHg119/5dixYwwZMoRjx47xzDPP4HQ6efTRR32bk1bGd9+p5bhxENDkHTeCIAiCUDcNroomTZpETk4OTz/9NJmZmQwZMoRvv/2WuLg4ANLT0zGb3YaH8vJynnrqKQ4ePEhYWBgTJ07knXfeISoqymeZaI3osZkmTPBvOgRBEAQBGulUOGPGDGbMmFHjtuXLl3v9HjNmDDt37mzMZc5YTp4EfUDG+PH+TYsgCIIggMxl4Bc++ACcTujXDxIS/J0aQRAEQRBB0OwUFsIzz6jv993n16QIgiAIgoEIgmZmzhwVprhPH/jtb/2dGkEQBEFQiCBoRtLT4aWX1Pe//Q3OsOGygiAIQitGBEEz8uGHUFEBo0fDlVf6OzWCIAiC4EYEQTOyb59aXnwxMl+BIAiC0KIQQdCM7N+vlj17+jcdgiAIglAVEQTNiC4IevTwbzoEQRAEoSoiCJqJsjI4elR9FwuBIAiC0NIQQdBMHDqklpGREB3t37QIgiAIQlVEEDQTnt0F4lAoCIIgtDREEDQTBw6opXQXCIIgCC0REQTNhIwwEARBEFoyIgiaCRlhIAiCILRkRBA0E2IhaJ3kl+dTai/1dzKEWiiqKKKwotDfyRBaKIUVhRTbiv2djGpUVFZQUF5g/NY0jYyiDDRN82OqRBA0C3Y7pKWp7yIIWg8lthJ6vtqT4f8Z7u+kCDVQXlnOkDeH0P+1/hRVFPk7OUILw+aw0feffRn8+mCcmtPfyfHipk9vIu7vcRwpOALAq7++Spe5XXh367t+TZcIgmYgLQ0cDggJgU6d/J0aob7sydtDXlkeO3J2UFFZ4e/kCFX4as9XHDx5kGNFx/h056f+To7QwjhaeJTjxcc5lH+I40XH/Z0cg715e/lqz1dUOCrYnLkZgA3HNwDw3YHv/Jk0EQTNgQw5bJ0czj9sfM8ry/NfQoQaeWvLWzV+b/E4m6G1qmnNc50WTGZxpvHd87/sb97e8rbxPa80z2u5JWuLX9KkI4KgGZAhh60TL0FQKoKgJZFZnMl3+92tqRVpKzh08pAfU1RP1q1T0cn0edCbikmTVAukuOX1nzcXLVEQODUn72x9x/itNzT05a6cXZRXlvslbSCCoFlYtUotRRC0LtLy04zvYiFoWby39T0cmoNzu57Lxd0uBvAqaFssS5eqSnrRoqa9znffweHDsHNn016nBdMSBcHyw8tJL0g3fle1EDg0Bzuyd/glbSCCoMnZuBE++kh9nzTJv2kRGsbhgsPG99osBDklOczbNI8SW0kzpco/LNy9kL15e5vk3Fsyt/D5rs/rvb+maSzYsgCAqSlTmZoyFVDdBv720j4l6eneyzrYl7ePd7a8U688fbXnKzYd3+ReUeoaGZObW++kLdy90Pscp8DhdPDe1vfIKcmp9zFNiVNz8u62dzlwQplks4qzjG1pBUrc7z+xn6/2fNWo8687to6v935t/D5ScIQFqQuwO+xe+2maxic7PqlRhOhdWwHmAKC6hQD8220ggqAJ0TR4+GH1ffJkOPts/6ZHaBj18SF4ftXz3LXoLt7Y8EYzpar5WXNkDdd+dC0T35vYJN7akz6dxPUfX8+unF312j+tII3t2duxmq1MGjCJ6/pdRztrOw6ePOj3PthTog83OnLklH380xdNZ8rCKSw9uLTuU+ancfWHV3PDxzeoFXY7VFaq7/UUBHty93DtR9dy3UfX1Wt/gPmb53PbF7fxyNJH6n1MU7KlaAu/WfQb7v76bqBmC8Hkzydz9YdXszp9dYPPf/WHV3PVB1dxtFDNUvfw9w8z7ctp1Rxa3936Ljd9ehO/W/w7r/VOzckXu74AYNIA1TrMK8uj0llJfnm+sV9qZmqD0+YrRBA0IYsWwYoVEBQEf/6zv1MjNJT6+BDoLY9fj/3aHEnyC2uPrAXgwMkDrExb6dNzOzUnB08eBGB37u56HaMX9F0iutA+pD3tAtsxousIADYf3+zT9Pkc3TJgs0FWVp277spVAmndsXV17nes6Jg6dUG6siaUesTNqKcg0K+RVpBGmb2sXsesOboGgLVH19Zr/6bmSLkawqe/R5kl3oKgvLKcjRkbAfc7XV9KbCVkFmeioRmWMv356EudeZvnAbAjx9v0f+DEAYpsRYQEhDCx10RAlSsny0567ScWgjOU119XywcegMRE/6ZFaBj55fleAW9qsxDoQsGfqr6pSc1KNb772pv/ZNlJ7E5lcq1vP69+z6ND3NOGpsSlAC38OWiad1dBHd0GpfZSskuygVPnSW9dOjQHBRUFUOLRfVVPQeB5DV3k1veYfXn7WkSXWbZN3a+MogwqKiu8LATpBelsz96OQ3MA3u90fcgqcYu3w/mH0TTNeF8939tDJw+xIm0FoIY9enYn6PdrYOxAYtvFAqpcqVq2pGam+q3rSwRBE7LZ1Vi59lr/pkNoOFUrp1oFgWv9/hP7W2RENF+wJdPdYvl056c+Lfwb4/il3/PoULcgGBI/BPD/sK06OXHCu7JOq73i9XRora8gAJdY8rQQ5NSvf9+zgqzPc7A5bOzMUQ6LGhrbsrfV6zpNiS4INDSOFB7xercqHBVeo1I83+n6UPU9PVF2wvi/e94vT8dWp+Y0uhfA/RyHxA8xxGxeaZ4hcBMiEgi0BFJYUeg3J0gRBE1EZqayCJpMMGiQv1MjNJRqgqCWLgN9vYbGtiz/F4q+pqKywjCJRodEU2wrbpAD4Knwanl5OHHWxaksBC3WsbCqRaAOC4Hn+3fg5IE6IzF6CoLc0twGdxlomuZVQdanMtqduxubw2b8bgmWGV0QgMqD7lRoNVsB+HLPl8b23bm7GzS8r6og8LxHukVF0zSvGAP6vjq6WE2JSzHErKeFID4sngEdB3jt29yIIGgitrieZ69e0K6df9NSlbT8NG74+IYG96P5k3e3vcvUhVO9CiFQnr43fXKT0TfoK/Q/cpAlCKjZQqBpmtd6nxSKDazMvtj1BWMWjOH8+edz2bsT2H9i/+mnwYOdOTupdFbSPrg994+4H2hct0F5ZTm3f3E7729732u9Z0Hr2SquC8NC4CEI+nXsh9VspaCioEaTd1ZxFjd+ciM/HvoRUM9uxpIZ/OPXf9R8kYaKCocDsrJ49/sX+cOi+2sWJacSBB7HVM3D1qyt1fd17Z9fnk+AA0xO173xEARaLYIgoyiDSZ/cxMq0lRwrOub1HuvP4T8b/8Mj3z9SoyNp1Ra28e6fphizO+w88M0DLEhdUOs+q9JWcfsXt1fre8+xu60h245sNKKL6taj9Rnrje368L6tWVu54eMbTjmCpuqIBc/nc7TwKPayEtYcWcOBkwcICwxjVMIota+HIKjJQmBz2IxhiNGh0aTE+7frSwRBE6ELgiFD/JqMGnlry1t8tuszXv71ZX8npd7MXjGbt7e8Xc07+J/r/sknOz/hwe8e9On19EJR/4PmllYvWItsRVQ6K43fp63qf/gBAgPhnfqNp3c4Hcz4ZgYr01bSb+HPfDn1Oz554/7TS0MVPAuxWwfdCqix1A0NnvLTyrd5afK7mO++x2t9o7oMSqt3GQRaAhkQ62pd1WAOfnvL23y681NeXPsioBy+Xlv/Gk/99FT1Czz6KHTo4A4xeiocDhg6FOLjuW38H/jDLf9g654anC+rdhF4/q6shHPOUR+Ho9q98Hq3Nm5UcdBffhmAipxM0l6CLz+s3mVQlnmkxiR/NP8hFtz6CWsfuaVa5XO44DB2h53ff/N7/r727zU6NerHxIfFu9N38iQkJ8NNN1W/4H33qXtaWzfJ/v0QGcnB31zLq+te5bHFD0FKCgwfru6vB08vf5p3t75rOO+BEkUlDtUdE1UG066ezWcfQWRQJH1j+nodr6c5NTOVP3z/Bz7b9RlP/vhkzelyUZeFYPJmJwFRHTj81isAXNP3Gvp37E90CVx36QNw223kluYazp+D4wYTag01Ghu6GIkOiWZI3BBALARnHKmpatkSBcGhfBXRrVVEdgPsTrvxZ6oak1zv+1ydvtoYf+wLdPP1sE7DgJq7DKquO21Vv2iRqhjmzTv1vsCyQ8vIKMqgfXB7njuQSKATgr77wad9/HqeUuJS6NG+Bx1COjQqeIpj4RfElMHV64twVrjFhGdBW1BR4GX+ro2aLAR6Gj3T7JUP13uivz/6srCi0EvUAbBwIeTnY/68nl0jW7fCNnd3UXwJlK5fU30/3SLQp4/3b1AtiI0bYcMGOHbMqHAigiKq52nJEqioMN6TTut20rkYxhxW96a84ISxqymnupC1O+1Ef76EkEoYtiHDsPjo1zqcf5g9eXuocFRUv7aeXFeFddug29QtyNqK89tvVZ6+/NJ7SGVlJbz1lhIMX3xR/b4AfPIJFBbS+bPvMTmh+6F8dV/Xr4ft243dNE0z0lObI+SwDIgqqOCy/RDXLpbkqGRjm9Vs5cb+NwKweN9ifjj4A6DiOJwoc9+3qni+p0cLj3pZ4ibsB5PNRru1aj6CEV1GkByVzGX7ISK3CBYuZMtxldYe7XsQHhSOyWQyBK2nILi89+W8d917/HXcX2tNS1MigqCJ0AVBSopfk1Ejeuu3vt7E/ibPnmeYLT3/mOBdKFTtvzsd9AJZFwQny09WM53qFZPFZAFgW/Y2HE7v1kyD0CuIX39Vw9JOgV6QT+15A/F7MwCIP2Hni921FLqNQC/4h8QPwWQyGebXhoqfiPXK5B1SCbmrvzfWV32e9bES1ORUqKfRM82e6OnVr+clRDymofUcCWBaU0OlXhOrldXq5wHhLO2uVtkO1WBd0J/v6NHevz3Ooa/X74M+PM3rfuvH7dgBJ06QuFX9j8NskFeSS8FJt2gOKiyp1sLeULiBsw+qyj6xAGMcvX6tw/mHva5X9Vl7Vso3DriRkIAQSu2lFPy4xJV5G2S7+/NJTXU7U66uZfy/a314iZ3+OXB+evVtoEYL6KKxNkGQ6HqcIZXQR4v2EgQDYgdwTudzAPhi9xdoqC4Om8PGR9s/qjlteA9hdGpOfj7yc7XrObJVt8KQ+CEkRyW781BSwu59a41tOrqgNQRBaDQ9O/Tk1kG30ju6d61paUpEEDQBZWWwZ4/63hItBHphk12STam9tO6dWwCezkKeBXlmcaYxNAvg7a1v+yxwji6azup0FqAKgaqtV91C0K9jP6NQPK0+fN2cWl6uWot1UFhRaAQ5udsxBJMrEE1Sge+GBnoW/HrXSaNMmppG713uPtjSZW5vb0+nQqifH0FNToVArWKlzF7Gnlz1h8wuycapOb3eI6/nmp2tWt+4BEF9JghyxSZfHFdEeqSekRryoa87/3y1PHHCPdeAHt/ctZ9ewV3T5xpAiU3DkuF57p9/ptdOdQ/NQGF+FkX57ntq1lAtcw/WH1vKINcuiQXK98DzWpnFmfx61B1Xo+qzzijKIK8sD4vJwuC4wQyKG1RjHgw8169aVd3PwOGAn90V7Oh0GO15+zyO90yLp2Og3g/fNbwrSR76bkBZGEmRScbvlLgUr0oZ3P/xuv43VYWr7tPRNcJ9vdACVZYOjhtMUmSSVx4yd6j76SUIXIJWL4+rvs/+QARBE7B9uypHYmJa3nTHDqeDI4XufsX6OnI1Bk3TKKoooqiiqLpZtgHk2NzOQp4ViF7wJ0clExEUweH8w40OnOMpJArKCzhZrgrRXtG9CAsMA9wVkb6v3lKNbRdrFIqn1ffn2WJ0FYK1CZxPdnxCWWUZ/WL60WenWxQlFsCyg8uMedbrS4mthKKKIq9pntML0imoKMBqttK/Y3+ARjk95e/aTHyBu5VqWfuL8V0vaCODVE16OhYCvcvgUP4hYyw6KH8Bffy5Q3OQV5rn9R55CQKPZ2DKzyfC9bu8stx4l70+5YU4V6l3bnUihiCwHnO30o1nqJ974EA1wZFrncNRaZwDwH7ogHFfLu5+Me2s7SivLGdf3r5qaWTJErqnu0cglOZnU5zv0TrH3XIttZeSVpBGux2pRsEf7ICOrjbBRd0uMt71r/a6w/tuzdrqZfnSn33fmL4EBwSTEpdCZBlE7vUoS1xp1DQN+4qf3Ouzs2HfPq/02VI3QYG7Fr8w3extIfAQEZ7vnWfXlS6gLki6wGixA/QsCvSyEAyJH0LfmL4EWgIBCAkI4cPrP8RisvDrsV9rDY5V9T3VubjrBXR23f6YUujevjsRQRF0d0TQ36O3pmCf6vbQ31FwCwDdSlH1ffYHIgiaAE+HwpY23XFGUYZX5dyU3QYT3ptAxF8iiPhLBJ1e7NTgSkqnNguB7jw2ossIbuqvHJnqUvn3Lb6Pbq90I6Mow2v90gNLiZgTYXg26/ckJjSGsMAw95jhsjx+8+Vv6DK3C7mluV4tVb3l3Gg/gpISyPPwSVi9muySbDq/2Jl7v7632u6esfxNHq2rzkVgcWi8u/Xdel/6ke8fIWxOGBF/iSDqr1H8dEgV4Hpe+nfsbxSgnmZ5T0/62StmM3X7VOPe/XDwByL/EslH2z/i+JKPATgZrPaN3rTLaHnrz1OPNFgvQVCLhaB9SHsSI1UEsC5zuxDxlwh+PPRjtWeSWZxZzXfBoIrnf4edO/lyz5dEzIkw3mXPz5DHIjFnZlFhgXVdIKyXEk6hmSqN/1z3T6L+EsUv+1eoschA94UXktNRDT0q2reDi55OxJztFr2FrspDf/cGxw0GUPMMaBqONI97tGABFo8Gd/nJXEoLvP0Gjh5M5W8//412f25Hr9d6cV6at8hMyofO4Z3p2K6j0Zr2nICn1F7KgZNu/xx9vgP9XRgSP4SRR13WCB2XheDaD6/h5FI1kVN+iKsw9OgCWH54OY88cx7gfj+u2WOiQzlUBAWA1QoZGWqiJqr/v3QBrr8353Q6h+RCd7WWWAAJkQmYMBlptVqsxvC+6/pdR6/oXlzW6zIA3kqtXn5ommaMMtDfU53xIYOM+x9T6q7w41K9RY/5yBGvewbV31+xEJyhtGSHwqoFblMFwMgpyeH7A+6+4tzSXP676b+NOldtgkB3FEuJS2HqEDXBTV2Bcz7c8SGH8w9XS8dnuz6jxF7Cor2q4NLviV446so9uySbD7d/SGZxJr8c/cXLua2xfesGVYegrV7Nz4dXkVWSxeJ9i702HThxgNXpqzGbzNzW/2ZY6x4+atGgS6ESDPUZj19YUchr618zfpdXlrNw90LAY9x0vLtV0zemL1az1St4SkF5AS/9+hIFlQVGlLav935NYUUhz618jsqVywH43xAosUJoYRns2kWls9KYGGdEF1XQnkqgltnLKKtUoXVralHpTm6g+oVf/uXlaqMOqgoCLwtBFVN/9K5dfL77cyOaYlV0s/D6ztA/YShnDb8GgPZZKsrll3u+pMhWxLpfPwPAHmTlkKWITVZVaW9Y+xk9tns7yurm5eSoZEwmE+cnqi6Gz3Z9Brm5WMrdVhzKvUd7VBTkUV7o7Rx3eN96/r7m7+40V9HlZ9mjmTZkmnFNHbPJzKBYZfnyfK8/26XyoqdrYq+JXJoR4nXOysMHKaooYtear4gthXIL/HeI63306AKYu3YuIw+pBsqbw8BhNhFoU9aIPb3aw7BhXsfo7+TQ+KFe6dLfm+SoZLoXWozzdzphJ9ASyNV9r6Z3dG+GdxkOwB1D7iCuXRwPj1STzegTZL2z9Z1qfkAFFQWGg+XwzsON9R1DOzKwLNz4HVPqrvDNP3v7n3Q96SQxMpGuEV2NdVXfX7EQnKHoFoKW6FDYXIJA/+P2aN+DBVcvABrfx38qC8GQ+CGMShhFj/Y9ag2cU1RRZHgRv73lba/KUi9U9Huhd6PohaOu3H85+otRGR3OP+w1/E2vNBvbZZC5Qw3t2hUDpUFmOHmSos2qYqgaA0F3nhzXfRxdDmQr60JUFHRXHm29igPZm7e3XvMr6F0PfaL78J8r/wO4hZYx5NBl/YAqw/tcef1k5ydGX25Vp72dOTsJW6fOs2NQLL/o5eHq1eSW5qKhYTaZObuzmvnrVO+jfi8CzAGEB4ZX2/78xc9T/mQ5qXeray7Zt4Rlh5Z57VOnINCFmWsmsuidO9niug+f3fQZpU+Uen3+234KACMmPczG324kotdAAGLzysEjvG35QeU4lhsTCibYH6acRg9tXcFofQLEXnHqi6s1qb9/tw++HVAiK22rqhiPh0F2ZEC1/FcWnMRelO+1bvnGT8kpzSG2XSx59x3j3IwArzy+MfgJ/nTRn7yuCUr8ndv1XMD9X9uSuYUtWVsItARy04CbjGMetKt++M1d1bmL9u9ka9ZWQzAd7deFZS6HS71yzy7J5pt9S4z8z3zmWyzD3DPArU22uB0wV62ioLzAmPdiSoq67/o7mF6oTpIUnkCnfLcFNCZXNQ4+v+lzdt+3m1BrKAD3j7ifzD9kMrSTEhZX9r6S9sHtOVZ0zIhVoePZXeA5hDE5Kpku+W7xEG6DoVH9vPK4vrP6mVignqPJw2R8xlgIXnvtNZKTkwkODmbEiBGsW1f35Bsvv/wyffr0ISQkhISEBB566CHKqyjbM4lDrtF8+uiiFoHLealqC8yngqC4WGX+0CG2HVVmxaGdhnLjgBsJDwzncP5hVqWtqvc59E+hR39vbmkulc5K5SiWpxzFUuJTMJlMhsrXzemeeOb7wMkDhpeww+kwwq7qQkC/J4YgcCl3z4olLT+N3DLVyosOiWZQ7CBMmMgoynBPB1tS4u1AVVlp5Ck0K0t9z88HYP1a5el9sD1sSFAtnOBf1TCmUnupUeE6NSdvb1WC4I6UO9zm11Gj1Bhw4Lp2qlA1zJ9FRe77Wenty6F3sdwx5A7D+3pL5hYvh0IvJ6yyMoZ2VCZsfbtnN43eN68Xoh2LoVuWal3FXXItq/U5PVatMvaJD+lIjyDlbGO8jxkZKr2Z3s5cJ7IOk3wShpZFYTp8uNq7wqFDBBWXkRKfwjmdz8GhOcg9vIvkkzAusC9oKo1ZJVlYHGB21iIIbrwRLSCAkLw8OqTuIa4IhncZTog1xOsTsEb5Q1jHXojJZCKql2pRh9g1nLk5huldc1kejrVXzzbN1RUddSiTC1yvZsDtqpJLzNdAg27hSWC3MyhuEEPjh2J32nnlk4eN49cku1vCRapHB0dRAZVF7jk4AMoz1ZDdyYMmE7F1N5bKSrT4eLjwQrWDbhWx2+kW7p50JSUuRZnANTi2fS0cOsQnq9Ssnlf2vpIOIR1UN9e+fZjWqaA/ay9Qtb7zsJp5UvcFKDt3GGsSwGkCDhyADRv4+rt/MDzNQZciwGol8LzRbgEAfNupxP17xQr2bPiW8HLlyHdRt4sA9a4WVhQaYj/ZFkpgpfs/F56lnq3JZPKqjKsSFBDEzQNvBqp3Oxrvabs4urVzt/CTo5KJyvaOIDk0IEH97zep8u89l79lYoFbxOhEh0aDBl0KIPkkdMzyKPvqGXLa1zRYEHz00UfMnDmT2bNns2nTJlJSUhg/fjzZ2dk17v/+++/z+OOPM3v2bHbt2sW8efP46KOPeOKJJ0478S2Ryko47rIAJiT4Ny0GTz4J7dvDr78aBa7eL+kzH4KsLOjSRbVSu3fn+ilzQFOty1BrqNGaqNMDvso56N4da58+/PL3XNq5rKQaGjklOWzP3o5Tc9IxtCOdwlRlcnuKakn9dOgnrz5QqC589Mpy/4n9xkiLvLI8iiqKjBgEVS0EGzI2uM9X4G0hCA8Kp0eHHoCr1ZKRAfHxcPPNroRrMGKEkadL7r4ba58+0LEjzo0bSNumhFJaJCzrqszTcZvd/ZD6tValreJw/mEigiK4pu81bvPr6NGQpLo4LglQQ5Y+3PEhFfv3qHTo9/S88wyRcvDkQValr8KEidsG3+YV7W9r1lYjXoXRZVBaCt2783/Pus23eveFjl546sLgPJd5elssXHL2JFa56htt9Wpj3/ffKaP/0EuJLVbDO8v/9Iz7PejUCd58Ux104AADBl3MoVdg3Zxcr/fE6xMbC9u3c8eQO7h1K2T/HQ69Akuf2M2fflTRLe15uRx+Gb5/p5Yug3790M5Srd5V85xkvghdPvoGL7KyYO9e5Sh0nuoHj4tO5Ljyy+Po1p+NyJrBR1VeD4QrQaY7H163G3qcBEwmOk+fCaiWZlQ5/GbeRvW/3bvXELtOV196WhQs66LOnRMKW13GhYDSCmzFyifC6ar/YlxOg3cMucMYSqmNGmW8L6Snq4o9MZFbn/jAyJ7ukb9gISx48Cfo3p1nJ73BmEMuE/vSpdCxI/TurYYaxsZSdsFIld+MbFIzU43Wf/jFl1EYDNviXVXOOefwm+v/xM/zXRcbNgxCQw0BUGmC7zsWUTTMVaPu28fw0TeT9Xe4jF6GY2BBRQErDqtuqnBLOOGZ3t0lwcfqnlHSE/0ef77rc6+JzfT39L3Xsxl+4WQiXG3ZpMgkzFW6+rrYgmDdOqispCA2klWuW9yz2FptKGF0SDR/+QGOvqTez9C+A93v8B/+UO90+5IGC4K5c+cyffp0pk2bRv/+/XnjjTcIDQ1l/vz5Ne6/Zs0aRo0axa233kpycjKXXnopt9xyyymtCq2V48eVv5TVqsqlFsGyZUqprFplVIxjk8YC1SvK/Sf2e1WmOSU51cOm1sSGDVBYiOZS4Ylp+fTNdVcm+p/tk52f1B44Z/164xyVIUE4Q0PQTCY6F8OFxwKMGcIyizO9Wq+68k+OSmZs8lg0NJ5Z/gwf7/jYCL6k51MXDx/v/Jgye1k1E39aQVp1HwKXIPDs7jicf7hagBwvP4Jff1XWjmUuq0J+vtFqqAwOwhZkRQsIgMpKjrz1DyKyVCGe2cFqVJq9d7oLM/1auvXjpv43ERIQ7LYQjB5tTKnZvchK14iu5Jfns2PB31RFbrEY9/jrpa/x8Y6PeXbFs4Dqeuga0ZVAS6AxmkCfpCUhIkG1BEFFk8vMpMvWQwTb1ZS5z618DsBwOtSHgeqFaP8T6ro7O1sZlTiKnXGuWurIEbIKlHPnsAOlmPLzue6Iqkm1j1zjwQNcpu1PPlHLRYuwVNioNEFZoFlVIFU/FgvY7fDll9w88GZu3qmuZ3eVdDftgK3ZW7noEHQtgosPgfOYewIaw0KQmIjznnsoCQ2iwnXrTJ96z3tv3PuBA1XFDYQHhnM0Sl3z0NYVxq49D6nnuyFKvfs7BsayO1r5VFSGBMFdd0F8PKXt1T3odhIGLN2iWptffMGtg24lwBxgeNAfiYSP+msUDerN30ZBWYi6V2E2jC4DW2wMAB1L1Ls5OG4wpr2q60JLSXFPwZqerir3zEw6rU4lxvX3HBI/hMHR/blezWGEw2LGosHt+0OY0HMCfPqpEpdWK4SHwwMP0HWQCtnbrqiCvC2/0OMkaGYTXcbfSKAlkH+c7cQREY4jJJgSq8q/s30U/O536iKXXAJjxvDm+cGUBkJaQDFFt0+iMiQIh9lESCVcsy9AdV25HAPf2KisFh0DOxrPb0dHdTpLbp4aB14PhncZTt+YvpRVlhmxGUC9y8F2GLYrn4Ajx7g4Xb0QyVHJ1Xx/THl5xpjzwn7dDeEXU2A3hrPqRIdGM8kVc6ksAO/3ODCwXmn2NQ0SBDabjY0bNzJu3Dj3Ccxmxo0bx1oPxyZPzjvvPDZu3GgIgIMHD7JkyRImTpxY63UqKiooLCz0+gDY7XaffprinIdcDjJdumg4HL49d2PzpLleWsdhd8jN8xOUQ1BmcSaFpYXY7XaOnDzCWW+exXnzzqPCVoHdbufmT29m6JtDWX14dZ3XPbxFDZv6cVAYlRcolT86DQZED8ButzOi0wi6R3Wn2FbMx9s/rvnepaoCdGEfDetjFcTODuXIlWMAuOx4OPHtVMjRowVH2ZShKtdBHQd5neO2gcqp7H+p/2PSp5M4b955lFeUc+iEEgY39LuBpMgkCisK+XTHp9XmQDiQd8DoOugS1gW73U5UUFS1d/TwSbeFIDIwErvdzqCOqjWzKWMTDr3fKC8Pe34+FftVaz87FKyPVxA8q5IDzz4EQOlP3xtjmSu6xPNrV3AGWOh00k5CvlqfVZiFzWbjs53KoWvywMnYd+yAnBy0oCDsgwdT2dVlzkxP59aBKsxw0TLVqs37wwx+7aoqqo/e+D2TPp1k+CLcNug24/4NjlWWI32UwqBY9/2t9DDfJxSoESv6OSYPmGy8T0VlRUare5zLWlHRNR6T00RwXBcATE4neccOEOCAsFL1n7nkWBAR5RC8QxWolV+poW/a2rXYS0txrlDvx1MXwc0LrsCen1/t4/i7cp5zrlxJuLkdY4+qivKVJ8ehmUz0OgHH9202Wq4A8Zv3qTwWFBiTAdk7d8Y2aRKT/jaO4dPVftqaNdjLyoz74Vip3nnHqFHue1RZSXaM6qfO3rXBlVc4L11ZZX7q6sBisnD7uIfp93tI+lM0FSfysL/2Gna7nYBkZWW6ci9YS8qMvEQFRnFZz8sMQXCiYxg5YfDOfx7g76PAHqIqkDAbBNmUcNVcJsqYUrhtoHrGetdFZZcu2DurDm4tLc3IC7iDAvWP7k/I1t2E2eFEMNx6rTrvhOPtwAma65jKDz7AnpeH/ZFH6J10NvkqIi9Df1KTYpX37wNhEQzoOIB5w+Dzn+fx8Gd3E/Yk3P72tTiysrHffLO6h4GB2Jcu5b+3qr76Xdm76DdkNdbHKnjyQnUPB+3J93pXl+xTAZHiAuMMC8rWOCgOdInBgwfrVXZWVla6y4/N/zPWZxRm0NWjF2ZipvJdSQhPMO6nLjgrjx83/vumxGRyQ6FUza2E/dAhr+tFZ5eSXKCsIWP+2s/7Xf7Xv3xeR9WH6l4pdZCbm4vD4SAuLs5rfVxcHLt31zx+89ZbbyU3N5fzzz8fTdOorKzknnvuqbPLYM6cOTz77LPV1n///feEhoY2JMmnZOnSpT493+rVnYFzCAk5wZIltUTlamI882S227nS1YeRvW4dabHqBc7flU+wOZhyZznvLnqXLsFdWJi9kCJbEUW2It778j06WDvwS/ovODUnzyx6ht8n/r7Wa1Ys/5RkYEdwEaURxVwJXHQkgC2rtrDVpCwMw4OHc5CDvPzTy3Q40qHaOWwrPqMbkNM+hHYWM3lleSwIOsDTwPADNv5bppT50jVLWXFCVQ7acY0lS5YY54hwRnBRh4vItmWzu2Q3mSWZzF84n1+Oq77esowyRgSPIK0gjZd+fMkYjqTz2arPjNb4nl/3cMRyhCMnqg+XzC3LNSq9beu2kReUh61AmXB/PvAzh9aU0NO178p336U8bRuXA0eioKO1Izn2HF4o+4U3geTdmUS4nLTzwtpRGgiHukbR43Aeo9Ph/Sj4Yc0PZG7NpMim+ixzt+SyfdmnDAXyevbk52XLiMnIYBRQsmMHSSeuAg167lDP/l+2bQQnaow4Clcdb0/qKFUxxwXGEXoolCVp6h5a81TppZv82xW1M+5v59WrOceVpzsDxvFumKtvNTCewRWqcD5acJQPv/4QgABTAL2zVEUVE3sWS5YsIYQI8oMgqgL2bVhjmLMBztlv47wkMGkaxfHxLCsv57KwMAKLi1nz2muM+OkngoFVSRCaW+r13I3nr2lcCDhWrWLVG29wUbGdcquZdn2uJKvrNuKPZDFwb4HXOPfYdXtZsmQJYUePcjFgDwlhyc8/g8nEobJD7I6FspBAQoqL+flf/6Kgp3qyYxYvJgrY3K4dxzzSkhcRCJRQvHsHdIGB2dC+HMqDAkiNr6RDQAeSspO4JPoShoYP5YfvfnDfg6AQOgN372gHqKZ65YoVfPP110wwT2BA8U9AMTmRoUAx3278FoDSAFUjhdkg1FX25wQFkwj0sEVQlNWVJUuWMG7PHtoB6zIzKQwK4nJUq7Zs4ULC9Geb25vQ+LPZtHITPb78koHAtm7hZPfvDOyh86E8fpg3j0tcZf7S0lJsrvxXapVYoiAqCyalKqGXkdSd7UuW0MGm/vMfr/6YpXmqjOpn61fjcwwuV2MQn//+eY4VHSPEHMKxvvGw7BAdN+xhyeLFDK0YyvaI7VQ4K7CarFwdezVH319Fd8AS342iEwWEZZxg/aefklPPIV/xtnhMmFh9ZDXzv5hPfFA8G9M3kpTv3ueStBAu7jAM264KHIcOEQAcjm9Hr4wSdq5cSft9+0gA8k3hXBV7FUXRqwjNPMm6Tz4hd/Bg4zwdln9HH2BzJyjTLDXeB/BNHVXqOQNmXWgN4NixYxqgrVmzxmv9I488og0fPrzGY3766SctLi5O+89//qNt3bpV+/zzz7WEhATtueeeq/U65eXlWkFBgfE5cuSIBmi5ubmazWbzyaekpERbuHChVlJS4rNz2mw27a9/rdRA0yZNcvj0vI3O065d+rxoWvmg/hrPoFmetWil5aVa/3+q34t3L9YqKiq0Aa8N0HgGjWfQlh9YrmUVZBm/w/8cruWX5Nd43aLSIu2TFKumgfbwJWjX3xmuaaBlxAR77bcne4/GM2imZ0zagdwD1c7xqescux67S3vqh6c0nkHrPUOl3WY1a9M+vFXjGbTnfnpOC/tzmMYzaJuObqr1fpz95tkaz6C9v+V9bdibwzSeQft0+6fazsydGs+gmZ81a5FzIjWeQRv+7+Eaz6BNeGeCxjNoHf7awTjPol2LjPvQ+9XexjH6J7MgU7PZbNrB3IPG/bVdc7Vx3+1ff62te3yKpoH209D22lc7vtJ4Bq39nCitLLKdsZ8G2pP/m6LSeVmypoH2+jB1jdd+eU3bfGyzV9oct92maaBVPv64SuuOHZoGmrNdO81WUaFd/ecUtT3AovX/W7J21c3qGs6+fWu9Z9/v/d4rbx9u/dDYVvnKK+48/fvfXscdzTtqHLNs/zKNZ9AS5yZqzoEDjXtgs9m0yZ9O1vZ2UOf443MXagPvxSv/b56lliW33KTyOHGiysP06a73wKIFPoX28LcP15yHsjLNGRHhdYxj7FjNZrNpWVNv0jTQ/jsUzW5yX3NvQjvNZrNp9sWL1f3p31+z2WxacXGxFvpcqMYzaPkXnqfO+eKL6jp5eZrTbFZpOnjQKw3/nqLy/Fl/k8YzaPdOVNfZOCBa4xm00fNG13r/K++/3+t+6B/bxo2azWbTnLGxmgba3c+d6/W+fnZxF00D7bkL0FYnqGMqZ85U+enWTZ2/vFxzWtV/rHT3bs1WUaE5w8OrXctx9tlGehxXq/e48vnn1fV79PC6tzW9SysGRXidz/7++5rNZtPm/jxX4xm06L+q+9Dxbx21krKay98Hljzg9R4+uORBzVZUpDmDgtT92LatxrLPfsUVKn3/+IfmmDBBXf/NNxtUjo57a5zGM2hP/fCUZrPZtPFvj9emXeXOjzMgQLPl52u2zEz3PZui/t+VTz6pOUaNUtd97z11D8eNU7//+1+v69h/+1tNA23uuWjXfnBt/crzRn5yc3M1QCsoKKizjm9Ql0FMTAwWi4WsLG9HjaysLOLj42s85o9//CO33347d911F4MGDeLaa6/lz3/+M3PmzMFZS1jQoKAgIiIivD4AVqvVp5+mOGdGhmrFJiWZfX7uRuXpuHuMs/mI6itNiEwgJCiEbu27AXC0+Cjb87azI8c9Yc2xkmMcKzlm/C6yFfH1/q9rvOZ3h74j/oRqlqRFwXexRThM0Cm3HGtWlrFf7469uSDpAjQ0Ptz5odc5vj30rXGOXmddwrSz1LjovdEuM7vdyTnHVWt+3fF1FNuKCbIEMTB+YK33Qh9StD1nu+E82TO6J/3i+nF+4vk4NScFFQWYMHFlnysBjNEHSZFJxnniwt0WsSGdhpAUleS+pyYzHcM7YrVaSe6QbEz+YzvsDmEckJFB6UFlBnckdOWSHpcQbY3mZEU+y7p49CtaLET3VN0On0ar1rfeks235XOiQjlMxYfFY7VaMbscxCxjxqi0uoYdmkpKsBYX8/sK5b+xsTPsLD1Mak8VDMe0ezfWgoIa79mwrsPwZFiXYcY2i0cI3ICjR72O6xjWkQCTMjjuyFXvUVxYHCbXELqA7t2xWq1079CdXJeRz5mT7WUhALhjq3rGa7tZVB7HqC4jy1vKCfRAr2hsARAbFlvzcw8OxuRy8NOPMV9wAVarleCxFwNw21YI0OBkqCr+ehwtwVpSQkCG8mkwJSdjtVo5VnqMUmcpgZZA2l2sAtdY1qxR19m4EZPTCcnJWLt180pDZYKyvnTNVybusUfUdb6JV9adbh261Zx2qxVLt27eN6SDalVbf/kFa2UlJpfztilJ9f/rI220MHVTPS0E+rlMubnq/CdOYLLb0cxmAhITsQYGYkpKqnYt8+bNWCsqsAYEuN+xsWOxWq2YXE5/+r01ue6tV/67dvbKQoDr2LM6KydN3QI3edBkQoNDa7wP3Tt09zrHtLOmYQ0LwzRcxQGw/vprjWWf5ehRI+9mV94Cjh1rUBl6x5A7AHhv+3tYAixklWZ5RT80VVZi3bTJXbbGxhrXspw8aQQh0t95s2v0T9V0BLju7apE6NiuY/3K89OsG05FgwRBYGAgw4YNY5nuKAU4nU6WLVvGyJEjazymtLQUs9n7MhaXg5PnWPAzBdf7iN6d63c8Aq1Y8wtpV+H2nted5g7nH64Woetw/uFqIxBqGyHw1pa3jD9MZZdOFAfBZl0fesYxxzVUznWM5/N/a8tbhlnOkpxM9/bdGZ04GkwYQ9UG7VU76OOEB8YOxGqp/UXXnfzWHF1jTF+sV+a6kyNAzw49GRirxo/rJnnP8dieAUOGxA3x2tY+uD1mk3q/PSf/0StCANLSMLmeQ0iPPljMFsa2H6vy0tVjGGDXriR2UIX4D52UK/PAHOhQqkYZGMOfwuLVKIaDB8FsNjzcCQ4GvTsvLY3z09T9Xd5VjZW++OyboL9yGvSMHe9Jh5AORrS/sMAwurf3KJg9h0JVdaYymYgKiALcwxG7m6PdIWldDmxJkUmGIDDl5VUTBPqQsX8Gu5w99WFnrsmetvRS/bd1jtmucoz+O2ycqtSDXEPHNw6JZV8HV4S9tWvd/xVXWnWH0/4x/QkYM1Zt08Poeo7uqII5KVnltQDQ4MKj6h39oatKT3Jkcu1p1x39QDmW3X23+r56tRGfgHbtCI9X+xlOwWHK4B9e4RYExiiCoiLl0OZ6ZmUdOrgdNj2vd/XV6rfDoZxi9+xRzzw42B0gqJZ760lQt17G94KuHY347Z4BrgCj4q0Jz/kHdIdIr+tVKVcMPJxCjbzVNuVyLVzb71rCA8M5lH+I1elqNIynIADU8/B8X2KUAyeZmXDsmHu959IzHSdPGrM4rk5sGUGJoBGjDGbOnMl//vMf3nrrLXbt2sW9995LSUkJ06apFt2UKVOYNWuWsf+VV17J66+/zocffsihQ4dYunQpf/zjH7nyyisNYXAmof9n/THk8O2tb7Mmv8oMbVUK7sQC959Nr9g+3fmpMbZdj+TlOee3HjTmh4M/MHXhVGYsmWFMH5tdks3SPUvUWGLgvFFqmJ3neHNPbuh/A6HWUPbm7WXSp5OY9uU07lh4B0t3LzFigut/IL3S1ofudNuhWnD6MEHPuOA1oW/XYx9EBUcRFRwFwI39byQ4QPVTpsSneFXynvcGvCuflPgUrwK9pnj6QXYIzfPwQkpPN0LZxvRVraQLO6gx4Ks8ymMSE43r5rZTQYoARqWrVpWXINDv6+DB4LKg6efQrxm0Vjny6s9iaspU98Q6tRWouO/b4LjBhthRifIIiVs1siLQ3qo87fWKtF+pskgQHQ3t1PfkqGRDEJCT6xYEHi2YrHaw0LGT7dnbVUUUHGxs+yVJVWR1FqB6HkGNOjhXBdcxd00grYO7zMk5q4/7/q9axdHtyjH60+J1TPtyGn9b8zfjPnD22aqC1mPx6/fP81ouArspH4O4EuiTCx1PVmA3w6/KcFDtXfPCs4IePhx0B+5Vq9wVSlIS8eGqktXj4JvD1DvgaSGgUyf36JLcXOP4so4da77e6NHeFa6ex+HDISio5vzWkP8OfYca320j3ZH9ooKjjLynxKVUEwieeN4jvRFhpBFqnDXRUlaG6YRr2GFSkvewygbgOUz6/m/uJ7sk2z1hkutdYtUqb/GhC4KtW5WgslrVcF99e9V0uAR5WlwwOWEtIygRNEIQTJo0ib///e88/fTTDBkyhNTUVL799lvD0TA9PZ3jHmbqp556iocffpinnnqK/v37c+eddzJ+/Hje1McWn2H4y0KwNWsrd319Fy8efpFiW7F7Qw2CQP+z6UPM9p3YR355Pp3DOzP9LOVS7Tn87sLkC7kw+UI0NN7e8javrX+NZ5Y/A0VFvL/1PWILHCqed2Ag1429F4vJwqaerlJ/+XLYudOI0x8eFG7MR/7Jzk9YkLqAt7a8RXyBU72MQUHGeE09oNEvSapQi03dZ8zMBlSbtawqeqtCn9jGaHU4HETazUY6zut6Xp2CICIogvbB7QkwB3BWp7O8xUJwB9VS37kTDh7k3C4jSCj0OhUVB/fSOU+V0gmDVAHaNbgrI7uOZHMnsAe7hhglJXmdW6+srtgLofsO49yxnX7ZkJJrBd0BqWoLTS8Ef/zRGP70c6KKGDk6aXTdLSxNg6IiRiWooWOjEkapGfl0S46nIPAIZsOuXbBzJ93KVKW/PVu1fHoUuyp5j0qnT0wfQxBEFVcS73IU5aKLjH0ODOgEJli0Z5GqhEe44sebTCx3jb2vswAdPtw9bGvoUKP1DLC1d6Tx3THqPEMsact+IGuT8pr/vHQTC1IXsOG4GiUwvPNwJUpc5mo++US1oKHGFnL7Lj3Qs36vaxDLxk5Qpj9mjy6nania8M8/X1VAAQGqYFnsCmGdmKhEoQeWcJUvL0HQrp27osrNNcqCUk9B4Hm90aPdFfwPP8C331bPY69e7vHUXbt6H+8iYeAo43vEuMu9tp2XoKxZvxn6m5pyb9C9fXdCraEEBwRzy6Bb3BtGjlRxHw4ccAd8AcjIIGaHq8szMlKJ5JoqYocDdu9W/9faPnv2MG2waoxsydqCU3OSXOhyPp6sRtOwdi1s3qy+JyWpeAyghuaCahHqlvGahInr/3dgoOpeqVMkNiMNGmWgM2PGDGbMmFHjtuXLl3tfICCA2bNnM3v27MZcqlVht/svKJFu8nfgYHv2dkZ3c/2Jq5jLkjwEwWW9LuOta94iszgTEybG9xxvRPw6nH+YkADl+p4clczvh/+eT3Z+wtHCo7z0y0tkf/EuXPVvKm/oTKIeQTYhgR4xvVh+x3LCT5bCe+NVZTFggKrod+6E7t158dIXOavTWUb0PYCk1MPA615/pIigCH647QeW/fQd2nt/xVpYTP8c2OGyip9KEIQHhdOjfQ9jYhbjT/fII/CPf/CvFUu59NpLubH/jQQFBBERFGEEJPE0WZpMJr6Z/A1FtiLiw+K9CvS7l+XD9B7G7xv++hci+v8BcMeOL9u1nXiXRgvp0Qe9vH73mnfZkLmBgDX/VvEKEhPpENKBdtZ2lNhLWJUEv92kPmxaAaxAhSvx6LqpWiHpheCrr6rlgAH87zd/pm9MX9Xa1/fftEmNcXe13AF46SV4+GEe+uZrEq57j6vzYlWFMnMm/P3v3oLgyBEVcGPCBPjxR6zAEhOcMx02d1Z+EQmuPnRPQdA1oitXjpoGa/7H+MiziIpLAr5QloCdO+HIEcrOHQZ8bYRQZvRoWLECUlJI09T7XKeFIDgYzjlHtcKq3J+DAzrDLyfIDYHYs8ewas0cAEzr1qN7T0y4aDpDhvTE4XBwdP9Rpgye4k7H6tXw1FPqd3Q09O1LVeLDO5EeCf1z4QHXxI6rPOrNOgv/mBgICVFj50ePVmPShw1TAuSVV4z7WVUQBERGAUoQtHNZ8wkNVefLylIfvcugJgtBfDz06OEeK+/ZpeR5D00m9fuzz9Syhuh/+gRPAEEXjvPaNvfSuVzd52pu6H9D7fcA9d/94fYfsJgtRvwRQFX2KSlqwpjVq+HGG+Gbb7BOnMi5VfPkKQgqK5Wwuukm+Lx6aPOqjLrjDj595FNVdjiddJszG7DBxInq+RcUwP/+576OLryqpqFqOhwOZbVxWTiG3vB73j2vI1f1ueqUaWoOGiUIhJo5fhwjTofnf66pqXRW8t6294zfW7K2uAWB55Sr27d7dRmYTeZq4TT1QD5p+WmGST0pMomEyARmjpyJU3Py2a7POG+POu/YnzPYfp4FcBhK+PzE8yERFWxl4UJ3H+Y338B99xEdGs39I+73zsTBd4DXq7U4hsYP5XjkcbQBAzCtW0evE25BYPQr1sGQ+CHegkDT4P33obKSsK++5ba//MXYNzkq2QjCVLXQ9pzlzHPb6A2ufnVXIW5e+CUT7roLgOPJMXQ6nEvUCTV8zGY1E9ixoxE+OCEige7R3eGRSNUSnzwZk8lEclQyO3J28HVvyO6bgCn9CBazBbPJjN1hJzwonGBLkCrAJ0zwzvBNN6l7XlioCsAHH/QubBITVcvu6FFVyXi0zHlXxR4I/Gklt/71r/C3v3n3l3v6ENhsygLxo/Ln0IKDsZSXc9Ue2OzyKYvPc1UuVZ5p377nA/9joDkebK7ugJgYeOIJePttLLfeBt9+7Z5Q5ze/gW+/xfngg+TvV9EoT2lifeQRFZDpt7/1Wr1/7GBW/LCdL/vA3VHJHIsN5sMB5VybocTgvlgLU+58FYKDsdvtLDm5xPgfcPvtqiI8cUKJ1pkza6wQ48PimXs2PLEKgk0BRMYn8/7QQ4ADs8nsNcFNNUwmePhhVeHpz+aBB1TkOptNWTtuuYW4dt4FTGCkuh+RFWpKY0CJvT59YMcO1ZqtqcvgsstUxT5pkrp2v35w3XWgxyXo3x9cjp0GDz6ouk3ur/If1unSBaZOVfeoZ0+vTXFhcYY5/lSMTKjZL40hQ9T90adRdlWulYGBWNq3x6Q/865dlaWgsFCZ8vv3h6+/Vtuio2ueitbpVM934UKunzdP5SEzEypmqe8JCerdevVVtW/HjnDlldUDCXm+84mJ6rkVFyu/gd69VRA2oP0lVzK5Rw9aCg3uMhBqR/cf6NrVbS1qDr7b/53X/O5bs1Wlhqa55yUfpcx4Sfl1t1C6RnTFbDJT4agw5hqvOgPalMFTDAfAoZlwVaXrT++pigH+8x9ViTz5pPpdR791VYeuarhMLj1cM5l1i+pGZHBkzft64GlFSI5KViY9fZRMlfR45rMus66+X7sKSDro6rPUTfgbNhim+uBRY6j0KHOK4jvUXAiNH69a7C6HP/38+SFwcOnHxD4Kvf8YxZi/9SP2UVi5/lN1X3/5RUWI82TECGVOzclRCtUlTgz0Fl7V/BcWumfl0p+FvkxPV++SbiHQHdI+cIW57d0bpysgkGfAnw45LqeQqs/U04ytnzMmBu65B9asod+AsQDsy9unolp26wbr13Pi2vFGn7kRPbE2rr5a3dMqLfiw+ETGToOXzlOVU1RIe265Ef6+8FFiH4X7Hh/k5bPgRb9+bke7rCx47LEad4trF8c/zoVOj8D9b9+Cad8+Cnqpe9AlvIsR1bFW/u//YNEidzpuuUU5quXkqDj3Y8dWsxAERSlB0NEzCGhoqLfPiN5l4Nmabd9eVf733ad+m81K9OTkqM+KFdXvx/nnq3fl3HOpEZMJFiyA+fObZv73qk56ruWeW26h8sgR0K3XFovb4Xb1avXfdIVYNvJX9ZOVpYRUfr4SUuBuVHXurFp7Tz6p9svJUVatHj3qthAEBKiuDj0d69Ypc3KnTsaEZC0FEQQ+RPcfaO7uAj2cre7sZoTjzclR06OaTBScpcJ8JhZSZwvFarEa242+9yqV45SUKYbXrUWD8Zu8Pcmr4VkB1TayxNNBpwY01/o+paoboy6HJE88HQ+TIpO8nZHWr/cKa6pbTiKDIg3nw5poH9yesMAwRhwDi1NTD3zMGFVg2O0qpCvQvt9Qsju4C38toX6OJbog6BDSwUjTyfKTZBQpp8qqlUGDqckxa80a1eIB97PQl5mZquLWPcsHueLLv/eecT6nS3CeexQCXC3UsEzXMMXaBIFeCHuuQ1XU8WHxaGjGxFPgns8hIiiiztEldaHfuyBLkNdz1v8zvujLDbGGEBGknPyqOvDW6T/QANqHtMdqdt+DkPaq1R/rKQiCg93P+uefwRXFr6w5zZdNQVXfgJp8I3Q8yx7PMN+1CZWqlbfndWor30BZCD2736ru29B0+AkRBD7E00LQXJwsO8lXe74C4E8XqilMt2VvU3N6eyjbo3FK5XcrtJyyMPUsFGNCYwgLDPPa3iu6Fz2L3edod8QV0rYGByNAOWNZrWqonKtQqoae1trO4RGnH7yn5K2LahYCz1ax3W6Y7oztnLpS0M36+tSuxh9b/9MfPKiWSUmGkAEI6VG9v7kmPCsRva/cqTmNoZNx7eJqPbZe6K3GtWvdsx963peqggDcDlShoe5Wt57P0aOhXz/KwkJoZ4ehLj+aoAyXJabqM9ULbk8LQZXC3HNeiD25e+j3Wj/Om69ae6fjka0LgviweDVU0iUI9O6JOocENuI6VYWAr5zHzCYzcWHu96Bde/U90KXpnCHBqrU/ZIi7xeuaWbPVC4KqTno1+UboeFpI6hgq6kVVC1p9BAF4WwmqvvM1paOGERr+RgSBD/GHhSA1MxWbw0b39t25vu/1BJmDKKssY/+J/V4v8oEw5crWqcBRbQrcqng61Hl+N6ioUJN1VKW2P4zuGAW1dxucostAj8veyyUILu1xac3nqULXiK4M6DiAjqEd6RPTx319vfDwSM/5iedjwqTiH5yCCxIvcJvH9QKk6h88MZGYPu4hWKH1FASe6Qi0BHoJMrPJTExoTB1H14MBA5SpuKTEXdF7WgsyMpQ1wNMh1TUxEzEx1Z/R+eeD2UxWH9V1NDodoiztMGe4lEFtFoKiInf3TRWTqy74tmRuYe7auezO3W04vOpDYxvDsM7DCDAHcG5XZe7Wu532n1De4b5qwY/sOhKLyWL0g1+QeAFAvd6t+uJpKQrr4G01MoW6WqueLV5Ai4qi0sfh35sdzy6Dykqj4K1REOgjTrKy1AROcOqK2LPy1jSv4Z514vkOV33nR4xwN4pcfjenFCZ+QASBD/GHhUAfGtirQy8sZgtJweqlTc1M9apkdwfkYzdDgBPv4To14NmKqbFFoyufqtSloOsYP+zp63AqQZBQANl/yK7d4agKJpOJDb/dwP7796vYAPv3q9b8739fLT3Duwwn+5FsXrnslVOe9x+XzOWiLFfBqhcgNXj8B3V3B2kxnapAcTEyYSTZj2Tz0viXAO8WcWy7WCzm04zfYTaDy8TP6tXK4VMfRgfqeezc6Q4qBLULAo9+0Pz+qivh/HRIqYxRXRCBgdWn/YyMdI+P1yddqSII9C6hX479wkc71OyH71z7Dtvv3c57171HY+nZoSeZD2ca59AtBLpvgq9a8POvnk/WH7KMob3Thk4j+w/Z/HbYb09xZP3xFATh0Z28tpk8K33P97LFzMl+Guh5KClR/fwOB5rVSnlUVPV99REnoN61sDA1SqEuPId6pqc3zkJQ9T57NorsduXsqHe9tSBEEPgQfwQl0gWBXpB1C1GR7lIzU73M8IeK0jmix685RaCOUwoC/fjevZW3rk5dGa9r/PuJE8ojvK5zuCpTU3Y2Hc1hNe9TC8EBwapPV6/8Bw+GK65Q39esUUOBXMSExngH46kFc+oWTKWlqqWtR/8bOFBVdqAq3S5dvFsV9RQEejr0aZ09h9idtv+Ajufz2LBBiYLYWOUgBdUjGXoKgqpj113pLByoCtrz02FgmcvZMTGxuoet2ez93gQHe/e/4t1lUFBRQFJkErcOupUBsQNOWxBFh0Yb56g6k6UvTfpVh0Z2bOdbU71n11F4+yrvhacg8GgRa6eq1FoDISFukan/p+vy5Pa0CJx3ntsptjaqWjQbKgg6dvS+/zWlY9QotyhuQYgg8CH+CEqkhxdOikyCwkL6mFQ4tC1ZW7xe5LSCNGNublauhI0ba/0MOmLjrAwVMtcoICsr3Q5guuUhOdn9ksfGqj9qbejevrt3K89lz2vqpry4uNo9vNu3d1canmGB6+LECe/ruKbT5fzzlSgID3cPSdLJyXG3WkFtL/Hw1KqoUJWjy3FQN5cD3l7NukdyTeORG4inheC0/Qd09Oe2erXyKtfXueKuVxNuB9TQzWoWAo9CLr97d8qsJjqWwhWbXQKvtjx7tqZiYqo5V/Xq0MuIgwFw++Db6yXUGkpV59Eau8haKLo4jAiKwGIN9K6EPL/rLV7OEEEA7vfK9Z7WmS9PC0l9++31/RYtcvs9nUrQ610Wp3Kubkg6mhkRBD7C4XB3h3buXPe+p0t2STZOTXkP6RaCHoHxBAwZwtPPL8LigE3HN1G6b6c6IDGRw/mH3YLgiSdUKNZaPudcfS8b/w0HXoGeuCqjRx5RwUt+/tlbMesv+akKmuho1XcNMHas9zVvueXU5zCZao48VhuFhSqqmud13nlHbRs9uvqQJFDD+OLj1fhy/Rx9+qg+WN0D//LLVevhhRfc5/Kk6v3wzFMjlWKTWAjOPluJr5wcFZAIVCFVpaCt5gXdsWP1cLcuNKuVXd2VZWDCUpfDYW3P1LO/t+qQLcBitjAozm1SrRovw1d4CoKIoIg6R5e0NPR3wUizR0TGauJAb/GeCV0GUP09rStfo0a53+P69tvr+338sWpYeF6zNvT3uLb99G66hqSjmRFB4CNyc1WdYTZX7zL1JZ/t/Iy4v8fx19V/BdyCYMDBYkxHjxJ9PIfB2VB4MgvrDjVf+e64ANLy01gwBGw9u6s/Tx0fLSGBcouas77vrmzVp/zRRyqDn33m7QB4++0q3vpDD5068Y8+qsaU13Tdbt3gd7+r+/iGxCZfvVr9kQMDva9zwQUq2hhU78b47DOVx48+cgfkycyEbdvU+POCArdDUNeucNZZbjGjM22aCijz4IPqd9++KtDLAw+448E3EE8Lgc8EQWAgzJqlnmFCghIIN9/sLsxcM/9V6+eMiVHdIjNnqoBBHvO7A3x3RX/2dYCC2EglpqZOrfn6VS0ENaA7Fp6XcB69onvVuM/p4hnLIjkq2eimaQ3UWxCAiq43ejTOSZOaJ3FNjV4WuN5TrS5BEBWl3vVJk7wr5bq45BK4+GJ3uXH33e7uwNq49lrlPHjnnTVvj46Gxx9X/7NaJgP0NxKp0EdkukbedezYtF1DL/2iWnOL9i7ikVGPcLRQ9VMkbj1s7PMkF/DRyZ1YnbkcDYdn09+irLKM5d1NMH8XnCIwignYNHEQ532zncStaWp4me6IuGqV+oOBqjxiY90m/1MxZYr6NJaGzF6mV/KTJ6sAKTVRNT6CZ0Q+zwls9H0SE9V+PXq4Y5ZXJT5ehSHW0QO9nAZNIggAnn5afTypabiUZ5eKXnm/+GKNp+x/5+NM7vM8/7v6f0TGDqj92p4ioJZhcHeffTebMzcz5+I5tZ/nNPG0CLSUePL15aJuF3FewnlMHuSKr+8pCKr4ZHDFFepjtyuB29qp0grXTmXOf/75hp0/NFTN59AQ+vdXVsa6mNN077IvEEHgI3RBEOejLt6a2H9iPz8fUc5eW7O2cqTgCA7NoYamrUs19rs2tyPXp9wPPM2qJPhox8cAdA7vfOooaS7Ou+VR+GYK5p/XwGAPr9zNm92zeDXASc4nNKTLoD5jffUhSZmZKqToxo3ex3uOiFi92m2WbOb+vybpMqiNqubO886D1193B5SqpTWvM7HnRK7ud/Wpr1MPC8FZnc5i3fR1pz7XaeApCFqT/wCowFU//8bD+bMuC8GZRtX3NCHB2/dHaBTSZeAjdEEQ34Tl9dtb3ja+l9hLWHZItUR7hCVi8lCmpp9/NirELb0iGjekSm89b9wI333nXu9wVJ/vu7mob5dBebk74FBdfXXBwcpUDqrF6xmfYelSr6BFDQps4mOaxKmwNqqKvJ49vZ1iTiEI6s0pfAiai9ZsIahGWxIEVd7TOrsMhHojgsBH6A6FTSUInJrTEAQWk+qTWLh7IQAX53eA0lK0qCgcAQGYMjPhp58AiL30WuMcDSrwkpJUP3llpZruFbzNHyZT88/xXN8ug/XrVWCduLhqk6tUQ6/c9TC8eh4/+0ydIzpamf0PH1ZDFD2PaSaa1UJQ9ZkmJnoLP19FuauHhaA5OKMEgee8Fme6IKjJQiCcNiIIfERTWQiOFBzhhZ9fYOZ3M0krSCMiKIKbB94MwA8HVR+XHjFPGzWKfL0CrKyEqCguvWamca4GFXieoXj1lrPuKAcqIE3VGb6aGr0Q0KferQ3P7oJTOYlVzeMDD6hj9N8XXaTCv4KyjsTGqtELzUiT+RDURHCwWxQFBqrvnoWvryrvevgQNAdnlCBoSxYCfZpo/XtVnwmhUYgg8BFN5UPw4HcP8ugPj/LKryp63k39b2JkV+WhWuFQ08um7FOzymnnn88JPUgOwKhRDIwfzLBOashRrw4NrMg8W8IDB8JVVabRbW66dFGtdZsNsrNr389z8pBTcd553qLh8su9I5mNHl19/HAze6J3CldR6MICw5pnWJxujk1IUPfb0zzb4RSzDNaXFmQhCDAHYDaZ6RbVzW/p8AltSRB4DkM+U2IrtABEEPiIprIQ/HpUhZS9vs+1vNDhZuYEXc6oTCsWV3A9kxOStqtAPdr555PnKQhczm/vXvcu/3fh/3HLoCpD5E6Fp/OcawIbI8KcP/6EVqu7P/vrr9Wc6FVnT3Q43FH26iMI2rdXYgfU6ImBA72PqyoI/DB+uGtEV16//HXeufad5hkWV7Wg1ZdRUeoZ+IIWIgiCA4J565q3+N/V/6N9SHu/pcMntCVBACIImgAZZeAjmkIQ5JbmcqxIOfC9e3gYwbOeAj4kBvi/8+GJcdAnD4LziyEkBG3oUE6kpaGZTJg0zai8+sb05akLnmp4AvRJcE6edLeMR41SEf/89SdMTFQhIadPV7/nz1dj/3W2bVMBhcLCqo2Rr5XRo9Vxo0apFvH558M//uGON97JI068nyKM3XP2Pc13sdoEgS9N+57n8vPse7cOutWv1/cZIgiE00QEgY9oCqfCLZlqjvYe7XsQvFp9JyICCgu5fr+VJ8bZ3VPwjhgBgYHYw8Jw/vGPWI4eVSFLTwezGf78Z+Vxf7VrKNmTT6pQvnqF3Nw8+KAKEJSbq276okXegkDvLqhPzHKdBx5QIZUff1z9vvJKuPFGGDNGBZWIi1P5zslRwYjOdKZMUaNL7rpL/b7oIpgwwbvL6HQJDVXBrIqKmnasbluirQmCO+9U8UBqC34lNBgRBD6gosId3dKXZZs+R/uQ+CFuz/oXXoC776Z3pp32pTDmiAnQvEzZzqeewuIr0+4996iPzvDhDQ/Y4UtuvFF91qxRLfrVq1W3gW5Kb8zQwN69vYMJhYSokKWe/OlPp5fu1kRKCixf7v7drh18843vrzN3ru/P2ZapKzDRmcjIke73VGIQ+ATxIfABun+b1aos7L4iNSsVcAkCfez9sGEqJCww6giMOeIKi9hCJ8toMoYNU6GAc3Jg7161zjPaYFu7H4LQloYdCk2CCAIf4DnCoLYZOBuD3mUwtH1/90WSkozW7y3boOuJSnXRFhobu8kIClLdJOAWAYcOqRDLVqt7myC0Fdpal4Hgc0QQ+ICmcCisqKxgV+4uAM6qdDldhYQoL39dEOx0Pb6hQ71bB20FvVtA9xvQhcHZZ9c9FbMgnImIIBBOExEEPqApHAp35uyk0llJ++D2xJ+wqZWJiaqv3GUON+nBedqqeVzPty4EpLtAaMuIIBBOExEEPqApghJ5OhSajqg4A0aAmG7dvOPLt9C5tZuc885T3SUHD6ppUBsSkEgQzjREEAiniQgCH9AUXQZbspT/gNcIA328rWdYYWi7LeKICHdUwVdfhT171Pf6znkuCGcSIgiE00QEgQ9oCkGgWwhS4lLcIww8A3DoIqBXr7Y9jlu/D3/9q1oOGOC78LqC0JqIiHB/bwvDDgWfI3EIfEBTCIJ9J/YB0K9jP0h/V630jCl/++2wciXcfLPvLtoa+d3vYMsWFZ0wIAAee8zfKRIE/xAeDn/4g7IgtkUnY+G0EUHgA3ztQ1BRWUFGUQaAmnClapcBQGRk9eA5bZG+fWHFCn+nQhBaBi+84O8UCK0YEQSnQVqasub72kJwpFA5EYZaQ4kJiXZ3GXhaCARBEATBh4ggaCSHD6uAgTabe52vLASH8w8Dan52U06Oio1sMqnpfwVBEAShCRBB0EgWLVJiICJCTYZ32WXePj2ngy4IkiKT3NaBTp0gMNA3FxAEQRCEKoggaCTffquWTz4Jjz7q23On5SufgeSoZOkuEARBEJoFGXbYCCoq3JNsjR/v+/MfLjgMuARBTQ6FgiAIguBjxELQCFavhtJS5UQ4eLDvz384/zCRZTAyNQ9WuoLtiCAQBEEQmpBGWQhee+01kpOTCQ4OZsSIEaxbt67WfceOHYvJZKr2ufzyyxudaH+jdxeMH698/XzN4fzDfPQpjJ7xN/jyS7VSugwEQRCEJqTBguCjjz5i5syZzJ49m02bNpGSksL48ePJzs6ucf/PP/+c48ePG5/t27djsVi48cYbTzvx/uK779RywgTfn9vmsJFRlEHvPNeKQYPgqqvghht8fzFBEARBcNHgLoO5c+cyffp0pk2bBsAbb7zB4sWLmT9/Po8//ni1/TtUCSP74YcfEhoaWqcgqKiooKKiwvhdWFgIgN1ux263NzTJNaKfp6Hny8iAbdusmEwaY8dW0pjkVFRW8Oef/8zEnhMZ0WWE17ZDJw/h1JxElbvS+e670K+fnug6z9vYPLVkJE+tA8lT6+BMzBOcmfnyZZ7qew6TpmlafU9qs9kIDQ3l008/5ZprrjHWT506lfz8fL7Uzdt1MGjQIEaOHMm///3vWvd55plnePbZZ6utf//99wn186Qdv/zSib/8ZTjduuXz0kuNi5C3Jn8Nfzv8N3qF9uKF3t6RxbYWbeXp/U9T+RxYNPhu/nzKJTa/IAiC0EhKS0u59dZbKSgoIKKO8fENshDk5ubicDiIqxKBJy4ujt27d5/y+HXr1rF9+3bmzZtX536zZs1i5syZxu/CwkISEhK49NJL68xMQ7Db7SxdupRLLrkEq9Va7+OyspTTQL9+EUycOLFR196xdgcchvSKdC6dcCkBZvdjyNmSQ9hOJQYALrr22npPVNLYPLVkJE+tA8lT6+BMzBOcmfnyZZ50K/upaNZRBvPmzWPQoEEMHz68zv2CgoIICgqqtt5qtfr8YTf0nEVFatm+vRmrtXGjNvXQxBWOCg4WHGRA7AD3tqIjRLq6CwgIwBoZ2WDPxaa4T/5G8tQ6kDy1Ds7EPMGZmS9f5Km+xzeoRouJicFisZCVleW1Pisri/hTBPIvKSnhww8/5M4772zIJVscBQVqGRXV+HOkFaQZ37dkbfHadrjgMJG6+0QjxIAgCIIgNIYGCYLAwECGDRvGsmXLjHVOp5Nly5YxcuTIOo/95JNPqKio4LbbbmtcSlsI+flqGRnZ+HPooYkBUjNTq20zLASncxFBEARBaAANtnnPnDmT//znP7z11lvs2rWLe++9l5KSEmPUwZQpU5g1a1a14+bNm8c111xDdHT06afaj+iCoLEWAk3TvARBNQtB/mFjhIEIAkEQBKG5aLAPwaRJk8jJyeHpp58mMzOTIUOG8O233xqOhunp6ZjN3jpjz549rF69mu+//943qfYjpysIcktzKassM357Wgh25+4mvSCd820mQBNBIAiCIDQbjXIqnDFjBjNmzKhx23I9yL8Hffr0oQGjG1s0pysIdOtAdEg0J8tPkl2STWZxJvFh8byV+hYA50UMBLadnqOCIAiCIDQAmdyogfhKEPSJ6UOf6D6AshI4nA7e2foOABdEuiZIEAuBIAiC0EzI5EYN5HRHGegjDJIik0iMTGRX7i62ZG7BYrJwrOgY7YPb06+ss9pZBIEgCILQTIggaCANsRA4NSc7c3Zid9gJtYbSO7q3YSFIjkomMiiSD7d/yMr0lfx67FcAbh54MwFLStQJRBAIgiAIzYQIggbgcIAe8Kk+dfWjSx/lxbUvGr//NfFfXoIgKVLNYLhk3xJjn6kpU+H9V+t/EUEQBEHwASIIGoBn9Mf61NWr01cDEB4YTpGtiNfWv2ZsS45KZnTiaC5MvpC9eXsBuKjbRQzvMtw30Y8EQRAEoQGIIGgAendBSAjUEFm5Gro14Mubv+Sy9y5jR84OTKjIg0mRSYRYQ/hx6o/VD9QFgVgIBEEQhGZCRhmcgtJSWLwYyssb5j9QZi8jq0SFeE6JT+GavtcAoKGGXyZGJtZ+sAgCQRAEoZkRQXAK/vIXuOIK+Oc/G2bJ10cThAeG0z64vfINcBEfFk+INaT2g0UQCIIgCM2MCIJT8PPParljR8MsBGn5ShAkRyVjMpm4pMclxIfFG+vqRASBIAiC0MyIIKgDTYMtrqkGjhxpmCDwHE0AEGAO4PbBtwPQq0Ov2g90Ohs2lEEQBEEQfIA4FdbBsWOQl6e+Hz3asJkOdUGgDy0EeHrM00QERXDroFtrP7CoSCkRkFEGgiAIQrMhgqAOUlPd348cgZMn1feG+BB4dg+EBYbx1AVP1X2g3l0QGAjBwfVOqyAIgiCcDtJlUAdbPGYmLi2Fw4fV98Z0GdQb8R8QBEEQ/IAIgjrwtBAAbN+uliIIBEEQhDMNEQR1oFsITCqWEDt3quWpBEF5ZTnHi48DkBSVVPfOVRFBIAiCIPgBEQS1UFQE+/er7yNGqGV5uVqeShAcKTgCQDtrO6JDoht2YQlbLAiCIPgBEQS1sG2bcvbv3BmGDvXedqq62rO7wKSbF+pLQ4YyCIIgCIKPEEFQC3p3wZAhkJDgve1UdbUx5LCh3QUgXQaCIAiCXxBBUAsbN6plSgp07eq9rd4Wgsjkhl9YBIEgCILgByQOQQ0cPQrvv6++jxlTfWbDUwmCmmIQ1BsRBIIgCIIfEAtBDTz5JJSVwejRcOml1bsMGuJD0GBEEAiCIAh+QARBFTZtgrffVt9ffFENOezSxb09KKjuAIKaprEzR41P7BVdx5wFtSGjDARBEAQ/IIKgCs8+q5a33grnnKO+BwdDx47q+ymHHBYe4WT5SQLMAfSL6dfwBMgoA0EQBMEPiCCowrZtannPPd7rdcfCUwmC1MxUAPp37E9QQFDdO9eEdBkIgiAIfkAEQRVyc9UyPt57ve5HcKp6ekumGq+YEpfSuASIIBAEQRD8gAgCDyoqVIRCgJgY7231thBkpQIwJH5IwxOgaQ2bUlEQBEEQfIQIAg9064DFUr2BnuSKMdShQ93nOC0LQUEBlJSo7507N/x4QRAEQWgkEofAA10QxMSAuYpUmjxZzX74+9/XfnxhRSEHTh4AICW+EYIgTcUvoGNHCA1t+PGCIAiC0EhEEHjgKQiq0qWLO1hRbWzN2qr2De9CTGgNJzkV6elqmZjY8GMFQRAE4TSQLgMPcnLUUh9i2FD07oJG+Q+ACAJBEATBb4gg8KAuC0F90IccNloQ6F0GSY2YFEkQBEEQTgMRBB7oFoLGCoItWac55FAsBIIgCIKfEEHggW4haEyXgaZp7MjZAcDguMGNS4AIAkEQBMFPiCDw4HS6DHJKcyi1l2LC1LhJjUC6DARBEAS/0ShB8Nprr5GcnExwcDAjRoxg3bp1de6fn5/PfffdR6dOnQgKCqJ3794sWbKkUQluSk7HqVCf4bBTeKfGhSy22eD4cfVdLASCIAhCM9PgYYcfffQRM2fO5I033mDEiBG8/PLLjB8/nj179hAbG1ttf5vNxiWXXEJsbCyffvopXbp0IS0tjagWGInvdCwEafmqdd9o68CxYypSoedMSoIgCILQTDRYEMydO5fp06czbdo0AN544w0WL17M/Pnzefzxx6vtP3/+fE6cOMGaNWuwWq0AJCcnn16qm4jTEQS6haDRgsDTf8Bkatw5BEEQBKGRNEgQ2Gw2Nm7cyKxZs4x1ZrOZcePGsXbt2hqP+eqrrxg5ciT33XcfX375JR07duTWW2/lsccew2Kx1HhMRUUFFRUVxu/CwkIA7HY7dru9IUmuFf08+lLTIDc3ADARFWWnoZc5eOIgAAnhCY1Ko+nAAQIAZ0ICjkbmsWqezgQkT60DyVPr4EzME5yZ+fJlnup7jgYJgtzcXBwOB3FxcV7r4+Li2L17d43HHDx4kB9//JHJkyezZMkS9u/fz+9+9zvsdjuzZ8+u8Zg5c+bw7LPPVlv//fffE+rjkL5Lly4FoKQkALv9cgDWr/+WrVudDTrPhoMbAChML2yUf0TvZcvoBxwBUk/Tv0LP05mE5Kl1IHlqHZyJeYIzM1++yFNpaWm99mvy0MVOp5PY2Fj+/e9/Y7FYGDZsGMeOHeOFF16oVRDMmjWLmTNnGr8LCwtJSEjg0ksvJSIiwifpstvtLF26lEsuuQSr1coBNQUB7dppXHvthAaf74l/PwHAFedfwSXdL2nw8ZZFiwDoOmoUnSdObPDxUD1PZwKSp9aB5Kl1cCbmCc7MfPkyT7qV/VQ0SBDExMRgsVjIysryWp+VlUV8fHyNx3Tq1Amr1erVPdCvXz8yMzOx2WwEBgZWOyYoKIigoOqe+lar1ecPWz9nfr763bGjqcHX0DSNtALlVNgzpmfj0njkCACWbt2wnGYem+I++RvJU+tA8tQ6OBPzBGdmvnyRp/oe36Bhh4GBgQwbNoxly5YZ65xOJ8uWLWPkyJE1HjNq1Cj279+P0+k2we/du5dOnTrVKAb8xek4FOaV5VFiV9MWJ0Y2csigBCUSBEEQ/EiD4xDMnDmT//znP7z11lvs2rWLe++9l5KSEmPUwZQpU7ycDu+9915OnDjBAw88wN69e1m8eDF//vOfue+++3yXCx9wOlEK9SGH8WHxBAcEN/wEmuYWBBKUSBAEQfADDfYhmDRpEjk5OTz99NNkZmYyZMgQvv32W8PRMD09HbPZrTMSEhL47rvveOihhxg8eDBdunThgQce4LHHHvNdLnzA6cxjcNpDDjMyoLQUzGbo2rVx5xAEQRCE06BRToUzZsxgxowZNW5bvnx5tXUjR47kl19+acylmpyMDAgM9HMMgp9/VsvBg6EG3wlBEARBaGqafJRBS8ZmM3POOQE4HKC7QDSqy8DlUJgU2Uhz/6pVajl6dOOOFwRBEITTpE0Lgry8YHJyVFTAxYvVOr9YCFavVksRBIIgCIKfaNOzHRYUVDfPn87ERo0SBAUFsGWL+n7++Q0/XhAEQRB8QJsWBCdPVh8R0FALgaZppycI1qxRowx69IBOnRp+vCAIgiD4gDYuCJSFICUFAlydJ126NOwc+eX5FNmKgEbGIJDuAkEQBKEF0KZ9CPLzlYVg5Eh4/nk14qBbt4adQ7cOxLaLJdTaiHkWxKFQEARBaAG0aUGgWwji4+Hyyxt3Dq/ugu++U6rCFaSpGoWFMHcunDzpXrdunVqK/4AgCILgR9q0INCdCmuZhqFeeA05vP12FeFo+HAYMKD6zv/7H9QwiyOdO0OvXo1PhCAIgiCcJm1aEOhOhacjCHQLQbfwRHe4w5UraxYE+/er5ejR7i4CkwmuvFItBUEQBMFPtGlBkJ9/+hYCXRD0tnqcZNUquPfe6junKWsCt94K99zT+IsKgiAIgo9ps6MMNM3tQ+CahqFR6IKguznavXLVKnWBqsiMhoIgCEILpc0KgoICqKy0AKcnCHQfggQi3CuPHnVX/p7IjIaCIAhCC6XNCoLMTLWMjNQICWncOfLL88kvzwegs7Od90Z9OKFOUZF7dEFCQuMuKAiCIAhNRJsVBFlZyonvtKwD+co6EBMaQ2ip3XtjVUGgWweioiAiAkEQBEFoSbRZp0LdQhAfrwH18/CvqKzA5rBhNplpF9jO6C5IjkpWfRCgQh5WVrojEOpId4EgCILQgmmzgiA7W4mA2Nj67f/ToZ+Y8N4EbA4bAA+OeJBu7VVYw6TIJMjOVzuOHg0//QQ7d0JeHkS7nA31EQbiUCgIgiC0QNpsl4G3heDUzP1lriEGAF7f8DpbMtUshV4Wgu7doX9/9d3TSiAjDARBEIQWTJsVBA3xIcgqzuKbfd8AsOWeLQzoOIAKRwXvbnsXqCIIIiPdYYhrEgTSZSAIgiC0QNqwIFDL+lgI3tv2Hg7NwfAuwxkcN5ipKVMBDItBNUGgRyH0dCyULgNBEAShBdNmBUFmZv0tBG9teQuAO1LuAOC2wbdhNrlvXVJkUs2CYONGKClR36XLQBAEQWjBtFlBkJ2tlnFxykKQUZTBntw9xvbCikI+3/U5r617ja1ZWwm0BDJp4CQAOoV3YnyP8ca+SVFJkJ+vfkRGqkq/a1c12mDdOrU8dsy1s3QZCIIgCC2PNikIHA5PQaCW498dz5A3h3DgxAEAfrvot1z/8fXM+GYGAFf1uYoOIR2Mc+jdBtEh0UQERbgtBFFRaqIiz26DjAx1Uav19CZOEARBEIQmok0Kgrw8cDhMmEwaHTuCpmnszNlJeWU5C1IXkFuay+e7PgdgdOJoJvaayDNjnvE6x3X9ruPhkQ/z8oSX1QrPLgNwOxauWuXuLujaFcxt8pYLgiAILZw2GYdAH3IYEWHDajVTbCvGqTkBeHvr28SExmB32hnWaRgrp62s8RxWi5W/X/p394qqgkC3EKxdCwcPqu/SXSAIgiC0UNpkczU0FKZMcXLuuRkAxnwEAOkF6Tyz4hkGZ8Kbq9tDYaHakJcHjz6qAg7VRFVBMGCA6j4oKYE5c9Q6cSgUBEEQWihtUhD07An//a+De+/dCngLAv3306tMDHvnB/jsM7Xy/ffhhRfgr3+tfsLKSvdoAl0QmM1w0UXq++7daqkHLBIEQRCEFkab7DKoii4IAswBVDorAehFNJALOTlqp6pLT3TrALgFAcA//wljx4LdDuHhcMstPk+7IAiCIPgCEQRAQYWq0IfED6HYVszu3N0kmNsDue7hhFWXXidwCYLQUDWSQKdTJ/j975so1YIgCILgO9pkl0FVdAtBVHAUi25ZxBeTvqB9pUsr6ZV91aUnVf0HBEEQBKGVIRYCvAVBzw496dmhJxTfrzaKIBAEoY3hcDiw2+3+Tka9sdvtBAQEUF5ejsPh8HdyfEJD8mS1WrFYLKd9TREEeAiCoCj3yuJitRRBIAhCG0HTNDIzM8mvqWu0BaNpGvHx8Rw5cgSTyeTv5PiEhuYpKiqK+Pj408q/CAK8LQQGtQmC4mIVddBTjXmGLRYEQWil6GIgNjaW0NDQVlO5Op1OiouLCQsLw3yGBH+rb540TaO0tJRsV/jdTp06NfqaIgioQRDYbGpkALgre0/LQGEhtG/v/u0ZtlgQBKEV4nA4DDEQHR3t7+Q0CKfTic1mIzg4+IwSBPXNU0hICADZ2dnExsY2uvvgzLhzp0l+eT43b4Nx760FTXNbB6DmroKq3QbSZSAIQitH9xkIDQ31c0qExqA/t9Px/RALAZBfdpLPF0GEbTE8csB76GBBgRIJnn1qVfvXRBAIgnCG0Fq6CQRvfPHcGmUheO2110hOTiY4OJgRI0awbt26WvddsGABJpPJ6xMcHNzoBDcJeXlE2Fzfc3O9LQSFhSoKoaeXp1gIBEEQhDOMBguCjz76iJkzZzJ79mw2bdpESkoK48ePNxwaaiIiIoLjx48bn7S0tNNKtK9pl5nn/lFQ4C0INA2OHvU+QASBIAiCcIbRYEEwd+5cpk+fzrRp0+jfvz9vvPEGoaGhzJ8/v9ZjTCYT8fHxxicuLu60Eu1rorKq+Ad4CgJwT1/suY8neheCOBUKgiAIjcBkMrFw4UK/pqFBPgQ2m42NGzcya9YsY53ZbGbcuHGsXbu21uOKi4tJSkrC6XRy1lln8ec//5kBAwbUun9FRQUVFRXG70LXjIN2u91nwTL089hsNqJzSoz1lXl5YLF43ZjKQ4e8fjtOnMDpkQ5Lfj5moLJdOzQ/BvPQ89SaAoqcCslT60Dy1DqoK092ux1N03A6nTidzuZO2mmhaRoAa9asYcyYMYwfP56vv/66Sa/ZvXt3HnjgAR544AGfnO/YsWO0b9/euPd6nvRnciqcTieapmG326uNMqjvO9wgQZCbm4vD4ajWwo+Li2O3PqNfFfr06cP8+fMZPHgwBQUF/P3vf+e8885jx44ddO3atcZj5syZw7PPPltt/ffff+9zD9ivv/uaLvnum73n118p79CBYR77HPjxR/p4/N67fj17lywxfl+UkUE48MuuXeT5IFrU6bJ06VJ/J8HnSJ5aB5Kn1kFNeQoICCA+Pp7i4mJsNlsNR7V8/v3vf/Pb3/6Wd999lz179pzWmPxT4XQ6KS8vNxqsNeFwODCZTPUaChkaGlqtMQxQVFRUr/TYbDbKyspYuXIllZWVXttKS0vrdY4mH2UwcuRIRo4cafw+77zz6NevH2+++Sb/93//V+Mxs2bNYubMmcbvwsJCEhISuPTSS4mIiPBJuux2O0uXLmXYqGFYnnKv79upEyQkeO3bKyjI63fvuDh6Tpxo/A5wORyOuPRSGDLEJ+lrDHqeLrnkEqyeIyVaMZKn1oHkqXVQV57Ky8s5cuQIYWFhhuO3pkE96xKfExoK9XWc1zSN48eP88UXX7Bu3TpOnDjB559/7mXNBli0aBF/+tOf2LZtG2FhYZx//vl8/vnngLJMz549mw8++IDs7GwSEhJ47LHHuPPOO6td76KLLuLIkSM88cQTPPHEE4Cq/BcsWMDMmTNZsGABTzzxBHv37mXv3r3k5OTw5JNPkpqait1uZ8iQIbz44oucddZZxjktFgufffYZ11xzDYcPH6ZHjx68/fbbzJs3j3Xr1tGrVy/+9a9/edWnnpSXlxMSEsIFF1xQzXG/LtHiSYMEQUxMDBaLhaysLK/1WVlZxMfH1+scVquVoUOHsn///lr3CQoKIqhKJawf6+s/ZrGjmEQPlwBLcTGUlXntYz5yxOu3pagIS9WhiYA1JsZ7yKKfaIr75G8kT60DyVProKY8ebZm9RZtSQn4qA3WYIqLoV27+u3rdDpZuHAhffv2pV+/ftx+++08+OCDPPHEE8ZwvMWLF3P99dfz5JNP8vbbb2Oz2ViyZImR1zvuuIO1a9fy6quvkpKSwqFDh8jNza2xdf/555+TkpLCb3/7W6ZPnw5g3LfS0lJeeOEF/vvf/xIdHU18fDyHDx/mjjvu4Oyzz0bTNF588UWuuOIK9u3bR3h4uHFe/Rz6Nf/0pz/x97//nT59+vDkk08yefJk9u/fT0BA9arbbDZjMplqfLb1fX8bJAgCAwMZNmwYy5Yt45prrgHUg1i2bBkzZsyo1zkcDgfbtm1jokcL258UVhTS29NHsKAAqppoqo6K8HQqtNncAkKcCgVBEPzCO++8w+TJkwGYMGECBQUFrFixgrFjxwLw/PPPc/PNN3t1R6ekpACwd+9ePv74Y5YuXcq4ceMA5SNQGx06dMBisRAeHl6tMWy32/nXv/5lnBuURcGTf//730RFRbFixQquuOKKWq8zY8YMLr/8csxmM88++ywDBgxg//799O3btx53pOE0uMtg5syZTJ06lbPPPpvhw4fz8ssvU1JSwrRp0wCYMmUKXbp0Yc6cOQA899xznHvuufTs2ZP8/HxeeOEF0tLSuOuuu3ybk0ZSWJBNfInHippGGejDDsPDlVjwFAT6CASr1X9SWhAEoQkIDa1eHDbntevLnj172LRpE19++SWg/CEmTZrEvHnzDEGQmppqtOarkpqaisViYcyYMaebbAIDAxk8eLDXuqysLJ566imWL19OdnY2DoeD0tJS0quOYKuCp/O97g+RnZ3dcgTBpEmTyMnJ4emnnyYzM5MhQ4bw7bffGo6G6enpXiaWkydPMn36dDIzM2nfvj3Dhg1jzZo19O/f33e5OA3saQe9V+TnV/8H6B6aSUmwfbu3IFi9Wi3POcd7wiNBEIRWjslUf7O9P5k/fz6VlZVejuqaphEUFMQ///lPIiMjjXj/NVHXtoYSEhJSLWrg1KlTycvL45VXXiEpKYmgoCBGjhx5SudNT1O/fs6mHAHSqEiFM2bMIC0tjYqKCn799VdGjBhhbFu+fDkLFiwwfr/00kvGvpmZmSxevJihQ4eedsJ9Rk0xBnRB4NG3AyhBoO+js2qVWp5/ftOkTxAEQaiVyspK3nnnHf70pz+xadMmUlNTSU1NZcuWLXTu3JkPPvgAgMGDB7Ns2bIazzFo0CCcTicrVqyo93UDAwNxeEawrYOff/6Z+++/n4kTJzJgwACCgoLIzc2t97WaizY/uZHlyDEAStoFqhWegqBLF++dExPd++joFoLRo5swlYIgCEJNfP3115w8eZLbbruNgQMHen2uv/565s2bB2CMIJg9eza7du1i27Zt/PWvfwUgOTmZqVOn8pvf/IaFCxdy6NAhli9fzscff1zrdZOTk1m5ciXHjh07ZeXeq1cv3nnnHXbt2sWvv/7K5MmTfWqV8BVtXhAEZWQCkNmto1pRH0GgRybMyoK9e9X3UaOaNqGCIAhCNebNm8fFF19MZA2h46+//no2bNjA1q1bGTt2LJ988glfffUVQ4YM4aKLLvKah+f111/nhhtu4He/+x19+/Zl+vTplJSUVDunznPPPWcMD+zYseMp03jy5EnOOussbr/9du6//35iY2Mbn+kmos3Pdhh2XM1jcKJnF3psP6YmM9JHGVQNnKQLgtJS5VegWwcGDoT27ZspxYIgCILOokWLcDqdNY61Hz58uBHxD+C6667juuuuq/E8wcHBzJ07l7lz59bruueeey5btmzxWnfHHXdwxx13VNt36NChrF+/3mvdDTfc4PXbM53Jyck4HA6vPEVFRXnt0xS0eQtBRHY+AEV9uqkVDodq+UN1QeAZsKiwULoLBEEQhDOGNi8IOuSo7gF7rx7uUQIZGWpZtcsgJsY9FqagwO1QKIJAEARBaOW0bUHgdNIxrxwAc3Kye/pifZhhVUEQGene59gx2LxZfZcRBoIgCEIrp00LgsCiIgIrVZ9McFIPd2WvU5Mg0KMRfvstOJ1qKGKVuQ8EQRAEobXRtgWBy2HjRDBEhsdUDz3sKQgsFtVdoIsGfWpN6S4QBEEQzgBEEAC5oRAVHFXdQtChA+izRkVFqbBd+j5bt6qldBcIgiAIZwBtWhCYC04AtQiCwED10ddVXeqIhUAQBEE4A2jTgsCUr6JL5bWDsMAw78o+LEwt6xIE0dHQr18zpFQQBEEQmpY2LQh0C0FhWCBmk7nhgmDUKNWNIAiCIAitnDYtCCyFJwEojnTFlPas7PWJjeoSBNJdIAiCIDQSk8nEwoUL/Z0MgzYtCKwup8LSKNf8np6jDGqzEHjuI4JAEAShxbB27VosFguXX365v5PSKmnTgiDYJQhs7atYA8AtCHQBoC/1fUJCoCVN4ywIgtDGmT9/Pr///e9ZuXIlGXrEWaHetG1BUKRmsqrsEKVW1CQI+vTxXvburZaXXqpGIQiCIJyhaJpGia3EL5+GTuRTXFzMxx9/zL333svll1/OggULqu2zaNEizjnnHIKDg4mJieHaa681tlVUVPDYY4+RkJBAUFAQPXv2NKZOrsoTTzzBiBEjqq1PSUnhueeeA2D9+vVccsklxMTEEBkZyZgxY9i0aVOD8tTctOnZDtsVlQLgjIlWK2oSBA89BBddBEOGqN8jRkBqKnTr1mzpFARB8Ael9lLC5oT55drFs4ppF9iu3vsvXLiQvn370qdPH2677TYefPBBZs2ahcnl+L148WKuvfZannzySd5++21sNhtLliwxjp8yZQpr167l1VdfJSUlhUOHDpGbm1vjtSZPnsycOXM4cOAAPXr0AGDHjh1s3bqVzz77DICioiKmTp3KP/7xDzRN48UXX2TixIns27ePcN1HrYXRpgVBWHEFAOYY17zUNQmCgAAYNsz7wJSUZkidIAiCUF/eeecdJk+eDMCECRMoKChgxYoVjB07FoDnn3+em2++mWeffdY4JsVVlu/du5ePP/6YpUuXMm7cOAC6d+9e67UGDBhASkoK77//Pn/84x8BeO+99xgxYgQ9e/YE4KKLLvI65t///jdRUVGsWLGCK664wjeZ9jFtVxCUlxNSUQmAJTZOratJEAiCILRRQq2hFM8q9tu168uePXvYtGkTX375JQABAQFMmjSJefPmGYIgNTWV6dOn13h8amoqFouFMWPG1PuakydPZv78+fzxj39E0zQ++OADZs6caWzPysriqaeeYvny5WRnZ+NwOCgtLSU9Pb3e12hu2q4gcJmCKk0QEh2v1nmOIGihJh1BEITmwmQyNchs7y/mz59PZWUlXbt2NdZpmkZQUBD//Oc/iYyMJCQkpNbj69pWG7fccguPPfYYmzZtoqysjCNHjjBp0iRj+9SpU8nLy+OVV14hKSmJoKAgRo4cic1ma/C1mou261ToEgS5oRAV0l6tEwuBIAhCq6KyspJ33nmHP/3pT2zatInU1FRSU1PZsmULnTt35oMPPgBg8ODBLFu2rMZzDBo0CKfTyYoVK+p93a5duzJmzBjee+893nvvPS655BJiY2ON7T///DP3338/EydOZMCAAQQFBdXqk9BSaLMWAlNeHgA57SAy2CUEgoPBagW7XQSBIAhCK+Drr7/m5MmT3HbbbSQkJGA2u9u5119/PfPmzeOee+5h9uzZXHzxxfTo0YObb76ZyspKlixZwmOPPUZycjJTp07lN7/5jeFUmJaWRnZ2NjfddFOt1548eTKzZ8/GZrPx0ksveW3r1asX77zzDmeffTaFhYU88sgjjbJENCdiIdAnNgLv2QxFEAiCILR45s2bx8UXX0xk1YnnUIJgw4YNbN26lbFjx/LJJ5/w1VdfMWTIEC666CLWrVtn7Pv6669zww038Lvf/Y6+ffsyffp0SkpK6rz2DTfcQF5eHqWlpVxzzTXV0nXy5EnOOussbr/9du6//34vC0JLpO1aCDwEQS9dEIASBLm5IggEQRBaAYsWLcLpdFLoCjTnyfDhw73iGVx33XVcd911NZ4nODiYuXPnMnfu3HpfOyoqivLy8hq3DR06lPXr13utu+GGG7x+NzTWQlMjFgJPCwHAgAFqqQcgEgRBEIQ2QJu1EDhysrFQgyD44AM4fhxcwSYEQRAEoS3QZi0ElTmZgBIEEUER7g2hoSIGBEEQhDZHmxUEzpxsAEoiQzCb2uxtEARBEASgDQsC3amwPEqcBwVBEAShzQoCS95JAOztqw9VEQRBEIS2RtsUBJqG9WQBAI4O7f2cGEEQBEHwP21TEBQVYal0qO8xMf5NiyAIgiC0ANqmIHD5D5RYISQy2s+JEQRBEAT/0zYFQU4O4IpBEBTl37QIgiAIPmHt2rVYLBYuv/zyJr9WcnIyL7/8sk/POXbsWB588EGfnrMhtE1BMGgQc+Zezy3Xe0xsJAiCILRq5s+fz+9//3tWrlxJRkaGv5PT6miUIHjttddITk4mODiYESNGeE0QURcffvghJpOp2iQQzU5oKFsTrKxNrBKlUBAEQWiVFBcX8/HHH3Pvvfdy+eWXs2DBgmr7LFq0iHPOOYfg4GBiYmK49tprjW0VFRU89thjJCQkEBQURM+ePZk3b16N1xo7dixpaWk89NBDmEwmTCaTsW316tWMHj2akJAQEhISuP/++70mSfrXv/5Fr169CA4OJi4uzpjf4I477mDFihW88sormEwmLBYL6enpPro79aPBguCjjz5i5syZzJ49m02bNpGSksL48ePJzs6u87jDhw/zhz/8gdGjRzc6sb6koFyNMogMEguBIAhCjWgalJT459PAiX8WLlxI37596dOnD7fddhvz58/3mjxo8eLFXHvttUycOJHNmzezbNkyhg8fbmyfMmUKH3zwAa+++iq7du3izTffJKyWSe4+//xzunbtynPPPcfx48c5fvw4AAcOHGDChAlcf/31bN26lY8++ojVq1czY8YMADZs2MD999/Pc889x549e/j222+54IILAHjllVcYOXIk06dP5/jx4xw7dowuXbo06B6cLg2ey2Du3LlMnz6dadOmAfDGG2+wePFi5s+fz+OPP17jMQ6Hg8mTJ/Pss8+yatUq8vPzTyvRviC/QqVBBIEgCEItlJb6b+bX4mJo167eu7/zzjtMnjwZgAkTJlBQUMCKFSsYO3YsAM8//zw333wzzz77rHFMSkoKAHv37uXjjz9m6dKljBs3DoDu3bvXeq0OHTpgsVgIDw8nPj7eWD9nzhwmT55s+AH06tWLV199lTFjxvD666+Tnp5Ou3btuOKKKwgPDycpKYmhQ4cCEBkZSWBgIKGhocTHx9c6g2NT0iBBYLPZ2LhxI7NmzTLWmc1mxo0bx9q1a2s97rnnniM2NpY777yTVatWnfI6FRUVVFRUGL/1m2K327Hb7Q1Jcq3kl+cDEBYQ5rNz+hs9H2dKfkDy1FqQPLUO6sqT3W5H0zScTidOp1OtdDr95mjmdDpBT8cp2L17N5s2bWLhwoU4nU7MZjM33XQT//3vf40WeGpqKnfeeac7bx5s2rQJi8XC6NGja9xeG/r90tmyZQtbt27lvffeq7bPgQMHuPjii0lKSqJ79+6MHz+e8ePHc+211xIaGlptf926UfUataEfY7fbsVgsXtvq+w43SBDk5ubicDiIi4vzWh8XF8fu3btrPGb16tXMmzeP1NTUel9nzpw5XipO5/vvv/e6cadDdoHq4tiduhv73jPnDw+wdOlSfyfB50ieWgeSp9ZBTXkKCAggPj6e4uJibDabWqlpcPRoM6fORWUl1LOF/MYbb1BZWUlCQoKxTtM0goKCeP7554mMjCQ4OJjy8vIaW9165VtYWIjVaq3XNZ1OZ7XzFRYWcscdd3D33XdX279jx45omsaPP/7I6tWr+fHHH3n66ad55pln+PHHH4mMjKSyshKbzeZ1zqKionqlx2azUVZWxsqVK6msrPTaVlpaWq9zNOn0x0VFRdx+++385z//IaYBAYBmzZrFzJkzjd+FhYUkJCRw6aWXEhERUceR9ad8RzkA48eMp3fH3j45p7+x2+0sXbqUSy65pN4vdUtH8tQ6kDy1DurKU3l5OUeOHCEsLIzg4GD3hsiW3a1aWVnJxx9/zJ/+9CeuuOIKLwe/6667jsWLF3PPPfeQkpLCmjVruPfee6udY8SIETidTjZv3mx0GZyK4OBgrFarV500bNgwDhw4wJAhQ+o89qqrruKqq67i+eefp0OHDqxfv57rrruOkJAQLBYLERERaJpGUVER4eHhXnmqjfLyckJCQrjgggu8nx/Uu+uhQYIgJiYGi8VCVlaW1/qsrCyvfhSdAwcOcPjwYa688kpjnW76CAgIYM+ePfSoYarhoKAggoKCqq23Wq0++WPaHXZK7MrrMyYs5oz5s+v46j61JCRPrQPJU+ugpjw5HA5MJhNmsxmzufWMSF+yZAknT57ktttuIyEhwSvt119/Pf/73//43e9+x+zZs7n44ovp2bMnN998M5WVlSxZsoTHHnuM7t27M3XqVO666y5effVVUlJSSEtLIzs7m5tuuqnG6yYnJ7Nq1SpuueUWgoKCiImJ4fHHH+fcc8/l/vvv56677qJdu3bs3LmTpUuX8s9//pOvv/6agwcPcsEFF9C+fXuWLFmC0+mkX79+mM1munXrxrp160hPTyc0NJSAgADjmZwKs9mMyWSq8dnW9/1t0FMPDAxk2LBhLFu2zFjndDpZtmwZI0eOrLZ/37592bZtG6mpqcbnqquu4sILLyQ1NdXLvNOcFFQUGN8jgnxjcRAEQRCan3nz5nHxxRcTWYMl4/rrr2fDhg1s3bqVsWPH8sknn/DVV18xZMgQLrroIq8h86+//jo33HADv/vd7+jbty/Tp0/3Gi5Yleeee47Dhw/To0cPOnbsCMDgwYNZsWIFe/fuZfTo0QwdOpSnn36azp07AxAVFcXnn3/ORRddRL9+/XjjjTf44IMPGDBgAAB/+MMfsFgs9O/fn7i4OI42c3dNg7sMZs6cydSpUzn77LMZPnw4L7/8MiUlJcaogylTptClSxfmzJlDcHAwAwcO9Do+KioKoNr65kR3KAwxhxBgbtJeE0EQBKEJWbRoUa0e+cOHD/caenjddddx3XXX1Xie4OBg5s6dy9y5c+t13XPPPZctW7ZUW3/OOefw/fff13jM+eefz/Lly2s9Z+/evQ0H/RY/ygBg0qRJ5OTk8PTTT5OZmcmQIUP49ttvDUfD9PT0Fm9u0gVBqMU3DoqCIAiC0NppVPN4xowZRqCFqtSlfoAao0c1Nz079OSLG79g/Yb1/k6KIAiCILQI2qS9PCo4ist7XY5p36k9NwVBEAShLdCybfuCIAiCIDQLIggEQRAEQRBBIAiCILhpSOheoeXgi+fWJn0IBEEQBG8CAwMxm81kZGTQsWNHAgMD6xUhryXgdDqx2WyUl5e3+FFu9aW+edI0DZvNRk5ODmazmcDAwEZfUwSBIAiCYETKO378OBkZGf5OToPQNI2ysjJCQkJajYg5FQ3NU2hoKImJiacliEQQCIIgCICyEiQmJlJZWYnD4fB3cuqN3W5n5cqVXHDBBWdMmOmG5MlisRhhjk8HEQSCIAiCQW3x8FsyFouFyspKY8KhMwF/5OnM6GwRBEEQBOG0EEEgCIIgCIIIAkEQBEEQWokPgT5blS9nfrLb7ZSWllJYWHjG9DlJnloHkqfWgeSp9XAm5suXedLrTs+ZH2uiVQiCoqIiABISEvycEkEQBEFonRQVFREZGVnrdpN2KsnQAnA6nWRkZBAeHu6zMaaFhYUkJCRw5MgRIiIifHJOfyN5ah1InloHkqfWw5mYL1/mSdM0ioqK6Ny5c51xClqFhcBsNtO1a9cmOXdERMQZ8wLpSJ5aB5Kn1oHkqfVwJubLV3mqyzKgI06FgiAIgiCIIBAEQRAEoQ0LgqCgIGbPnk1QUJC/k+IzJE+tA8lT60Dy1Ho4E/Pljzy1CqdCQRAEQRCaljZrIRAEQRAEwY0IAkEQBEEQRBAIgiAIgiCCQBAEQRAERBAIgiAIgkAbFQSvvfYaycnJBAcHM2LECNatW+fvJNWbOXPmcM455xAeHk5sbCzXXHMNe/bs8dpn7NixmEwmr88999zjpxSfmmeeeaZaevv27WtsLy8v57777iM6OpqwsDCuv/56srKy/Jji+pGcnFwtXyaTifvuuw9oHc9p5cqVXHnllXTu3BmTycTChQu9tmuaxtNPP02nTp0ICQlh3Lhx7Nu3z2ufEydOMHnyZCIiIoiKiuLOO++kuLi4GXPhTV15stvtPPbYYwwaNIh27drRuXNnpkyZQkZGhtc5anq2f/nLX5o5J25O9ZzuuOOOaumdMGGC1z6t6TkBNf63TCYTL7zwgrFPS3tO9Sm/61Pepaenc/nllxMaGkpsbCyPPPIIlZWVp52+NicIPvroI2bOnMns2bPZtGkTKSkpjB8/nuzsbH8nrV6sWLGC++67j19++YWlS5dit9u59NJLKSkp8dpv+vTpHD9+3Pj87W9/81OK68eAAQO80rt69Wpj20MPPcSiRYv45JNPWLFiBRkZGVx33XV+TG39WL9+vVeeli5dCsCNN95o7NPSn1NJSQkpKSm89tprNW7/29/+xquvvsobb7zBr7/+Srt27Rg/fjzl5eXGPpMnT2bHjh0sXbqUr7/+mpUrV/Lb3/62ubJQjbryVFpayqZNm/jjH//Ipk2b+Pzzz9mzZw9XXXVVtX2fe+45r2f3+9//vjmSXyOnek4AEyZM8ErvBx984LW9NT0nwCsvx48fZ/78+ZhMJq6//nqv/VrSc6pP+X2q8s7hcHD55Zdjs9lYs2YNb731FgsWLODpp58+/QRqbYzhw4dr9913n/Hb4XBonTt31ubMmePHVDWe7OxsDdBWrFhhrBszZoz2wAMP+C9RDWT27NlaSkpKjdvy8/M1q9WqffLJJ8a6Xbt2aYC2du3aZkqhb3jggQe0Hj16aE6nU9O01vecAO2LL74wfjudTi0+Pl574YUXjHX5+flaUFCQ9sEHH2iapmk7d+7UAG39+vXGPt98841mMpm0Y8eONVvaa6Nqnmpi3bp1GqClpaUZ65KSkrSXXnqpaRPXSGrK09SpU7Wrr7661mPOhOd09dVXaxdddJHXupb8nDStevldn/JuyZIlmtls1jIzM419Xn/9dS0iIkKrqKg4rfS0KQuBzWZj48aNjBs3zlhnNpsZN24ca9eu9WPKGk9BQQEAHTp08Fr/3nvvERMTw8CBA5k1axalpaX+SF692bdvH507d6Z79+5MnjyZ9PR0ADZu3Ijdbvd6Zn379iUxMbFVPTObzca7777Lb37zG68ZO1vbc/Lk0KFDZGZmej2byMhIRowYYTybtWvXEhUVxdlnn23sM27cOMxmM7/++muzp7kxFBQUYDKZiIqK8lr/l7/8hejoaIYOHcoLL7zgE5NtU7J8+XJiY2Pp06cP9957L3l5eca21v6csrKyWLx4MXfeeWe1bS35OVUtv+tT3q1du5ZBgwYRFxdn7DN+/HgKCwvZsWPHaaWnVcx26Ctyc3NxOBxeNxIgLi6O3bt3+ylVjcfpdPLggw8yatQoBg4caKy/9dZbSUpKonPnzmzdupXHHnuMPXv28Pnnn/sxtbUzYsQIFixYQJ8+fTh+/DjPPvsso0ePZvv27WRmZhIYGFitMI6LiyMzM9M/CW4ECxcuJD8/nzvuuMNY19qeU1X0+1/T/0nflpmZSWxsrNf2gIAAOnTo0CqeX3l5OY899hi33HKL14xz999/P2eddRYdOnRgzZo1zJo1i+PHjzN37lw/prZ2JkyYwHXXXUe3bt04cOAATzzxBJdddhlr167FYrG0+uf01ltvER4eXq0rsSU/p5rK7/qUd5mZmTX+5/Rtp0ObEgRnGvfddx/bt2/36m8HvPr9Bg0aRKdOnbj44os5cOAAPXr0aO5knpLLLrvM+D548GBGjBhBUlISH3/8MSEhIX5Mme+YN28el112GZ07dzbWtbbn1Naw2+3cdNNNaJrG66+/7rVt5syZxvfBgwcTGBjI3XffzZw5c1pkPP2bb77Z+D5o0CAGDx5Mjx49WL58ORdffLEfU+Yb5s+fz+TJkwkODvZa35KfU23ltz9pU10GMTExWCyWah6bWVlZxMfH+ylVjWPGjBl8/fXX/PTTT3Tt2rXOfUeMGAHA/v37myNpp01UVBS9e/dm//79xMfHY7PZyM/P99qnNT2ztLQ0fvjhB+66664692ttz0m//3X9n+Lj46s57FZWVnLixIkW/fx0MZCWlsbSpUtPOR/9iBEjqKys5PDhw82TwNOke/fuxMTEGO9aa31OAKtWrWLPnj2n/H9By3lOtZXf9Snv4uPja/zP6dtOhzYlCAIDAxk2bBjLli0z1jmdTpYtW8bIkSP9mLL6o2kaM2bM4IsvvuDHH3+kW7dupzwmNTUVgE6dOjVx6nxDcXExBw4coFOnTgwbNgyr1er1zPbs2UN6enqreWb/+9//iI2N5fLLL69zv9b2nLp160Z8fLzXsyksLOTXX381ns3IkSPJz89n48aNxj4//vgjTqfTEEAtDV0M7Nu3jx9++IHo6OhTHpOamorZbK5mdm+pHD16lLy8PONda43PSWfevHkMGzaMlJSUU+7r7+d0qvK7PuXdyJEj2bZtm5eA00Vr//79TzuBbYoPP/xQCwoK0hYsWKDt3LlT++1vf6tFRUV5eWy2ZO69914tMjJSW758uXb8+HHjU1paqmmapu3fv1977rnntA0bNmiHDh3SvvzyS6179+7aBRdc4OeU187DDz+sLV++XDt06JD2888/a+PGjdNiYmK07OxsTdM07Z577tESExO1H3/8UduwYYM2cuRIbeTIkX5Odf1wOBxaYmKi9thjj3mtby3PqaioSNu8efP/t2//LunEcRzHP99BTYl+0RGRJEEOQYsOtbkEkkvRJC5Kg0trNTS0RlMNDdHU0j/QViDcIv0Y4g63oLhoaQoUIaOC53foqyD6raBBo9cDblHueL95e597CZ/DcRyMMezs7OA4TmPH/fb2NgMDAxwfH1MqlVhcXGRiYoJarda4xvz8PLFYjMvLS4rFItFolEwm06mWPuzp5eWFhYUFwuEwrus23WP1HdxnZ2fs7u7iui63t7ccHR1hWRbZbLYre6pWq6ytrXF+fo7neRQKBeLxONFolOfn58Y1ftKc6iqVCqFQiP39/Zbzu3FOn63f8Pl69/b2xvT0NMlkEtd1OTk5wbIsNjY2vl3frwsEAHt7e4yPj+P3+5mZmeHi4qLTJX2ZMabtcXh4CMD9/T2JRIKhoSECgQCTk5Osr69TqVQ6W/gH0uk0o6Oj+P1+xsbGSKfT3NzcNL6v1WqsrKwwODhIKBRiaWmJh4eHDlb8daenpxhjuL6+bvr8p8zJtu22v7dcLge8v3q4ubnJyMgIgUCAubm5ll4fHx/JZDL09vbS19fH8vIy1Wq1A928+6gnz/P+e4/Ztg3A1dUVs7Oz9Pf309PTw9TUFFtbW00P127q6enpiWQyiWVZ+Hw+IpEI+Xy+5U/QT5pT3cHBAcFgkHK53HJ+N87ps/Ubvrbe3d3dkUqlCAaDDA8Ps7q6yuvr67fr+/OvSBEREfnFftUeAhEREWlPgUBEREQUCERERESBQERERIwCgYiIiBgFAhERETEKBCIiImIUCERERMQoEIiIiIhRIBARERGjQCAiIiLGmL+zpt174UiDaAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'Acc at best val acc: {err1.mean():.3f} +- {err1.std():.3f}')\n",
    "print(f'Acc at test: {err2.mean():.3f} +- {err1.std():.3f}')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=[6,4])\n",
    "plt.plot(acc['train'], 'b-', label='Acc train')\n",
    "plt.plot(acc['val'], 'g-', label='Acc val')\n",
    "plt.plot(acc['test'], 'r-', label='Acc test')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=[6,4])\n",
    "plt.plot(loss['train'], 'b-', label='Loss train')\n",
    "plt.plot(loss['val'], 'g-', label='Loss val')\n",
    "plt.plot(loss['test'], 'r-', label='Loss test')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training paramerters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 200-0.005-0.0001-0.25: 0.765 (0.863)\n",
      "-1: 200-0.005-0.0005-0.25: 0.843 (0.863)\n",
      "-1: 200-0.01-0.001-0.25: 0.824 (0.882)\n",
      "-1: 200-0.005-0.001-0.25: 0.804 (0.882)\n",
      "-1: 200-0.005-0.001-0.5: 0.784 (0.843)\n",
      "-1: 200-0.001-0.001-0.25: 0.608 (0.647)\n",
      "-1: 200-0.05-0.005-0.25: 0.824 (0.882)\n",
      "-1: 200-0.01-0.005-0.25: 0.804 (0.882)\n",
      "-1: 200-0.005-0.005-0.25: 0.804 (0.843)\n",
      "-1: 200-0.001-0.005-0.25: 0.784 (0.804)\n",
      "-1: 200-0.05-0.01-0.25: 0.824 (0.882)\n",
      "-1: 200-0.005-0.01-0.25: 0.765 (0.843)\n",
      "-1: 200-0.05-0.01-0.5: 0.745 (0.863)\n",
      "-1: 200-0.05-0.005-0.5: 0.863 (0.882)\n",
      "-1: 200-0.01-0.005-0.5: 0.784 (0.843)\n",
      "-1: 200-0.05-0.01-0: 0.784 (0.843)\n",
      "-1: 200-0.05-0.005-0: 0.824 (0.824)\n",
      "-1: 200-0.01-0.005-0: 0.765 (0.804)\n",
      "-1: 500-0.005-0.001-0.5: 0.784 (0.843)\n",
      "-2: 200-0.005-0.0001-0.25: 0.804 (0.902)\n",
      "-2: 200-0.005-0.0005-0.25: 0.902 (0.922)\n",
      "-2: 200-0.01-0.001-0.25: 0.784 (0.941)\n",
      "-2: 200-0.005-0.001-0.25: 0.902 (0.941)\n",
      "-2: 200-0.005-0.001-0.5: 0.804 (0.882)\n",
      "-2: 200-0.001-0.001-0.25: 0.784 (0.784)\n",
      "-2: 200-0.05-0.005-0.25: 0.843 (0.941)\n",
      "-2: 200-0.01-0.005-0.25: 0.882 (0.922)\n",
      "-2: 200-0.005-0.005-0.25: 0.843 (0.922)\n",
      "-2: 200-0.001-0.005-0.25: 0.902 (0.922)\n",
      "-2: 200-0.05-0.01-0.25: 0.882 (0.941)\n",
      "-2: 200-0.005-0.01-0.25: 0.882 (0.941)\n",
      "-2: 200-0.05-0.01-0.5: 0.863 (0.941)\n",
      "-2: 200-0.05-0.005-0.5: 0.804 (0.922)\n",
      "-2: 200-0.01-0.005-0.5: 0.843 (0.902)\n",
      "-2: 200-0.05-0.01-0: 0.882 (0.922)\n",
      "-2: 200-0.05-0.005-0: 0.902 (0.902)\n",
      "-2: 200-0.01-0.005-0: 0.902 (0.902)\n",
      "-2: 500-0.005-0.001-0.5: 0.804 (0.902)\n",
      "-3: 200-0.005-0.0001-0.25: 0.902 (0.902)\n",
      "-3: 200-0.005-0.0005-0.25: 0.863 (0.941)\n",
      "-3: 200-0.01-0.001-0.25: 0.863 (0.902)\n",
      "-3: 200-0.005-0.001-0.25: 0.882 (0.941)\n",
      "-3: 200-0.005-0.001-0.5: 0.745 (0.804)\n",
      "-3: 200-0.001-0.001-0.25: 0.490 (0.510)\n",
      "-3: 200-0.05-0.005-0.25: 0.882 (0.922)\n",
      "-3: 200-0.01-0.005-0.25: 0.843 (0.922)\n",
      "-3: 200-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-3: 200-0.001-0.005-0.25: 0.863 (0.863)\n",
      "-3: 200-0.05-0.01-0.25: 0.902 (0.922)\n",
      "-3: 200-0.005-0.01-0.25: 0.863 (0.922)\n",
      "-3: 200-0.05-0.01-0.5: 0.784 (0.882)\n",
      "-3: 200-0.05-0.005-0.5: 0.784 (0.902)\n",
      "-3: 200-0.01-0.005-0.5: 0.902 (0.902)\n",
      "-3: 200-0.05-0.01-0: 0.882 (0.902)\n",
      "-3: 200-0.05-0.005-0: 0.824 (0.922)\n",
      "-3: 200-0.01-0.005-0: 0.843 (0.902)\n",
      "-3: 500-0.005-0.001-0.5: 0.784 (0.863)\n",
      "-4: 200-0.005-0.0001-0.25: 0.863 (0.882)\n",
      "-4: 200-0.005-0.0005-0.25: 0.922 (0.961)\n",
      "-4: 200-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-4: 200-0.005-0.001-0.25: 0.922 (0.941)\n",
      "-4: 200-0.005-0.001-0.5: 0.863 (0.882)\n",
      "-4: 200-0.001-0.001-0.25: 0.804 (0.824)\n",
      "-4: 200-0.05-0.005-0.25: 0.863 (0.882)\n",
      "-4: 200-0.01-0.005-0.25: 0.922 (0.961)\n",
      "-4: 200-0.005-0.005-0.25: 0.882 (0.941)\n",
      "-4: 200-0.001-0.005-0.25: 0.922 (0.922)\n",
      "-4: 200-0.05-0.01-0.25: 0.863 (0.941)\n",
      "-4: 200-0.005-0.01-0.25: 0.922 (0.922)\n",
      "-4: 200-0.05-0.01-0.5: 0.824 (0.882)\n",
      "-4: 200-0.05-0.005-0.5: 0.765 (0.882)\n",
      "-4: 200-0.01-0.005-0.5: 0.902 (0.902)\n",
      "-4: 200-0.05-0.01-0: 0.804 (0.961)\n",
      "-4: 200-0.05-0.005-0: 0.843 (0.922)\n",
      "-4: 200-0.01-0.005-0: 0.902 (0.922)\n",
      "-4: 500-0.005-0.001-0.5: 0.843 (0.863)\n",
      "-5: 200-0.005-0.0001-0.25: 0.863 (0.902)\n",
      "-5: 200-0.005-0.0005-0.25: 0.824 (0.902)\n",
      "-5: 200-0.01-0.001-0.25: 0.882 (0.961)\n",
      "-5: 200-0.005-0.001-0.25: 0.902 (0.922)\n",
      "-5: 200-0.005-0.001-0.5: 0.824 (0.882)\n",
      "-5: 200-0.001-0.001-0.25: 0.725 (0.765)\n",
      "-5: 200-0.05-0.005-0.25: 0.863 (0.922)\n",
      "-5: 200-0.01-0.005-0.25: 0.863 (0.941)\n",
      "-5: 200-0.005-0.005-0.25: 0.843 (0.902)\n",
      "-5: 200-0.001-0.005-0.25: 0.804 (0.843)\n",
      "-5: 200-0.05-0.01-0.25: 0.745 (0.941)\n",
      "-5: 200-0.005-0.01-0.25: 0.882 (0.922)\n",
      "-5: 200-0.05-0.01-0.5: 0.882 (0.922)\n",
      "-5: 200-0.05-0.005-0.5: 0.843 (0.941)\n",
      "-5: 200-0.01-0.005-0.5: 0.863 (0.902)\n",
      "-5: 200-0.05-0.01-0: 0.843 (0.941)\n",
      "-5: 200-0.05-0.005-0: 0.824 (0.922)\n",
      "-5: 200-0.01-0.005-0: 0.824 (0.843)\n",
      "-5: 500-0.005-0.001-0.5: 0.863 (0.922)\n",
      "-6: 200-0.005-0.0001-0.25: 0.725 (0.824)\n",
      "-6: 200-0.005-0.0005-0.25: 0.745 (0.863)\n",
      "-6: 200-0.01-0.001-0.25: 0.804 (0.863)\n",
      "-6: 200-0.005-0.001-0.25: 0.843 (0.882)\n",
      "-6: 200-0.005-0.001-0.5: 0.765 (0.804)\n",
      "-6: 200-0.001-0.001-0.25: 0.706 (0.706)\n",
      "-6: 200-0.05-0.005-0.25: 0.804 (0.863)\n",
      "-6: 200-0.01-0.005-0.25: 0.804 (0.863)\n",
      "-6: 200-0.005-0.005-0.25: 0.804 (0.863)\n",
      "-6: 200-0.001-0.005-0.25: 0.843 (0.863)\n",
      "-6: 200-0.05-0.01-0.25: 0.745 (0.863)\n",
      "-6: 200-0.005-0.01-0.25: 0.843 (0.863)\n",
      "-6: 200-0.05-0.01-0.5: 0.804 (0.843)\n",
      "-6: 200-0.05-0.005-0.5: 0.824 (0.843)\n",
      "-6: 200-0.01-0.005-0.5: 0.824 (0.863)\n",
      "-6: 200-0.05-0.01-0: 0.804 (0.902)\n",
      "-6: 200-0.05-0.005-0: 0.824 (0.882)\n",
      "-6: 200-0.01-0.005-0: 0.725 (0.824)\n",
      "-6: 500-0.005-0.001-0.5: 0.765 (0.824)\n",
      "-7: 200-0.005-0.0001-0.25: 0.863 (0.922)\n",
      "-7: 200-0.005-0.0005-0.25: 0.824 (0.922)\n",
      "-7: 200-0.01-0.001-0.25: 0.882 (0.922)\n",
      "-7: 200-0.005-0.001-0.25: 0.882 (0.902)\n",
      "-7: 200-0.005-0.001-0.5: 0.843 (0.941)\n",
      "-7: 200-0.001-0.001-0.25: 0.745 (0.745)\n",
      "-7: 200-0.05-0.005-0.25: 0.863 (0.902)\n",
      "-7: 200-0.01-0.005-0.25: 0.863 (0.922)\n",
      "-7: 200-0.005-0.005-0.25: 0.843 (0.902)\n",
      "-7: 200-0.001-0.005-0.25: 0.902 (0.902)\n",
      "-7: 200-0.05-0.01-0.25: 0.882 (0.922)\n",
      "-7: 200-0.005-0.01-0.25: 0.863 (0.882)\n",
      "-7: 200-0.05-0.01-0.5: 0.843 (0.902)\n",
      "-7: 200-0.05-0.005-0.5: 0.843 (0.902)\n",
      "-7: 200-0.01-0.005-0.5: 0.882 (0.941)\n",
      "-7: 200-0.05-0.01-0: 0.804 (0.902)\n",
      "-7: 200-0.05-0.005-0: 0.784 (0.863)\n",
      "-7: 200-0.01-0.005-0: 0.843 (0.863)\n",
      "-7: 500-0.005-0.001-0.5: 0.882 (0.922)\n",
      "-8: 200-0.005-0.0001-0.25: 0.824 (0.843)\n",
      "-8: 200-0.005-0.0005-0.25: 0.843 (0.882)\n",
      "-8: 200-0.01-0.001-0.25: 0.824 (0.902)\n",
      "-8: 200-0.005-0.001-0.25: 0.863 (0.902)\n",
      "-8: 200-0.005-0.001-0.5: 0.784 (0.843)\n",
      "-8: 200-0.001-0.001-0.25: 0.647 (0.725)\n",
      "-8: 200-0.05-0.005-0.25: 0.784 (0.863)\n",
      "-8: 200-0.01-0.005-0.25: 0.843 (0.922)\n",
      "-8: 200-0.005-0.005-0.25: 0.843 (0.882)\n",
      "-8: 200-0.001-0.005-0.25: 0.784 (0.824)\n",
      "-8: 200-0.05-0.01-0.25: 0.843 (0.863)\n",
      "-8: 200-0.005-0.01-0.25: 0.843 (0.863)\n",
      "-8: 200-0.05-0.01-0.5: 0.784 (0.882)\n",
      "-8: 200-0.05-0.005-0.5: 0.804 (0.882)\n",
      "-8: 200-0.01-0.005-0.5: 0.804 (0.882)\n",
      "-8: 200-0.05-0.01-0: 0.824 (0.882)\n",
      "-8: 200-0.05-0.005-0: 0.843 (0.902)\n",
      "-8: 200-0.01-0.005-0: 0.765 (0.765)\n",
      "-8: 500-0.005-0.001-0.5: 0.824 (0.902)\n",
      "-9: 200-0.005-0.0001-0.25: 0.843 (0.882)\n",
      "-9: 200-0.005-0.0005-0.25: 0.804 (0.902)\n",
      "-9: 200-0.01-0.001-0.25: 0.843 (0.922)\n",
      "-9: 200-0.005-0.001-0.25: 0.804 (0.922)\n",
      "-9: 200-0.005-0.001-0.5: 0.804 (0.902)\n",
      "-9: 200-0.001-0.001-0.25: 0.725 (0.725)\n",
      "-9: 200-0.05-0.005-0.25: 0.804 (0.882)\n",
      "-9: 200-0.01-0.005-0.25: 0.863 (0.902)\n",
      "-9: 200-0.005-0.005-0.25: 0.804 (0.882)\n",
      "-9: 200-0.001-0.005-0.25: 0.784 (0.824)\n",
      "-9: 200-0.05-0.01-0.25: 0.824 (0.882)\n",
      "-9: 200-0.005-0.01-0.25: 0.824 (0.843)\n",
      "-9: 200-0.05-0.01-0.5: 0.804 (0.863)\n",
      "-9: 200-0.05-0.005-0.5: 0.804 (0.863)\n",
      "-9: 200-0.01-0.005-0.5: 0.804 (0.863)\n",
      "-9: 200-0.05-0.01-0: 0.824 (0.863)\n",
      "-9: 200-0.05-0.005-0: 0.824 (0.882)\n",
      "-9: 200-0.01-0.005-0: 0.824 (0.824)\n",
      "-9: 500-0.005-0.001-0.5: 0.804 (0.843)\n",
      "-10: 200-0.005-0.0001-0.25: 0.843 (0.882)\n",
      "-10: 200-0.005-0.0005-0.25: 0.882 (0.902)\n",
      "-10: 200-0.01-0.001-0.25: 0.882 (0.922)\n",
      "-10: 200-0.005-0.001-0.25: 0.882 (0.922)\n",
      "-10: 200-0.005-0.001-0.5: 0.804 (0.902)\n",
      "-10: 200-0.001-0.001-0.25: 0.804 (0.824)\n",
      "-10: 200-0.05-0.005-0.25: 0.902 (0.941)\n",
      "-10: 200-0.01-0.005-0.25: 0.863 (0.922)\n",
      "-10: 200-0.005-0.005-0.25: 0.843 (0.882)\n",
      "-10: 200-0.001-0.005-0.25: 0.804 (0.824)\n",
      "-10: 200-0.05-0.01-0.25: 0.922 (0.922)\n",
      "-10: 200-0.005-0.01-0.25: 0.843 (0.882)\n",
      "-10: 200-0.05-0.01-0.5: 0.882 (0.882)\n",
      "-10: 200-0.05-0.005-0.5: 0.824 (0.863)\n",
      "-10: 200-0.01-0.005-0.5: 0.902 (0.922)\n",
      "-10: 200-0.05-0.01-0: 0.882 (0.902)\n",
      "-10: 200-0.05-0.005-0: 0.882 (0.922)\n",
      "-10: 200-0.01-0.005-0: 0.882 (0.882)\n",
      "-10: 500-0.005-0.001-0.5: 0.804 (0.882)\n",
      "-11: 200-0.005-0.0001-0.25: 0.804 (0.863)\n",
      "-11: 200-0.005-0.0005-0.25: 0.824 (0.882)\n",
      "-11: 200-0.01-0.001-0.25: 0.804 (0.902)\n",
      "-11: 200-0.005-0.001-0.25: 0.765 (0.843)\n",
      "-11: 200-0.005-0.001-0.5: 0.765 (0.863)\n",
      "-11: 200-0.001-0.001-0.25: 0.608 (0.706)\n",
      "-11: 200-0.05-0.005-0.25: 0.784 (0.882)\n",
      "-11: 200-0.01-0.005-0.25: 0.765 (0.863)\n",
      "-11: 200-0.005-0.005-0.25: 0.784 (0.843)\n",
      "-11: 200-0.001-0.005-0.25: 0.765 (0.804)\n",
      "-11: 200-0.05-0.01-0.25: 0.843 (0.882)\n",
      "-11: 200-0.005-0.01-0.25: 0.784 (0.824)\n",
      "-11: 200-0.05-0.01-0.5: 0.824 (0.863)\n",
      "-11: 200-0.05-0.005-0.5: 0.784 (0.863)\n",
      "-11: 200-0.01-0.005-0.5: 0.765 (0.863)\n",
      "-11: 200-0.05-0.01-0: 0.804 (0.843)\n",
      "-11: 200-0.05-0.005-0: 0.745 (0.843)\n",
      "-11: 200-0.01-0.005-0: 0.725 (0.804)\n",
      "-11: 500-0.005-0.001-0.5: 0.784 (0.843)\n",
      "-12: 200-0.005-0.0001-0.25: 0.784 (0.902)\n",
      "-12: 200-0.005-0.0005-0.25: 0.784 (0.882)\n",
      "-12: 200-0.01-0.001-0.25: 0.784 (0.922)\n",
      "-12: 200-0.005-0.001-0.25: 0.863 (0.922)\n",
      "-12: 200-0.005-0.001-0.5: 0.863 (0.922)\n",
      "-12: 200-0.001-0.001-0.25: 0.863 (0.863)\n",
      "-12: 200-0.05-0.005-0.25: 0.843 (0.922)\n",
      "-12: 200-0.01-0.005-0.25: 0.902 (0.922)\n",
      "-12: 200-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-12: 200-0.001-0.005-0.25: 0.902 (0.902)\n",
      "-12: 200-0.05-0.01-0.25: 0.804 (0.941)\n",
      "-12: 200-0.005-0.01-0.25: 0.882 (0.922)\n",
      "-12: 200-0.05-0.01-0.5: 0.922 (0.941)\n",
      "-12: 200-0.05-0.005-0.5: 0.824 (0.922)\n",
      "-12: 200-0.01-0.005-0.5: 0.922 (0.941)\n",
      "-12: 200-0.05-0.01-0: 0.824 (0.941)\n",
      "-12: 200-0.05-0.005-0: 0.843 (0.902)\n",
      "-12: 200-0.01-0.005-0: 0.882 (0.902)\n",
      "-12: 500-0.005-0.001-0.5: 0.863 (0.941)\n",
      "-13: 200-0.005-0.0001-0.25: 0.804 (0.863)\n",
      "-13: 200-0.005-0.0005-0.25: 0.922 (0.922)\n",
      "-13: 200-0.01-0.001-0.25: 0.804 (0.941)\n",
      "-13: 200-0.005-0.001-0.25: 0.863 (0.922)\n",
      "-13: 200-0.005-0.001-0.5: 0.824 (0.843)\n",
      "-13: 200-0.001-0.001-0.25: 0.588 (0.627)\n",
      "-13: 200-0.05-0.005-0.25: 0.843 (0.922)\n",
      "-13: 200-0.01-0.005-0.25: 0.902 (0.922)\n",
      "-13: 200-0.005-0.005-0.25: 0.902 (0.922)\n",
      "-13: 200-0.001-0.005-0.25: 0.863 (0.882)\n",
      "-13: 200-0.05-0.01-0.25: 0.843 (0.922)\n",
      "-13: 200-0.005-0.01-0.25: 0.882 (0.922)\n",
      "-13: 200-0.05-0.01-0.5: 0.843 (0.882)\n",
      "-13: 200-0.05-0.005-0.5: 0.824 (0.882)\n",
      "-13: 200-0.01-0.005-0.5: 0.824 (0.902)\n",
      "-13: 200-0.05-0.01-0: 0.824 (0.902)\n",
      "-13: 200-0.05-0.005-0: 0.843 (0.922)\n",
      "-13: 200-0.01-0.005-0: 0.882 (0.902)\n",
      "-13: 500-0.005-0.001-0.5: 0.745 (0.824)\n",
      "-14: 200-0.005-0.0001-0.25: 0.804 (0.902)\n",
      "-14: 200-0.005-0.0005-0.25: 0.882 (0.941)\n",
      "-14: 200-0.01-0.001-0.25: 0.961 (0.961)\n",
      "-14: 200-0.005-0.001-0.25: 0.882 (0.922)\n",
      "-14: 200-0.005-0.001-0.5: 0.902 (0.941)\n",
      "-14: 200-0.001-0.001-0.25: 0.804 (0.824)\n",
      "-14: 200-0.05-0.005-0.25: 0.784 (0.922)\n",
      "-14: 200-0.01-0.005-0.25: 0.902 (0.980)\n",
      "-14: 200-0.005-0.005-0.25: 0.902 (0.941)\n",
      "-14: 200-0.001-0.005-0.25: 0.922 (0.922)\n",
      "-14: 200-0.05-0.01-0.25: 0.843 (0.902)\n",
      "-14: 200-0.005-0.01-0.25: 0.902 (0.941)\n",
      "-14: 200-0.05-0.01-0.5: 0.824 (0.863)\n",
      "-14: 200-0.05-0.005-0.5: 0.706 (0.902)\n",
      "-14: 200-0.01-0.005-0.5: 0.843 (0.902)\n",
      "-14: 200-0.05-0.01-0: 0.882 (0.922)\n",
      "-14: 200-0.05-0.005-0: 0.863 (0.922)\n",
      "-14: 200-0.01-0.005-0: 0.882 (0.922)\n",
      "-14: 500-0.005-0.001-0.5: 0.863 (0.902)\n",
      "-15: 200-0.005-0.0001-0.25: 0.784 (0.863)\n",
      "-15: 200-0.005-0.0005-0.25: 0.863 (0.922)\n",
      "-15: 200-0.01-0.001-0.25: 0.863 (0.922)\n",
      "-15: 200-0.005-0.001-0.25: 0.882 (0.941)\n",
      "-15: 200-0.005-0.001-0.5: 0.824 (0.922)\n",
      "-15: 200-0.001-0.001-0.25: 0.765 (0.784)\n",
      "-15: 200-0.05-0.005-0.25: 0.863 (0.922)\n",
      "-15: 200-0.01-0.005-0.25: 0.882 (0.961)\n",
      "-15: 200-0.005-0.005-0.25: 0.882 (0.922)\n",
      "-15: 200-0.001-0.005-0.25: 0.804 (0.863)\n",
      "-15: 200-0.05-0.01-0.25: 0.843 (0.922)\n",
      "-15: 200-0.005-0.01-0.25: 0.902 (0.902)\n",
      "-15: 200-0.05-0.01-0.5: 0.902 (0.902)\n",
      "-15: 200-0.05-0.005-0.5: 0.843 (0.922)\n",
      "-15: 200-0.01-0.005-0.5: 0.863 (0.902)\n",
      "-15: 200-0.05-0.01-0: 0.824 (0.902)\n",
      "-15: 200-0.05-0.005-0: 0.863 (0.922)\n",
      "-15: 200-0.01-0.005-0: 0.863 (0.863)\n",
      "-15: 500-0.005-0.001-0.5: 0.824 (0.922)\n",
      "-16: 200-0.005-0.0001-0.25: 0.765 (0.824)\n",
      "-16: 200-0.005-0.0005-0.25: 0.784 (0.843)\n",
      "-16: 200-0.01-0.001-0.25: 0.784 (0.843)\n",
      "-16: 200-0.005-0.001-0.25: 0.824 (0.863)\n",
      "-16: 200-0.005-0.001-0.5: 0.745 (0.843)\n",
      "-16: 200-0.001-0.001-0.25: 0.784 (0.824)\n",
      "-16: 200-0.05-0.005-0.25: 0.824 (0.843)\n",
      "-16: 200-0.01-0.005-0.25: 0.824 (0.882)\n",
      "-16: 200-0.005-0.005-0.25: 0.824 (0.863)\n",
      "-16: 200-0.001-0.005-0.25: 0.863 (0.882)\n",
      "-16: 200-0.05-0.01-0.25: 0.765 (0.863)\n",
      "-16: 200-0.005-0.01-0.25: 0.824 (0.863)\n",
      "-16: 200-0.05-0.01-0.5: 0.745 (0.824)\n",
      "-16: 200-0.05-0.005-0.5: 0.804 (0.843)\n",
      "-16: 200-0.01-0.005-0.5: 0.784 (0.843)\n",
      "-16: 200-0.05-0.01-0: 0.745 (0.863)\n",
      "-16: 200-0.05-0.005-0: 0.843 (0.882)\n",
      "-16: 200-0.01-0.005-0: 0.784 (0.863)\n",
      "-16: 500-0.005-0.001-0.5: 0.824 (0.843)\n",
      "-17: 200-0.005-0.0001-0.25: 0.882 (0.941)\n",
      "-17: 200-0.005-0.0005-0.25: 0.863 (0.922)\n",
      "-17: 200-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-17: 200-0.005-0.001-0.25: 0.863 (0.922)\n",
      "-17: 200-0.005-0.001-0.5: 0.863 (0.902)\n",
      "-17: 200-0.001-0.001-0.25: 0.804 (0.843)\n",
      "-17: 200-0.05-0.005-0.25: 0.843 (0.922)\n",
      "-17: 200-0.01-0.005-0.25: 0.863 (0.902)\n",
      "-17: 200-0.005-0.005-0.25: 0.882 (0.882)\n",
      "-17: 200-0.001-0.005-0.25: 0.804 (0.863)\n",
      "-17: 200-0.05-0.01-0.25: 0.882 (0.902)\n",
      "-17: 200-0.005-0.01-0.25: 0.843 (0.902)\n",
      "-17: 200-0.05-0.01-0.5: 0.804 (0.902)\n",
      "-17: 200-0.05-0.005-0.5: 0.824 (0.902)\n",
      "-17: 200-0.01-0.005-0.5: 0.882 (0.922)\n",
      "-17: 200-0.05-0.01-0: 0.765 (0.882)\n",
      "-17: 200-0.05-0.005-0: 0.824 (0.843)\n",
      "-17: 200-0.01-0.005-0: 0.784 (0.843)\n",
      "-17: 500-0.005-0.001-0.5: 0.863 (0.922)\n",
      "-18: 200-0.005-0.0001-0.25: 0.725 (0.824)\n",
      "-18: 200-0.005-0.0005-0.25: 0.882 (0.902)\n",
      "-18: 200-0.01-0.001-0.25: 0.863 (0.902)\n",
      "-18: 200-0.005-0.001-0.25: 0.902 (0.922)\n",
      "-18: 200-0.005-0.001-0.5: 0.824 (0.902)\n",
      "-18: 200-0.001-0.001-0.25: 0.706 (0.706)\n",
      "-18: 200-0.05-0.005-0.25: 0.804 (0.902)\n",
      "-18: 200-0.01-0.005-0.25: 0.863 (0.902)\n",
      "-18: 200-0.005-0.005-0.25: 0.882 (0.882)\n",
      "-18: 200-0.001-0.005-0.25: 0.804 (0.804)\n",
      "-18: 200-0.05-0.01-0.25: 0.843 (0.863)\n",
      "-18: 200-0.005-0.01-0.25: 0.843 (0.902)\n",
      "-18: 200-0.05-0.01-0.5: 0.784 (0.843)\n",
      "-18: 200-0.05-0.005-0.5: 0.784 (0.863)\n",
      "-18: 200-0.01-0.005-0.5: 0.745 (0.882)\n",
      "-18: 200-0.05-0.01-0: 0.824 (0.843)\n",
      "-18: 200-0.05-0.005-0: 0.804 (0.824)\n",
      "-18: 200-0.01-0.005-0: 0.843 (0.843)\n",
      "-18: 500-0.005-0.001-0.5: 0.863 (0.922)\n",
      "-19: 200-0.005-0.0001-0.25: 0.922 (0.922)\n",
      "-19: 200-0.005-0.0005-0.25: 0.863 (0.922)\n",
      "-19: 200-0.01-0.001-0.25: 0.824 (0.902)\n",
      "-19: 200-0.005-0.001-0.25: 0.824 (0.882)\n",
      "-19: 200-0.005-0.001-0.5: 0.784 (0.922)\n",
      "-19: 200-0.001-0.001-0.25: 0.745 (0.765)\n",
      "-19: 200-0.05-0.005-0.25: 0.863 (0.882)\n",
      "-19: 200-0.01-0.005-0.25: 0.824 (0.902)\n",
      "-19: 200-0.005-0.005-0.25: 0.824 (0.882)\n",
      "-19: 200-0.001-0.005-0.25: 0.765 (0.804)\n",
      "-19: 200-0.05-0.01-0.25: 0.882 (0.882)\n",
      "-19: 200-0.005-0.01-0.25: 0.804 (0.863)\n",
      "-19: 200-0.05-0.01-0.5: 0.843 (0.902)\n",
      "-19: 200-0.05-0.005-0.5: 0.863 (0.882)\n",
      "-19: 200-0.01-0.005-0.5: 0.784 (0.863)\n",
      "-19: 200-0.05-0.01-0: 0.784 (0.863)\n",
      "-19: 200-0.05-0.005-0: 0.824 (0.902)\n",
      "-19: 200-0.01-0.005-0: 0.824 (0.824)\n",
      "-19: 500-0.005-0.001-0.5: 0.804 (0.863)\n",
      "-20: 200-0.005-0.0001-0.25: 0.824 (0.863)\n",
      "-20: 200-0.005-0.0005-0.25: 0.922 (0.922)\n",
      "-20: 200-0.01-0.001-0.25: 0.902 (0.922)\n",
      "-20: 200-0.005-0.001-0.25: 0.882 (0.902)\n",
      "-20: 200-0.005-0.001-0.5: 0.765 (0.882)\n",
      "-20: 200-0.001-0.001-0.25: 0.824 (0.843)\n",
      "-20: 200-0.05-0.005-0.25: 0.863 (0.922)\n",
      "-20: 200-0.01-0.005-0.25: 0.902 (0.902)\n",
      "-20: 200-0.005-0.005-0.25: 0.824 (0.882)\n",
      "-20: 200-0.001-0.005-0.25: 0.784 (0.824)\n",
      "-20: 200-0.05-0.01-0.25: 0.863 (0.922)\n",
      "-20: 200-0.005-0.01-0.25: 0.863 (0.882)\n",
      "-20: 200-0.05-0.01-0.5: 0.863 (0.882)\n",
      "-20: 200-0.05-0.005-0.5: 0.824 (0.902)\n",
      "-20: 200-0.01-0.005-0.5: 0.824 (0.882)\n",
      "-20: 200-0.05-0.01-0: 0.902 (0.902)\n",
      "-20: 200-0.05-0.005-0: 0.843 (0.902)\n",
      "-20: 200-0.01-0.005-0: 0.843 (0.863)\n",
      "-20: 500-0.005-0.001-0.5: 0.863 (0.902)\n",
      "----- 106.19 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-4, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-4, 'drop': .25},\n",
    "\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "        \n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-2, 'drop': .25},\n",
    "\n",
    "        # {'epochs': 500, 'lr': .01, 'wd': 1e-3, 'drop': .25},\n",
    "        # {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        # {'epochs': 500, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "\n",
    "        # {'epochs': 500, 'lr': .01, 'wd': 5e-3, 'drop': .25},\n",
    "        # {'epochs': 500, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        # {'epochs': 500, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "\n",
    "\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': 0},\n",
    "\n",
    "        \n",
    "        # {'epochs': 500, 'lr': .005, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        # {'epochs': 500, 'lr': .001, 'wd': 1e-3, 'drop': .5},\n",
    "\n",
    "        # {'epochs': 750, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        # {'epochs': 750, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "        # {'epochs': 750, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "        ]\n",
    "\n",
    "best_accs1 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs1 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=exp['drop'], init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S, K, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, exp['epochs'], exp['lr'], exp['wd'],\n",
    "                             epochs_h=EPOCHS_h, epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs1[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs1[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}: {best_val_accs1[j,i]:.3f} ({best_accs1[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}' for exp in EXPS]\n",
    "table_over1 = summary_table(best_accs1, index_name)\n",
    "table1 = summary_table(best_val_accs1, index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0001-0.25</th>\n",
       "      <td>0.819608</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.052467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0005-0.25</th>\n",
       "      <td>0.851961</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.047819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.25</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.044281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.25</th>\n",
       "      <td>0.861765</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.038957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.5</th>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.041997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.001-0.25</th>\n",
       "      <td>0.726471</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.092485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0.25</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.033448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.25</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.039019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.005-0.25</th>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.033735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.25</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.052796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.25</th>\n",
       "      <td>0.842157</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.047008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.01-0.25</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.038970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.5</th>\n",
       "      <td>0.828431</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.047171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0.5</th>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.035294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.5</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.050029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0</th>\n",
       "      <td>0.825490</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.041548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.032516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0</th>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.053734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0.5</th>\n",
       "      <td>0.822549</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.037958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean accs       med       std\n",
       "200-0.005-0.0001-0.25   0.819608  0.813725  0.052467\n",
       "200-0.005-0.0005-0.25   0.851961  0.862745  0.047819\n",
       "200-0.01-0.001-0.25     0.843137  0.843137  0.044281\n",
       "200-0.005-0.001-0.25    0.861765  0.872549  0.038957\n",
       "200-0.005-0.001-0.5     0.808824  0.803922  0.041997\n",
       "200-0.001-0.001-0.25    0.726471  0.745098  0.092485\n",
       "200-0.05-0.005-0.25     0.837255  0.843137  0.033448\n",
       "200-0.01-0.005-0.25     0.858824  0.862745  0.039019\n",
       "200-0.005-0.005-0.25    0.847059  0.843137  0.033735\n",
       "200-0.001-0.005-0.25    0.833333  0.803922  0.052796\n",
       "200-0.05-0.01-0.25      0.842157  0.843137  0.047008\n",
       "200-0.005-0.01-0.25     0.852941  0.852941  0.038970\n",
       "200-0.05-0.01-0.5       0.828431  0.823529  0.047171\n",
       "200-0.05-0.005-0.5      0.811765  0.823529  0.035294\n",
       "200-0.01-0.005-0.5      0.837255  0.833333  0.050029\n",
       "200-0.05-0.01-0         0.825490  0.823529  0.041548\n",
       "200-0.05-0.005-0        0.833333  0.833333  0.032516\n",
       "200-0.01-0.005-0        0.829412  0.843137  0.053734\n",
       "500-0.005-0.001-0.5     0.822549  0.823529  0.037958"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0001-0.25</th>\n",
       "      <td>0.878431</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.033160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0005-0.25</th>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.028818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.25</th>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.028074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.25</th>\n",
       "      <td>0.909804</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.026597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.5</th>\n",
       "      <td>0.881373</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.039932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.001-0.25</th>\n",
       "      <td>0.751961</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.085080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0.25</th>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.027028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.25</th>\n",
       "      <td>0.914706</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.029914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.005-0.25</th>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.028684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.25</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.041176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.25</th>\n",
       "      <td>0.903922</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.028347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.01-0.25</th>\n",
       "      <td>0.890196</td>\n",
       "      <td>0.892157</td>\n",
       "      <td>0.033620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.5</th>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.030042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0.5</th>\n",
       "      <td>0.888235</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.026380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.5</th>\n",
       "      <td>0.891176</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.028057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0</th>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.033044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0</th>\n",
       "      <td>0.890196</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.033044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0</th>\n",
       "      <td>0.857843</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.041997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0.5</th>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.892157</td>\n",
       "      <td>0.036683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean accs       med       std\n",
       "200-0.005-0.0001-0.25   0.878431  0.882353  0.033160\n",
       "200-0.005-0.0005-0.25   0.905882  0.911765  0.028818\n",
       "200-0.01-0.001-0.25     0.911765  0.911765  0.028074\n",
       "200-0.005-0.001-0.25    0.909804  0.921569  0.026597\n",
       "200-0.005-0.001-0.5     0.881373  0.882353  0.039932\n",
       "200-0.001-0.001-0.25    0.751961  0.764706  0.085080\n",
       "200-0.05-0.005-0.25     0.901961  0.911765  0.027028\n",
       "200-0.01-0.005-0.25     0.914706  0.921569  0.029914\n",
       "200-0.005-0.005-0.25    0.894118  0.882353  0.028684\n",
       "200-0.001-0.005-0.25    0.856863  0.862745  0.041176\n",
       "200-0.05-0.01-0.25      0.903922  0.911765  0.028347\n",
       "200-0.005-0.01-0.25     0.890196  0.892157  0.033620\n",
       "200-0.05-0.01-0.5       0.883333  0.882353  0.030042\n",
       "200-0.05-0.005-0.5      0.888235  0.882353  0.026380\n",
       "200-0.01-0.005-0.5      0.891176  0.901961  0.028057\n",
       "200-0.05-0.01-0         0.894118  0.901961  0.033044\n",
       "200-0.05-0.005-0        0.890196  0.901961  0.033044\n",
       "200-0.01-0.005-0        0.857843  0.862745  0.041997\n",
       "500-0.005-0.001-0.5     0.882353  0.892157  0.036683"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_over1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 0.01-1-1-True: 0.373 (0.549)\n",
      "-1: 0.1-1-1-True: 0.529 (0.569)\n",
      "-1: 1-1-1-True: 0.510 (0.569)\n",
      "-1: 1-1-1-False: 0.510 (0.588)\n",
      "\tDone at iteration: 728\n",
      "-1: 1-1-1-False: 0.608 (0.667)\n",
      "-1: 1-5-1-True: 0.804 (0.804)\n",
      "-1: 1-10-1-True: 0.725 (0.765)\n",
      "-1: 1-10-5-True: 0.824 (0.824)\n",
      "-1: 1-1-5-True: 0.549 (0.569)\n",
      "-1: 1-1-10-True: 0.529 (0.569)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 1-5-10-True: 0.784 (0.804)\n",
      "-1: 0.1-10-10-True: 0.804 (0.863)\n",
      "-1: 1-10-10-True: 0.765 (0.804)\n",
      "-1: 1-25-25-True: 0.706 (0.745)\n",
      "-1: 1-25-25-True: 0.824 (0.882)\n",
      "-1: 0.1-25-25-True: 0.824 (0.882)\n",
      "-1: 1-25-25-True: 0.824 (0.882)\n",
      "-1: 1-50-50-True: 0.843 (0.863)\n",
      "-2: 0.01-1-1-True: 0.588 (0.667)\n",
      "-2: 0.1-1-1-True: 0.627 (0.647)\n",
      "-2: 1-1-1-True: 0.647 (0.686)\n",
      "-2: 1-1-1-False: 0.667 (0.667)\n",
      "\tDone at iteration: 873\n",
      "-2: 1-1-1-False: 0.863 (0.882)\n",
      "-2: 1-5-1-True: 0.804 (0.863)\n",
      "-2: 1-10-1-True: 0.843 (0.863)\n",
      "-2: 1-10-5-True: 0.784 (0.882)\n",
      "-2: 1-1-5-True: 0.647 (0.706)\n",
      "-2: 1-1-10-True: 0.608 (0.686)\n",
      "-2: 1-5-10-True: 0.882 (0.922)\n",
      "-2: 0.1-10-10-True: 0.863 (0.922)\n",
      "-2: 1-10-10-True: 0.843 (0.922)\n",
      "-2: 1-25-25-True: 0.784 (0.863)\n",
      "-2: 1-25-25-True: 0.843 (0.902)\n",
      "-2: 0.1-25-25-True: 0.824 (0.902)\n",
      "-2: 1-25-25-True: 0.863 (0.902)\n",
      "-2: 1-50-50-True: 0.843 (0.902)\n",
      "-3: 0.01-1-1-True: 0.431 (0.490)\n",
      "-3: 0.1-1-1-True: 0.431 (0.451)\n",
      "-3: 1-1-1-True: 0.490 (0.569)\n",
      "\tDone at iteration: 154\n",
      "-3: 1-1-1-False: 0.510 (0.549)\n",
      "\tDone at iteration: 306\n",
      "-3: 1-1-1-False: 0.451 (0.529)\n",
      "-3: 1-5-1-True: 0.451 (0.451)\n",
      "-3: 1-10-1-True: 0.608 (0.725)\n",
      "-3: 1-10-5-True: 0.843 (0.882)\n",
      "-3: 1-1-5-True: 0.490 (0.569)\n",
      "-3: 1-1-10-True: 0.510 (0.569)\n",
      "-3: 1-5-10-True: 0.765 (0.804)\n",
      "-3: 0.1-10-10-True: 0.863 (0.882)\n",
      "-3: 1-10-10-True: 0.863 (0.902)\n",
      "-3: 1-25-25-True: 0.784 (0.804)\n",
      "-3: 1-25-25-True: 0.863 (0.902)\n",
      "-3: 0.1-25-25-True: 0.902 (0.941)\n",
      "-3: 1-25-25-True: 0.863 (0.922)\n",
      "-3: 1-50-50-True: 0.863 (0.922)\n",
      "-4: 0.01-1-1-True: 0.490 (0.529)\n",
      "-4: 0.1-1-1-True: 0.490 (0.510)\n",
      "-4: 1-1-1-True: 0.392 (0.412)\n",
      "-4: 1-1-1-False: 0.451 (0.510)\n",
      "\tDone at iteration: 250\n",
      "-4: 1-1-1-False: 0.471 (0.569)\n",
      "-4: 1-5-1-True: 0.745 (0.745)\n",
      "-4: 1-10-1-True: 0.725 (0.765)\n",
      "-4: 1-10-5-True: 0.784 (0.843)\n",
      "-4: 1-1-5-True: 0.451 (0.569)\n",
      "-4: 1-1-10-True: 0.569 (0.647)\n",
      "-4: 1-5-10-True: 0.843 (0.902)\n",
      "-4: 0.1-10-10-True: 0.902 (0.961)\n",
      "-4: 1-10-10-True: 0.922 (0.922)\n",
      "-4: 1-25-25-True: 0.804 (0.804)\n",
      "-4: 1-25-25-True: 0.941 (0.941)\n",
      "-4: 0.1-25-25-True: 0.882 (0.941)\n",
      "-4: 1-25-25-True: 0.902 (0.922)\n",
      "-4: 1-50-50-True: 0.922 (0.941)\n",
      "-5: 0.01-1-1-True: 0.431 (0.490)\n",
      "-5: 0.1-1-1-True: 0.412 (0.510)\n",
      "-5: 1-1-1-True: 0.412 (0.431)\n",
      "\tDone at iteration: 154\n",
      "-5: 1-1-1-False: 0.412 (0.412)\n",
      "\tDone at iteration: 229\n",
      "-5: 1-1-1-False: 0.373 (0.529)\n",
      "-5: 1-5-1-True: 0.686 (0.686)\n",
      "-5: 1-10-1-True: 0.686 (0.745)\n",
      "-5: 1-10-5-True: 0.784 (0.824)\n",
      "-5: 1-1-5-True: 0.451 (0.529)\n",
      "-5: 1-1-10-True: 0.451 (0.627)\n",
      "-5: 1-5-10-True: 0.824 (0.902)\n",
      "-5: 0.1-10-10-True: 0.902 (0.902)\n",
      "-5: 1-10-10-True: 0.804 (0.902)\n",
      "-5: 1-25-25-True: 0.765 (0.843)\n",
      "-5: 1-25-25-True: 0.882 (0.941)\n",
      "-5: 0.1-25-25-True: 0.902 (0.922)\n",
      "-5: 1-25-25-True: 0.902 (0.922)\n",
      "-5: 1-50-50-True: 0.882 (0.922)\n",
      "-6: 0.01-1-1-True: 0.490 (0.529)\n",
      "-6: 0.1-1-1-True: 0.510 (0.549)\n",
      "-6: 1-1-1-True: 0.627 (0.706)\n",
      "-6: 1-1-1-False: 0.569 (0.647)\n",
      "\tDone at iteration: 962\n",
      "-6: 1-1-1-False: 0.765 (0.784)\n",
      "-6: 1-5-1-True: 0.706 (0.784)\n",
      "-6: 1-10-1-True: 0.706 (0.725)\n",
      "-6: 1-10-5-True: 0.804 (0.843)\n",
      "-6: 1-1-5-True: 0.588 (0.627)\n",
      "-6: 1-1-10-True: 0.569 (0.647)\n",
      "-6: 1-5-10-True: 0.784 (0.843)\n",
      "-6: 0.1-10-10-True: 0.765 (0.843)\n",
      "-6: 1-10-10-True: 0.784 (0.824)\n",
      "-6: 1-25-25-True: 0.843 (0.863)\n",
      "-6: 1-25-25-True: 0.804 (0.824)\n",
      "-6: 0.1-25-25-True: 0.863 (0.902)\n",
      "-6: 1-25-25-True: 0.745 (0.902)\n",
      "-6: 1-50-50-True: 0.784 (0.843)\n",
      "-7: 0.01-1-1-True: 0.471 (0.529)\n",
      "-7: 0.1-1-1-True: 0.510 (0.529)\n",
      "-7: 1-1-1-True: 0.471 (0.471)\n",
      "\tDone at iteration: 187\n",
      "-7: 1-1-1-False: 0.451 (0.471)\n",
      "\tDone at iteration: 195\n",
      "-7: 1-1-1-False: 0.510 (0.529)\n",
      "-7: 1-5-1-True: 0.784 (0.824)\n",
      "-7: 1-10-1-True: 0.804 (0.824)\n",
      "-7: 1-10-5-True: 0.902 (0.922)\n",
      "-7: 1-1-5-True: 0.549 (0.608)\n",
      "-7: 1-1-10-True: 0.569 (0.627)\n",
      "-7: 1-5-10-True: 0.784 (0.843)\n",
      "-7: 0.1-10-10-True: 0.843 (0.902)\n",
      "-7: 1-10-10-True: 0.863 (0.902)\n",
      "-7: 1-25-25-True: 0.882 (0.922)\n",
      "-7: 1-25-25-True: 0.902 (0.902)\n",
      "-7: 0.1-25-25-True: 0.863 (0.902)\n",
      "-7: 1-25-25-True: 0.882 (0.922)\n",
      "-7: 1-50-50-True: 0.882 (0.922)\n",
      "-8: 0.01-1-1-True: 0.549 (0.608)\n",
      "-8: 0.1-1-1-True: 0.529 (0.529)\n",
      "-8: 1-1-1-True: 0.549 (0.588)\n",
      "\tDone at iteration: 177\n",
      "-8: 1-1-1-False: 0.529 (0.529)\n",
      "\tDone at iteration: 195\n",
      "-8: 1-1-1-False: 0.529 (0.549)\n",
      "-8: 1-5-1-True: 0.510 (0.549)\n",
      "-8: 1-10-1-True: 0.706 (0.706)\n",
      "-8: 1-10-5-True: 0.745 (0.804)\n",
      "-8: 1-1-5-True: 0.510 (0.569)\n",
      "-8: 1-1-10-True: 0.490 (0.569)\n",
      "-8: 1-5-10-True: 0.784 (0.863)\n",
      "-8: 0.1-10-10-True: 0.843 (0.902)\n",
      "-8: 1-10-10-True: 0.765 (0.882)\n",
      "-8: 1-25-25-True: 0.725 (0.765)\n",
      "-8: 1-25-25-True: 0.824 (0.882)\n",
      "-8: 0.1-25-25-True: 0.824 (0.902)\n",
      "-8: 1-25-25-True: 0.824 (0.902)\n",
      "-8: 1-50-50-True: 0.863 (0.882)\n",
      "-9: 0.01-1-1-True: 0.451 (0.451)\n",
      "-9: 0.1-1-1-True: 0.471 (0.471)\n",
      "-9: 1-1-1-True: 0.510 (0.608)\n",
      "\tDone at iteration: 131\n",
      "-9: 1-1-1-False: 0.471 (0.510)\n",
      "\tDone at iteration: 262\n",
      "-9: 1-1-1-False: 0.471 (0.529)\n",
      "-9: 1-5-1-True: 0.686 (0.686)\n",
      "-9: 1-10-1-True: 0.745 (0.804)\n",
      "-9: 1-10-5-True: 0.863 (0.882)\n",
      "-9: 1-1-5-True: 0.549 (0.608)\n",
      "-9: 1-1-10-True: 0.510 (0.608)\n",
      "-9: 1-5-10-True: 0.804 (0.824)\n",
      "-9: 0.1-10-10-True: 0.843 (0.882)\n",
      "-9: 1-10-10-True: 0.804 (0.863)\n",
      "-9: 1-25-25-True: 0.824 (0.863)\n",
      "-9: 1-25-25-True: 0.863 (0.902)\n",
      "-9: 0.1-25-25-True: 0.824 (0.902)\n",
      "-9: 1-25-25-True: 0.843 (0.902)\n",
      "-9: 1-50-50-True: 0.765 (0.902)\n",
      "-10: 0.01-1-1-True: 0.647 (0.647)\n",
      "-10: 0.1-1-1-True: 0.471 (0.490)\n",
      "-10: 1-1-1-True: 0.529 (0.588)\n",
      "-10: 1-1-1-False: 0.510 (0.569)\n",
      "\tDone at iteration: 794\n",
      "-10: 1-1-1-False: 0.784 (0.784)\n",
      "-10: 1-5-1-True: 0.745 (0.765)\n",
      "-10: 1-10-1-True: 0.725 (0.725)\n",
      "-10: 1-10-5-True: 0.863 (0.882)\n",
      "-10: 1-1-5-True: 0.569 (0.627)\n",
      "-10: 1-1-10-True: 0.569 (0.706)\n",
      "-10: 1-5-10-True: 0.804 (0.824)\n",
      "-10: 0.1-10-10-True: 0.882 (0.922)\n",
      "-10: 1-10-10-True: 0.824 (0.882)\n",
      "-10: 1-25-25-True: 0.863 (0.922)\n",
      "-10: 1-25-25-True: 0.824 (0.882)\n",
      "-10: 0.1-25-25-True: 0.922 (0.922)\n",
      "-10: 1-25-25-True: 0.863 (0.902)\n",
      "-10: 1-50-50-True: 0.902 (0.922)\n",
      "-11: 0.01-1-1-True: 0.373 (0.412)\n",
      "-11: 0.1-1-1-True: 0.510 (0.529)\n",
      "-11: 1-1-1-True: 0.510 (0.549)\n",
      "-11: 1-1-1-False: 0.373 (0.490)\n",
      "\tDone at iteration: 156\n",
      "-11: 1-1-1-False: 0.529 (0.549)\n",
      "-11: 1-5-1-True: 0.647 (0.686)\n",
      "-11: 1-10-1-True: 0.706 (0.725)\n",
      "-11: 1-10-5-True: 0.824 (0.843)\n",
      "-11: 1-1-5-True: 0.529 (0.569)\n",
      "-11: 1-1-10-True: 0.510 (0.588)\n",
      "-11: 1-5-10-True: 0.784 (0.824)\n",
      "-11: 0.1-10-10-True: 0.784 (0.824)\n",
      "-11: 1-10-10-True: 0.824 (0.824)\n",
      "-11: 1-25-25-True: 0.804 (0.804)\n",
      "-11: 1-25-25-True: 0.784 (0.863)\n",
      "-11: 0.1-25-25-True: 0.843 (0.863)\n",
      "-11: 1-25-25-True: 0.804 (0.863)\n",
      "-11: 1-50-50-True: 0.804 (0.882)\n",
      "-12: 0.01-1-1-True: 0.510 (0.647)\n",
      "-12: 0.1-1-1-True: 0.471 (0.627)\n",
      "-12: 1-1-1-True: 0.490 (0.627)\n",
      "-12: 1-1-1-False: 0.627 (0.706)\n",
      "\tDone at iteration: 230\n",
      "-12: 1-1-1-False: 0.588 (0.686)\n",
      "-12: 1-5-1-True: 0.765 (0.784)\n",
      "-12: 1-10-1-True: 0.863 (0.863)\n",
      "-12: 1-10-5-True: 0.843 (0.882)\n",
      "-12: 1-1-5-True: 0.647 (0.706)\n",
      "-12: 1-1-10-True: 0.608 (0.725)\n",
      "-12: 1-5-10-True: 0.863 (0.882)\n",
      "-12: 0.1-10-10-True: 0.863 (0.922)\n",
      "-12: 1-10-10-True: 0.863 (0.922)\n",
      "-12: 1-25-25-True: 0.863 (0.922)\n",
      "-12: 1-25-25-True: 0.902 (0.922)\n",
      "-12: 0.1-25-25-True: 0.824 (0.922)\n",
      "-12: 1-25-25-True: 0.882 (0.941)\n",
      "-12: 1-50-50-True: 0.843 (0.922)\n",
      "-13: 0.01-1-1-True: 0.412 (0.431)\n",
      "-13: 0.1-1-1-True: 0.471 (0.529)\n",
      "-13: 1-1-1-True: 0.490 (0.549)\n",
      "-13: 1-1-1-False: 0.490 (0.529)\n",
      "\tDone at iteration: 156\n",
      "-13: 1-1-1-False: 0.451 (0.510)\n",
      "-13: 1-5-1-True: 0.647 (0.647)\n",
      "-13: 1-10-1-True: 0.765 (0.784)\n",
      "-13: 1-10-5-True: 0.745 (0.804)\n",
      "-13: 1-1-5-True: 0.529 (0.569)\n",
      "-13: 1-1-10-True: 0.510 (0.569)\n",
      "-13: 1-5-10-True: 0.843 (0.882)\n",
      "-13: 0.1-10-10-True: 0.843 (0.882)\n",
      "-13: 1-10-10-True: 0.843 (0.882)\n",
      "-13: 1-25-25-True: 0.882 (0.902)\n",
      "-13: 1-25-25-True: 0.863 (0.922)\n",
      "-13: 0.1-25-25-True: 0.804 (0.941)\n",
      "-13: 1-25-25-True: 0.902 (0.941)\n",
      "-13: 1-50-50-True: 0.882 (0.941)\n",
      "-14: 0.01-1-1-True: 0.412 (0.412)\n",
      "-14: 0.1-1-1-True: 0.490 (0.490)\n",
      "-14: 1-1-1-True: 0.529 (0.588)\n",
      "-14: 1-1-1-False: 0.529 (0.569)\n",
      "\tDone at iteration: 195\n",
      "-14: 1-1-1-False: 0.451 (0.608)\n",
      "-14: 1-5-1-True: 0.843 (0.863)\n",
      "-14: 1-10-1-True: 0.804 (0.804)\n",
      "-14: 1-10-5-True: 0.824 (0.882)\n",
      "-14: 1-1-5-True: 0.569 (0.667)\n",
      "-14: 1-1-10-True: 0.490 (0.647)\n",
      "-14: 1-5-10-True: 0.843 (0.882)\n",
      "-14: 0.1-10-10-True: 0.863 (0.922)\n",
      "-14: 1-10-10-True: 0.922 (0.922)\n",
      "-14: 1-25-25-True: 0.843 (0.882)\n",
      "-14: 1-25-25-True: 0.863 (0.882)\n",
      "-14: 0.1-25-25-True: 0.824 (0.902)\n",
      "-14: 1-25-25-True: 0.882 (0.941)\n",
      "-14: 1-50-50-True: 0.941 (0.961)\n",
      "-15: 0.01-1-1-True: 0.412 (0.412)\n",
      "-15: 0.1-1-1-True: 0.490 (0.510)\n",
      "-15: 1-1-1-True: 0.392 (0.529)\n",
      "-15: 1-1-1-False: 0.412 (0.510)\n",
      "\tDone at iteration: 153\n",
      "-15: 1-1-1-False: 0.373 (0.392)\n",
      "-15: 1-5-1-True: 0.706 (0.725)\n",
      "-15: 1-10-1-True: 0.706 (0.745)\n",
      "-15: 1-10-5-True: 0.882 (0.882)\n",
      "-15: 1-1-5-True: 0.431 (0.510)\n",
      "-15: 1-1-10-True: 0.529 (0.529)\n",
      "-15: 1-5-10-True: 0.882 (0.922)\n",
      "-15: 0.1-10-10-True: 0.824 (0.902)\n",
      "-15: 1-10-10-True: 0.824 (0.882)\n",
      "-15: 1-25-25-True: 0.725 (0.824)\n",
      "-15: 1-25-25-True: 0.882 (0.902)\n",
      "-15: 0.1-25-25-True: 0.863 (0.941)\n",
      "-15: 1-25-25-True: 0.902 (0.941)\n",
      "-15: 1-50-50-True: 0.882 (0.922)\n",
      "-16: 0.01-1-1-True: 0.549 (0.588)\n",
      "-16: 0.1-1-1-True: 0.549 (0.588)\n",
      "-16: 1-1-1-True: 0.549 (0.647)\n",
      "-16: 1-1-1-False: 0.569 (0.608)\n",
      "\tDone at iteration: 281\n",
      "-16: 1-1-1-False: 0.569 (0.647)\n",
      "-16: 1-5-1-True: 0.765 (0.784)\n",
      "-16: 1-10-1-True: 0.725 (0.745)\n",
      "-16: 1-10-5-True: 0.765 (0.804)\n",
      "-16: 1-1-5-True: 0.608 (0.686)\n",
      "-16: 1-1-10-True: 0.588 (0.647)\n",
      "-16: 1-5-10-True: 0.804 (0.804)\n",
      "-16: 0.1-10-10-True: 0.765 (0.843)\n",
      "-16: 1-10-10-True: 0.804 (0.843)\n",
      "-16: 1-25-25-True: 0.765 (0.765)\n",
      "-16: 1-25-25-True: 0.784 (0.863)\n",
      "-16: 0.1-25-25-True: 0.922 (0.922)\n",
      "-16: 1-25-25-True: 0.784 (0.863)\n",
      "-16: 1-50-50-True: 0.784 (0.863)\n",
      "-17: 0.01-1-1-True: 0.510 (0.529)\n",
      "-17: 0.1-1-1-True: 0.529 (0.529)\n",
      "-17: 1-1-1-True: 0.529 (0.588)\n",
      "\tDone at iteration: 185\n",
      "-17: 1-1-1-False: 0.471 (0.549)\n",
      "\tDone at iteration: 195\n",
      "-17: 1-1-1-False: 0.529 (0.588)\n",
      "-17: 1-5-1-True: 0.784 (0.784)\n",
      "-17: 1-10-1-True: 0.804 (0.804)\n",
      "-17: 1-10-5-True: 0.843 (0.863)\n",
      "-17: 1-1-5-True: 0.529 (0.549)\n",
      "-17: 1-1-10-True: 0.647 (0.686)\n",
      "-17: 1-5-10-True: 0.784 (0.863)\n",
      "-17: 0.1-10-10-True: 0.863 (0.922)\n",
      "-17: 1-10-10-True: 0.843 (0.882)\n",
      "-17: 1-25-25-True: 0.863 (0.902)\n",
      "-17: 1-25-25-True: 0.902 (0.941)\n",
      "-17: 0.1-25-25-True: 0.863 (0.922)\n",
      "-17: 1-25-25-True: 0.882 (0.902)\n",
      "-17: 1-50-50-True: 0.882 (0.922)\n",
      "-18: 0.01-1-1-True: 0.510 (0.529)\n",
      "-18: 0.1-1-1-True: 0.451 (0.510)\n",
      "-18: 1-1-1-True: 0.529 (0.588)\n",
      "-18: 1-1-1-False: 0.510 (0.529)\n",
      "\tDone at iteration: 245\n",
      "-18: 1-1-1-False: 0.510 (0.529)\n",
      "-18: 1-5-1-True: 0.686 (0.706)\n",
      "-18: 1-10-1-True: 0.725 (0.784)\n",
      "-18: 1-10-5-True: 0.765 (0.843)\n",
      "-18: 1-1-5-True: 0.529 (0.569)\n",
      "-18: 1-1-10-True: 0.431 (0.588)\n",
      "-18: 1-5-10-True: 0.824 (0.843)\n",
      "-18: 0.1-10-10-True: 0.824 (0.843)\n",
      "-18: 1-10-10-True: 0.784 (0.824)\n",
      "-18: 1-25-25-True: 0.804 (0.863)\n",
      "-18: 1-25-25-True: 0.824 (0.902)\n",
      "-18: 0.1-25-25-True: 0.804 (0.902)\n",
      "-18: 1-25-25-True: 0.863 (0.902)\n",
      "-18: 1-50-50-True: 0.863 (0.902)\n",
      "-19: 0.01-1-1-True: 0.451 (0.451)\n",
      "-19: 0.1-1-1-True: 0.471 (0.471)\n",
      "-19: 1-1-1-True: 0.549 (0.588)\n",
      "-19: 1-1-1-False: 0.529 (0.588)\n",
      "\tDone at iteration: 562\n",
      "-19: 1-1-1-False: 0.843 (0.863)\n",
      "-19: 1-5-1-True: 0.686 (0.686)\n",
      "-19: 1-10-1-True: 0.804 (0.804)\n",
      "-19: 1-10-5-True: 0.902 (0.922)\n",
      "-19: 1-1-5-True: 0.588 (0.627)\n",
      "-19: 1-1-10-True: 0.490 (0.490)\n",
      "-19: 1-5-10-True: 0.784 (0.804)\n",
      "-19: 0.1-10-10-True: 0.824 (0.902)\n",
      "-19: 1-10-10-True: 0.843 (0.882)\n",
      "-19: 1-25-25-True: 0.804 (0.843)\n",
      "-19: 1-25-25-True: 0.824 (0.863)\n",
      "-19: 0.1-25-25-True: 0.804 (0.902)\n",
      "-19: 1-25-25-True: 0.824 (0.882)\n",
      "-19: 1-50-50-True: 0.843 (0.882)\n",
      "-20: 0.01-1-1-True: 0.588 (0.608)\n",
      "-20: 0.1-1-1-True: 0.471 (0.569)\n",
      "-20: 1-1-1-True: 0.529 (0.588)\n",
      "\tDone at iteration: 166\n",
      "-20: 1-1-1-False: 0.529 (0.588)\n",
      "\tDone at iteration: 800\n",
      "-20: 1-1-1-False: 0.863 (0.863)\n",
      "-20: 1-5-1-True: 0.667 (0.706)\n",
      "-20: 1-10-1-True: 0.765 (0.765)\n",
      "-20: 1-10-5-True: 0.784 (0.843)\n",
      "-20: 1-1-5-True: 0.549 (0.569)\n",
      "-20: 1-1-10-True: 0.510 (0.549)\n",
      "-20: 1-5-10-True: 0.804 (0.843)\n",
      "-20: 0.1-10-10-True: 0.843 (0.863)\n",
      "-20: 1-10-10-True: 0.784 (0.863)\n",
      "-20: 1-25-25-True: 0.784 (0.824)\n",
      "-20: 1-25-25-True: 0.882 (0.882)\n",
      "-20: 0.1-25-25-True: 0.882 (0.922)\n",
      "-20: 1-25-25-True: 0.863 (0.882)\n",
      "-20: 1-50-50-True: 0.902 (0.922)\n",
      "----- 42.37 mins -----\n"
     ]
    }
   ],
   "source": [
    "# h0, norm (norm, not norm, norm H), sep vs joint training, optimizers iters (W vs h)\n",
    "EXPS = [\n",
    "        # {'h0': .001, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': .01, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': .1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        \n",
    "        # {'h0': .1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        {'h0': 1, 'epochs': 1000, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 5, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 1, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 5, 'alt': True},\n",
    "\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 5, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 10, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 5, 'epochs_W': 10, 'alt': True},\n",
    "        \n",
    "        {'h0': .1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 50, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 100, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': .1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 50, 'epochs_W': 50, 'alt': True},\n",
    "    ]\n",
    "\n",
    "\n",
    "best_accs2 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs2 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=exp['h0'])\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        if not exp['alt']:\n",
    "            model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "            _, acc = model.train(feat, labels, exp['epochs'], LR, WD)\n",
    "        else:\n",
    "            model = GF_NodeClassModel(arch, S,  K, masks, LOSS_FN, device=device)\n",
    "            _, acc = model.train(feat, labels, exp['epochs'], LR, WD, epochs_h=exp['epochs_h'],\n",
    "                                 epochs_W=exp['epochs_W'])\n",
    "\n",
    "        best_accs2[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs2[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"h0\"]}-{exp[\"epochs_h\"]}-{exp[\"epochs_W\"]}-{exp[\"alt\"]}: {best_val_accs2[j,i]:.3f} ({best_accs2[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"h0\"]}-{exp[\"epochs_h\"]}-{exp[\"epochs_W\"]}-{exp[\"alt\"]}' for exp in EXPS]\n",
    "table_over2 = summary_table(best_accs2, index_name)\n",
    "table2 = summary_table(best_val_accs2, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.01-1-1-True</th>\n",
       "      <td>0.482353</td>\n",
       "      <td>0.480392</td>\n",
       "      <td>0.073208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-1-1-True</th>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.045395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-1-True</th>\n",
       "      <td>0.511765</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.062898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-1-False</th>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.068935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000-1-1-1-False</th>\n",
       "      <td>0.576471</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.155435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-5-1-True</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.093215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-1-True</th>\n",
       "      <td>0.747059</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.058463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-5-True</th>\n",
       "      <td>0.818627</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.047577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-5-True</th>\n",
       "      <td>0.543137</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.057200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-10-True</th>\n",
       "      <td>0.534314</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.053654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-5-10-True</th>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.034244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-10-10-True</th>\n",
       "      <td>0.840196</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.037856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-10-True</th>\n",
       "      <td>0.828431</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.043348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50-1-25-25-True</th>\n",
       "      <td>0.805882</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.051093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100-1-25-25-True</th>\n",
       "      <td>0.853922</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.041351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-25-25-True</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.037970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-25-25-True</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.041779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-50-50-True</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.045395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mean accs       med       std\n",
       "200-0.01-1-1-True    0.482353  0.480392  0.073208\n",
       "200-0.1-1-1-True     0.494118  0.490196  0.045395\n",
       "200-1-1-1-True       0.511765  0.519608  0.062898\n",
       "200-1-1-1-False      0.505882  0.509804  0.068935\n",
       "1000-1-1-1-False     0.576471  0.529412  0.155435\n",
       "200-1-5-1-True       0.705882  0.705882  0.093215\n",
       "200-1-10-1-True      0.747059  0.725490  0.058463\n",
       "200-1-10-5-True      0.818627  0.823529  0.047577\n",
       "200-1-1-5-True       0.543137  0.549020  0.057200\n",
       "200-1-1-10-True      0.534314  0.519608  0.053654\n",
       "200-1-5-10-True      0.813725  0.803922  0.034244\n",
       "200-0.1-10-10-True   0.840196  0.843137  0.037856\n",
       "200-1-10-10-True     0.828431  0.823529  0.043348\n",
       "50-1-25-25-True      0.805882  0.803922  0.051093\n",
       "100-1-25-25-True     0.853922  0.862745  0.041351\n",
       "200-0.1-25-25-True   0.852941  0.852941  0.037970\n",
       "200-1-25-25-True     0.854902  0.862745  0.041779\n",
       "200-1-50-50-True     0.858824  0.862745  0.045395"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.01-1-1-True</th>\n",
       "      <td>0.525490</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.080273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-1-1-True</th>\n",
       "      <td>0.530392</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.049010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-1-True</th>\n",
       "      <td>0.573529</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.071205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-1-False</th>\n",
       "      <td>0.555882</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.067148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000-1-1-1-False</th>\n",
       "      <td>0.629412</td>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.135124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-5-1-True</th>\n",
       "      <td>0.726471</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.097148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-1-True</th>\n",
       "      <td>0.773529</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.044052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-5-True</th>\n",
       "      <td>0.857843</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.035007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-5-True</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.054902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-10-True</th>\n",
       "      <td>0.613725</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.060148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-5-10-True</th>\n",
       "      <td>0.853922</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.038461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-10-10-True</th>\n",
       "      <td>0.890196</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.034187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-10-True</th>\n",
       "      <td>0.876471</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.035673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50-1-25-25-True</th>\n",
       "      <td>0.846078</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.052714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100-1-25-25-True</th>\n",
       "      <td>0.895098</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.029264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-25-25-True</th>\n",
       "      <td>0.912745</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.020068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-25-25-True</th>\n",
       "      <td>0.906863</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.023914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-50-50-True</th>\n",
       "      <td>0.906863</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mean accs       med       std\n",
       "200-0.01-1-1-True    0.525490  0.529412  0.080273\n",
       "200-0.1-1-1-True     0.530392  0.529412  0.049010\n",
       "200-1-1-1-True       0.573529  0.588235  0.071205\n",
       "200-1-1-1-False      0.555882  0.549020  0.067148\n",
       "1000-1-1-1-False     0.629412  0.578431  0.135124\n",
       "200-1-5-1-True       0.726471  0.735294  0.097148\n",
       "200-1-10-1-True      0.773529  0.764706  0.044052\n",
       "200-1-10-5-True      0.857843  0.852941  0.035007\n",
       "200-1-1-5-True       0.600000  0.568627  0.054902\n",
       "200-1-1-10-True      0.613725  0.617647  0.060148\n",
       "200-1-5-10-True      0.853922  0.843137  0.038461\n",
       "200-0.1-10-10-True   0.890196  0.901961  0.034187\n",
       "200-1-10-10-True     0.876471  0.882353  0.035673\n",
       "50-1-25-25-True      0.846078  0.852941  0.052714\n",
       "100-1-25-25-True     0.895098  0.901961  0.029264\n",
       "200-0.1-25-25-True   0.912745  0.911765  0.020068\n",
       "200-1-25-25-True     0.906863  0.901961  0.023914\n",
       "200-1-50-50-True     0.906863  0.921569  0.029000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_over2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design - Bias True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 2-2-16: 0.863 (0.882)\n",
      "-1: 2-3-8: 0.804 (0.824)\n",
      "-1: 2-3-16: 0.804 (0.882)\n",
      "-1: 2-3-32: 0.863 (0.882)\n",
      "-1: 2-4-16: 0.765 (0.882)\n",
      "-1: 3-2-16: 0.843 (0.882)\n",
      "-1: 4-2-16: 0.863 (0.863)\n",
      "-1: 3-3-16: 0.784 (0.863)\n",
      "-1: 4-3-16: 0.863 (0.882)\n",
      "-1: 2-2-8: 0.745 (0.824)\n",
      "-1: 2-2-32: 0.863 (0.882)\n",
      "-1: 2-2-50: 0.725 (0.863)\n",
      "-1: 2-2-75: 0.804 (0.863)\n",
      "-1: 2-2-100: 0.784 (0.863)\n",
      "-1: 2-3-32: 0.784 (0.882)\n",
      "-1: 2-3-50: 0.824 (0.882)\n",
      "-1: 2-3-75: 0.804 (0.863)\n",
      "-1: 2-3-100: 0.824 (0.863)\n",
      "-1: 3-2-50: 0.882 (0.882)\n",
      "-1: 3-3-50: 0.824 (0.882)\n",
      "-2: 2-2-16: 0.863 (0.902)\n",
      "-2: 2-3-8: 0.843 (0.902)\n",
      "-2: 2-3-16: 0.882 (0.922)\n",
      "-2: 2-3-32: 0.824 (0.941)\n",
      "-2: 2-4-16: 0.804 (0.902)\n",
      "-2: 3-2-16: 0.843 (0.902)\n",
      "-2: 4-2-16: 0.882 (0.902)\n",
      "-2: 3-3-16: 0.863 (0.902)\n",
      "-2: 4-3-16: 0.804 (0.902)\n",
      "-2: 2-2-8: 0.902 (0.902)\n",
      "-2: 2-2-32: 0.902 (0.922)\n",
      "-2: 2-2-50: 0.824 (0.922)\n",
      "-2: 2-2-75: 0.922 (0.941)\n",
      "-2: 2-2-100: 0.902 (0.941)\n",
      "-2: 2-3-32: 0.843 (0.922)\n",
      "-2: 2-3-50: 0.863 (0.922)\n",
      "-2: 2-3-75: 0.843 (0.941)\n",
      "-2: 2-3-100: 0.824 (0.922)\n",
      "-2: 3-2-50: 0.863 (0.941)\n",
      "-2: 3-3-50: 0.882 (0.922)\n",
      "-3: 2-2-16: 0.902 (0.922)\n",
      "-3: 2-3-8: 0.843 (0.882)\n",
      "-3: 2-3-16: 0.882 (0.922)\n",
      "-3: 2-3-32: 0.882 (0.922)\n",
      "-3: 2-4-16: 0.902 (0.922)\n",
      "-3: 3-2-16: 0.863 (0.941)\n",
      "-3: 4-2-16: 0.863 (0.922)\n",
      "-3: 3-3-16: 0.882 (0.902)\n",
      "-3: 4-3-16: 0.902 (0.902)\n",
      "-3: 2-2-8: 0.804 (0.804)\n",
      "-3: 2-2-32: 0.902 (0.922)\n",
      "-3: 2-2-50: 0.902 (0.922)\n",
      "-3: 2-2-75: 0.882 (0.922)\n",
      "-3: 2-2-100: 0.882 (0.922)\n",
      "-3: 2-3-32: 0.843 (0.941)\n",
      "-3: 2-3-50: 0.882 (0.941)\n",
      "-3: 2-3-75: 0.863 (0.922)\n",
      "-3: 2-3-100: 0.902 (0.922)\n",
      "-3: 3-2-50: 0.902 (0.941)\n",
      "-3: 3-3-50: 0.902 (0.922)\n",
      "-4: 2-2-16: 0.843 (0.961)\n",
      "-4: 2-3-8: 0.804 (0.843)\n",
      "-4: 2-3-16: 0.882 (0.961)\n",
      "-4: 2-3-32: 0.882 (0.961)\n",
      "-4: 2-4-16: 0.863 (0.922)\n",
      "-4: 3-2-16: 0.882 (0.922)\n",
      "-4: 4-2-16: 0.824 (0.902)\n",
      "-4: 3-3-16: 0.804 (0.882)\n",
      "-4: 4-3-16: 0.824 (0.882)\n",
      "-4: 2-2-8: 0.824 (0.863)\n",
      "-4: 2-2-32: 0.863 (0.980)\n",
      "-4: 2-2-50: 0.941 (0.961)\n",
      "-4: 2-2-75: 0.882 (0.961)\n",
      "-4: 2-2-100: 0.922 (0.961)\n",
      "-4: 2-3-32: 0.902 (0.941)\n",
      "-4: 2-3-50: 0.863 (0.941)\n",
      "-4: 2-3-75: 0.941 (0.961)\n",
      "-4: 2-3-100: 0.941 (0.961)\n",
      "-4: 3-2-50: 0.941 (0.941)\n",
      "-4: 3-3-50: 0.882 (0.941)\n",
      "-5: 2-2-16: 0.902 (0.922)\n",
      "-5: 2-3-8: 0.784 (0.824)\n",
      "-5: 2-3-16: 0.882 (0.902)\n",
      "-5: 2-3-32: 0.902 (0.922)\n",
      "-5: 2-4-16: 0.824 (0.922)\n",
      "-5: 3-2-16: 0.882 (0.941)\n",
      "-5: 4-2-16: 0.863 (0.922)\n",
      "-5: 3-3-16: 0.882 (0.922)\n",
      "-5: 4-3-16: 0.863 (0.902)\n",
      "-5: 2-2-8: 0.745 (0.824)\n",
      "-5: 2-2-32: 0.843 (0.941)\n",
      "-5: 2-2-50: 0.843 (0.941)\n",
      "-5: 2-2-75: 0.902 (0.922)\n",
      "-5: 2-2-100: 0.863 (0.902)\n",
      "-5: 2-3-32: 0.882 (0.922)\n",
      "-5: 2-3-50: 0.882 (0.941)\n",
      "-5: 2-3-75: 0.922 (0.922)\n",
      "-5: 2-3-100: 0.843 (0.922)\n",
      "-5: 3-2-50: 0.922 (0.941)\n",
      "-5: 3-3-50: 0.882 (0.902)\n",
      "-6: 2-2-16: 0.863 (0.882)\n",
      "-6: 2-3-8: 0.686 (0.804)\n",
      "-6: 2-3-16: 0.804 (0.863)\n",
      "-6: 2-3-32: 0.863 (0.922)\n",
      "-6: 2-4-16: 0.804 (0.843)\n",
      "-6: 3-2-16: 0.804 (0.882)\n",
      "-6: 4-2-16: 0.784 (0.824)\n",
      "-6: 3-3-16: 0.765 (0.824)\n",
      "-6: 4-3-16: 0.745 (0.824)\n",
      "-6: 2-2-8: 0.627 (0.745)\n",
      "-6: 2-2-32: 0.902 (0.922)\n",
      "-6: 2-2-50: 0.863 (0.922)\n",
      "-6: 2-2-75: 0.882 (0.922)\n",
      "-6: 2-2-100: 0.824 (0.902)\n",
      "-6: 2-3-32: 0.882 (0.902)\n",
      "-6: 2-3-50: 0.863 (0.902)\n",
      "-6: 2-3-75: 0.804 (0.902)\n",
      "-6: 2-3-100: 0.863 (0.902)\n",
      "-6: 3-2-50: 0.863 (0.902)\n",
      "-6: 3-3-50: 0.510 (0.569)\n",
      "-7: 2-2-16: 0.863 (0.922)\n",
      "-7: 2-3-8: 0.765 (0.804)\n",
      "-7: 2-3-16: 0.863 (0.922)\n",
      "-7: 2-3-32: 0.863 (0.902)\n",
      "-7: 2-4-16: 0.863 (0.882)\n",
      "-7: 3-2-16: 0.843 (0.902)\n",
      "-7: 4-2-16: 0.843 (0.922)\n",
      "-7: 3-3-16: 0.863 (0.941)\n",
      "-7: 4-3-16: 0.490 (0.549)\n",
      "-7: 2-2-8: 0.922 (0.941)\n",
      "-7: 2-2-32: 0.843 (0.922)\n",
      "-7: 2-2-50: 0.882 (0.902)\n",
      "-7: 2-2-75: 0.882 (0.902)\n",
      "-7: 2-2-100: 0.843 (0.922)\n",
      "-7: 2-3-32: 0.863 (0.902)\n",
      "-7: 2-3-50: 0.804 (0.902)\n",
      "-7: 2-3-75: 0.941 (0.941)\n",
      "-7: 2-3-100: 0.863 (0.902)\n",
      "-7: 3-2-50: 0.863 (0.922)\n",
      "-7: 3-3-50: 0.863 (0.902)\n",
      "-8: 2-2-16: 0.882 (0.882)\n",
      "-8: 2-3-8: 0.745 (0.824)\n",
      "-8: 2-3-16: 0.882 (0.882)\n",
      "-8: 2-3-32: 0.902 (0.922)\n",
      "-8: 2-4-16: 0.765 (0.882)\n",
      "-8: 3-2-16: 0.843 (0.882)\n",
      "-8: 4-2-16: 0.882 (0.882)\n",
      "-8: 3-3-16: 0.863 (0.902)\n",
      "-8: 4-3-16: 0.843 (0.843)\n",
      "-8: 2-2-8: 0.824 (0.843)\n",
      "-8: 2-2-32: 0.824 (0.922)\n",
      "-8: 2-2-50: 0.863 (0.882)\n",
      "-8: 2-2-75: 0.863 (0.902)\n",
      "-8: 2-2-100: 0.824 (0.882)\n",
      "-8: 2-3-32: 0.824 (0.882)\n",
      "-8: 2-3-50: 0.804 (0.882)\n",
      "-8: 2-3-75: 0.882 (0.922)\n",
      "-8: 2-3-100: 0.882 (0.882)\n",
      "-8: 3-2-50: 0.882 (0.922)\n",
      "-8: 3-3-50: 0.824 (0.922)\n",
      "-9: 2-2-16: 0.804 (0.882)\n",
      "-9: 2-3-8: 0.725 (0.863)\n",
      "-9: 2-3-16: 0.863 (0.922)\n",
      "-9: 2-3-32: 0.843 (0.902)\n",
      "-9: 2-4-16: 0.882 (0.902)\n",
      "-9: 3-2-16: 0.863 (0.882)\n",
      "-9: 4-2-16: 0.824 (0.882)\n",
      "-9: 3-3-16: 0.824 (0.882)\n",
      "-9: 4-3-16: 0.843 (0.882)\n",
      "-9: 2-2-8: 0.765 (0.804)\n",
      "-9: 2-2-32: 0.824 (0.902)\n",
      "-9: 2-2-50: 0.843 (0.882)\n",
      "-9: 2-2-75: 0.843 (0.882)\n",
      "-9: 2-2-100: 0.824 (0.882)\n",
      "-9: 2-3-32: 0.863 (0.902)\n",
      "-9: 2-3-50: 0.804 (0.882)\n",
      "-9: 2-3-75: 0.863 (0.902)\n",
      "-9: 2-3-100: 0.804 (0.882)\n",
      "-9: 3-2-50: 0.863 (0.902)\n",
      "-9: 3-3-50: 0.882 (0.902)\n",
      "-10: 2-2-16: 0.882 (0.922)\n",
      "-10: 2-3-8: 0.784 (0.863)\n",
      "-10: 2-3-16: 0.882 (0.882)\n",
      "-10: 2-3-32: 0.922 (0.922)\n",
      "-10: 2-4-16: 0.863 (0.902)\n",
      "-10: 3-2-16: 0.902 (0.902)\n",
      "-10: 4-2-16: 0.843 (0.922)\n",
      "-10: 3-3-16: 0.824 (0.882)\n",
      "-10: 4-3-16: 0.824 (0.882)\n",
      "-10: 2-2-8: 0.824 (0.863)\n",
      "-10: 2-2-32: 0.882 (0.922)\n",
      "-10: 2-2-50: 0.922 (0.922)\n",
      "-10: 2-2-75: 0.922 (0.922)\n",
      "-10: 2-2-100: 0.882 (0.922)\n",
      "-10: 2-3-32: 0.902 (0.922)\n",
      "-10: 2-3-50: 0.882 (0.922)\n",
      "-10: 2-3-75: 0.824 (0.882)\n",
      "-10: 2-3-100: 0.882 (0.941)\n",
      "-10: 3-2-50: 0.882 (0.922)\n",
      "-10: 3-3-50: 0.549 (0.569)\n",
      "-11: 2-2-16: 0.843 (0.882)\n",
      "-11: 2-3-8: 0.784 (0.804)\n",
      "-11: 2-3-16: 0.765 (0.843)\n",
      "-11: 2-3-32: 0.804 (0.882)\n",
      "-11: 2-4-16: 0.824 (0.863)\n",
      "-11: 3-2-16: 0.863 (0.882)\n",
      "-11: 4-2-16: 0.824 (0.882)\n",
      "-11: 3-3-16: 0.804 (0.843)\n",
      "-11: 4-3-16: 0.765 (0.843)\n",
      "-11: 2-2-8: 0.725 (0.784)\n",
      "-11: 2-2-32: 0.824 (0.882)\n",
      "-11: 2-2-50: 0.824 (0.882)\n",
      "-11: 2-2-75: 0.863 (0.882)\n",
      "-11: 2-2-100: 0.843 (0.882)\n",
      "-11: 2-3-32: 0.863 (0.882)\n",
      "-11: 2-3-50: 0.804 (0.863)\n",
      "-11: 2-3-75: 0.784 (0.882)\n",
      "-11: 2-3-100: 0.824 (0.863)\n",
      "-11: 3-2-50: 0.784 (0.882)\n",
      "-11: 3-3-50: 0.843 (0.902)\n",
      "-12: 2-2-16: 0.804 (0.902)\n",
      "-12: 2-3-8: 0.843 (0.902)\n",
      "-12: 2-3-16: 0.824 (0.922)\n",
      "-12: 2-3-32: 0.863 (0.922)\n",
      "-12: 2-4-16: 0.824 (0.902)\n",
      "-12: 3-2-16: 0.804 (0.922)\n",
      "-12: 4-2-16: 0.863 (0.922)\n",
      "-12: 3-3-16: 0.843 (0.922)\n",
      "-12: 4-3-16: 0.843 (0.843)\n",
      "-12: 2-2-8: 0.863 (0.902)\n",
      "-12: 2-2-32: 0.843 (0.922)\n",
      "-12: 2-2-50: 0.882 (0.941)\n",
      "-12: 2-2-75: 0.824 (0.922)\n",
      "-12: 2-2-100: 0.804 (0.902)\n",
      "-12: 2-3-32: 0.863 (0.922)\n",
      "-12: 2-3-50: 0.843 (0.922)\n",
      "-12: 2-3-75: 0.863 (0.941)\n",
      "-12: 2-3-100: 0.863 (0.922)\n",
      "-12: 3-2-50: 0.863 (0.922)\n",
      "-12: 3-3-50: 0.882 (0.922)\n",
      "-13: 2-2-16: 0.863 (0.922)\n",
      "-13: 2-3-8: 0.725 (0.824)\n",
      "-13: 2-3-16: 0.863 (0.922)\n",
      "-13: 2-3-32: 0.863 (0.922)\n",
      "-13: 2-4-16: 0.902 (0.941)\n",
      "-13: 3-2-16: 0.882 (0.941)\n",
      "-13: 4-2-16: 0.824 (0.824)\n",
      "-13: 3-3-16: 0.843 (0.902)\n",
      "-13: 4-3-16: 0.745 (0.765)\n",
      "-13: 2-2-8: 0.667 (0.725)\n",
      "-13: 2-2-32: 0.882 (0.922)\n",
      "-13: 2-2-50: 0.863 (0.941)\n",
      "-13: 2-2-75: 0.804 (0.922)\n",
      "-13: 2-2-100: 0.882 (0.922)\n",
      "-13: 2-3-32: 0.882 (0.922)\n",
      "-13: 2-3-50: 0.863 (0.922)\n",
      "-13: 2-3-75: 0.863 (0.941)\n",
      "-13: 2-3-100: 0.902 (0.941)\n",
      "-13: 3-2-50: 0.922 (0.922)\n",
      "-13: 3-3-50: 0.902 (0.922)\n",
      "-14: 2-2-16: 0.843 (0.961)\n",
      "-14: 2-3-8: 0.824 (0.882)\n",
      "-14: 2-3-16: 0.882 (0.961)\n",
      "-14: 2-3-32: 0.902 (0.941)\n",
      "-14: 2-4-16: 0.843 (0.902)\n",
      "-14: 3-2-16: 0.902 (0.961)\n",
      "-14: 4-2-16: 0.863 (0.902)\n",
      "-14: 3-3-16: 0.863 (0.922)\n",
      "-14: 4-3-16: 0.824 (0.843)\n",
      "-14: 2-2-8: 0.902 (0.941)\n",
      "-14: 2-2-32: 0.882 (0.961)\n",
      "-14: 2-2-50: 0.941 (0.961)\n",
      "-14: 2-2-75: 0.882 (0.980)\n",
      "-14: 2-2-100: 0.902 (0.961)\n",
      "-14: 2-3-32: 0.863 (0.961)\n",
      "-14: 2-3-50: 0.882 (0.961)\n",
      "-14: 2-3-75: 0.882 (0.961)\n",
      "-14: 2-3-100: 0.922 (0.961)\n",
      "-14: 3-2-50: 0.902 (0.961)\n",
      "-14: 3-3-50: 0.882 (0.941)\n",
      "-15: 2-2-16: 0.882 (0.941)\n",
      "-15: 2-3-8: 0.804 (0.843)\n",
      "-15: 2-3-16: 0.863 (0.902)\n",
      "-15: 2-3-32: 0.902 (0.922)\n",
      "-15: 2-4-16: 0.882 (0.902)\n",
      "-15: 3-2-16: 0.882 (0.922)\n",
      "-15: 4-2-16: 0.863 (0.922)\n",
      "-15: 3-3-16: 0.863 (0.941)\n",
      "-15: 4-3-16: 0.863 (0.922)\n",
      "-15: 2-2-8: 0.765 (0.824)\n",
      "-15: 2-2-32: 0.941 (0.941)\n",
      "-15: 2-2-50: 0.843 (0.941)\n",
      "-15: 2-2-75: 0.882 (0.922)\n",
      "-15: 2-2-100: 0.902 (0.941)\n",
      "-15: 2-3-32: 0.941 (0.941)\n",
      "-15: 2-3-50: 0.863 (0.922)\n",
      "-15: 2-3-75: 0.902 (0.922)\n",
      "-15: 2-3-100: 0.902 (0.922)\n",
      "-15: 3-2-50: 0.882 (0.941)\n",
      "-15: 3-3-50: 0.882 (0.941)\n",
      "-16: 2-2-16: 0.804 (0.902)\n",
      "-16: 2-3-8: 0.765 (0.784)\n",
      "-16: 2-3-16: 0.804 (0.882)\n",
      "-16: 2-3-32: 0.882 (0.902)\n",
      "-16: 2-4-16: 0.863 (0.922)\n",
      "-16: 3-2-16: 0.824 (0.863)\n",
      "-16: 4-2-16: 0.804 (0.882)\n",
      "-16: 3-3-16: 0.824 (0.843)\n",
      "-16: 4-3-16: 0.745 (0.784)\n",
      "-16: 2-2-8: 0.706 (0.745)\n",
      "-16: 2-2-32: 0.882 (0.922)\n",
      "-16: 2-2-50: 0.882 (0.922)\n",
      "-16: 2-2-75: 0.843 (0.902)\n",
      "-16: 2-2-100: 0.882 (0.902)\n",
      "-16: 2-3-32: 0.843 (0.922)\n",
      "-16: 2-3-50: 0.843 (0.922)\n",
      "-16: 2-3-75: 0.824 (0.882)\n",
      "-16: 2-3-100: 0.804 (0.902)\n",
      "-16: 3-2-50: 0.863 (0.902)\n",
      "-16: 3-3-50: 0.824 (0.902)\n",
      "-17: 2-2-16: 0.863 (0.922)\n",
      "-17: 2-3-8: 0.824 (0.902)\n",
      "-17: 2-3-16: 0.824 (0.902)\n",
      "-17: 2-3-32: 0.863 (0.922)\n",
      "-17: 2-4-16: 0.882 (0.902)\n",
      "-17: 3-2-16: 0.843 (0.902)\n",
      "-17: 4-2-16: 0.882 (0.922)\n",
      "-17: 3-3-16: 0.863 (0.922)\n",
      "-17: 4-3-16: 0.784 (0.843)\n",
      "-17: 2-2-8: 0.824 (0.882)\n",
      "-17: 2-2-32: 0.882 (0.922)\n",
      "-17: 2-2-50: 0.843 (0.922)\n",
      "-17: 2-2-75: 0.804 (0.902)\n",
      "-17: 2-2-100: 0.804 (0.882)\n",
      "-17: 2-3-32: 0.843 (0.902)\n",
      "-17: 2-3-50: 0.843 (0.902)\n",
      "-17: 2-3-75: 0.882 (0.882)\n",
      "-17: 2-3-100: 0.843 (0.882)\n",
      "-17: 3-2-50: 0.863 (0.922)\n",
      "-17: 3-3-50: 0.863 (0.882)\n",
      "-18: 2-2-16: 0.863 (0.922)\n",
      "-18: 2-3-8: 0.765 (0.824)\n",
      "-18: 2-3-16: 0.804 (0.863)\n",
      "-18: 2-3-32: 0.824 (0.882)\n",
      "-18: 2-4-16: 0.804 (0.863)\n",
      "-18: 3-2-16: 0.804 (0.882)\n",
      "-18: 4-2-16: 0.824 (0.863)\n",
      "-18: 3-3-16: 0.863 (0.882)\n",
      "-18: 4-3-16: 0.843 (0.882)\n",
      "-18: 2-2-8: 0.765 (0.863)\n",
      "-18: 2-2-32: 0.843 (0.902)\n",
      "-18: 2-2-50: 0.843 (0.902)\n",
      "-18: 2-2-75: 0.863 (0.882)\n",
      "-18: 2-2-100: 0.824 (0.882)\n",
      "-18: 2-3-32: 0.902 (0.922)\n",
      "-18: 2-3-50: 0.804 (0.882)\n",
      "-18: 2-3-75: 0.843 (0.902)\n",
      "-18: 2-3-100: 0.843 (0.902)\n",
      "-18: 3-2-50: 0.902 (0.902)\n",
      "-18: 3-3-50: 0.824 (0.902)\n",
      "-19: 2-2-16: 0.784 (0.922)\n",
      "-19: 2-3-8: 0.824 (0.843)\n",
      "-19: 2-3-16: 0.843 (0.882)\n",
      "-19: 2-3-32: 0.824 (0.902)\n",
      "-19: 2-4-16: 0.863 (0.902)\n",
      "-19: 3-2-16: 0.804 (0.863)\n",
      "-19: 4-2-16: 0.843 (0.863)\n",
      "-19: 3-3-16: 0.784 (0.863)\n",
      "-19: 4-3-16: 0.490 (0.490)\n",
      "-19: 2-2-8: 0.824 (0.882)\n",
      "-19: 2-2-32: 0.863 (0.902)\n",
      "-19: 2-2-50: 0.882 (0.902)\n",
      "-19: 2-2-75: 0.843 (0.882)\n",
      "-19: 2-2-100: 0.843 (0.882)\n",
      "-19: 2-3-32: 0.843 (0.882)\n",
      "-19: 2-3-50: 0.824 (0.902)\n",
      "-19: 2-3-75: 0.784 (0.882)\n",
      "-19: 2-3-100: 0.843 (0.902)\n",
      "-19: 3-2-50: 0.863 (0.902)\n",
      "-19: 3-3-50: 0.863 (0.882)\n",
      "-20: 2-2-16: 0.922 (0.941)\n",
      "-20: 2-3-8: 0.824 (0.824)\n",
      "-20: 2-3-16: 0.863 (0.882)\n",
      "-20: 2-3-32: 0.922 (0.922)\n",
      "-20: 2-4-16: 0.843 (0.922)\n",
      "-20: 3-2-16: 0.863 (0.922)\n",
      "-20: 4-2-16: 0.784 (0.863)\n",
      "-20: 3-3-16: 0.843 (0.882)\n",
      "-20: 4-3-16: 0.902 (0.922)\n",
      "-20: 2-2-8: 0.922 (0.922)\n",
      "-20: 2-2-32: 0.902 (0.941)\n",
      "-20: 2-2-50: 0.922 (0.922)\n",
      "-20: 2-2-75: 0.922 (0.922)\n",
      "-20: 2-2-100: 0.882 (0.902)\n",
      "-20: 2-3-32: 0.922 (0.922)\n",
      "-20: 2-3-50: 0.882 (0.902)\n",
      "-20: 2-3-75: 0.902 (0.922)\n",
      "-20: 2-3-100: 0.863 (0.941)\n",
      "-20: 3-2-50: 0.902 (0.922)\n",
      "-20: 3-3-50: 0.882 (0.941)\n",
      "----- 107.77 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'L': 2, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 8},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 8},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 50},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 75},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 100},\n",
    "\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 50},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 75},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 100},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 50},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 50},\n",
    "        ]\n",
    "\n",
    "best_accs3 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs3 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, exp['hid_dim'], OUT_DIM, exp['L'], exp['K'], act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  exp['K'], masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs3[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs3[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}: {best_val_accs3[j,i]:.3f} ({best_accs3[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}' for exp in EXPS]\n",
    "table_over3 = summary_table(best_accs3, index_name)\n",
    "table3 = summary_table(best_val_accs3, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.035130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-8</th>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.042779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.848039</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.035552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.869608</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.032971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-16</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.039703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.851961</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.031296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-16</th>\n",
       "      <td>0.842157</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.029395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-16</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.033448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-16</th>\n",
       "      <td>0.790196</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.110066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-8</th>\n",
       "      <td>0.797059</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.080195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.869608</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.031173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-50</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.047869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-75</th>\n",
       "      <td>0.865686</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.036827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-100</th>\n",
       "      <td>0.855882</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.038361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.867647</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.035007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-50</th>\n",
       "      <td>0.846078</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.029914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-75</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.046359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-100</th>\n",
       "      <td>0.861765</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.037448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-50</th>\n",
       "      <td>0.880392</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.032159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-50</th>\n",
       "      <td>0.832353</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.104213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean accs       med       std\n",
       "2-2-16    0.856863  0.862745  0.035130\n",
       "2-3-8     0.788235  0.794118  0.042779\n",
       "2-3-16    0.848039  0.862745  0.035552\n",
       "2-3-32    0.869608  0.862745  0.032971\n",
       "2-4-16    0.843137  0.852941  0.039703\n",
       "3-2-16    0.851961  0.852941  0.031296\n",
       "4-2-16    0.842157  0.843137  0.029395\n",
       "3-3-16    0.837255  0.843137  0.033448\n",
       "4-3-16    0.790196  0.823529  0.110066\n",
       "2-2-8     0.797059  0.813725  0.080195\n",
       "2-2-32    0.869608  0.872549  0.031173\n",
       "2-2-50    0.866667  0.862745  0.047869\n",
       "2-2-75    0.865686  0.872549  0.036827\n",
       "2-2-100   0.855882  0.852941  0.038361\n",
       "2-3-32    0.867647  0.862745  0.035007\n",
       "2-3-50    0.846078  0.852941  0.029914\n",
       "2-3-75    0.860784  0.862745  0.046359\n",
       "2-3-100   0.861765  0.862745  0.037448\n",
       "3-2-50    0.880392  0.882353  0.032159\n",
       "3-3-50    0.832353  0.872549  0.104213"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.914706</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.024234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-8</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.035076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.900980</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.030042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.915686</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.019706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-16</th>\n",
       "      <td>0.899020</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.023427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.904902</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.027222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-16</th>\n",
       "      <td>0.889216</td>\n",
       "      <td>0.892157</td>\n",
       "      <td>0.031173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-16</th>\n",
       "      <td>0.891176</td>\n",
       "      <td>0.892157</td>\n",
       "      <td>0.031904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-16</th>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.111282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-8</th>\n",
       "      <td>0.844118</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.062153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.922549</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.022761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-50</th>\n",
       "      <td>0.917647</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.026013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-75</th>\n",
       "      <td>0.912745</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.027363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-100</th>\n",
       "      <td>0.907843</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.027099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.914706</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.021724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-50</th>\n",
       "      <td>0.910784</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.024392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-75</th>\n",
       "      <td>0.913725</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.028006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-100</th>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.028074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-50</th>\n",
       "      <td>0.919608</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.020471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-50</th>\n",
       "      <td>0.878431</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.104971</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean accs       med       std\n",
       "2-2-16    0.914706  0.921569  0.024234\n",
       "2-3-8     0.843137  0.833333  0.035076\n",
       "2-3-16    0.900980  0.901961  0.030042\n",
       "2-3-32    0.915686  0.921569  0.019706\n",
       "2-4-16    0.899020  0.901961  0.023427\n",
       "3-2-16    0.904902  0.901961  0.027222\n",
       "4-2-16    0.889216  0.892157  0.031173\n",
       "3-3-16    0.891176  0.892157  0.031904\n",
       "4-3-16    0.829412  0.862745  0.111282\n",
       "2-2-8     0.844118  0.852941  0.062153\n",
       "2-2-32    0.922549  0.921569  0.022761\n",
       "2-2-50    0.917647  0.921569  0.026013\n",
       "2-2-75    0.912745  0.921569  0.027363\n",
       "2-2-100   0.907843  0.901961  0.027099\n",
       "2-3-32    0.914706  0.921569  0.021724\n",
       "2-3-50    0.910784  0.911765  0.024392\n",
       "2-3-75    0.913725  0.921569  0.028006\n",
       "2-3-100   0.911765  0.911765  0.028074\n",
       "3-2-50    0.919608  0.921569  0.020471\n",
       "3-3-50    0.878431  0.901961  0.104971"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_over3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design - Bias False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 2-2-16: 0.824 (0.863)\n",
      "-1: 2-3-16: 0.804 (0.863)\n",
      "-1: 2-4-16: 0.784 (0.863)\n",
      "-1: 3-2-16: 0.784 (0.863)\n",
      "-1: 4-2-16: 0.784 (0.882)\n",
      "-1: 3-3-16: 0.784 (0.824)\n",
      "-1: 4-3-16: 0.843 (0.863)\n",
      "-1: 2-2-8: 0.745 (0.804)\n",
      "-1: 2-2-32: 0.824 (0.882)\n",
      "-1: 2-2-50: 0.863 (0.882)\n",
      "-1: 2-2-75: 0.863 (0.882)\n",
      "-1: 2-2-100: 0.824 (0.882)\n",
      "-1: 2-3-32: 0.863 (0.882)\n",
      "-1: 2-3-50: 0.882 (0.882)\n",
      "-1: 2-3-75: 0.804 (0.843)\n",
      "-1: 2-3-100: 0.804 (0.902)\n",
      "-1: 3-2-50: 0.863 (0.882)\n",
      "-1: 3-3-50: 0.804 (0.882)\n",
      "-2: 2-2-16: 0.902 (0.922)\n",
      "-2: 2-3-16: 0.824 (0.922)\n",
      "-2: 2-4-16: 0.804 (0.902)\n",
      "-2: 3-2-16: 0.863 (0.902)\n",
      "-2: 4-2-16: 0.863 (0.902)\n",
      "-2: 3-3-16: 0.824 (0.902)\n",
      "-2: 4-3-16: 0.824 (0.922)\n",
      "-2: 2-2-8: 0.804 (0.863)\n",
      "-2: 2-2-32: 0.843 (0.922)\n",
      "-2: 2-2-50: 0.882 (0.922)\n",
      "-2: 2-2-75: 0.882 (0.922)\n",
      "-2: 2-2-100: 0.804 (0.902)\n",
      "-2: 2-3-32: 0.804 (0.922)\n",
      "-2: 2-3-50: 0.843 (0.922)\n",
      "-2: 2-3-75: 0.863 (0.922)\n",
      "-2: 2-3-100: 0.804 (0.922)\n",
      "-2: 3-2-50: 0.922 (0.941)\n",
      "-2: 3-3-50: 0.882 (0.922)\n",
      "-3: 2-2-16: 0.882 (0.941)\n",
      "-3: 2-3-16: 0.902 (0.922)\n",
      "-3: 2-4-16: 0.843 (0.922)\n",
      "-3: 3-2-16: 0.843 (0.922)\n",
      "-3: 4-2-16: 0.863 (0.922)\n",
      "-3: 3-3-16: 0.843 (0.902)\n",
      "-3: 4-3-16: 0.510 (0.549)\n",
      "-3: 2-2-8: 0.784 (0.804)\n",
      "-3: 2-2-32: 0.863 (0.922)\n",
      "-3: 2-2-50: 0.824 (0.922)\n",
      "-3: 2-2-75: 0.784 (0.922)\n",
      "-3: 2-2-100: 0.843 (0.922)\n",
      "-3: 2-3-32: 0.882 (0.922)\n",
      "-3: 2-3-50: 0.863 (0.941)\n",
      "-3: 2-3-75: 0.863 (0.922)\n",
      "-3: 2-3-100: 0.882 (0.902)\n",
      "-3: 3-2-50: 0.902 (0.922)\n",
      "-3: 3-3-50: 0.549 (0.549)\n",
      "-4: 2-2-16: 0.902 (0.961)\n",
      "-4: 2-3-16: 0.882 (0.922)\n",
      "-4: 2-4-16: 0.902 (0.902)\n",
      "-4: 3-2-16: 0.902 (0.922)\n",
      "-4: 4-2-16: 0.863 (0.882)\n",
      "-4: 3-3-16: 0.882 (0.902)\n",
      "-4: 4-3-16: 0.745 (0.745)\n",
      "-4: 2-2-8: 0.824 (0.863)\n",
      "-4: 2-2-32: 0.902 (0.961)\n",
      "-4: 2-2-50: 0.902 (0.961)\n",
      "-4: 2-2-75: 0.882 (0.961)\n",
      "-4: 2-2-100: 0.922 (0.961)\n",
      "-4: 2-3-32: 0.902 (0.941)\n",
      "-4: 2-3-50: 0.902 (0.961)\n",
      "-4: 2-3-75: 0.804 (0.941)\n",
      "-4: 2-3-100: 0.941 (0.961)\n",
      "-4: 3-2-50: 0.902 (0.961)\n",
      "-4: 3-3-50: 0.882 (0.922)\n",
      "-5: 2-2-16: 0.882 (0.941)\n",
      "-5: 2-3-16: 0.922 (0.941)\n",
      "-5: 2-4-16: 0.804 (0.882)\n",
      "-5: 3-2-16: 0.902 (0.922)\n",
      "-5: 4-2-16: 0.784 (0.902)\n",
      "-5: 3-3-16: 0.882 (0.922)\n",
      "-5: 4-3-16: 0.843 (0.902)\n",
      "-5: 2-2-8: 0.745 (0.843)\n",
      "-5: 2-2-32: 0.922 (0.941)\n",
      "-5: 2-2-50: 0.882 (0.922)\n",
      "-5: 2-2-75: 0.843 (0.922)\n",
      "-5: 2-2-100: 0.882 (0.922)\n",
      "-5: 2-3-32: 0.863 (0.922)\n",
      "-5: 2-3-50: 0.902 (0.922)\n",
      "-5: 2-3-75: 0.824 (0.922)\n",
      "-5: 2-3-100: 0.902 (0.902)\n",
      "-5: 3-2-50: 0.882 (0.941)\n",
      "-5: 3-3-50: 0.863 (0.902)\n",
      "-6: 2-2-16: 0.863 (0.902)\n",
      "-6: 2-3-16: 0.804 (0.843)\n",
      "-6: 2-4-16: 0.804 (0.843)\n",
      "-6: 3-2-16: 0.745 (0.863)\n",
      "-6: 4-2-16: 0.882 (0.882)\n",
      "-6: 3-3-16: 0.804 (0.804)\n",
      "-6: 4-3-16: 0.765 (0.804)\n",
      "-6: 2-2-8: 0.686 (0.725)\n",
      "-6: 2-2-32: 0.902 (0.922)\n",
      "-6: 2-2-50: 0.863 (0.922)\n",
      "-6: 2-2-75: 0.863 (0.902)\n",
      "-6: 2-2-100: 0.882 (0.922)\n",
      "-6: 2-3-32: 0.843 (0.922)\n",
      "-6: 2-3-50: 0.784 (0.902)\n",
      "-6: 2-3-75: 0.902 (0.902)\n",
      "-6: 2-3-100: 0.843 (0.902)\n",
      "-6: 3-2-50: 0.863 (0.922)\n",
      "-6: 3-3-50: 0.882 (0.882)\n",
      "-7: 2-2-16: 0.843 (0.902)\n",
      "-7: 2-3-16: 0.863 (0.922)\n",
      "-7: 2-4-16: 0.882 (0.902)\n",
      "-7: 3-2-16: 0.902 (0.941)\n",
      "-7: 4-2-16: 0.882 (0.902)\n",
      "-7: 3-3-16: 0.824 (0.902)\n",
      "-7: 4-3-16: 0.882 (0.902)\n",
      "-7: 2-2-8: 0.824 (0.863)\n",
      "-7: 2-2-32: 0.824 (0.902)\n",
      "-7: 2-2-50: 0.863 (0.882)\n",
      "-7: 2-2-75: 0.824 (0.902)\n",
      "-7: 2-2-100: 0.843 (0.902)\n",
      "-7: 2-3-32: 0.863 (0.922)\n",
      "-7: 2-3-50: 0.843 (0.882)\n",
      "-7: 2-3-75: 0.863 (0.902)\n",
      "-7: 2-3-100: 0.902 (0.902)\n",
      "-7: 3-2-50: 0.863 (0.922)\n",
      "-7: 3-3-50: 0.843 (0.922)\n",
      "-8: 2-2-16: 0.824 (0.902)\n",
      "-8: 2-3-16: 0.863 (0.902)\n",
      "-8: 2-4-16: 0.863 (0.882)\n",
      "-8: 3-2-16: 0.824 (0.882)\n",
      "-8: 4-2-16: 0.824 (0.902)\n",
      "-8: 3-3-16: 0.843 (0.882)\n",
      "-8: 4-3-16: 0.824 (0.882)\n",
      "-8: 2-2-8: 0.804 (0.824)\n",
      "-8: 2-2-32: 0.882 (0.922)\n",
      "-8: 2-2-50: 0.863 (0.902)\n",
      "-8: 2-2-75: 0.843 (0.882)\n",
      "-8: 2-2-100: 0.824 (0.922)\n",
      "-8: 2-3-32: 0.863 (0.882)\n",
      "-8: 2-3-50: 0.824 (0.882)\n",
      "-8: 2-3-75: 0.824 (0.902)\n",
      "-8: 2-3-100: 0.843 (0.882)\n",
      "-8: 3-2-50: 0.824 (0.922)\n",
      "-8: 3-3-50: 0.882 (0.922)\n",
      "-9: 2-2-16: 0.804 (0.882)\n",
      "-9: 2-3-16: 0.824 (0.902)\n",
      "-9: 2-4-16: 0.863 (0.882)\n",
      "-9: 3-2-16: 0.863 (0.882)\n",
      "-9: 4-2-16: 0.804 (0.863)\n",
      "-9: 3-3-16: 0.824 (0.843)\n",
      "-9: 4-3-16: 0.824 (0.882)\n",
      "-9: 2-2-8: 0.784 (0.824)\n",
      "-9: 2-2-32: 0.882 (0.922)\n",
      "-9: 2-2-50: 0.843 (0.902)\n",
      "-9: 2-2-75: 0.843 (0.902)\n",
      "-9: 2-2-100: 0.863 (0.882)\n",
      "-9: 2-3-32: 0.902 (0.902)\n",
      "-9: 2-3-50: 0.843 (0.902)\n",
      "-9: 2-3-75: 0.824 (0.902)\n",
      "-9: 2-3-100: 0.863 (0.882)\n",
      "-9: 3-2-50: 0.824 (0.902)\n",
      "-9: 3-3-50: 0.863 (0.902)\n",
      "-10: 2-2-16: 0.902 (0.922)\n",
      "-10: 2-3-16: 0.882 (0.902)\n",
      "-10: 2-4-16: 0.863 (0.882)\n",
      "-10: 3-2-16: 0.902 (0.902)\n",
      "-10: 4-2-16: 0.922 (0.922)\n",
      "-10: 3-3-16: 0.902 (0.941)\n",
      "-10: 4-3-16: 0.824 (0.882)\n",
      "-10: 2-2-8: 0.863 (0.922)\n",
      "-10: 2-2-32: 0.882 (0.922)\n",
      "-10: 2-2-50: 0.902 (0.922)\n",
      "-10: 2-2-75: 0.902 (0.941)\n",
      "-10: 2-2-100: 0.902 (0.922)\n",
      "-10: 2-3-32: 0.902 (0.922)\n",
      "-10: 2-3-50: 0.882 (0.902)\n",
      "-10: 2-3-75: 0.882 (0.922)\n",
      "-10: 2-3-100: 0.902 (0.902)\n",
      "-10: 3-2-50: 0.902 (0.922)\n",
      "-10: 3-3-50: 0.922 (0.922)\n",
      "-11: 2-2-16: 0.824 (0.882)\n",
      "-11: 2-3-16: 0.804 (0.863)\n",
      "-11: 2-4-16: 0.804 (0.843)\n",
      "-11: 3-2-16: 0.745 (0.863)\n",
      "-11: 4-2-16: 0.745 (0.863)\n",
      "-11: 3-3-16: 0.882 (0.882)\n",
      "-11: 4-3-16: 0.804 (0.824)\n",
      "-11: 2-2-8: 0.686 (0.725)\n",
      "-11: 2-2-32: 0.824 (0.882)\n",
      "-11: 2-2-50: 0.863 (0.882)\n",
      "-11: 2-2-75: 0.863 (0.882)\n",
      "-11: 2-2-100: 0.843 (0.882)\n",
      "-11: 2-3-32: 0.804 (0.863)\n",
      "-11: 2-3-50: 0.863 (0.882)\n",
      "-11: 2-3-75: 0.843 (0.863)\n",
      "-11: 2-3-100: 0.745 (0.843)\n",
      "-11: 3-2-50: 0.804 (0.882)\n",
      "-11: 3-3-50: 0.863 (0.922)\n",
      "-12: 2-2-16: 0.863 (0.922)\n",
      "-12: 2-3-16: 0.843 (0.902)\n",
      "-12: 2-4-16: 0.804 (0.941)\n",
      "-12: 3-2-16: 0.804 (0.902)\n",
      "-12: 4-2-16: 0.902 (0.941)\n",
      "-12: 3-3-16: 0.863 (0.922)\n",
      "-12: 4-3-16: 0.863 (0.922)\n",
      "-12: 2-2-8: 0.824 (0.902)\n",
      "-12: 2-2-32: 0.863 (0.922)\n",
      "-12: 2-2-50: 0.863 (0.922)\n",
      "-12: 2-2-75: 0.863 (0.922)\n",
      "-12: 2-2-100: 0.824 (0.922)\n",
      "-12: 2-3-32: 0.843 (0.922)\n",
      "-12: 2-3-50: 0.824 (0.922)\n",
      "-12: 2-3-75: 0.804 (0.922)\n",
      "-12: 2-3-100: 0.843 (0.922)\n",
      "-12: 3-2-50: 0.843 (0.922)\n",
      "-12: 3-3-50: 0.824 (0.902)\n",
      "-13: 2-2-16: 0.882 (0.941)\n",
      "-13: 2-3-16: 0.843 (0.941)\n",
      "-13: 2-4-16: 0.902 (0.941)\n",
      "-13: 3-2-16: 0.843 (0.922)\n",
      "-13: 4-2-16: 0.824 (0.902)\n",
      "-13: 3-3-16: 0.843 (0.922)\n",
      "-13: 4-3-16: 0.902 (0.941)\n",
      "-13: 2-2-8: 0.706 (0.765)\n",
      "-13: 2-2-32: 0.824 (0.922)\n",
      "-13: 2-2-50: 0.824 (0.941)\n",
      "-13: 2-2-75: 0.882 (0.922)\n",
      "-13: 2-2-100: 0.824 (0.922)\n",
      "-13: 2-3-32: 0.863 (0.941)\n",
      "-13: 2-3-50: 0.824 (0.922)\n",
      "-13: 2-3-75: 0.843 (0.922)\n",
      "-13: 2-3-100: 0.824 (0.922)\n",
      "-13: 3-2-50: 0.902 (0.922)\n",
      "-13: 3-3-50: 0.902 (0.922)\n",
      "-14: 2-2-16: 0.941 (0.961)\n",
      "-14: 2-3-16: 0.922 (0.961)\n",
      "-14: 2-4-16: 0.824 (0.882)\n",
      "-14: 3-2-16: 0.843 (0.941)\n",
      "-14: 4-2-16: 0.804 (0.941)\n",
      "-14: 3-3-16: 0.804 (0.882)\n",
      "-14: 4-3-16: 0.784 (0.863)\n",
      "-14: 2-2-8: 0.902 (0.922)\n",
      "-14: 2-2-32: 0.941 (0.961)\n",
      "-14: 2-2-50: 0.902 (0.961)\n",
      "-14: 2-2-75: 0.941 (0.961)\n",
      "-14: 2-2-100: 0.863 (0.941)\n",
      "-14: 2-3-32: 0.941 (0.961)\n",
      "-14: 2-3-50: 0.863 (0.961)\n",
      "-14: 2-3-75: 0.784 (0.941)\n",
      "-14: 2-3-100: 0.902 (0.941)\n",
      "-14: 3-2-50: 0.922 (0.961)\n",
      "-14: 3-3-50: 0.863 (0.961)\n",
      "-15: 2-2-16: 0.882 (0.941)\n",
      "-15: 2-3-16: 0.922 (0.941)\n",
      "-15: 2-4-16: 0.902 (0.902)\n",
      "-15: 3-2-16: 0.882 (0.902)\n",
      "-15: 4-2-16: 0.843 (0.922)\n",
      "-15: 3-3-16: 0.863 (0.902)\n",
      "-15: 4-3-16: 0.843 (0.922)\n",
      "-15: 2-2-8: 0.824 (0.863)\n",
      "-15: 2-2-32: 0.882 (0.941)\n",
      "-15: 2-2-50: 0.824 (0.941)\n",
      "-15: 2-2-75: 0.922 (0.961)\n",
      "-15: 2-2-100: 0.863 (0.922)\n",
      "-15: 2-3-32: 0.922 (0.941)\n",
      "-15: 2-3-50: 0.863 (0.922)\n",
      "-15: 2-3-75: 0.902 (0.922)\n",
      "-15: 2-3-100: 0.882 (0.902)\n",
      "-15: 3-2-50: 0.863 (0.941)\n",
      "-15: 3-3-50: 0.902 (0.922)\n",
      "-16: 2-2-16: 0.843 (0.902)\n",
      "-16: 2-3-16: 0.824 (0.843)\n",
      "-16: 2-4-16: 0.824 (0.863)\n",
      "-16: 3-2-16: 0.824 (0.863)\n",
      "-16: 4-2-16: 0.765 (0.824)\n",
      "-16: 3-3-16: 0.745 (0.804)\n",
      "-16: 4-3-16: 0.667 (0.686)\n",
      "-16: 2-2-8: 0.725 (0.804)\n",
      "-16: 2-2-32: 0.882 (0.922)\n",
      "-16: 2-2-50: 0.824 (0.922)\n",
      "-16: 2-2-75: 0.882 (0.922)\n",
      "-16: 2-2-100: 0.882 (0.922)\n",
      "-16: 2-3-32: 0.804 (0.882)\n",
      "-16: 2-3-50: 0.882 (0.902)\n",
      "-16: 2-3-75: 0.804 (0.902)\n",
      "-16: 2-3-100: 0.863 (0.902)\n",
      "-16: 3-2-50: 0.843 (0.922)\n",
      "-16: 3-3-50: 0.824 (0.882)\n",
      "-17: 2-2-16: 0.863 (0.882)\n",
      "-17: 2-3-16: 0.863 (0.922)\n",
      "-17: 2-4-16: 0.863 (0.902)\n",
      "-17: 3-2-16: 0.863 (0.922)\n",
      "-17: 4-2-16: 0.824 (0.882)\n",
      "-17: 3-3-16: 0.824 (0.902)\n",
      "-17: 4-3-16: 0.451 (0.471)\n",
      "-17: 2-2-8: 0.784 (0.843)\n",
      "-17: 2-2-32: 0.843 (0.902)\n",
      "-17: 2-2-50: 0.843 (0.922)\n",
      "-17: 2-2-75: 0.863 (0.902)\n",
      "-17: 2-2-100: 0.863 (0.882)\n",
      "-17: 2-3-32: 0.902 (0.902)\n",
      "-17: 2-3-50: 0.863 (0.902)\n",
      "-17: 2-3-75: 0.882 (0.902)\n",
      "-17: 2-3-100: 0.863 (0.882)\n",
      "-17: 3-2-50: 0.882 (0.922)\n",
      "-17: 3-3-50: 0.882 (0.902)\n",
      "-18: 2-2-16: 0.843 (0.882)\n",
      "-18: 2-3-16: 0.863 (0.902)\n",
      "-18: 2-4-16: 0.863 (0.902)\n",
      "-18: 3-2-16: 0.843 (0.882)\n",
      "-18: 4-2-16: 0.843 (0.863)\n",
      "-18: 3-3-16: 0.843 (0.863)\n",
      "-18: 4-3-16: 0.745 (0.824)\n",
      "-18: 2-2-8: 0.725 (0.824)\n",
      "-18: 2-2-32: 0.863 (0.882)\n",
      "-18: 2-2-50: 0.824 (0.882)\n",
      "-18: 2-2-75: 0.824 (0.902)\n",
      "-18: 2-2-100: 0.843 (0.902)\n",
      "-18: 2-3-32: 0.843 (0.922)\n",
      "-18: 2-3-50: 0.843 (0.922)\n",
      "-18: 2-3-75: 0.863 (0.882)\n",
      "-18: 2-3-100: 0.882 (0.902)\n",
      "-18: 3-2-50: 0.882 (0.902)\n",
      "-18: 3-3-50: 0.882 (0.922)\n",
      "-19: 2-2-16: 0.882 (0.902)\n",
      "-19: 2-3-16: 0.824 (0.902)\n",
      "-19: 2-4-16: 0.843 (0.882)\n",
      "-19: 3-2-16: 0.824 (0.882)\n",
      "-19: 4-2-16: 0.824 (0.882)\n",
      "-19: 3-3-16: 0.824 (0.863)\n",
      "-19: 4-3-16: 0.843 (0.863)\n",
      "-19: 2-2-8: 0.765 (0.843)\n",
      "-19: 2-2-32: 0.804 (0.902)\n",
      "-19: 2-2-50: 0.843 (0.902)\n",
      "-19: 2-2-75: 0.882 (0.902)\n",
      "-19: 2-2-100: 0.843 (0.902)\n",
      "-19: 2-3-32: 0.902 (0.922)\n",
      "-19: 2-3-50: 0.824 (0.902)\n",
      "-19: 2-3-75: 0.843 (0.902)\n",
      "-19: 2-3-100: 0.804 (0.863)\n",
      "-19: 3-2-50: 0.843 (0.882)\n",
      "-19: 3-3-50: 0.843 (0.902)\n",
      "-20: 2-2-16: 0.902 (0.922)\n",
      "-20: 2-3-16: 0.863 (0.902)\n",
      "-20: 2-4-16: 0.882 (0.902)\n",
      "-20: 3-2-16: 0.863 (0.922)\n",
      "-20: 4-2-16: 0.882 (0.902)\n",
      "-20: 3-3-16: 0.843 (0.922)\n",
      "-20: 4-3-16: 0.510 (0.549)\n",
      "-20: 2-2-8: 0.863 (0.902)\n",
      "-20: 2-2-32: 0.941 (0.941)\n",
      "-20: 2-2-50: 0.922 (0.922)\n",
      "-20: 2-2-75: 0.902 (0.922)\n",
      "-20: 2-2-100: 0.902 (0.941)\n",
      "-20: 2-3-32: 0.902 (0.922)\n",
      "-20: 2-3-50: 0.922 (0.922)\n",
      "-20: 2-3-75: 0.922 (0.922)\n",
      "-20: 2-3-100: 0.863 (0.882)\n",
      "-20: 3-2-50: 0.922 (0.922)\n",
      "-20: 3-3-50: 0.922 (0.922)\n",
      "----- 85.91 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'L': 2, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 8},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 50},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 75},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 100},\n",
    "\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 50},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 75},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 100},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 50},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 50},\n",
    "        ]\n",
    "\n",
    "best_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, exp['hid_dim'], OUT_DIM, exp['L'], exp['K'], act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0, bias=False)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  exp['K'], masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs3b[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs3b[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}: {best_val_accs3b[j,i]:.3f} ({best_accs3b[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}' for exp in EXPS]\n",
    "table_over3b = summary_table(best_accs3b, index_name)\n",
    "table3b = summary_table(best_val_accs3b, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.867647</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.033891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.038273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-16</th>\n",
       "      <td>0.846078</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.036827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.045984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-16</th>\n",
       "      <td>0.836275</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.045680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-16</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.036208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-16</th>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.126770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-8</th>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.058652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.869608</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.038859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-50</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.029672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-75</th>\n",
       "      <td>0.867647</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.035007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-100</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.030440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.870588</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.038423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-50</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.032279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-75</th>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.037512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-100</th>\n",
       "      <td>0.857843</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.044659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-50</th>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.034244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-50</th>\n",
       "      <td>0.853922</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.076313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean accs       med       std\n",
       "2-2-16    0.867647  0.872549  0.033891\n",
       "2-3-16    0.856863  0.862745  0.038273\n",
       "2-4-16    0.846078  0.852941  0.036827\n",
       "3-2-16    0.843137  0.843137  0.045984\n",
       "4-2-16    0.836275  0.833333  0.045680\n",
       "3-3-16    0.837255  0.843137  0.036208\n",
       "4-3-16    0.764706  0.823529  0.126770\n",
       "2-2-8     0.783333  0.784314  0.058652\n",
       "2-2-32    0.869608  0.872549  0.038859\n",
       "2-2-50    0.860784  0.862745  0.029672\n",
       "2-2-75    0.867647  0.862745  0.035007\n",
       "2-2-100   0.856863  0.852941  0.030440\n",
       "2-3-32    0.870588  0.862745  0.038423\n",
       "2-3-50    0.856863  0.862745  0.032279\n",
       "2-3-75    0.847059  0.843137  0.037512\n",
       "2-3-100   0.857843  0.862745  0.044659\n",
       "3-2-50    0.872549  0.872549  0.034244\n",
       "3-3-50    0.853922  0.872549  0.076313"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.913725</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.027311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.031373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-16</th>\n",
       "      <td>0.891176</td>\n",
       "      <td>0.892157</td>\n",
       "      <td>0.025920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.025490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-16</th>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.028006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-16</th>\n",
       "      <td>0.884314</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.038673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-16</th>\n",
       "      <td>0.809804</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.135436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-8</th>\n",
       "      <td>0.836275</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.054507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.919608</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.022270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-50</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.023097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-75</th>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.023914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-100</th>\n",
       "      <td>0.913725</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.020935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.915686</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.023283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-50</th>\n",
       "      <td>0.912745</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.022761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-75</th>\n",
       "      <td>0.907843</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.023283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-100</th>\n",
       "      <td>0.900980</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.025168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-50</th>\n",
       "      <td>0.920588</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.021900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-50</th>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.081177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean accs       med       std\n",
       "2-2-16    0.913725  0.911765  0.027311\n",
       "2-3-16    0.905882  0.901961  0.031373\n",
       "2-4-16    0.891176  0.892157  0.025920\n",
       "3-2-16    0.900000  0.901961  0.025490\n",
       "4-2-16    0.894118  0.901961  0.028006\n",
       "3-3-16    0.884314  0.901961  0.038673\n",
       "4-3-16    0.809804  0.862745  0.135436\n",
       "2-2-8     0.836275  0.843137  0.054507\n",
       "2-2-32    0.919608  0.921569  0.022270\n",
       "2-2-50    0.916667  0.921569  0.023097\n",
       "2-2-75    0.916667  0.921569  0.023914\n",
       "2-2-100   0.913725  0.921569  0.020935\n",
       "2-3-32    0.915686  0.921569  0.023283\n",
       "2-3-50    0.912745  0.911765  0.022761\n",
       "2-3-75    0.907843  0.911765  0.023283\n",
       "2-3-100   0.900980  0.901961  0.025168\n",
       "3-2-50    0.920588  0.921569  0.021900\n",
       "3-3-50    0.894118  0.921569  0.081177"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_over3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearities and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.882)\n",
      "-1: ReLU()-Softmax(dim=1)-NLLLoss(): 0.804 (0.902)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.294 (0.529)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.294 (0.451)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.882)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.863)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.863)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.824 (0.863)\n",
      "-1: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.863)\n",
      "-1: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.882)\n",
      "-1: Identity()-Identity()-CrossEntropyLoss(): 0.608 (0.686)\n",
      "-2: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-2: ReLU()-Softmax(dim=1)-NLLLoss(): 0.922 (0.922)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.529 (0.588)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.471 (0.569)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.922)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.902)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.784 (0.902)\n",
      "-2: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.941)\n",
      "-2: Identity()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-2: Identity()-Identity()-CrossEntropyLoss(): 0.608 (0.725)\n",
      "-3: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.902)\n",
      "-3: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.941)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.510 (0.588)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.412 (0.471)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.902 (0.941)\n",
      "-3: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.922)\n",
      "-3: Identity()-Softmax(dim=1)-NLLLoss(): 0.882 (0.902)\n",
      "-3: Identity()-Identity()-CrossEntropyLoss(): 0.647 (0.647)\n",
      "-4: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.961)\n",
      "-4: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.333 (0.471)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.412 (0.490)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.961)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.902 (0.941)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.902 (0.941)\n",
      "-4: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.961)\n",
      "-4: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.961)\n",
      "-4: Identity()-Identity()-CrossEntropyLoss(): 0.686 (0.706)\n",
      "-5: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.941)\n",
      "-5: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.373 (0.412)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.490 (0.529)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.922)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-5: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.941)\n",
      "-5: Identity()-Softmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-5: Identity()-Identity()-CrossEntropyLoss(): 0.647 (0.686)\n",
      "-6: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.902)\n",
      "-6: ReLU()-Softmax(dim=1)-NLLLoss(): 0.784 (0.902)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.490 (0.569)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.608 (0.647)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.902)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.863)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.804 (0.882)\n",
      "-6: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-6: Identity()-Softmax(dim=1)-NLLLoss(): 0.824 (0.902)\n",
      "-6: Identity()-Identity()-CrossEntropyLoss(): 0.608 (0.627)\n",
      "-7: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-7: ReLU()-Softmax(dim=1)-NLLLoss(): 0.824 (0.882)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.431 (0.549)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.412 (0.510)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.922)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-7: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-7: Identity()-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-7: Identity()-Identity()-CrossEntropyLoss(): 0.725 (0.745)\n",
      "-8: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.941)\n",
      "-8: ReLU()-Softmax(dim=1)-NLLLoss(): 0.882 (0.902)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.392 (0.431)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.451 (0.471)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.882)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.902)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-8: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-8: Identity()-Softmax(dim=1)-NLLLoss(): 0.804 (0.902)\n",
      "-8: Identity()-Identity()-CrossEntropyLoss(): 0.627 (0.627)\n",
      "-9: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.882)\n",
      "-9: ReLU()-Softmax(dim=1)-NLLLoss(): 0.882 (0.902)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.451 (0.471)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.510 (0.627)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.882)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.804 (0.902)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-9: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-9: Identity()-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-9: Identity()-Identity()-CrossEntropyLoss(): 0.667 (0.686)\n",
      "-10: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-10: ReLU()-Softmax(dim=1)-NLLLoss(): 0.902 (0.902)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.588 (0.588)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.451 (0.608)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.902 (0.941)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.922 (0.922)\n",
      "-10: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-10: Identity()-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-10: Identity()-Identity()-CrossEntropyLoss(): 0.392 (0.608)\n",
      "-11: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-11: ReLU()-Softmax(dim=1)-NLLLoss(): 0.784 (0.863)\n",
      "-11: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.353 (0.471)\n",
      "-11: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.412 (0.588)\n",
      "-11: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.863)\n",
      "-11: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.804 (0.863)\n",
      "-11: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-11: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.882)\n",
      "-11: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.882)\n",
      "-11: Identity()-Softmax(dim=1)-NLLLoss(): 0.804 (0.863)\n",
      "-11: Identity()-Identity()-CrossEntropyLoss(): 0.490 (0.588)\n",
      "-12: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-12: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-12: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.451 (0.529)\n",
      "-12: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.569 (0.647)\n",
      "-12: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.941)\n",
      "-12: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-12: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.922)\n",
      "-12: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-12: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-12: Identity()-Softmax(dim=1)-NLLLoss(): 0.824 (0.922)\n",
      "-12: Identity()-Identity()-CrossEntropyLoss(): 0.706 (0.725)\n",
      "-13: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-13: ReLU()-Softmax(dim=1)-NLLLoss(): 0.882 (0.902)\n",
      "-13: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.353 (0.510)\n",
      "-13: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.373 (0.471)\n",
      "-13: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.922)\n",
      "-13: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-13: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.922)\n",
      "-13: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-13: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-13: Identity()-Softmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-13: Identity()-Identity()-CrossEntropyLoss(): 0.529 (0.588)\n",
      "-14: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.961)\n",
      "-14: ReLU()-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-14: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.451 (0.529)\n",
      "-14: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.412 (0.490)\n",
      "-14: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.922 (0.941)\n",
      "-14: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.922 (0.961)\n",
      "-14: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.922 (0.961)\n",
      "-14: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-14: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.961 (0.961)\n",
      "-14: Identity()-Softmax(dim=1)-NLLLoss(): 0.882 (0.941)\n",
      "-14: Identity()-Identity()-CrossEntropyLoss(): 0.706 (0.706)\n",
      "-15: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-15: ReLU()-Softmax(dim=1)-NLLLoss(): 0.902 (0.902)\n",
      "-15: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.431 (0.490)\n",
      "-15: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.392 (0.431)\n",
      "-15: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-15: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-15: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-15: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-15: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-15: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-15: Identity()-Identity()-CrossEntropyLoss(): 0.608 (0.667)\n",
      "-16: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-16: ReLU()-Softmax(dim=1)-NLLLoss(): 0.765 (0.824)\n",
      "-16: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.392 (0.529)\n",
      "-16: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.431 (0.510)\n",
      "-16: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-16: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-16: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-16: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.804 (0.843)\n",
      "-16: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-16: Identity()-Softmax(dim=1)-NLLLoss(): 0.902 (0.902)\n",
      "-16: Identity()-Identity()-CrossEntropyLoss(): 0.569 (0.647)\n",
      "-17: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-17: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-17: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.490 (0.588)\n",
      "-17: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.373 (0.529)\n",
      "-17: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.902)\n",
      "-17: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.804 (0.902)\n",
      "-17: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-17: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-17: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-17: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-17: Identity()-Identity()-CrossEntropyLoss(): 0.431 (0.569)\n",
      "-18: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.863)\n",
      "-18: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.863)\n",
      "-18: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.333 (0.490)\n",
      "-18: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.431 (0.490)\n",
      "-18: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-18: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.902)\n",
      "-18: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.882)\n",
      "-18: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.824 (0.863)\n",
      "-18: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-18: Identity()-Softmax(dim=1)-NLLLoss(): 0.804 (0.902)\n",
      "-18: Identity()-Identity()-CrossEntropyLoss(): 0.569 (0.647)\n",
      "-19: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.863)\n",
      "-19: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-19: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.471 (0.569)\n",
      "-19: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.471 (0.627)\n",
      "-19: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.882)\n",
      "-19: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-19: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.863)\n",
      "-19: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.824 (0.902)\n",
      "-19: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-19: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-19: Identity()-Identity()-CrossEntropyLoss(): 0.569 (0.765)\n",
      "-20: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-20: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-20: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.431 (0.588)\n",
      "-20: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.549 (0.608)\n",
      "-20: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-20: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-20: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.882)\n",
      "-20: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-20: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-20: Identity()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-20: Identity()-Identity()-CrossEntropyLoss(): 0.627 (0.647)\n",
      "----- 51.33 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},]\n",
    "\n",
    "best_accs4 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs4 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=exp['act'], last_act=exp['lact'],\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  K, masks, exp['loss'], device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs4[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs4[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}: {best_val_accs4[j,i]:.3f} ({best_accs4[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}' for exp in EXPS]\n",
    "table_over4 = summary_table(best_accs4, index_name)\n",
    "table4 = summary_table(best_val_accs4, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.859804</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.029914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.042237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.427451</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.072998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.446078</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.071475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.851961</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.035875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.853922</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.036931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.853922</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.033087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.034187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.031373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.031859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.600980</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.085979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean accs       med  \\\n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()             0.859804  0.862745   \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                      0.850980  0.843137   \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()          0.427451  0.431373   \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                   0.446078  0.431373   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()     0.851961  0.852941   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()              0.853922  0.843137   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...   0.853922  0.843137   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...   0.850980  0.852941   \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()         0.866667  0.862745   \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                  0.854902  0.852941   \n",
       "Identity()-Identity()-CrossEntropyLoss()             0.600980  0.607843   \n",
       "\n",
       "                                                         std  \n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()            0.029914  \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                     0.042237  \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()         0.072998  \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                  0.071475  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()    0.035875  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()             0.036931  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...  0.033087  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...  0.034187  \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()        0.031373  \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                 0.031859  \n",
       "Identity()-Identity()-CrossEntropyLoss()            0.085979  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.907843</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.028482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.899020</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.025014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.524510</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.053654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.538235</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.068620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.905882</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.023692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.913725</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.022698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.024802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.904902</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.025771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.912745</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.025920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.020092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.664706</td>\n",
       "      <td>0.656863</td>\n",
       "      <td>0.053303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean accs       med  \\\n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()             0.907843  0.901961   \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                      0.899020  0.901961   \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()          0.524510  0.529412   \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                   0.538235  0.519608   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()     0.905882  0.901961   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()              0.913725  0.921569   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...   0.901961  0.901961   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...   0.904902  0.911765   \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()         0.912745  0.921569   \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                  0.911765  0.911765   \n",
       "Identity()-Identity()-CrossEntropyLoss()             0.664706  0.656863   \n",
       "\n",
       "                                                         std  \n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()            0.028482  \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                     0.025014  \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()         0.053654  \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                  0.068620  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()    0.023692  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()             0.022698  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...  0.024802  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...  0.025771  \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()        0.025920  \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                 0.020092  \n",
       "Identity()-Identity()-CrossEntropyLoss()            0.053303  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_over4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the GSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RUNS = 3\n",
    "NORM = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training paramerters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:43: RuntimeWarning: divide by zero encountered in divide\n",
      "  D_inv = np.diag(1/S.sum(1))\n",
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:51: RuntimeWarning: invalid value encountered in matmul\n",
      "  return D_inv_sqr @ S @ D_inv_sqr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 200-0.005-0.005-0.25: 0.020 (0.020)\n",
      "-2: 200-0.005-0.005-0.25: 0.039 (0.039)\n",
      "-3: 200-0.005-0.005-0.25: 0.039 (0.039)\n",
      "----- 0.40 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [\n",
    "    {'epochs': 200, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        ]\n",
    "\n",
    "best_accs5 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs5 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=exp['drop'], init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S, K, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, exp['epochs'], exp['lr'], exp['wd'], epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs5[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs5[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}: {best_val_accs5[j,i]:.3f} ({best_accs5[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}' for exp in EXPS]\n",
    "table_over5 = summary_table(best_accs5, index_name)\n",
    "table5 = summary_table(best_val_accs5, index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.005-0.25</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      mean accs       med       std\n",
       "200-0.005-0.005-0.25    0.03268  0.039216  0.009243"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:43: RuntimeWarning: divide by zero encountered in divide\n",
      "  D_inv = np.diag(1/S.sum(1))\n",
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:51: RuntimeWarning: invalid value encountered in matmul\n",
      "  return D_inv_sqr @ S @ D_inv_sqr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 1-25-25-True: 0.020 (0.020)\n",
      "-2: 1-25-25-True: 0.039 (0.039)\n",
      "-3: 1-25-25-True: 0.039 (0.039)\n",
      "----- 0.40 mins -----\n"
     ]
    }
   ],
   "source": [
    "# h0, norm (norm, not norm, norm H), sep vs joint training, optimizers iters (W vs h)\n",
    "EXPS = [\n",
    "    # {'h0': .001, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "    #     {'h0': .01, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "    #     {'h0': .1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        \n",
    "        # {'h0': .01, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 1, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 10, 'alt': True},\n",
    "        # {'h0': .01, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 50, 'epochs_h': 50, 'epochs_W': 50, 'alt': True},\n",
    "        ]\n",
    "\n",
    "\n",
    "best_accs6 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs6 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=exp['h0'])\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        if not exp['alt']:\n",
    "            model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "            _, acc = model.train(feat, labels, exp['epochs'], LR, WD)\n",
    "        else:\n",
    "            model = GF_NodeClassModel(arch, S,  K, masks, LOSS_FN, device=device)\n",
    "            _, acc = model.train(feat, labels, exp['epochs'], LR, WD, epochs_h=exp['epochs_h'],\n",
    "                                 epochs_W=exp['epochs_W'])\n",
    "\n",
    "        best_accs6[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs6[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"h0\"]}-{exp[\"epochs_h\"]}-{exp[\"epochs_W\"]}-{exp[\"alt\"]}: {best_val_accs6[j,i]:.3f} ({best_accs6[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"h0\"]}-{exp[\"epochs_h\"]}-{exp[\"epochs_W\"]}-{exp[\"alt\"]}' for exp in EXPS]\n",
    "table_over6 = summary_table(best_accs6, index_name)\n",
    "table6 = summary_table(best_val_accs6, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-1-25-25-True</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean accs       med       std\n",
       "200-1-25-25-True    0.03268  0.039216  0.009243"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:43: RuntimeWarning: divide by zero encountered in divide\n",
      "  D_inv = np.diag(1/S.sum(1))\n",
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:51: RuntimeWarning: invalid value encountered in matmul\n",
      "  return D_inv_sqr @ S @ D_inv_sqr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 2-2-16: 0.020 (0.020)\n",
      "-1: 2-3-16: 0.020 (0.020)\n",
      "-1: 2-4-16: 0.020 (0.020)\n",
      "-2: 2-2-16: 0.039 (0.039)\n",
      "-2: 2-3-16: 0.039 (0.039)\n",
      "-2: 2-4-16: 0.039 (0.039)\n",
      "-3: 2-2-16: 0.039 (0.039)\n",
      "-3: 2-3-16: 0.039 (0.039)\n",
      "-3: 2-4-16: 0.039 (0.039)\n",
      "----- 1.20 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'L': 2, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 16},\n",
    "\n",
    "        # {'L': 3, 'K': 2, 'hid_dim': 16},\n",
    "        # {'L': 4, 'K': 2, 'hid_dim': 16},\n",
    "        # {'L': 3, 'K': 3, 'hid_dim': 16},\n",
    "        # {'L': 4, 'K': 3, 'hid_dim': 16},\n",
    "\n",
    "        # {'L': 2, 'K': 2, 'hid_dim': 8},\n",
    "        # {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        # {'L': 2, 'K': 2, 'hid_dim': 50},\n",
    "\n",
    "        # {'L': 2, 'K': 3, 'hid_dim': 8},\n",
    "        # {'L': 2, 'K': 3, 'hid_dim': 32},\n",
    "        # {'L': 2, 'K': 3, 'hid_dim': 50},\n",
    "\n",
    "        # {'L': 3, 'K': 2, 'hid_dim': 8},\n",
    "        # {'L': 3, 'K': 2, 'hid_dim': 32},\n",
    "        # {'L': 3, 'K': 2, 'hid_dim': 50},\n",
    "\n",
    "        # {'L': 3, 'K': 3, 'hid_dim': 8},\n",
    "        # {'L': 3, 'K': 3, 'hid_dim': 32},\n",
    "        # {'L': 3, 'K': 3, 'hid_dim': 50},\n",
    "        ]\n",
    "\n",
    "best_accs7 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs7 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, exp['hid_dim'], OUT_DIM, exp['L'], exp['K'], act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  exp['K'], masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs7[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs7[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}: {best_val_accs7[j,i]:.3f} ({best_accs7[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}' for exp in EXPS]\n",
    "table_over7 = summary_table(best_accs7, index_name)\n",
    "table7 = summary_table(best_val_accs7, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-16</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean accs       med       std\n",
       "2-2-16    0.03268  0.039216  0.009243\n",
       "2-3-16    0.03268  0.039216  0.009243\n",
       "2-4-16    0.03268  0.039216  0.009243"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearities and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:43: RuntimeWarning: divide by zero encountered in divide\n",
      "  D_inv = np.diag(1/S.sum(1))\n",
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:51: RuntimeWarning: invalid value encountered in matmul\n",
      "  return D_inv_sqr @ S @ D_inv_sqr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.020 (0.020)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.020 (0.020)\n",
      "-2: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.039 (0.039)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.039 (0.039)\n",
      "-3: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.039 (0.039)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.039 (0.039)\n",
      "----- 0.80 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        # {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        # {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        # {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        # {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        # {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        # {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        # {'act': nn.Identity(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "    ]\n",
    "\n",
    "best_accs8 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs8 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=exp['act'], last_act=exp['lact'],\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  K, masks, exp['loss'], device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs8[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs8[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}: {best_val_accs8[j,i]:.3f} ({best_accs8[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}' for exp in EXPS]\n",
    "table_over8 = summary_table(best_accs8, index_name)\n",
    "table8 = summary_table(best_val_accs8, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean accs       med  \\\n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()              0.03268  0.039216   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...    0.03268  0.039216   \n",
       "\n",
       "                                                         std  \n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()            0.009243  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...  0.009243  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPS = [\n",
    "        {'name': 'Kipf', 'norm': 'none'},\n",
    "        {'name': 'Kipf', 'norm': 'both'},\n",
    "\n",
    "        {'name': 'A-GCNN', 'norm': False},\n",
    "        {'name': 'A-GCNN', 'norm': True},\n",
    "\n",
    "        {'name': 'H-GCNN', 'norm': False}, # This should be the same as A-GCNN not norm\n",
    "        {'name': 'H-GCNN', 'norm': True},\n",
    "\n",
    "        {'name': 'W-GCN-A', 'norm': False},\n",
    "        {'name': 'W-GCN-A', 'norm': True},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- RUN: 1\n",
      "\tDone at iteration: 156\n",
      "\tKipf-none: acc = 0.529  -  acc2 = 0.529  -  acc (over) = 0.549\n",
      "\tDone at iteration: 188\n",
      "\tKipf-both: acc = 0.569  -  acc2 = 0.549  -  acc (over) = 0.569\n",
      "\tA-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:43: RuntimeWarning: divide by zero encountered in divide\n",
      "  D_inv = np.diag(1/S.sum(1))\n",
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:51: RuntimeWarning: invalid value encountered in matmul\n",
      "  return D_inv_sqr @ S @ D_inv_sqr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tA-GCNN-True: acc = 0.020  -  acc2 = 0.020  -  acc (over) = 0.020\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tH-GCNN-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.882\n",
      "\tW-GCN-A-False: acc = 0.608  -  acc2 = 0.588  -  acc (over) = 0.686\n",
      "\tDone at iteration: 99\n",
      "\tW-GCN-A-True: acc = 0.020  -  acc2 = 0.020  -  acc (over) = 0.020\n",
      "- RUN: 2\n",
      "\tDone at iteration: 105\n",
      "\tKipf-none: acc = 0.235  -  acc2 = 0.235  -  acc (over) = 0.235\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.667  -  acc (over) = 0.706\n",
      "\tA-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.922\n",
      "\tA-GCNN-True: acc = 0.039  -  acc2 = 0.039  -  acc (over) = 0.039\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.843  -  acc2 = 0.882  -  acc (over) = 0.922\n",
      "\tW-GCN-A-False: acc = 0.765  -  acc2 = 0.784  -  acc (over) = 0.804\n",
      "\tDone at iteration: 99\n",
      "\tW-GCN-A-True: acc = 0.039  -  acc2 = 0.039  -  acc (over) = 0.039\n",
      "- RUN: 3\n",
      "\tDone at iteration: 109\n",
      "\tKipf-none: acc = 0.412  -  acc2 = 0.412  -  acc (over) = 0.510\n",
      "\tDone at iteration: 165\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.608  -  acc (over) = 0.627\n",
      "\tA-GCNN-False: acc = 0.863  -  acc2 = 0.902  -  acc (over) = 0.922\n",
      "\tA-GCNN-True: acc = 0.039  -  acc2 = 0.039  -  acc (over) = 0.039\n",
      "\tH-GCNN-False: acc = 0.882  -  acc2 = 0.863  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.667  -  acc2 = 0.667  -  acc (over) = 0.706\n",
      "\tDone at iteration: 99\n",
      "\tW-GCN-A-True: acc = 0.039  -  acc2 = 0.039  -  acc (over) = 0.039\n",
      "- RUN: 4\n",
      "\tDone at iteration: 112\n",
      "\tKipf-none: acc = 0.431  -  acc2 = 0.431  -  acc (over) = 0.431\n",
      "\tKipf-both: acc = 0.529  -  acc2 = 0.431  -  acc (over) = 0.549\n",
      "\tA-GCNN-False: acc = 0.941  -  acc2 = 0.902  -  acc (over) = 0.961\n",
      "\tA-GCNN-True: acc = 0.039  -  acc2 = 0.039  -  acc (over) = 0.039\n",
      "\tH-GCNN-False: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.941\n",
      "\tH-GCNN-True: acc = 0.922  -  acc2 = 0.902  -  acc (over) = 0.961\n",
      "\tW-GCN-A-False: acc = 0.725  -  acc2 = 0.725  -  acc (over) = 0.765\n",
      "\tDone at iteration: 99\n",
      "\tW-GCN-A-True: acc = 0.039  -  acc2 = 0.039  -  acc (over) = 0.039\n",
      "- RUN: 5\n",
      "\tKipf-none: acc = 0.471  -  acc2 = 0.451  -  acc (over) = 0.569\n",
      "\tDone at iteration: 125\n",
      "\tKipf-both: acc = 0.510  -  acc2 = 0.549  -  acc (over) = 0.608\n",
      "\tA-GCNN-False: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.961\n",
      "\tA-GCNN-True: acc = 0.020  -  acc2 = 0.020  -  acc (over) = 0.020\n",
      "\tH-GCNN-False: acc = 0.882  -  acc2 = 0.863  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tDone at iteration: 186\n",
      "\tW-GCN-A-False: acc = 0.686  -  acc2 = 0.686  -  acc (over) = 0.745\n",
      "\tDone at iteration: 99\n",
      "\tW-GCN-A-True: acc = 0.020  -  acc2 = 0.020  -  acc (over) = 0.020\n",
      "- RUN: 6\n",
      "\tDone at iteration: 135\n",
      "\tKipf-none: acc = 0.627  -  acc2 = 0.510  -  acc (over) = 0.667\n",
      "\tDone at iteration: 116\n",
      "\tKipf-both: acc = 0.529  -  acc2 = 0.529  -  acc (over) = 0.588\n",
      "\tA-GCNN-False: acc = 0.902  -  acc2 = 0.804  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.078  -  acc2 = 0.078  -  acc (over) = 0.078\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.804  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.667  -  acc2 = 0.686  -  acc (over) = 0.745\n",
      "\tDone at iteration: 99\n",
      "\tW-GCN-A-True: acc = 0.078  -  acc2 = 0.078  -  acc (over) = 0.078\n",
      "- RUN: 7\n",
      "\tKipf-none: acc = 0.627  -  acc2 = 0.667  -  acc (over) = 0.667\n",
      "\tDone at iteration: 186\n",
      "\tKipf-both: acc = 0.569  -  acc2 = 0.588  -  acc (over) = 0.647\n",
      "\tA-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.039  -  acc2 = 0.039  -  acc (over) = 0.039\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.843  -  acc2 = 0.863  -  acc (over) = 0.882\n",
      "\tW-GCN-A-False: acc = 0.804  -  acc2 = 0.804  -  acc (over) = 0.824\n",
      "\tDone at iteration: 99\n",
      "\tW-GCN-A-True: acc = 0.039  -  acc2 = 0.039  -  acc (over) = 0.039\n",
      "- RUN: 8\n",
      "\tDone at iteration: 117\n",
      "\tKipf-none: acc = 0.510  -  acc2 = 0.510  -  acc (over) = 0.510\n",
      "\tKipf-both: acc = 0.647  -  acc2 = 0.667  -  acc (over) = 0.686\n",
      "\tA-GCNN-False: acc = 0.902  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.059  -  acc2 = 0.059  -  acc (over) = 0.059\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tH-GCNN-True: acc = 0.843  -  acc2 = 0.863  -  acc (over) = 0.863\n",
      "\tW-GCN-A-False: acc = 0.569  -  acc2 = 0.588  -  acc (over) = 0.647\n",
      "\tDone at iteration: 99\n",
      "\tW-GCN-A-True: acc = 0.059  -  acc2 = 0.059  -  acc (over) = 0.059\n",
      "- RUN: 9\n",
      "\tKipf-none: acc = 0.549  -  acc2 = 0.549  -  acc (over) = 0.588\n",
      "\tKipf-both: acc = 0.510  -  acc2 = 0.510  -  acc (over) = 0.588\n",
      "\tA-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.020  -  acc2 = 0.020  -  acc (over) = 0.020\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.843  -  acc2 = 0.863  -  acc (over) = 0.863\n",
      "\tW-GCN-A-False: acc = 0.686  -  acc2 = 0.647  -  acc (over) = 0.706\n",
      "\tDone at iteration: 99\n",
      "\tW-GCN-A-True: acc = 0.020  -  acc2 = 0.020  -  acc (over) = 0.020\n",
      "- RUN: 10\n",
      "\tDone at iteration: 188\n",
      "\tKipf-none: acc = 0.588  -  acc2 = 0.588  -  acc (over) = 0.588\n",
      "\tKipf-both: acc = 0.490  -  acc2 = 0.510  -  acc (over) = 0.588\n",
      "\tA-GCNN-False: acc = 0.863  -  acc2 = 0.843  -  acc (over) = 0.922\n",
      "\tA-GCNN-True: acc = 0.020  -  acc2 = 0.020  -  acc (over) = 0.020\n",
      "\tH-GCNN-False: acc = 0.922  -  acc2 = 0.922  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.765  -  acc2 = 0.765  -  acc (over) = 0.843\n",
      "\tW-GCN-A-False: acc = 0.706  -  acc2 = 0.706  -  acc (over) = 0.725\n",
      "\tDone at iteration: 99\n",
      "\tW-GCN-A-True: acc = 0.020  -  acc2 = 0.020  -  acc (over) = 0.020\n",
      "- RUN: 11\n",
      "\tKipf-none: acc = 0.510  -  acc2 = 0.471  -  acc (over) = 0.549\n",
      "\tDone at iteration: 165\n",
      "\tKipf-both: acc = 0.647  -  acc2 = 0.569  -  acc (over) = 0.647\n",
      "\tA-GCNN-False: acc = 0.882  -  acc2 = 0.824  -  acc (over) = 0.882\n",
      "\tA-GCNN-True: acc = 0.020  -  acc2 = 0.020  -  acc (over) = 0.020\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.824  -  acc (over) = 0.882\n",
      "\tH-GCNN-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.863\n",
      "\tW-GCN-A-False: acc = 0.647  -  acc2 = 0.647  -  acc (over) = 0.706\n",
      "\tDone at iteration: 99\n",
      "\tW-GCN-A-True: acc = 0.020  -  acc2 = 0.020  -  acc (over) = 0.020\n",
      "- RUN: 12\n",
      "\tKipf-none: acc = 0.667  -  acc2 = 0.667  -  acc (over) = 0.686\n",
      "\tDone at iteration: 191\n",
      "\tKipf-both: acc = 0.569  -  acc2 = 0.569  -  acc (over) = 0.627\n",
      "\tA-GCNN-False: acc = 0.902  -  acc2 = 0.902  -  acc (over) = 0.941\n",
      "\tA-GCNN-True: acc = 0.039  -  acc2 = 0.039  -  acc (over) = 0.039\n",
      "\tH-GCNN-False: acc = 0.804  -  acc2 = 0.843  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.922\n",
      "\tDone at iteration: 139\n",
      "\tW-GCN-A-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.843\n",
      "\tDone at iteration: 99\n",
      "\tW-GCN-A-True: acc = 0.039  -  acc2 = 0.039  -  acc (over) = 0.039\n",
      "- RUN: 13\n",
      "\tDone at iteration: 100\n",
      "\tKipf-none: acc = 0.314  -  acc2 = 0.314  -  acc (over) = 0.314\n",
      "\tDone at iteration: 149\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.588  -  acc (over) = 0.627\n",
      "\tA-GCNN-False: acc = 0.863  -  acc2 = 0.882  -  acc (over) = 0.941\n",
      "\tA-GCNN-True: acc = 0.039  -  acc2 = 0.039  -  acc (over) = 0.039\n",
      "\tH-GCNN-False: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.941\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.922\n",
      "\tW-GCN-A-False: acc = 0.725  -  acc2 = 0.725  -  acc (over) = 0.725\n",
      "\tDone at iteration: 99\n",
      "\tW-GCN-A-True: acc = 0.039  -  acc2 = 0.039  -  acc (over) = 0.039\n",
      "- RUN: 14\n",
      "\tKipf-none: acc = 0.392  -  acc2 = 0.431  -  acc (over) = 0.431\n",
      "\tDone at iteration: 199\n",
      "\tKipf-both: acc = 0.529  -  acc2 = 0.529  -  acc (over) = 0.569\n",
      "\tA-GCNN-False: acc = 0.882  -  acc2 = 0.941  -  acc (over) = 0.961\n",
      "\tA-GCNN-True: acc = 0.039  -  acc2 = 0.039  -  acc (over) = 0.039\n",
      "\tH-GCNN-False: acc = 0.765  -  acc2 = 0.765  -  acc (over) = 0.980\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.902  -  acc (over) = 0.941\n",
      "\tW-GCN-A-False: acc = 0.706  -  acc2 = 0.706  -  acc (over) = 0.765\n",
      "\tDone at iteration: 99\n",
      "\tW-GCN-A-True: acc = 0.039  -  acc2 = 0.039  -  acc (over) = 0.039\n",
      "- RUN: 15\n",
      "\tDone at iteration: 102\n",
      "\tKipf-none: acc = 0.392  -  acc2 = 0.392  -  acc (over) = 0.392\n",
      "\tDone at iteration: 154\n",
      "\tKipf-both: acc = 0.549  -  acc2 = 0.510  -  acc (over) = 0.667\n",
      "\tA-GCNN-False: acc = 0.902  -  acc2 = 0.902  -  acc (over) = 0.941\n",
      "\tA-GCNN-True: acc = 0.020  -  acc2 = 0.020  -  acc (over) = 0.020\n",
      "\tH-GCNN-False: acc = 0.824  -  acc2 = 0.804  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.863  -  acc (over) = 0.922\n",
      "\tDone at iteration: 135\n",
      "\tW-GCN-A-False: acc = 0.549  -  acc2 = 0.569  -  acc (over) = 0.745\n",
      "\tDone at iteration: 99\n",
      "\tW-GCN-A-True: acc = 0.020  -  acc2 = 0.020  -  acc (over) = 0.020\n",
      "- RUN: 16\n",
      "\tDone at iteration: 165\n",
      "\tKipf-none: acc = 0.667  -  acc2 = 0.706  -  acc (over) = 0.706\n",
      "\tKipf-both: acc = 0.510  -  acc2 = 0.549  -  acc (over) = 0.588\n",
      "\tA-GCNN-False: acc = 0.863  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tA-GCNN-True: acc = 0.078  -  acc2 = 0.078  -  acc (over) = 0.078\n",
      "\tH-GCNN-False: acc = 0.824  -  acc2 = 0.843  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.824  -  acc2 = 0.784  -  acc (over) = 0.843\n",
      "\tW-GCN-A-False: acc = 0.627  -  acc2 = 0.627  -  acc (over) = 0.686\n",
      "\tDone at iteration: 99\n",
      "\tW-GCN-A-True: acc = 0.078  -  acc2 = 0.078  -  acc (over) = 0.078\n",
      "- RUN: 17\n",
      "\tDone at iteration: 137\n",
      "\tKipf-none: acc = 0.569  -  acc2 = 0.588  -  acc (over) = 0.627\n",
      "\tDone at iteration: 156\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.588  -  acc (over) = 0.647\n",
      "\tA-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.039  -  acc2 = 0.039  -  acc (over) = 0.039\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.843  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.745  -  acc2 = 0.745  -  acc (over) = 0.824\n",
      "\tDone at iteration: 99\n",
      "\tW-GCN-A-True: acc = 0.039  -  acc2 = 0.039  -  acc (over) = 0.039\n",
      "- RUN: 18\n",
      "\tKipf-none: acc = 0.667  -  acc2 = 0.667  -  acc (over) = 0.667\n",
      "\tDone at iteration: 150\n",
      "\tKipf-both: acc = 0.627  -  acc2 = 0.549  -  acc (over) = 0.667\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.059  -  acc2 = 0.059  -  acc (over) = 0.059\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.843\n",
      "\tDone at iteration: 158\n",
      "\tW-GCN-A-False: acc = 0.627  -  acc2 = 0.627  -  acc (over) = 0.706\n",
      "\tDone at iteration: 99\n",
      "\tW-GCN-A-True: acc = 0.059  -  acc2 = 0.059  -  acc (over) = 0.059\n",
      "- RUN: 19\n",
      "\tDone at iteration: 114\n",
      "\tKipf-none: acc = 0.588  -  acc2 = 0.588  -  acc (over) = 0.588\n",
      "\tKipf-both: acc = 0.490  -  acc2 = 0.510  -  acc (over) = 0.569\n",
      "\tA-GCNN-False: acc = 0.843  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.020  -  acc2 = 0.020  -  acc (over) = 0.020\n",
      "\tH-GCNN-False: acc = 0.902  -  acc2 = 0.902  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.863\n",
      "\tDone at iteration: 191\n",
      "\tW-GCN-A-False: acc = 0.667  -  acc2 = 0.627  -  acc (over) = 0.706\n",
      "\tDone at iteration: 99\n",
      "\tW-GCN-A-True: acc = 0.020  -  acc2 = 0.020  -  acc (over) = 0.020\n",
      "- RUN: 20\n",
      "\tDone at iteration: 100\n",
      "\tKipf-none: acc = 0.314  -  acc2 = 0.314  -  acc (over) = 0.314\n",
      "\tKipf-both: acc = 0.510  -  acc2 = 0.510  -  acc (over) = 0.569\n",
      "\tA-GCNN-False: acc = 0.902  -  acc2 = 0.882  -  acc (over) = 0.922\n",
      "\tA-GCNN-True: acc = 0.020  -  acc2 = 0.020  -  acc (over) = 0.020\n",
      "\tH-GCNN-False: acc = 0.882  -  acc2 = 0.902  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.824  -  acc2 = 0.863  -  acc (over) = 0.922\n",
      "\tW-GCN-A-False: acc = 0.745  -  acc2 = 0.745  -  acc (over) = 0.745\n",
      "\tDone at iteration: 99\n",
      "\tW-GCN-A-True: acc = 0.020  -  acc2 = 0.020  -  acc (over) = 0.020\n"
     ]
    }
   ],
   "source": [
    "N_RUNS = 20\n",
    "\n",
    "best_accs = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs2 = np.zeros((len(EXPS), N_RUNS))\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    print(f'- RUN: {i+1}')\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        # t_i = time.time()\n",
    "        if exp['name'] == 'Kipf':\n",
    "            arch = GCNN_2L(IN_DIM, HID_DIM, OUT_DIM, act=ACT, last_act=LAST_ACT,\n",
    "                           dropout=DROPOUT, norm=exp['norm'])\n",
    "            S = dgl.from_networkx(nx.from_numpy_array(A)).add_self_loop().to(device)\n",
    "            \n",
    "        elif exp['name'] == 'A-GCNN':\n",
    "            arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "            dropout=DROPOUT, diff_layer=GFGCNLayer, init_h0=h0)\n",
    "            if exp['norm']:\n",
    "                S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "            else:\n",
    "                S = torch.Tensor(A).to(device)\n",
    "\n",
    "        elif exp['name'] == 'H-GCNN':\n",
    "            arch = GFGCN_Spows(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                               dropout=DROPOUT, norm=exp['norm'], dev=device)\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        elif exp['name'] == 'W-GCN-A':\n",
    "            arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                         dropout=DROPOUT, diff_layer=GFGCN_noh_Layer)\n",
    "            if exp['norm']:\n",
    "                S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "            else:\n",
    "                S = torch.Tensor(A).to(device)  \n",
    "            \n",
    "        else:\n",
    "            raise Exception(f'ERROR: Unknown architecture: {exp[\"name\"]}')\n",
    "\n",
    "        if exp['name'] in ['Kipf', 'W-GCN-A']:\n",
    "            model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "            loss, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "        else:\n",
    "            model = GF_NodeClassModel(arch, S, K, masks, LOSS_FN, device=device)\n",
    "            loss, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                                    epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs[j,i] = model.test(feat, model.S, labels, masks['test'])\n",
    "        best_val_accs2[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'\\t{exp[\"name\"]}-{exp[\"norm\"]}: acc = {best_val_accs[j,i]:.3f}  -  acc2 = {best_val_accs2[j,i]:.3f}  -  acc (over) = {best_accs[j,i]:.3f}')\n",
    "\n",
    "\n",
    "# Print results\n",
    "index_name = [f'{exp[\"name\"]}-{exp[\"norm\"]}' for exp in EXPS]\n",
    "table_comp_over = summary_table(best_accs, index_name)\n",
    "table_comp = summary_table(best_val_accs, index_name)\n",
    "table_comp2 = summary_table(best_val_accs2, index_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kipf-none</th>\n",
       "      <td>0.502941</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.125594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kipf-both</th>\n",
       "      <td>0.560784</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.050905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-False</th>\n",
       "      <td>0.874510</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.027311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-True</th>\n",
       "      <td>0.037255</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.018498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-False</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.034745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-True</th>\n",
       "      <td>0.848039</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.036089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-False</th>\n",
       "      <td>0.688235</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.072017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-True</th>\n",
       "      <td>0.037255</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.018498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean accs       med       std\n",
       "Kipf-none       0.502941  0.519608  0.125594\n",
       "Kipf-both       0.560784  0.558824  0.050905\n",
       "A-GCNN-False    0.874510  0.862745  0.027311\n",
       "A-GCNN-True     0.037255  0.039216  0.018498\n",
       "H-GCNN-False    0.854902  0.852941  0.034745\n",
       "H-GCNN-True     0.848039  0.843137  0.036089\n",
       "W-GCN-A-False   0.688235  0.686275  0.072017\n",
       "W-GCN-A-True    0.037255  0.039216  0.018498"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kipf-none</th>\n",
       "      <td>0.500980</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.127296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kipf-both</th>\n",
       "      <td>0.553922</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.054011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-False</th>\n",
       "      <td>0.867647</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.031541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-True</th>\n",
       "      <td>0.037255</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.018498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-False</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.034244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-True</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.038772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-False</th>\n",
       "      <td>0.687255</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.072701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-True</th>\n",
       "      <td>0.037255</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.018498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean accs       med       std\n",
       "Kipf-none       0.500980  0.509804  0.127296\n",
       "Kipf-both       0.553922  0.549020  0.054011\n",
       "A-GCNN-False    0.867647  0.862745  0.031541\n",
       "A-GCNN-True     0.037255  0.039216  0.018498\n",
       "H-GCNN-False    0.852941  0.843137  0.034244\n",
       "H-GCNN-True     0.849020  0.862745  0.038772\n",
       "W-GCN-A-False   0.687255  0.686275  0.072701\n",
       "W-GCN-A-True    0.037255  0.039216  0.018498"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_comp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kipf-none</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.132842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kipf-both</th>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.043614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-False</th>\n",
       "      <td>0.917647</td>\n",
       "      <td>0.911765</td>\n",
       "      <td>0.025263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-True</th>\n",
       "      <td>0.037255</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.018498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-False</th>\n",
       "      <td>0.915686</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.022442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-True</th>\n",
       "      <td>0.893137</td>\n",
       "      <td>0.901961</td>\n",
       "      <td>0.033663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-False</th>\n",
       "      <td>0.740196</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.050326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-True</th>\n",
       "      <td>0.037255</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.018498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean accs       med       std\n",
       "Kipf-none       0.529412  0.558824  0.132842\n",
       "Kipf-both       0.616667  0.617647  0.043614\n",
       "A-GCNN-False    0.917647  0.911765  0.025263\n",
       "A-GCNN-True     0.037255  0.039216  0.018498\n",
       "H-GCNN-False    0.915686  0.921569  0.022442\n",
       "H-GCNN-True     0.893137  0.901961  0.033663\n",
       "W-GCN-A-False   0.740196  0.735294  0.050326\n",
       "W-GCN-A-True    0.037255  0.039216  0.018498"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_comp_over"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
