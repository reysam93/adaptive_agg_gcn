{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f162202a7d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import sys\n",
    "import os\n",
    "src_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(src_path)\n",
    "\n",
    "from src import utils\n",
    "from src.baselines_models import NodeClassModel, GF_NodeClassModel\n",
    "from src.data import normalize_gso\n",
    "from src.arch import GFGCN, GFGCNLayer, GFGCN_noh_Layer, GFGCN_Spows\n",
    "\n",
    "SEED = 15\n",
    "# PATH = 'results/diff_filters/'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "def summary_table(acc, index_name):\n",
    "    mean_accs = acc.mean(axis=1)\n",
    "    med_accs = np.median(acc, axis=1)\n",
    "    std_accs = acc.std(axis=1)\n",
    "    return DataFrame(np.vstack((mean_accs, med_accs, std_accs)).T, columns=['mean accs', 'med', 'std'], index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: WisconsinDataset\n",
      "Number of nodes: 251\n",
      "Number of features: 1703\n",
      "Shape of signals: torch.Size([251, 1703])\n",
      "Number of classes: 5\n",
      "Norm of A: 22.69361114501953\n",
      "Max value of A: 1.0\n",
      "Proportion of validation data: 0.32\n",
      "Proportion of test data: 0.20\n",
      "Node homophily: 0.13\n",
      "Edge homophily: 0.20\n"
     ]
    }
   ],
   "source": [
    "# Dataset must be from DGL\n",
    "dataset_name = 'WisconsinDataset'\n",
    "\n",
    "A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device,\n",
    "                                                     verb=True)\n",
    "N = A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without normalizing the GSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PARAMETERS  -  0.874\n",
    "## Training params\n",
    "N_RUNS = 10\n",
    "N_EPOCHS = 200  # 100\n",
    "EPOCHS_h = 25\n",
    "EPOCHS_W = 25\n",
    "LR = .005\n",
    "WD = .001\n",
    "DROPOUT = .25\n",
    "\n",
    "# BEST PARAMETERS\n",
    "## Architecture params\n",
    "N_LAYERS = 2\n",
    "K = 3  # 2\n",
    "HID_DIM = 32  # 32\n",
    "\n",
    "## Model params\n",
    "h0 = 1  # 1\n",
    "NORM = False\n",
    "\n",
    "IN_DIM = feat.shape[1]\n",
    "OUT_DIM = n_class\n",
    "\n",
    "ACT = nn.ReLU()   # nn.LeakyReLU() \n",
    "LAST_ACT = nn.Softmax(dim=1)\n",
    "LOSS_FN = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting eval/test acc/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val acc: 0.863  -  Best test acc: 0.843\n",
      "Test acc at best val: 0.804\n",
      "Val acc: 0.863  -  Test acc: 0.804\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = N_EPOCHS\n",
    "epochs_h = EPOCHS_h\n",
    "epochs_W = EPOCHS_W\n",
    "lr =  .005 #LR\n",
    "wd = .005  #WD\n",
    "drop = DROPOUT\n",
    "L = N_LAYERS\n",
    "K_aux = K\n",
    "hid_dim = HID_DIM\n",
    "h0_aux = 1\n",
    "norm = False\n",
    "act = ACT\n",
    "lact = LAST_ACT\n",
    "loss_fn = LOSS_FN\n",
    "patience = 200\n",
    "bias = True\n",
    "\n",
    "iters_aux = 1\n",
    "\n",
    "err1 = np.zeros(iters_aux)\n",
    "err2 = np.zeros(iters_aux)\n",
    "for i in range(iters_aux):\n",
    "\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "\n",
    "    # Create model\n",
    "    arch = GFGCN(IN_DIM, hid_dim, OUT_DIM, L, K_aux, act=act, l_act=lact,\n",
    "                dropout=drop, bias=bias)\n",
    "    S = torch.Tensor(A).to(device)\n",
    "    model = GF_NodeClassModel(arch, S, K, masks, loss_fn, device=device)\n",
    "    loss, acc = model.train(feat, labels, epochs, lr, wd, epochs_h=epochs_h, epochs_W=epochs_W,\n",
    "                            patience=patience)\n",
    "\n",
    "    idx_max_acc = np.argmax(acc[\"val\"])\n",
    "    print(f'Best val acc: {acc[\"val\"][idx_max_acc]:.3f}  -  Best test acc: {np.max(acc[\"test\"]):.3f}')\n",
    "    print(f'Test acc at best val: {acc[\"test\"][idx_max_acc]:.3f}')\n",
    "\n",
    "    acc_val = model.test(feat, S, labels, masks['val'])\n",
    "    acc_test = model.test(feat, S, labels, masks['test'])\n",
    "    print(f'Val acc: {acc_val:.3f}  -  Test acc: {acc_test:.3f}')\n",
    "\n",
    "    err1[i] = acc[\"test\"][idx_max_acc]\n",
    "    err2[i] = acc_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc at best val acc: 0.804 +- 0.000\n",
      "Acc at test: 0.804 +- 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f14c4f6e800>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'Acc at best val acc: {err1.mean():.3f} +- {err1.std():.3f}')\n",
    "print(f'Acc at test: {err2.mean():.3f} +- {err1.std():.3f}')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=[6,4])\n",
    "plt.plot(acc['train'], 'b-', label='Acc train')\n",
    "plt.plot(acc['val'], 'g-', label='Acc val')\n",
    "plt.plot(acc['test'], 'r-', label='Acc test')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=[6,4])\n",
    "plt.plot(loss['train'], 'b-', label='Loss train')\n",
    "plt.plot(loss['val'], 'g-', label='Loss val')\n",
    "plt.plot(loss['test'], 'r-', label='Loss test')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training paramerters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 200-0.005-0.0001-0.25: 0.863 (0.882)\n",
      "-1: 200-0.005-0.0005-0.25: 0.863 (0.882)\n",
      "-1: 200-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-1: 200-0.005-0.001-0.25: 0.824 (0.882)\n",
      "-1: 200-0.005-0.001-0.5: 0.804 (0.882)\n",
      "-1: 200-0.001-0.001-0.25: 0.745 (0.765)\n",
      "-1: 200-0.05-0.005-0.25: 0.804 (0.882)\n",
      "-1: 200-0.01-0.005-0.25: 0.863 (0.863)\n",
      "-1: 200-0.005-0.005-0.25: 0.824 (0.863)\n",
      "-1: 200-0.001-0.005-0.25: 0.784 (0.804)\n",
      "-1: 200-0.05-0.01-0.25: 0.843 (0.882)\n",
      "-1: 200-0.005-0.01-0.25: 0.804 (0.824)\n",
      "-1: 200-0.05-0.01-0.5: 0.784 (0.882)\n",
      "-1: 200-0.05-0.005-0.5: 0.804 (0.882)\n",
      "-1: 200-0.01-0.005-0.5: 0.882 (0.882)\n",
      "-1: 200-0.05-0.01-0: 0.804 (0.843)\n",
      "-1: 200-0.05-0.005-0: 0.804 (0.863)\n",
      "-1: 200-0.01-0.005-0: 0.784 (0.784)\n",
      "-1: 500-0.005-0.001-0.5: 0.824 (0.882)\n",
      "-2: 200-0.005-0.0001-0.25: 0.902 (0.922)\n",
      "-2: 200-0.005-0.0005-0.25: 0.824 (0.922)\n",
      "-2: 200-0.01-0.001-0.25: 0.725 (0.941)\n",
      "-2: 200-0.005-0.001-0.25: 0.784 (0.922)\n",
      "-2: 200-0.005-0.001-0.5: 0.843 (0.922)\n",
      "-2: 200-0.001-0.001-0.25: 0.824 (0.882)\n",
      "-2: 200-0.05-0.005-0.25: 0.863 (0.941)\n",
      "-2: 200-0.01-0.005-0.25: 0.882 (0.902)\n",
      "-2: 200-0.005-0.005-0.25: 0.882 (0.922)\n",
      "-2: 200-0.001-0.005-0.25: 0.902 (0.922)\n",
      "-2: 200-0.05-0.01-0.25: 0.882 (0.922)\n",
      "-2: 200-0.005-0.01-0.25: 0.843 (0.922)\n",
      "-2: 200-0.05-0.01-0.5: 0.882 (0.941)\n",
      "-2: 200-0.05-0.005-0.5: 0.843 (0.902)\n",
      "-2: 200-0.01-0.005-0.5: 0.843 (0.941)\n",
      "-2: 200-0.05-0.01-0: 0.843 (0.922)\n",
      "-2: 200-0.05-0.005-0: 0.824 (0.902)\n",
      "-2: 200-0.01-0.005-0: 0.882 (0.882)\n",
      "-2: 500-0.005-0.001-0.5: 0.824 (0.902)\n",
      "-3: 200-0.005-0.0001-0.25: 0.863 (0.902)\n",
      "-3: 200-0.005-0.0005-0.25: 0.863 (0.922)\n",
      "-3: 200-0.01-0.001-0.25: 0.843 (0.922)\n",
      "-3: 200-0.005-0.001-0.25: 0.922 (0.922)\n",
      "-3: 200-0.005-0.001-0.5: 0.902 (0.941)\n",
      "-3: 200-0.001-0.001-0.25: 0.745 (0.765)\n",
      "-3: 200-0.05-0.005-0.25: 0.843 (0.882)\n",
      "-3: 200-0.01-0.005-0.25: 0.882 (0.922)\n",
      "-3: 200-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-3: 200-0.001-0.005-0.25: 0.863 (0.882)\n",
      "-3: 200-0.05-0.01-0.25: 0.902 (0.922)\n",
      "-3: 200-0.005-0.01-0.25: 0.863 (0.922)\n",
      "-3: 200-0.05-0.01-0.5: 0.824 (0.902)\n",
      "-3: 200-0.05-0.005-0.5: 0.824 (0.902)\n",
      "-3: 200-0.01-0.005-0.5: 0.882 (0.922)\n",
      "-3: 200-0.05-0.01-0: 0.863 (0.902)\n",
      "-3: 200-0.05-0.005-0: 0.863 (0.902)\n",
      "-3: 200-0.01-0.005-0: 0.882 (0.902)\n",
      "-3: 500-0.005-0.001-0.5: 0.882 (0.922)\n",
      "-4: 200-0.005-0.0001-0.25: 0.922 (0.922)\n",
      "-4: 200-0.005-0.0005-0.25: 0.902 (0.941)\n",
      "-4: 200-0.01-0.001-0.25: 0.863 (0.961)\n",
      "-4: 200-0.005-0.001-0.25: 0.941 (0.961)\n",
      "-4: 200-0.005-0.001-0.5: 0.902 (0.961)\n",
      "-4: 200-0.001-0.001-0.25: 0.725 (0.745)\n",
      "-4: 200-0.05-0.005-0.25: 0.824 (0.922)\n",
      "-4: 200-0.01-0.005-0.25: 0.922 (0.961)\n",
      "-4: 200-0.005-0.005-0.25: 0.902 (0.961)\n",
      "-4: 200-0.001-0.005-0.25: 0.843 (0.922)\n",
      "-4: 200-0.05-0.01-0.25: 0.843 (0.922)\n",
      "-4: 200-0.005-0.01-0.25: 0.941 (0.961)\n",
      "-4: 200-0.05-0.01-0.5: 0.863 (0.882)\n",
      "-4: 200-0.05-0.005-0.5: 0.824 (0.863)\n",
      "-4: 200-0.01-0.005-0.5: 0.922 (0.961)\n",
      "-4: 200-0.05-0.01-0: 0.824 (0.922)\n",
      "-4: 200-0.05-0.005-0: 0.824 (0.922)\n",
      "-4: 200-0.01-0.005-0: 0.922 (0.922)\n",
      "-4: 500-0.005-0.001-0.5: 0.902 (0.941)\n",
      "-5: 200-0.005-0.0001-0.25: 0.843 (0.961)\n",
      "-5: 200-0.005-0.0005-0.25: 0.882 (0.941)\n",
      "-5: 200-0.01-0.001-0.25: 0.882 (0.941)\n",
      "-5: 200-0.005-0.001-0.25: 0.882 (0.922)\n",
      "-5: 200-0.005-0.001-0.5: 0.902 (0.922)\n",
      "-5: 200-0.001-0.001-0.25: 0.725 (0.745)\n",
      "-5: 200-0.05-0.005-0.25: 0.882 (0.922)\n",
      "-5: 200-0.01-0.005-0.25: 0.922 (0.922)\n",
      "-5: 200-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-5: 200-0.001-0.005-0.25: 0.843 (0.882)\n",
      "-5: 200-0.05-0.01-0.25: 0.882 (0.922)\n",
      "-5: 200-0.005-0.01-0.25: 0.922 (0.922)\n",
      "-5: 200-0.05-0.01-0.5: 0.882 (0.922)\n",
      "-5: 200-0.05-0.005-0.5: 0.765 (0.922)\n",
      "-5: 200-0.01-0.005-0.5: 0.843 (0.941)\n",
      "-5: 200-0.05-0.01-0: 0.843 (0.922)\n",
      "-5: 200-0.05-0.005-0: 0.882 (0.922)\n",
      "-5: 200-0.01-0.005-0: 0.804 (0.863)\n",
      "-5: 500-0.005-0.001-0.5: 0.863 (0.961)\n",
      "-6: 200-0.005-0.0001-0.25: 0.824 (0.863)\n",
      "-6: 200-0.005-0.0005-0.25: 0.843 (0.902)\n",
      "-6: 200-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-6: 200-0.005-0.001-0.25: 0.882 (0.902)\n",
      "-6: 200-0.005-0.001-0.5: 0.765 (0.843)\n",
      "-6: 200-0.001-0.001-0.25: 0.686 (0.745)\n",
      "-6: 200-0.05-0.005-0.25: 0.804 (0.843)\n",
      "-6: 200-0.01-0.005-0.25: 0.824 (0.902)\n",
      "-6: 200-0.005-0.005-0.25: 0.843 (0.902)\n",
      "-6: 200-0.001-0.005-0.25: 0.863 (0.882)\n",
      "-6: 200-0.05-0.01-0.25: 0.804 (0.863)\n",
      "-6: 200-0.005-0.01-0.25: 0.882 (0.882)\n",
      "-6: 200-0.05-0.01-0.5: 0.784 (0.843)\n",
      "-6: 200-0.05-0.005-0.5: 0.784 (0.843)\n",
      "-6: 200-0.01-0.005-0.5: 0.824 (0.882)\n",
      "-6: 200-0.05-0.01-0: 0.863 (0.863)\n",
      "-6: 200-0.05-0.005-0: 0.863 (0.863)\n",
      "-6: 200-0.01-0.005-0: 0.843 (0.863)\n",
      "-6: 500-0.005-0.001-0.5: 0.824 (0.843)\n",
      "-7: 200-0.005-0.0001-0.25: 0.863 (0.902)\n",
      "-7: 200-0.005-0.0005-0.25: 0.882 (0.922)\n",
      "-7: 200-0.01-0.001-0.25: 0.843 (0.922)\n",
      "-7: 200-0.005-0.001-0.25: 0.882 (0.902)\n",
      "-7: 200-0.005-0.001-0.5: 0.902 (0.941)\n",
      "-7: 200-0.001-0.001-0.25: 0.843 (0.863)\n",
      "-7: 200-0.05-0.005-0.25: 0.843 (0.902)\n",
      "-7: 200-0.01-0.005-0.25: 0.843 (0.902)\n",
      "-7: 200-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-7: 200-0.001-0.005-0.25: 0.843 (0.843)\n",
      "-7: 200-0.05-0.01-0.25: 0.843 (0.902)\n",
      "-7: 200-0.005-0.01-0.25: 0.902 (0.902)\n",
      "-7: 200-0.05-0.01-0.5: 0.863 (0.902)\n",
      "-7: 200-0.05-0.005-0.5: 0.882 (0.902)\n",
      "-7: 200-0.01-0.005-0.5: 0.863 (0.922)\n",
      "-7: 200-0.05-0.01-0: 0.824 (0.882)\n",
      "-7: 200-0.05-0.005-0: 0.824 (0.863)\n",
      "-7: 200-0.01-0.005-0: 0.824 (0.882)\n",
      "-7: 500-0.005-0.001-0.5: 0.863 (0.922)\n",
      "-8: 200-0.005-0.0001-0.25: 0.843 (0.882)\n",
      "-8: 200-0.005-0.0005-0.25: 0.882 (0.941)\n",
      "-8: 200-0.01-0.001-0.25: 0.824 (0.922)\n",
      "-8: 200-0.005-0.001-0.25: 0.843 (0.882)\n",
      "-8: 200-0.005-0.001-0.5: 0.843 (0.922)\n",
      "-8: 200-0.001-0.001-0.25: 0.784 (0.804)\n",
      "-8: 200-0.05-0.005-0.25: 0.824 (0.882)\n",
      "-8: 200-0.01-0.005-0.25: 0.843 (0.882)\n",
      "-8: 200-0.005-0.005-0.25: 0.824 (0.922)\n",
      "-8: 200-0.001-0.005-0.25: 0.784 (0.804)\n",
      "-8: 200-0.05-0.01-0.25: 0.804 (0.863)\n",
      "-8: 200-0.005-0.01-0.25: 0.843 (0.863)\n",
      "-8: 200-0.05-0.01-0.5: 0.824 (0.843)\n",
      "-8: 200-0.05-0.005-0.5: 0.784 (0.863)\n",
      "-8: 200-0.01-0.005-0.5: 0.824 (0.902)\n",
      "-8: 200-0.05-0.01-0: 0.784 (0.882)\n",
      "-8: 200-0.05-0.005-0: 0.784 (0.902)\n",
      "-8: 200-0.01-0.005-0: 0.804 (0.863)\n",
      "-8: 500-0.005-0.001-0.5: 0.863 (0.922)\n",
      "-9: 200-0.005-0.0001-0.25: 0.863 (0.882)\n",
      "-9: 200-0.005-0.0005-0.25: 0.824 (0.902)\n",
      "-9: 200-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-9: 200-0.005-0.001-0.25: 0.843 (0.902)\n",
      "-9: 200-0.005-0.001-0.5: 0.804 (0.882)\n",
      "-9: 200-0.001-0.001-0.25: 0.784 (0.824)\n",
      "-9: 200-0.05-0.005-0.25: 0.824 (0.882)\n",
      "-9: 200-0.01-0.005-0.25: 0.843 (0.922)\n",
      "-9: 200-0.005-0.005-0.25: 0.804 (0.882)\n",
      "-9: 200-0.001-0.005-0.25: 0.824 (0.824)\n",
      "-9: 200-0.05-0.01-0.25: 0.843 (0.863)\n",
      "-9: 200-0.005-0.01-0.25: 0.784 (0.863)\n",
      "-9: 200-0.05-0.01-0.5: 0.824 (0.882)\n",
      "-9: 200-0.05-0.005-0.5: 0.784 (0.863)\n",
      "-9: 200-0.01-0.005-0.5: 0.824 (0.902)\n",
      "-9: 200-0.05-0.01-0: 0.804 (0.882)\n",
      "-9: 200-0.05-0.005-0: 0.824 (0.863)\n",
      "-9: 200-0.01-0.005-0: 0.824 (0.843)\n",
      "-9: 500-0.005-0.001-0.5: 0.843 (0.902)\n",
      "-10: 200-0.005-0.0001-0.25: 0.902 (0.922)\n",
      "-10: 200-0.005-0.0005-0.25: 0.902 (0.922)\n",
      "-10: 200-0.01-0.001-0.25: 0.922 (0.922)\n",
      "-10: 200-0.005-0.001-0.25: 0.922 (0.922)\n",
      "-10: 200-0.005-0.001-0.5: 0.882 (0.922)\n",
      "-10: 200-0.001-0.001-0.25: 0.804 (0.804)\n",
      "-10: 200-0.05-0.005-0.25: 0.922 (0.922)\n",
      "-10: 200-0.01-0.005-0.25: 0.824 (0.922)\n",
      "-10: 200-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-10: 200-0.001-0.005-0.25: 0.824 (0.824)\n",
      "-10: 200-0.05-0.01-0.25: 0.902 (0.922)\n",
      "-10: 200-0.005-0.01-0.25: 0.882 (0.882)\n",
      "-10: 200-0.05-0.01-0.5: 0.824 (0.902)\n",
      "-10: 200-0.05-0.005-0.5: 0.843 (0.902)\n",
      "-10: 200-0.01-0.005-0.5: 0.863 (0.922)\n",
      "-10: 200-0.05-0.01-0: 0.902 (0.902)\n",
      "-10: 200-0.05-0.005-0: 0.902 (0.922)\n",
      "-10: 200-0.01-0.005-0: 0.843 (0.863)\n",
      "-10: 500-0.005-0.001-0.5: 0.882 (0.922)\n",
      "----- 62.25 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-4, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-4, 'drop': .25},\n",
    "\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "        \n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-2, 'drop': .25},\n",
    "\n",
    "        # {'epochs': 500, 'lr': .01, 'wd': 1e-3, 'drop': .25},\n",
    "        # {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        # {'epochs': 500, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "\n",
    "        # {'epochs': 500, 'lr': .01, 'wd': 5e-3, 'drop': .25},\n",
    "        # {'epochs': 500, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        # {'epochs': 500, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "\n",
    "\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': 0},\n",
    "\n",
    "        \n",
    "        # {'epochs': 500, 'lr': .005, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        # {'epochs': 500, 'lr': .001, 'wd': 1e-3, 'drop': .5},\n",
    "\n",
    "        # {'epochs': 750, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        # {'epochs': 750, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "        # {'epochs': 750, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "        ]\n",
    "\n",
    "best_accs1 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs1 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, l_act=LAST_ACT,\n",
    "                     dropout=exp['drop'], init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S, K, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, exp['epochs'], exp['lr'], exp['wd'],\n",
    "                             epochs_h=EPOCHS_h, epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs1[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs1[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}: {best_val_accs1[j,i]:.3f} ({best_accs1[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}' for exp in EXPS]\n",
    "table_over1 = summary_table(best_accs1, index_name)\n",
    "table1 = summary_table(best_val_accs1, index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0001-0.25</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.029149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0005-0.25</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.027451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.25</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.047222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.25</th>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.046607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.5</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.048189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.001-0.25</th>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.046772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0.25</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.035076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.25</th>\n",
       "      <td>0.864706</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.034467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.005-0.25</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.028074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.25</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.034018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.25</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.034187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.01-0.25</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.047059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.5</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.034187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0.5</th>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.034244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.5</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.030440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.033044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0</th>\n",
       "      <td>0.839216</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.034856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.040612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0.5</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.026380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean accs       med       std\n",
       "200-0.005-0.0001-0.25   0.868627  0.862745  0.029149\n",
       "200-0.005-0.0005-0.25   0.866667  0.872549  0.027451\n",
       "200-0.01-0.001-0.25     0.843137  0.843137  0.047222\n",
       "200-0.005-0.001-0.25    0.872549  0.882353  0.046607\n",
       "200-0.005-0.001-0.5     0.854902  0.862745  0.048189\n",
       "200-0.001-0.001-0.25    0.766667  0.764706  0.046772\n",
       "200-0.05-0.005-0.25     0.843137  0.833333  0.035076\n",
       "200-0.01-0.005-0.25     0.864706  0.852941  0.034467\n",
       "200-0.005-0.005-0.25    0.852941  0.862745  0.028074\n",
       "200-0.001-0.005-0.25    0.837255  0.843137  0.034018\n",
       "200-0.05-0.01-0.25      0.854902  0.843137  0.034187\n",
       "200-0.005-0.01-0.25     0.866667  0.872549  0.047059\n",
       "200-0.05-0.01-0.5       0.835294  0.823529  0.034187\n",
       "200-0.05-0.005-0.5      0.813725  0.813725  0.034244\n",
       "200-0.01-0.005-0.5      0.856863  0.852941  0.030440\n",
       "200-0.05-0.01-0         0.835294  0.833333  0.033044\n",
       "200-0.05-0.005-0        0.839216  0.823529  0.034856\n",
       "200-0.01-0.005-0        0.841176  0.833333  0.040612\n",
       "500-0.005-0.001-0.5     0.856863  0.862745  0.026380"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 1-1-1-False: 0.490 (0.569)\n",
      "-1: 1-1-1-False: 0.588 (0.588)\n",
      "-1: 1-1-1-False: 0.608 (0.647)\n",
      "-1: 1-1-1-False: 0.804 (0.804)\n",
      "-1: 1-1-1-True: 0.451 (0.569)\n",
      "-1: 1-10-10-True: 0.863 (0.882)\n",
      "-1: 1-25-25-True: 0.824 (0.863)\n",
      "-1: 1-50-50-True: 0.804 (0.882)\n",
      "-1: 1-100-100-True: 0.824 (0.902)\n",
      "-1: 1-25-25-True: 0.882 (0.902)\n",
      "-1: 1-10-5-True: 0.784 (0.824)\n",
      "-1: 1-25-5-True: 0.804 (0.843)\n",
      "-1: 1-5-10-True: 0.804 (0.843)\n",
      "-1: 1-5-25-True: 0.765 (0.804)\n",
      "-2: 1-1-1-False: 0.647 (0.706)\n",
      "-2: 1-1-1-False: 0.784 (0.882)\n",
      "-2: 1-1-1-False: 0.608 (0.706)\n",
      "-2: 1-1-1-False: 0.529 (0.667)\n",
      "-2: 1-1-1-True: 0.510 (0.667)\n",
      "-2: 1-10-10-True: 0.843 (0.941)\n",
      "-2: 1-25-25-True: 0.882 (0.941)\n",
      "-2: 1-50-50-True: 0.843 (0.922)\n",
      "-2: 1-100-100-True: 0.824 (0.941)\n",
      "-2: 1-25-25-True: 0.804 (0.922)\n",
      "-2: 1-10-5-True: 0.824 (0.902)\n",
      "-2: 1-25-5-True: 0.882 (0.882)\n",
      "-2: 1-5-10-True: 0.902 (0.922)\n",
      "-2: 1-5-25-True: 0.843 (0.882)\n",
      "-3: 1-1-1-False: 0.471 (0.510)\n",
      "-3: 1-1-1-False: 0.510 (0.569)\n",
      "-3: 1-1-1-False: 0.843 (0.882)\n",
      "-3: 1-1-1-False: 0.765 (0.824)\n",
      "-3: 1-1-1-True: 0.549 (0.569)\n",
      "-3: 1-10-10-True: 0.863 (0.902)\n",
      "-3: 1-25-25-True: 0.882 (0.922)\n",
      "-3: 1-50-50-True: 0.902 (0.922)\n",
      "-3: 1-100-100-True: 0.863 (0.922)\n",
      "-3: 1-25-25-True: 0.863 (0.922)\n",
      "-3: 1-10-5-True: 0.863 (0.922)\n",
      "-3: 1-25-5-True: 0.902 (0.941)\n",
      "-3: 1-5-10-True: 0.804 (0.882)\n",
      "-3: 1-5-25-True: 0.902 (0.902)\n",
      "-4: 1-1-1-False: 0.412 (0.529)\n",
      "-4: 1-1-1-False: 0.451 (0.510)\n",
      "-4: 1-1-1-False: 0.392 (0.412)\n",
      "-4: 1-1-1-False: 0.510 (0.569)\n",
      "-4: 1-1-1-True: 0.529 (0.529)\n",
      "-4: 1-10-10-True: 0.902 (0.922)\n",
      "-4: 1-25-25-True: 0.902 (0.961)\n",
      "-4: 1-50-50-True: 0.922 (0.961)\n",
      "-4: 1-100-100-True: 0.902 (0.961)\n",
      "-4: 1-25-25-True: 0.922 (0.941)\n",
      "-4: 1-10-5-True: 0.824 (0.941)\n",
      "-4: 1-25-5-True: 0.902 (0.961)\n",
      "-4: 1-5-10-True: 0.902 (0.922)\n",
      "-4: 1-5-25-True: 0.922 (0.941)\n",
      "-5: 1-1-1-False: 0.431 (0.529)\n",
      "-5: 1-1-1-False: 0.804 (0.863)\n",
      "-5: 1-1-1-False: 0.412 (0.588)\n",
      "-5: 1-1-1-False: 0.392 (0.412)\n",
      "-5: 1-1-1-True: 0.392 (0.392)\n",
      "-5: 1-10-10-True: 0.882 (0.922)\n",
      "-5: 1-25-25-True: 0.843 (0.922)\n",
      "-5: 1-50-50-True: 0.843 (0.941)\n",
      "-5: 1-100-100-True: 0.882 (0.941)\n",
      "-5: 1-25-25-True: 0.863 (0.941)\n",
      "-5: 1-10-5-True: 0.765 (0.843)\n",
      "-5: 1-25-5-True: 0.784 (0.882)\n",
      "-5: 1-5-10-True: 0.784 (0.843)\n",
      "-5: 1-5-25-True: 0.843 (0.902)\n",
      "-6: 1-1-1-False: 0.569 (0.627)\n",
      "-6: 1-1-1-False: 0.765 (0.765)\n",
      "-6: 1-1-1-False: 0.627 (0.647)\n",
      "-6: 1-1-1-False: 0.569 (0.608)\n",
      "-6: 1-1-1-True: 0.588 (0.647)\n",
      "-6: 1-10-10-True: 0.843 (0.863)\n",
      "-6: 1-25-25-True: 0.804 (0.902)\n",
      "-6: 1-50-50-True: 0.882 (0.902)\n",
      "-6: 1-100-100-True: 0.882 (0.902)\n",
      "-6: 1-25-25-True: 0.902 (0.902)\n",
      "-6: 1-10-5-True: 0.745 (0.824)\n",
      "-6: 1-25-5-True: 0.745 (0.804)\n",
      "-6: 1-5-10-True: 0.706 (0.745)\n",
      "-6: 1-5-25-True: 0.784 (0.882)\n",
      "-7: 1-1-1-False: 0.549 (0.588)\n",
      "-7: 1-1-1-False: 0.471 (0.529)\n",
      "-7: 1-1-1-False: 0.529 (0.588)\n",
      "-7: 1-1-1-False: 0.608 (0.667)\n",
      "-7: 1-1-1-True: 0.569 (0.588)\n",
      "-7: 1-10-10-True: 0.824 (0.882)\n",
      "-7: 1-25-25-True: 0.863 (0.902)\n",
      "-7: 1-50-50-True: 0.863 (0.922)\n",
      "-7: 1-100-100-True: 0.863 (0.902)\n",
      "-7: 1-25-25-True: 0.882 (0.922)\n",
      "-7: 1-10-5-True: 0.863 (0.902)\n",
      "-7: 1-25-5-True: 0.843 (0.902)\n",
      "-7: 1-5-10-True: 0.529 (0.569)\n",
      "-7: 1-5-25-True: 0.804 (0.902)\n",
      "-8: 1-1-1-False: 0.490 (0.608)\n",
      "-8: 1-1-1-False: 0.765 (0.784)\n",
      "-8: 1-1-1-False: 0.588 (0.588)\n",
      "-8: 1-1-1-False: 0.529 (0.569)\n",
      "-8: 1-1-1-True: 0.490 (0.569)\n",
      "-8: 1-10-10-True: 0.863 (0.902)\n",
      "-8: 1-25-25-True: 0.824 (0.922)\n",
      "-8: 1-50-50-True: 0.882 (0.902)\n",
      "-8: 1-100-100-True: 0.863 (0.922)\n",
      "-8: 1-25-25-True: 0.804 (0.902)\n",
      "-8: 1-10-5-True: 0.824 (0.824)\n",
      "-8: 1-25-5-True: 0.863 (0.902)\n",
      "-8: 1-5-10-True: 0.784 (0.843)\n",
      "-8: 1-5-25-True: 0.843 (0.882)\n",
      "-9: 1-1-1-False: 0.431 (0.451)\n",
      "-9: 1-1-1-False: 0.765 (0.784)\n",
      "-9: 1-1-1-False: 0.510 (0.549)\n",
      "-9: 1-1-1-False: 0.451 (0.451)\n",
      "-9: 1-1-1-True: 0.471 (0.549)\n",
      "-9: 1-10-10-True: 0.843 (0.882)\n",
      "-9: 1-25-25-True: 0.824 (0.902)\n",
      "-9: 1-50-50-True: 0.863 (0.902)\n",
      "-9: 1-100-100-True: 0.843 (0.882)\n",
      "-9: 1-25-25-True: 0.824 (0.902)\n",
      "-9: 1-10-5-True: 0.804 (0.882)\n",
      "-9: 1-25-5-True: 0.804 (0.902)\n",
      "-9: 1-5-10-True: 0.824 (0.843)\n",
      "-9: 1-5-25-True: 0.824 (0.882)\n",
      "-10: 1-1-1-False: 0.510 (0.529)\n",
      "-10: 1-1-1-False: 0.510 (0.608)\n",
      "-10: 1-1-1-False: 0.804 (0.824)\n",
      "-10: 1-1-1-False: 0.294 (0.353)\n",
      "-10: 1-1-1-True: 0.549 (0.588)\n",
      "-10: 1-10-10-True: 0.863 (0.922)\n",
      "-10: 1-25-25-True: 0.843 (0.922)\n",
      "-10: 1-50-50-True: 0.882 (0.922)\n",
      "-10: 1-100-100-True: 0.882 (0.922)\n",
      "-10: 1-25-25-True: 0.902 (0.922)\n",
      "-10: 1-10-5-True: 0.843 (0.882)\n",
      "-10: 1-25-5-True: 0.902 (0.922)\n",
      "-10: 1-5-10-True: 0.804 (0.863)\n",
      "-10: 1-5-25-True: 0.882 (0.922)\n",
      "----- 34.14 mins -----\n"
     ]
    }
   ],
   "source": [
    "# h0, norm (norm, not norm, norm H), sep vs joint training, optimizers iters (W vs h)\n",
    "EXPS = [\n",
    "        # {'h0': .1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        {'h0': 1, 'epochs': 1000, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        {'h0': 1, 'epochs': 5000, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        {'h0': 1, 'epochs': 10000, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},        \n",
    "\n",
    "        # {'h0': .1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},   # Current best\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 50, 'epochs_W': 50, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 100, 'epochs_W': 100, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 400, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 5, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 5, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 5, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 5, 'epochs_W': 25, 'alt': True},\n",
    "    ]\n",
    "\n",
    "\n",
    "best_accs2 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs2 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, l_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=exp['h0'])\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        if not exp['alt']:\n",
    "            model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "            _, acc = model.train(feat, labels, exp['epochs'], LR, WD)\n",
    "        else:\n",
    "            model = GF_NodeClassModel(arch, S,  K, masks, LOSS_FN, device=device)\n",
    "            _, acc = model.train(feat, labels, exp['epochs'], LR, WD, epochs_h=exp['epochs_h'],\n",
    "                                 epochs_W=exp['epochs_W'])\n",
    "\n",
    "        best_accs2[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs2[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"h0\"]}-{exp[\"epochs_h\"]}-{exp[\"epochs_W\"]}-{exp[\"alt\"]}: {best_val_accs2[j,i]:.3f} ({best_accs2[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"h0\"]}-{exp[\"epochs_h\"]}-{exp[\"epochs_W\"]}-{exp[\"alt\"]}' for exp in EXPS]\n",
    "table_over2 = summary_table(best_accs2, index_name)\n",
    "table2 = summary_table(best_val_accs2, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-1-1-1-False</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.068627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000-1-1-1-False</th>\n",
       "      <td>0.641176</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.139767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000-1-1-1-False</th>\n",
       "      <td>0.592157</td>\n",
       "      <td>0.598039</td>\n",
       "      <td>0.138870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000-1-1-1-False</th>\n",
       "      <td>0.545098</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.147203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-1-True</th>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.056829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-10-True</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.021118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-25-25-True</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.030440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-50-50-True</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.031677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-100-100-True</th>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.024802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400-1-25-25-True</th>\n",
       "      <td>0.864706</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.039654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-5-True</th>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.037461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-25-5-True</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.053339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-5-10-True</th>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.100747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-5-25-True</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.047587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mean accs       med       std\n",
       "200-1-1-1-False      0.500000  0.490196  0.068627\n",
       "1000-1-1-1-False     0.641176  0.676471  0.139767\n",
       "5000-1-1-1-False     0.592157  0.598039  0.138870\n",
       "10000-1-1-1-False    0.545098  0.529412  0.147203\n",
       "200-1-1-1-True       0.509804  0.519608  0.056829\n",
       "200-1-10-10-True     0.858824  0.862745  0.021118\n",
       "200-1-25-25-True     0.849020  0.843137  0.030440\n",
       "200-1-50-50-True     0.868627  0.872549  0.031677\n",
       "200-1-100-100-True   0.862745  0.862745  0.024802\n",
       "400-1-25-25-True     0.864706  0.872549  0.039654\n",
       "200-1-10-5-True      0.813725  0.823529  0.037461\n",
       "200-1-25-5-True      0.843137  0.852941  0.053339\n",
       "200-1-5-10-True      0.784314  0.803922  0.100747\n",
       "200-1-5-25-True      0.841176  0.843137  0.047587"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design - Bias True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 2-2-32: 0.804 (0.882)\n",
      "-1: 2-3-32: 0.843 (0.863)\n",
      "-1: 2-4-32: 0.843 (0.882)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 3-2-32: 0.863 (0.882)\n",
      "-1: 4-2-32: 0.804 (0.882)\n",
      "-1: 5-2-32: 0.804 (0.882)\n",
      "-1: 6-2-32: 0.451 (0.549)\n",
      "-1: 3-3-32: 0.843 (0.863)\n",
      "-1: 4-3-32: 0.824 (0.863)\n",
      "-1: 2-2-8: 0.706 (0.725)\n",
      "-1: 2-2-32: 0.882 (0.902)\n",
      "-1: 2-2-50: 0.804 (0.882)\n",
      "-1: 2-2-75: 0.824 (0.882)\n",
      "-1: 2-2-100: 0.843 (0.882)\n",
      "-1: 2-3-50: 0.824 (0.882)\n",
      "-1: 2-3-75: 0.804 (0.863)\n",
      "-1: 2-3-100: 0.843 (0.882)\n",
      "-1: 3-2-32: 0.843 (0.882)\n",
      "-1: 3-2-50: 0.824 (0.882)\n",
      "-1: 3-3-50: 0.882 (0.882)\n",
      "-2: 2-2-32: 0.882 (0.922)\n",
      "-2: 2-3-32: 0.882 (0.922)\n",
      "-2: 2-4-32: 0.843 (0.902)\n",
      "-2: 3-2-32: 0.882 (0.941)\n",
      "-2: 4-2-32: 0.882 (0.922)\n",
      "-2: 5-2-32: 0.843 (0.941)\n",
      "-2: 6-2-32: 0.824 (0.843)\n",
      "-2: 3-3-32: 0.843 (0.922)\n",
      "-2: 4-3-32: 0.843 (0.922)\n",
      "-2: 2-2-8: 0.824 (0.902)\n",
      "-2: 2-2-32: 0.863 (0.922)\n",
      "-2: 2-2-50: 0.863 (0.922)\n",
      "-2: 2-2-75: 0.843 (0.902)\n",
      "-2: 2-2-100: 0.902 (0.941)\n",
      "-2: 2-3-50: 0.882 (0.922)\n",
      "-2: 2-3-75: 0.863 (0.941)\n",
      "-2: 2-3-100: 0.882 (0.941)\n",
      "-2: 3-2-32: 0.863 (0.922)\n",
      "-2: 3-2-50: 0.882 (0.941)\n",
      "-2: 3-3-50: 0.824 (0.922)\n",
      "-3: 2-2-32: 0.863 (0.922)\n",
      "-3: 2-3-32: 0.863 (0.941)\n",
      "-3: 2-4-32: 0.902 (0.902)\n",
      "-3: 3-2-32: 0.882 (0.941)\n",
      "-3: 4-2-32: 0.863 (0.922)\n",
      "-3: 5-2-32: 0.843 (0.922)\n",
      "-3: 6-2-32: 0.843 (0.902)\n",
      "-3: 3-3-32: 0.824 (0.902)\n",
      "-3: 4-3-32: 0.490 (0.490)\n",
      "-3: 2-2-8: 0.667 (0.784)\n",
      "-3: 2-2-32: 0.882 (0.941)\n",
      "-3: 2-2-50: 0.863 (0.922)\n",
      "-3: 2-2-75: 0.902 (0.922)\n",
      "-3: 2-2-100: 0.882 (0.922)\n",
      "-3: 2-3-50: 0.882 (0.922)\n",
      "-3: 2-3-75: 0.804 (0.922)\n",
      "-3: 2-3-100: 0.804 (0.922)\n",
      "-3: 3-2-32: 0.843 (0.922)\n",
      "-3: 3-2-50: 0.882 (0.922)\n",
      "-3: 3-3-50: 0.863 (0.902)\n",
      "-4: 2-2-32: 0.902 (0.961)\n",
      "-4: 2-3-32: 0.922 (0.961)\n",
      "-4: 2-4-32: 0.882 (0.941)\n",
      "-4: 3-2-32: 0.882 (0.941)\n",
      "-4: 4-2-32: 0.882 (0.961)\n",
      "-4: 5-2-32: 0.902 (0.941)\n",
      "-4: 6-2-32: 0.490 (0.510)\n",
      "-4: 3-3-32: 0.843 (0.882)\n",
      "-4: 4-3-32: 0.451 (0.490)\n",
      "-4: 2-2-8: 0.804 (0.882)\n",
      "-4: 2-2-32: 0.843 (0.961)\n",
      "-4: 2-2-50: 0.902 (0.961)\n",
      "-4: 2-2-75: 0.922 (0.961)\n",
      "-4: 2-2-100: 0.941 (0.961)\n",
      "-4: 2-3-50: 0.843 (0.941)\n",
      "-4: 2-3-75: 0.902 (0.941)\n",
      "-4: 2-3-100: 0.922 (0.961)\n",
      "-4: 3-2-32: 0.882 (0.941)\n",
      "-4: 3-2-50: 0.922 (0.961)\n",
      "-4: 3-3-50: 0.882 (0.941)\n",
      "-5: 2-2-32: 0.882 (0.922)\n",
      "-5: 2-3-32: 0.922 (0.941)\n",
      "-5: 2-4-32: 0.902 (0.941)\n",
      "-5: 3-2-32: 0.882 (0.922)\n",
      "-5: 4-2-32: 0.922 (0.941)\n",
      "-5: 5-2-32: 0.902 (0.922)\n",
      "-5: 6-2-32: 0.490 (0.510)\n",
      "-5: 3-3-32: 0.882 (0.941)\n",
      "-5: 4-3-32: 0.863 (0.902)\n",
      "-5: 2-2-8: 0.725 (0.745)\n",
      "-5: 2-2-32: 0.922 (0.941)\n",
      "-5: 2-2-50: 0.843 (0.922)\n",
      "-5: 2-2-75: 0.843 (0.941)\n",
      "-5: 2-2-100: 0.922 (0.922)\n",
      "-5: 2-3-50: 0.882 (0.922)\n",
      "-5: 2-3-75: 0.902 (0.922)\n",
      "-5: 2-3-100: 0.824 (0.902)\n",
      "-5: 3-2-32: 0.882 (0.922)\n",
      "-5: 3-2-50: 0.902 (0.941)\n",
      "-5: 3-3-50: 0.843 (0.941)\n",
      "-6: 2-2-32: 0.882 (0.922)\n",
      "-6: 2-3-32: 0.824 (0.902)\n",
      "-6: 2-4-32: 0.882 (0.902)\n",
      "-6: 3-2-32: 0.863 (0.922)\n",
      "-6: 4-2-32: 0.765 (0.843)\n",
      "-6: 5-2-32: 0.804 (0.863)\n",
      "-6: 6-2-32: 0.804 (0.863)\n",
      "-6: 3-3-32: 0.804 (0.863)\n",
      "-6: 4-3-32: 0.784 (0.863)\n",
      "-6: 2-2-8: 0.627 (0.745)\n",
      "-6: 2-2-32: 0.843 (0.922)\n",
      "-6: 2-2-50: 0.843 (0.922)\n",
      "-6: 2-2-75: 0.863 (0.922)\n",
      "-6: 2-2-100: 0.863 (0.922)\n",
      "-6: 2-3-50: 0.863 (0.922)\n",
      "-6: 2-3-75: 0.843 (0.902)\n",
      "-6: 2-3-100: 0.843 (0.902)\n",
      "-6: 3-2-32: 0.863 (0.922)\n",
      "-6: 3-2-50: 0.863 (0.902)\n",
      "-6: 3-3-50: 0.588 (0.588)\n",
      "-7: 2-2-32: 0.863 (0.902)\n",
      "-7: 2-3-32: 0.863 (0.902)\n",
      "-7: 2-4-32: 0.863 (0.902)\n",
      "-7: 3-2-32: 0.882 (0.902)\n",
      "-7: 4-2-32: 0.882 (0.902)\n",
      "-7: 5-2-32: 0.863 (0.922)\n",
      "-7: 6-2-32: 0.784 (0.902)\n",
      "-7: 3-3-32: 0.902 (0.902)\n",
      "-7: 4-3-32: 0.529 (0.529)\n",
      "-7: 2-2-8: 0.843 (0.863)\n",
      "-7: 2-2-32: 0.882 (0.922)\n",
      "-7: 2-2-50: 0.882 (0.902)\n",
      "-7: 2-2-75: 0.843 (0.863)\n",
      "-7: 2-2-100: 0.882 (0.882)\n",
      "-7: 2-3-50: 0.863 (0.922)\n",
      "-7: 2-3-75: 0.824 (0.882)\n",
      "-7: 2-3-100: 0.843 (0.902)\n",
      "-7: 3-2-32: 0.843 (0.902)\n",
      "-7: 3-2-50: 0.863 (0.922)\n",
      "-7: 3-3-50: 0.843 (0.902)\n",
      "-8: 2-2-32: 0.843 (0.902)\n",
      "-8: 2-3-32: 0.863 (0.882)\n",
      "-8: 2-4-32: 0.843 (0.922)\n",
      "-8: 3-2-32: 0.882 (0.902)\n",
      "-8: 4-2-32: 0.804 (0.863)\n",
      "-8: 5-2-32: 0.863 (0.922)\n",
      "-8: 6-2-32: 0.843 (0.882)\n",
      "-8: 3-3-32: 0.804 (0.941)\n",
      "-8: 4-3-32: 0.804 (0.902)\n",
      "-8: 2-2-8: 0.882 (0.902)\n",
      "-8: 2-2-32: 0.824 (0.902)\n",
      "-8: 2-2-50: 0.902 (0.941)\n",
      "-8: 2-2-75: 0.863 (0.882)\n",
      "-8: 2-2-100: 0.824 (0.902)\n",
      "-8: 2-3-50: 0.863 (0.902)\n",
      "-8: 2-3-75: 0.863 (0.902)\n",
      "-8: 2-3-100: 0.843 (0.902)\n",
      "-8: 3-2-32: 0.843 (0.922)\n",
      "-8: 3-2-50: 0.824 (0.922)\n",
      "-8: 3-3-50: 0.745 (0.863)\n",
      "-9: 2-2-32: 0.824 (0.902)\n",
      "-9: 2-3-32: 0.882 (0.882)\n",
      "-9: 2-4-32: 0.824 (0.902)\n",
      "-9: 3-2-32: 0.843 (0.902)\n",
      "-9: 4-2-32: 0.784 (0.882)\n",
      "-9: 5-2-32: 0.824 (0.922)\n",
      "-9: 6-2-32: 0.804 (0.882)\n",
      "-9: 3-3-32: 0.824 (0.902)\n",
      "-9: 4-3-32: 0.843 (0.902)\n",
      "-9: 2-2-8: 0.765 (0.824)\n",
      "-9: 2-2-32: 0.843 (0.922)\n",
      "-9: 2-2-50: 0.843 (0.882)\n",
      "-9: 2-2-75: 0.843 (0.922)\n",
      "-9: 2-2-100: 0.824 (0.902)\n",
      "-9: 2-3-50: 0.804 (0.902)\n",
      "-9: 2-3-75: 0.843 (0.882)\n",
      "-9: 2-3-100: 0.863 (0.902)\n",
      "-9: 3-2-32: 0.843 (0.902)\n",
      "-9: 3-2-50: 0.843 (0.882)\n",
      "-9: 3-3-50: 0.824 (0.902)\n",
      "-10: 2-2-32: 0.922 (0.922)\n",
      "-10: 2-3-32: 0.902 (0.922)\n",
      "-10: 2-4-32: 0.902 (0.922)\n",
      "-10: 3-2-32: 0.902 (0.922)\n",
      "-10: 4-2-32: 0.529 (0.569)\n",
      "-10: 5-2-32: 0.863 (0.941)\n",
      "-10: 6-2-32: 0.863 (0.922)\n",
      "-10: 3-3-32: 0.902 (0.922)\n",
      "-10: 4-3-32: 0.902 (0.922)\n",
      "-10: 2-2-8: 0.882 (0.922)\n",
      "-10: 2-2-32: 0.922 (0.922)\n",
      "-10: 2-2-50: 0.882 (0.922)\n",
      "-10: 2-2-75: 0.922 (0.922)\n",
      "-10: 2-2-100: 0.922 (0.922)\n",
      "-10: 2-3-50: 0.882 (0.922)\n",
      "-10: 2-3-75: 0.902 (0.922)\n",
      "-10: 2-3-100: 0.902 (0.922)\n",
      "-10: 3-2-32: 0.882 (0.922)\n",
      "-10: 3-2-50: 0.882 (0.922)\n",
      "-10: 3-3-50: 0.804 (0.824)\n",
      "----- 75.62 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [\n",
    "        # {'L': 2, 'K': 3, 'hid_dim': 8},\n",
    "        # {'L': 2, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 32},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 5, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 6, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 32},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 8},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 50},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 75},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 100},\n",
    "\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 50},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 75},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 100},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 50},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 50},\n",
    "        ]\n",
    "\n",
    "best_accs3 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs3 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, exp['hid_dim'], OUT_DIM, exp['L'], exp['K'], act=ACT, l_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  exp['K'], masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs3[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs3[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}: {best_val_accs3[j,i]:.3f} ({best_accs3[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}' for exp in EXPS]\n",
    "table_over3 = summary_table(best_accs3, index_name)\n",
    "table3 = summary_table(best_val_accs3, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.033735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.876471</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.030440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-32</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.027799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-32</th>\n",
       "      <td>0.876471</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.015314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-32</th>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.106027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5-2-32</th>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.033044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6-2-32</th>\n",
       "      <td>0.719608</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.160509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-32</th>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.034856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-32</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.162922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-8</th>\n",
       "      <td>0.772549</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.084655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.870588</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.031859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-50</th>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.029083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-75</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.033735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-100</th>\n",
       "      <td>0.880392</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.039654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-50</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.026013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-75</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.036367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-100</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.034018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-32</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.017094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-50</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.030440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-50</th>\n",
       "      <td>0.809804</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.083212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean accs       med       std\n",
       "2-2-32    0.866667  0.872549  0.033735\n",
       "2-3-32    0.876471  0.872549  0.030440\n",
       "2-4-32    0.868627  0.872549  0.027799\n",
       "3-2-32    0.876471  0.882353  0.015314\n",
       "4-2-32    0.811765  0.833333  0.106027\n",
       "5-2-32    0.850980  0.852941  0.033044\n",
       "6-2-32    0.719608  0.803922  0.160509\n",
       "3-3-32    0.847059  0.843137  0.034856\n",
       "4-3-32    0.733333  0.813725  0.162922\n",
       "2-2-8     0.772549  0.784314  0.084655\n",
       "2-2-32    0.870588  0.872549  0.031859\n",
       "2-2-50    0.862745  0.862745  0.029083\n",
       "2-2-75    0.866667  0.852941  0.033735\n",
       "2-2-100   0.880392  0.882353  0.039654\n",
       "2-3-50    0.858824  0.862745  0.026013\n",
       "2-3-75    0.854902  0.852941  0.036367\n",
       "2-3-100   0.856863  0.843137  0.034018\n",
       "3-2-32    0.858824  0.852941  0.017094\n",
       "3-2-50    0.868627  0.872549  0.030440\n",
       "3-3-50    0.809804  0.833333  0.083212"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSO, bias, and batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1: orig-True-False: 0.824 (0.882)\n",
      "- 1: trans-True-False: 0.824 (0.882)\n",
      "- 1: sym-True-False: 0.549 (0.549)\n",
      "- 1: orig-True-True: 0.804 (0.824)\n",
      "- 1: trans-True-True: 0.804 (0.863)\n",
      "- 1: sym-True-True: 0.843 (0.863)\n",
      "- 1: orig-False-False: 0.824 (0.882)\n",
      "- 1: sym-False-False: 0.824 (0.863)\n",
      "- 2: orig-True-False: 0.882 (0.902)\n",
      "- 2: trans-True-False: 0.843 (0.882)\n",
      "- 2: sym-True-False: 0.863 (0.902)\n",
      "- 2: orig-True-True: 0.725 (0.863)\n",
      "- 2: trans-True-True: 0.725 (0.824)\n",
      "- 2: sym-True-True: 0.745 (0.863)\n",
      "- 2: orig-False-False: 0.804 (0.941)\n",
      "- 2: sym-False-False: 0.863 (0.902)\n",
      "- 3: orig-True-False: 0.882 (0.922)\n",
      "- 3: trans-True-False: 0.882 (0.941)\n",
      "- 3: sym-True-False: 0.412 (0.412)\n",
      "- 3: orig-True-True: 0.804 (0.882)\n",
      "- 3: trans-True-True: 0.784 (0.863)\n",
      "- 3: sym-True-True: 0.843 (0.902)\n",
      "- 3: orig-False-False: 0.902 (0.922)\n",
      "- 3: sym-False-False: 0.902 (0.922)\n",
      "- 4: orig-True-False: 0.882 (0.961)\n",
      "- 4: trans-True-False: 0.902 (0.941)\n",
      "- 4: sym-True-False: 0.431 (0.431)\n",
      "- 4: orig-True-True: 0.902 (0.941)\n",
      "- 4: trans-True-True: 0.706 (0.922)\n",
      "- 4: sym-True-True: 0.784 (0.961)\n",
      "- 4: orig-False-False: 0.902 (0.941)\n",
      "- 4: sym-False-False: 0.431 (0.431)\n",
      "- 5: orig-True-False: 0.902 (0.922)\n",
      "- 5: trans-True-False: 0.843 (0.843)\n",
      "- 5: sym-True-False: 0.392 (0.392)\n",
      "- 5: orig-True-True: 0.882 (0.941)\n",
      "- 5: trans-True-True: 0.863 (0.863)\n",
      "- 5: sym-True-True: 0.882 (0.941)\n",
      "- 5: orig-False-False: 0.902 (0.941)\n",
      "- 5: sym-False-False: 0.765 (0.863)\n",
      "- 6: orig-True-False: 0.863 (0.902)\n",
      "- 6: trans-True-False: 0.863 (0.902)\n",
      "- 6: sym-True-False: 0.824 (0.863)\n",
      "- 6: orig-True-True: 0.784 (0.882)\n",
      "- 6: trans-True-True: 0.765 (0.824)\n",
      "- 6: sym-True-True: 0.745 (0.843)\n",
      "- 6: orig-False-False: 0.804 (0.902)\n",
      "- 6: sym-False-False: 0.510 (0.529)\n",
      "- 7: orig-True-False: 0.882 (0.902)\n",
      "- 7: trans-True-False: 0.843 (0.882)\n",
      "- 7: sym-True-False: 0.882 (0.882)\n",
      "- 7: orig-True-True: 0.784 (0.902)\n",
      "- 7: trans-True-True: 0.765 (0.843)\n",
      "- 7: sym-True-True: 0.784 (0.843)\n",
      "- 7: orig-False-False: 0.882 (0.922)\n",
      "- 7: sym-False-False: 0.471 (0.471)\n",
      "- 8: orig-True-False: 0.863 (0.902)\n",
      "- 8: trans-True-False: 0.843 (0.922)\n",
      "- 8: sym-True-False: 0.824 (0.922)\n",
      "- 8: orig-True-True: 0.784 (0.843)\n",
      "- 8: trans-True-True: 0.706 (0.824)\n",
      "- 8: sym-True-True: 0.804 (0.902)\n",
      "- 8: orig-False-False: 0.843 (0.902)\n",
      "- 8: sym-False-False: 0.510 (0.510)\n",
      "- 9: orig-True-False: 0.824 (0.902)\n",
      "- 9: trans-True-False: 0.765 (0.843)\n",
      "- 9: sym-True-False: 0.451 (0.471)\n",
      "- 9: orig-True-True: 0.824 (0.882)\n",
      "- 9: trans-True-True: 0.784 (0.882)\n",
      "- 9: sym-True-True: 0.765 (0.863)\n",
      "- 9: orig-False-False: 0.843 (0.902)\n",
      "- 9: sym-False-False: 0.451 (0.451)\n",
      "- 10: orig-True-False: 0.922 (0.922)\n",
      "- 10: trans-True-False: 0.843 (0.882)\n",
      "- 10: sym-True-False: 0.451 (0.451)\n",
      "- 10: orig-True-True: 0.863 (0.882)\n",
      "- 10: trans-True-True: 0.882 (0.922)\n",
      "- 10: sym-True-True: 0.686 (0.686)\n",
      "- 10: orig-False-False: 0.922 (0.922)\n",
      "- 10: sym-False-False: 0.451 (0.451)\n",
      "----- 23.54 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [{'GSO': 'orig', 'bias': True, 'bn': False},\n",
    "        {'GSO': 'trans', 'bias': True, 'bn': False},\n",
    "        {'GSO': 'sym', 'bias': True, 'bn': False},\n",
    "\n",
    "        {'GSO': 'orig', 'bias': True, 'bn': True},\n",
    "        {'GSO': 'trans', 'bias': True, 'bn': True},\n",
    "        {'GSO': 'sym', 'bias': True, 'bn': True},\n",
    "\n",
    "        {'GSO': 'orig', 'bias': False, 'bn': False},\n",
    "        {'GSO': 'sym', 'bias': False, 'bn': False},\n",
    "        ]\n",
    "\n",
    "best_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, l_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0, bias=False, batch_norm=exp['bn'])\n",
    "        \n",
    "        if exp['GSO'] == 'sym':\n",
    "            A_aux = (A + A.T)/2\n",
    "            A_aux = np.where(A_aux > 0, 1, 0)\n",
    "        elif exp['GSO'] == 'trans':\n",
    "            A_aux = A.T\n",
    "        else:\n",
    "            A_aux = A\n",
    "\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A_aux, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A_aux).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  K, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs3b[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs3b[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'- {i+1}: {exp[\"GSO\"]}-{exp[\"bias\"]}-{exp[\"bn\"]}: {best_val_accs3b[j,i]:.3f} ({best_accs3b[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"GSO\"]}-{exp[\"bias\"]}-{exp[\"bn\"]}' for exp in EXPS]\n",
    "table_over3b = summary_table(best_accs3b, index_name)\n",
    "table3b = summary_table(best_val_accs3b, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>orig-True-False</th>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.029412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trans-True-False</th>\n",
       "      <td>0.845098</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.034467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sym-True-False</th>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig-True-True</th>\n",
       "      <td>0.815686</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.050526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trans-True-True</th>\n",
       "      <td>0.778431</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.056863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sym-True-True</th>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.054621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig-False-False</th>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.042054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sym-False-False</th>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.184408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean accs       med       std\n",
       "orig-True-False    0.872549  0.882353  0.029412\n",
       "trans-True-False   0.845098  0.843137  0.034467\n",
       "sym-True-False     0.607843  0.500000  0.200538\n",
       "orig-True-True     0.815686  0.803922  0.050526\n",
       "trans-True-True    0.778431  0.774510  0.056863\n",
       "sym-True-True      0.788235  0.784314  0.054621\n",
       "orig-False-False   0.862745  0.862745  0.042054\n",
       "sym-False-False    0.617647  0.509804  0.184408"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearities and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.863)\n",
      "-1: ReLU()-Softmax(dim=1)-NLLLoss(): 0.882 (0.882)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.412 (0.549)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.431 (0.471)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.843)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.804 (0.863)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.804 (0.882)\n",
      "-1: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.725 (0.824)\n",
      "-1: Identity()-Softmax(dim=1)-NLLLoss(): 0.824 (0.843)\n",
      "-1: Identity()-Identity()-CrossEntropyLoss(): 0.588 (0.588)\n",
      "-2: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.922)\n",
      "-2: ReLU()-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.608 (0.667)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.529 (0.588)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.941)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.941)\n",
      "-2: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.922)\n",
      "-2: Identity()-Softmax(dim=1)-NLLLoss(): 0.824 (0.941)\n",
      "-2: Identity()-Identity()-CrossEntropyLoss(): 0.706 (0.725)\n",
      "-3: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-3: ReLU()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.431 (0.608)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.471 (0.549)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.804 (0.922)\n",
      "-3: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-3: Identity()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-3: Identity()-Identity()-CrossEntropyLoss(): 0.706 (0.706)\n",
      "-4: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.922 (0.961)\n",
      "-4: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.392 (0.549)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.451 (0.627)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.922 (0.961)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.922 (0.961)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.922 (0.961)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-4: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.961)\n",
      "-4: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-4: Identity()-Identity()-CrossEntropyLoss(): 0.784 (0.784)\n",
      "-5: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.882)\n",
      "-5: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.667 (0.706)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.431 (0.510)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.902)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.961)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-5: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-5: Identity()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-5: Identity()-Identity()-CrossEntropyLoss(): 0.608 (0.627)\n",
      "-6: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.902)\n",
      "-6: ReLU()-Softmax(dim=1)-NLLLoss(): 0.882 (0.902)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.294 (0.569)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.569 (0.667)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.902)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-6: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.882)\n",
      "-6: Identity()-Softmax(dim=1)-NLLLoss(): 0.765 (0.843)\n",
      "-6: Identity()-Identity()-CrossEntropyLoss(): 0.588 (0.608)\n",
      "-7: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-7: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.471 (0.529)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.431 (0.490)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.804 (0.922)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.941)\n",
      "-7: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-7: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.863)\n",
      "-7: Identity()-Identity()-CrossEntropyLoss(): 0.824 (0.824)\n",
      "-8: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.902)\n",
      "-8: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.412 (0.471)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.510 (0.569)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.882)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.902)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.902 (0.902)\n",
      "-8: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.863)\n",
      "-8: Identity()-Softmax(dim=1)-NLLLoss(): 0.804 (0.863)\n",
      "-8: Identity()-Identity()-CrossEntropyLoss(): 0.647 (0.667)\n",
      "-9: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-9: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.412 (0.569)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.431 (0.647)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.902)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-9: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.902)\n",
      "-9: Identity()-Softmax(dim=1)-NLLLoss(): 0.804 (0.863)\n",
      "-9: Identity()-Identity()-CrossEntropyLoss(): 0.686 (0.706)\n",
      "-10: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-10: ReLU()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.451 (0.569)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.471 (0.490)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.863 (0.882)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-10: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.902)\n",
      "-10: Identity()-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-10: Identity()-Identity()-CrossEntropyLoss(): 0.706 (0.706)\n",
      "----- 35.75 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        # {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},]\n",
    "\n",
    "best_accs4 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs4 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=exp['act'], l_act=exp['lact'],\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  K, masks, exp['loss'], device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs4[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs4[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}: {best_val_accs4[j,i]:.3f} ({best_accs4[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}' for exp in EXPS]\n",
    "table_over4 = summary_table(best_accs4, index_name)\n",
    "table4 = summary_table(best_val_accs4, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.044063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.019706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.454902</td>\n",
       "      <td>0.421569</td>\n",
       "      <td>0.102187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.472549</td>\n",
       "      <td>0.460784</td>\n",
       "      <td>0.045943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.037255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.038273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.019608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.029346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.051281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.040232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.684314</td>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.075152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean accs       med  \\\n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()             0.852941  0.843137   \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                      0.868627  0.872549   \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()          0.454902  0.421569   \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                   0.472549  0.460784   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()     0.856863  0.852941   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()              0.856863  0.872549   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...   0.882353  0.882353   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...   0.850980  0.852941   \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()         0.835294  0.843137   \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                  0.837255  0.833333   \n",
       "Identity()-Identity()-CrossEntropyLoss()             0.684314  0.696078   \n",
       "\n",
       "                                                         std  \n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()            0.044063  \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                     0.019706  \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()         0.102187  \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                  0.045943  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()    0.037255  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()             0.038273  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...  0.019608  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...  0.029346  \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()        0.051281  \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                 0.040232  \n",
       "Identity()-Identity()-CrossEntropyLoss()            0.075152  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the GSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PARAMETERS  -  0.874\n",
    "## Training params\n",
    "N_RUNS = 10\n",
    "NORM = True\n",
    "N_EPOCHS = 200  # 750? --> repeat training params section\n",
    "EPOCHS_h = 10 # 10\n",
    "EPOCHS_W = 10 # 5\n",
    "LR = .005\n",
    "WD = .001\n",
    "DROPOUT = .25\n",
    "\n",
    "# BEST PARAMETERS\n",
    "## Architecture params\n",
    "N_LAYERS = 3\n",
    "K = 2\n",
    "HID_DIM = 50  # 32\n",
    "\n",
    "## Model params\n",
    "h0 = 1  # 1\n",
    "\n",
    "IN_DIM = feat.shape[1]\n",
    "OUT_DIM = n_class\n",
    "\n",
    "ACT = nn.ReLU() \n",
    "LAST_ACT = nn.Softmax(dim=1)\n",
    "LOSS_FN = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training paramerters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 200-0.005-0.0001-0.25: 0.647 (0.725)\n",
      "-1: 200-0.005-0.0005-0.25: 0.804 (0.863)\n",
      "-1: 200-0.01-0.001-0.25: 0.804 (0.863)\n",
      "-1: 200-0.005-0.001-0.25: 0.824 (0.863)\n",
      "-1: 200-0.005-0.001-0.5: 0.843 (0.882)\n",
      "-1: 200-0.001-0.001-0.25: 0.647 (0.686)\n",
      "-1: 200-0.05-0.005-0.25: 0.804 (0.902)\n",
      "-1: 200-0.01-0.005-0.25: 0.843 (0.882)\n",
      "-1: 200-0.005-0.005-0.25: 0.824 (0.863)\n",
      "-1: 200-0.001-0.005-0.25: 0.765 (0.804)\n",
      "-1: 200-0.05-0.01-0.25: 0.804 (0.882)\n",
      "-1: 200-0.005-0.01-0.25: 0.824 (0.863)\n",
      "-1: 500-0.01-0.001-0.25: 0.784 (0.863)\n",
      "-1: 500-0.005-0.001-0.25: 0.843 (0.863)\n",
      "-1: 500-0.001-0.001-0.25: 0.745 (0.804)\n",
      "-1: 500-0.005-0.005-0.25: 0.843 (0.863)\n",
      "-1: 500-0.001-0.005-0.25: 0.804 (0.843)\n",
      "-1: 200-0.05-0.01-0.5: 0.824 (0.843)\n",
      "-1: 200-0.05-0.005-0.5: 0.824 (0.863)\n",
      "-1: 200-0.01-0.005-0.5: 0.804 (0.882)\n",
      "-1: 200-0.05-0.01-0: 0.784 (0.863)\n",
      "-1: 200-0.05-0.005-0: 0.804 (0.863)\n",
      "-1: 200-0.01-0.005-0: 0.784 (0.824)\n",
      "-1: 500-0.005-0.005-0.5: 0.784 (0.843)\n",
      "-1: 500-0.005-0.001-0.5: 0.745 (0.784)\n",
      "-1: 500-0.001-0.001-0.5: 0.667 (0.667)\n",
      "-1: 750-0.005-0.005-0.25: 0.804 (0.863)\n",
      "-1: 750-0.005-0.001-0.25: 0.843 (0.882)\n",
      "-1: 750-0.001-0.001-0.25: 0.784 (0.843)\n",
      "-2: 200-0.005-0.0001-0.25: 0.863 (0.902)\n",
      "-2: 200-0.005-0.0005-0.25: 0.882 (0.922)\n",
      "-2: 200-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-2: 200-0.005-0.001-0.25: 0.863 (0.922)\n",
      "-2: 200-0.005-0.001-0.5: 0.824 (0.882)\n",
      "-2: 200-0.001-0.001-0.25: 0.804 (0.824)\n",
      "-2: 200-0.05-0.005-0.25: 0.824 (0.902)\n",
      "-2: 200-0.01-0.005-0.25: 0.824 (0.922)\n",
      "-2: 200-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-2: 200-0.001-0.005-0.25: 0.863 (0.882)\n",
      "-2: 200-0.05-0.01-0.25: 0.882 (0.922)\n",
      "-2: 200-0.005-0.01-0.25: 0.863 (0.922)\n",
      "-2: 500-0.01-0.001-0.25: 0.824 (0.902)\n",
      "-2: 500-0.005-0.001-0.25: 0.863 (0.922)\n",
      "-2: 500-0.001-0.001-0.25: 0.843 (0.882)\n",
      "-2: 500-0.005-0.005-0.25: 0.882 (0.922)\n",
      "-2: 500-0.001-0.005-0.25: 0.843 (0.902)\n",
      "-2: 200-0.05-0.01-0.5: 0.863 (0.922)\n",
      "-2: 200-0.05-0.005-0.5: 0.863 (0.922)\n",
      "-2: 200-0.01-0.005-0.5: 0.863 (0.902)\n",
      "-2: 200-0.05-0.01-0: 0.824 (0.902)\n",
      "-2: 200-0.05-0.005-0: 0.804 (0.922)\n",
      "-2: 200-0.01-0.005-0: 0.863 (0.882)\n",
      "-2: 500-0.005-0.005-0.5: 0.843 (0.902)\n",
      "-2: 500-0.005-0.001-0.5: 0.843 (0.902)\n",
      "-2: 500-0.001-0.001-0.5: 0.706 (0.725)\n",
      "-2: 750-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-2: 750-0.005-0.001-0.25: 0.824 (0.902)\n",
      "-2: 750-0.001-0.001-0.25: 0.863 (0.922)\n",
      "-3: 200-0.005-0.0001-0.25: 0.725 (0.784)\n",
      "-3: 200-0.005-0.0005-0.25: 0.804 (0.882)\n",
      "-3: 200-0.01-0.001-0.25: 0.843 (0.941)\n",
      "-3: 200-0.005-0.001-0.25: 0.843 (0.902)\n",
      "-3: 200-0.005-0.001-0.5: 0.824 (0.843)\n",
      "-3: 200-0.001-0.001-0.25: 0.686 (0.686)\n",
      "-3: 200-0.05-0.005-0.25: 0.824 (0.922)\n",
      "-3: 200-0.01-0.005-0.25: 0.922 (0.922)\n",
      "-3: 200-0.005-0.005-0.25: 0.902 (0.941)\n",
      "-3: 200-0.001-0.005-0.25: 0.843 (0.863)\n",
      "-3: 200-0.05-0.01-0.25: 0.863 (0.941)\n",
      "-3: 200-0.005-0.01-0.25: 0.863 (0.922)\n",
      "-3: 500-0.01-0.001-0.25: 0.922 (0.922)\n",
      "-3: 500-0.005-0.001-0.25: 0.882 (0.941)\n",
      "-3: 500-0.001-0.001-0.25: 0.784 (0.843)\n",
      "-3: 500-0.005-0.005-0.25: 0.882 (0.941)\n",
      "-3: 500-0.001-0.005-0.25: 0.863 (0.922)\n",
      "-3: 200-0.05-0.01-0.5: 0.804 (0.902)\n",
      "-3: 200-0.05-0.005-0.5: 0.863 (0.882)\n",
      "-3: 200-0.01-0.005-0.5: 0.882 (0.922)\n",
      "-3: 200-0.05-0.01-0: 0.882 (0.941)\n",
      "-3: 200-0.05-0.005-0: 0.804 (0.922)\n",
      "-3: 200-0.01-0.005-0: 0.863 (0.882)\n",
      "-3: 500-0.005-0.005-0.5: 0.882 (0.922)\n",
      "-3: 500-0.005-0.001-0.5: 0.706 (0.804)\n",
      "-3: 500-0.001-0.001-0.5: 0.667 (0.667)\n",
      "-3: 750-0.005-0.005-0.25: 0.882 (0.922)\n",
      "-3: 750-0.005-0.001-0.25: 0.863 (0.922)\n",
      "-3: 750-0.001-0.001-0.25: 0.863 (0.882)\n",
      "-4: 200-0.005-0.0001-0.25: 0.824 (0.863)\n",
      "-4: 200-0.005-0.0005-0.25: 0.882 (0.922)\n",
      "-4: 200-0.01-0.001-0.25: 0.863 (0.922)\n",
      "-4: 200-0.005-0.001-0.25: 0.882 (0.961)\n",
      "-4: 200-0.005-0.001-0.5: 0.784 (0.882)\n",
      "-4: 200-0.001-0.001-0.25: 0.745 (0.765)\n",
      "-4: 200-0.05-0.005-0.25: 0.745 (0.902)\n",
      "-4: 200-0.01-0.005-0.25: 0.902 (0.961)\n",
      "-4: 200-0.005-0.005-0.25: 0.902 (0.961)\n",
      "-4: 200-0.001-0.005-0.25: 0.824 (0.882)\n",
      "-4: 200-0.05-0.01-0.25: 0.882 (0.922)\n",
      "-4: 200-0.005-0.01-0.25: 0.922 (0.922)\n",
      "-4: 500-0.01-0.001-0.25: 0.804 (0.922)\n",
      "-4: 500-0.005-0.001-0.25: 0.882 (0.941)\n",
      "-4: 500-0.001-0.001-0.25: 0.765 (0.961)\n",
      "-4: 500-0.005-0.005-0.25: 0.902 (0.941)\n",
      "-4: 500-0.001-0.005-0.25: 0.863 (0.922)\n",
      "-4: 200-0.05-0.01-0.5: 0.843 (0.863)\n",
      "-4: 200-0.05-0.005-0.5: 0.824 (0.843)\n",
      "-4: 200-0.01-0.005-0.5: 0.882 (0.902)\n",
      "-4: 200-0.05-0.01-0: 0.902 (0.922)\n",
      "-4: 200-0.05-0.005-0: 0.804 (0.941)\n",
      "-4: 200-0.01-0.005-0: 0.922 (0.941)\n",
      "-4: 500-0.005-0.005-0.5: 0.882 (0.941)\n",
      "-4: 500-0.005-0.001-0.5: 0.784 (0.882)\n",
      "-4: 500-0.001-0.001-0.5: 0.725 (0.745)\n",
      "-4: 750-0.005-0.005-0.25: 0.922 (0.961)\n",
      "-4: 750-0.005-0.001-0.25: 0.922 (0.922)\n",
      "-4: 750-0.001-0.001-0.25: 0.941 (0.961)\n",
      "-5: 200-0.005-0.0001-0.25: 0.824 (0.843)\n",
      "-5: 200-0.005-0.0005-0.25: 0.804 (0.902)\n",
      "-5: 200-0.01-0.001-0.25: 0.882 (0.941)\n",
      "-5: 200-0.005-0.001-0.25: 0.922 (0.922)\n",
      "-5: 200-0.005-0.001-0.5: 0.804 (0.882)\n",
      "-5: 200-0.001-0.001-0.25: 0.608 (0.725)\n",
      "-5: 200-0.05-0.005-0.25: 0.824 (0.922)\n",
      "-5: 200-0.01-0.005-0.25: 0.863 (0.922)\n",
      "-5: 200-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-5: 200-0.001-0.005-0.25: 0.784 (0.784)\n",
      "-5: 200-0.05-0.01-0.25: 0.804 (0.961)\n",
      "-5: 200-0.005-0.01-0.25: 0.863 (0.902)\n",
      "-5: 500-0.01-0.001-0.25: 0.843 (0.941)\n",
      "-5: 500-0.005-0.001-0.25: 0.843 (0.922)\n",
      "-5: 500-0.001-0.001-0.25: 0.765 (0.843)\n",
      "-5: 500-0.005-0.005-0.25: 0.804 (0.902)\n",
      "-5: 500-0.001-0.005-0.25: 0.784 (0.882)\n",
      "-5: 200-0.05-0.01-0.5: 0.804 (0.922)\n",
      "-5: 200-0.05-0.005-0.5: 0.725 (0.882)\n",
      "-5: 200-0.01-0.005-0.5: 0.784 (0.922)\n",
      "-5: 200-0.05-0.01-0: 0.843 (0.922)\n",
      "-5: 200-0.05-0.005-0: 0.843 (0.882)\n",
      "-5: 200-0.01-0.005-0: 0.784 (0.843)\n",
      "-5: 500-0.005-0.005-0.5: 0.745 (0.882)\n",
      "-5: 500-0.005-0.001-0.5: 0.824 (0.882)\n",
      "-5: 500-0.001-0.001-0.5: 0.706 (0.725)\n",
      "-5: 750-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-5: 750-0.005-0.001-0.25: 0.922 (0.941)\n",
      "-5: 750-0.001-0.001-0.25: 0.784 (0.804)\n",
      "-6: 200-0.005-0.0001-0.25: 0.784 (0.824)\n",
      "-6: 200-0.005-0.0005-0.25: 0.824 (0.863)\n",
      "-6: 200-0.01-0.001-0.25: 0.824 (0.882)\n",
      "-6: 200-0.005-0.001-0.25: 0.824 (0.882)\n",
      "-6: 200-0.005-0.001-0.5: 0.784 (0.804)\n",
      "-6: 200-0.001-0.001-0.25: 0.667 (0.725)\n",
      "-6: 200-0.05-0.005-0.25: 0.804 (0.882)\n",
      "-6: 200-0.01-0.005-0.25: 0.843 (0.922)\n",
      "-6: 200-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-6: 200-0.001-0.005-0.25: 0.863 (0.863)\n",
      "-6: 200-0.05-0.01-0.25: 0.824 (0.882)\n",
      "-6: 200-0.005-0.01-0.25: 0.863 (0.882)\n",
      "-6: 500-0.01-0.001-0.25: 0.863 (0.882)\n",
      "-6: 500-0.005-0.001-0.25: 0.863 (0.922)\n",
      "-6: 500-0.001-0.001-0.25: 0.765 (0.843)\n",
      "-6: 500-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-6: 500-0.001-0.005-0.25: 0.882 (0.882)\n",
      "-6: 200-0.05-0.01-0.5: 0.745 (0.843)\n",
      "-6: 200-0.05-0.005-0.5: 0.804 (0.824)\n",
      "-6: 200-0.01-0.005-0.5: 0.863 (0.882)\n",
      "-6: 200-0.05-0.01-0: 0.843 (0.902)\n",
      "-6: 200-0.05-0.005-0: 0.843 (0.882)\n",
      "-6: 200-0.01-0.005-0: 0.824 (0.843)\n",
      "-6: 500-0.005-0.005-0.5: 0.863 (0.882)\n",
      "-6: 500-0.005-0.001-0.5: 0.725 (0.843)\n",
      "-6: 500-0.001-0.001-0.5: 0.667 (0.706)\n",
      "-6: 750-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-6: 750-0.005-0.001-0.25: 0.824 (0.902)\n",
      "-6: 750-0.001-0.001-0.25: 0.824 (0.843)\n",
      "-7: 200-0.005-0.0001-0.25: 0.824 (0.902)\n",
      "-7: 200-0.005-0.0005-0.25: 0.843 (0.902)\n",
      "-7: 200-0.01-0.001-0.25: 0.843 (0.922)\n",
      "-7: 200-0.005-0.001-0.25: 0.843 (0.902)\n",
      "-7: 200-0.005-0.001-0.5: 0.843 (0.882)\n",
      "-7: 200-0.001-0.001-0.25: 0.804 (0.843)\n",
      "-7: 200-0.05-0.005-0.25: 0.863 (0.902)\n",
      "-7: 200-0.01-0.005-0.25: 0.882 (0.902)\n",
      "-7: 200-0.005-0.005-0.25: 0.843 (0.902)\n",
      "-7: 200-0.001-0.005-0.25: 0.843 (0.863)\n",
      "-7: 200-0.05-0.01-0.25: 0.863 (0.902)\n",
      "-7: 200-0.005-0.01-0.25: 0.863 (0.902)\n",
      "-7: 500-0.01-0.001-0.25: 0.843 (0.922)\n",
      "-7: 500-0.005-0.001-0.25: 0.922 (0.922)\n",
      "-7: 500-0.001-0.001-0.25: 0.863 (0.863)\n",
      "-7: 500-0.005-0.005-0.25: 0.843 (0.902)\n",
      "-7: 500-0.001-0.005-0.25: 0.863 (0.882)\n",
      "-7: 200-0.05-0.01-0.5: 0.843 (0.902)\n",
      "-7: 200-0.05-0.005-0.5: 0.882 (0.922)\n",
      "-7: 200-0.01-0.005-0.5: 0.843 (0.902)\n",
      "-7: 200-0.05-0.01-0: 0.843 (0.882)\n",
      "-7: 200-0.05-0.005-0: 0.863 (0.882)\n",
      "-7: 200-0.01-0.005-0: 0.824 (0.824)\n",
      "-7: 500-0.005-0.005-0.5: 0.863 (0.902)\n",
      "-7: 500-0.005-0.001-0.5: 0.863 (0.902)\n",
      "-7: 500-0.001-0.001-0.5: 0.824 (0.824)\n",
      "-7: 750-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-7: 750-0.005-0.001-0.25: 0.863 (0.902)\n",
      "-7: 750-0.001-0.001-0.25: 0.882 (0.882)\n",
      "-8: 200-0.005-0.0001-0.25: 0.804 (0.843)\n",
      "-8: 200-0.005-0.0005-0.25: 0.765 (0.843)\n",
      "-8: 200-0.01-0.001-0.25: 0.765 (0.882)\n",
      "-8: 200-0.005-0.001-0.25: 0.843 (0.902)\n",
      "-8: 200-0.005-0.001-0.5: 0.725 (0.784)\n",
      "-8: 200-0.001-0.001-0.25: 0.627 (0.647)\n",
      "-8: 200-0.05-0.005-0.25: 0.804 (0.882)\n",
      "-8: 200-0.01-0.005-0.25: 0.863 (0.922)\n",
      "-8: 200-0.005-0.005-0.25: 0.824 (0.882)\n",
      "-8: 200-0.001-0.005-0.25: 0.784 (0.804)\n",
      "-8: 200-0.05-0.01-0.25: 0.882 (0.882)\n",
      "-8: 200-0.005-0.01-0.25: 0.843 (0.882)\n",
      "-8: 500-0.01-0.001-0.25: 0.824 (0.902)\n",
      "-8: 500-0.005-0.001-0.25: 0.863 (0.902)\n",
      "-8: 500-0.001-0.001-0.25: 0.804 (0.804)\n",
      "-8: 500-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-8: 500-0.001-0.005-0.25: 0.804 (0.843)\n",
      "-8: 200-0.05-0.01-0.5: 0.765 (0.882)\n",
      "-8: 200-0.05-0.005-0.5: 0.843 (0.882)\n",
      "-8: 200-0.01-0.005-0.5: 0.824 (0.843)\n",
      "-8: 200-0.05-0.01-0: 0.843 (0.902)\n",
      "-8: 200-0.05-0.005-0: 0.784 (0.863)\n",
      "-8: 200-0.01-0.005-0: 0.765 (0.784)\n",
      "-8: 500-0.005-0.005-0.5: 0.843 (0.863)\n",
      "-8: 500-0.005-0.001-0.5: 0.784 (0.863)\n",
      "-8: 500-0.001-0.001-0.5: 0.627 (0.706)\n",
      "-8: 750-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-8: 750-0.005-0.001-0.25: 0.804 (0.882)\n",
      "-8: 750-0.001-0.001-0.25: 0.765 (0.784)\n",
      "-9: 200-0.005-0.0001-0.25: 0.824 (0.843)\n",
      "-9: 200-0.005-0.0005-0.25: 0.843 (0.902)\n",
      "-9: 200-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-9: 200-0.005-0.001-0.25: 0.824 (0.902)\n",
      "-9: 200-0.005-0.001-0.5: 0.824 (0.863)\n",
      "-9: 200-0.001-0.001-0.25: 0.667 (0.725)\n",
      "-9: 200-0.05-0.005-0.25: 0.824 (0.863)\n",
      "-9: 200-0.01-0.005-0.25: 0.824 (0.882)\n",
      "-9: 200-0.005-0.005-0.25: 0.843 (0.882)\n",
      "-9: 200-0.001-0.005-0.25: 0.765 (0.824)\n",
      "-9: 200-0.05-0.01-0.25: 0.804 (0.882)\n",
      "-9: 200-0.005-0.01-0.25: 0.824 (0.863)\n",
      "-9: 500-0.01-0.001-0.25: 0.784 (0.882)\n",
      "-9: 500-0.005-0.001-0.25: 0.863 (0.902)\n",
      "-9: 500-0.001-0.001-0.25: 0.706 (0.784)\n",
      "-9: 500-0.005-0.005-0.25: 0.843 (0.882)\n",
      "-9: 500-0.001-0.005-0.25: 0.824 (0.824)\n",
      "-9: 200-0.05-0.01-0.5: 0.804 (0.882)\n",
      "-9: 200-0.05-0.005-0.5: 0.824 (0.863)\n",
      "-9: 200-0.01-0.005-0.5: 0.863 (0.882)\n",
      "-9: 200-0.05-0.01-0: 0.824 (0.843)\n",
      "-9: 200-0.05-0.005-0: 0.784 (0.843)\n",
      "-9: 200-0.01-0.005-0: 0.824 (0.843)\n",
      "-9: 500-0.005-0.005-0.5: 0.784 (0.843)\n",
      "-9: 500-0.005-0.001-0.5: 0.863 (0.882)\n",
      "-9: 500-0.001-0.001-0.5: 0.686 (0.725)\n",
      "-9: 750-0.005-0.005-0.25: 0.824 (0.882)\n",
      "-9: 750-0.005-0.001-0.25: 0.843 (0.882)\n",
      "-9: 750-0.001-0.001-0.25: 0.824 (0.863)\n",
      "-10: 200-0.005-0.0001-0.25: 0.824 (0.882)\n",
      "-10: 200-0.005-0.0005-0.25: 0.863 (0.902)\n",
      "-10: 200-0.01-0.001-0.25: 0.804 (0.922)\n",
      "-10: 200-0.005-0.001-0.25: 0.902 (0.902)\n",
      "-10: 200-0.005-0.001-0.5: 0.824 (0.882)\n",
      "-10: 200-0.001-0.001-0.25: 0.725 (0.765)\n",
      "-10: 200-0.05-0.005-0.25: 0.863 (0.902)\n",
      "-10: 200-0.01-0.005-0.25: 0.902 (0.902)\n",
      "-10: 200-0.005-0.005-0.25: 0.882 (0.902)\n",
      "-10: 200-0.001-0.005-0.25: 0.765 (0.804)\n",
      "-10: 200-0.05-0.01-0.25: 0.882 (0.941)\n",
      "-10: 200-0.005-0.01-0.25: 0.882 (0.882)\n",
      "-10: 500-0.01-0.001-0.25: 0.882 (0.922)\n",
      "-10: 500-0.005-0.001-0.25: 0.824 (0.902)\n",
      "-10: 500-0.001-0.001-0.25: 0.863 (0.882)\n",
      "-10: 500-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-10: 500-0.001-0.005-0.25: 0.843 (0.882)\n",
      "-10: 200-0.05-0.01-0.5: 0.843 (0.902)\n",
      "-10: 200-0.05-0.005-0.5: 0.804 (0.882)\n",
      "-10: 200-0.01-0.005-0.5: 0.843 (0.902)\n",
      "-10: 200-0.05-0.01-0: 0.843 (0.902)\n",
      "-10: 200-0.05-0.005-0: 0.843 (0.922)\n",
      "-10: 200-0.01-0.005-0: 0.843 (0.843)\n",
      "-10: 500-0.005-0.005-0.5: 0.824 (0.863)\n",
      "-10: 500-0.005-0.001-0.5: 0.804 (0.863)\n",
      "-10: 500-0.001-0.001-0.5: 0.745 (0.804)\n",
      "-10: 750-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-10: 750-0.005-0.001-0.25: 0.863 (0.902)\n",
      "-10: 750-0.001-0.001-0.25: 0.843 (0.882)\n",
      "----- 55.59 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-4, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-4, 'drop': .25},\n",
    "\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "        \n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-2, 'drop': .25},\n",
    "\n",
    "        {'epochs': 500, 'lr': .01, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "\n",
    "\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': 0},\n",
    "\n",
    "        \n",
    "        {'epochs': 500, 'lr': .005, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 500, 'lr': .001, 'wd': 1e-3, 'drop': .5},\n",
    "\n",
    "        {'epochs': 750, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 750, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 750, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "        ]\n",
    "\n",
    "best_accs5 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs5 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, l_act=LAST_ACT,\n",
    "                     dropout=exp['drop'], init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S, K, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, exp['epochs'], exp['lr'], exp['wd'], epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs5[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs5[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}: {best_val_accs5[j,i]:.3f} ({best_accs5[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}' for exp in EXPS]\n",
    "table_over5 = summary_table(best_accs5, index_name)\n",
    "table5 = summary_table(best_val_accs5, index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0001-0.25</th>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.059635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0005-0.25</th>\n",
       "      <td>0.831373</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.036367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.25</th>\n",
       "      <td>0.831373</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.031859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.25</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.032869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.5</th>\n",
       "      <td>0.807843</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.033735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.001-0.25</th>\n",
       "      <td>0.698039</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.065737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0.25</th>\n",
       "      <td>0.817647</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.031677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.25</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.032575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.005-0.25</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.026956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.25</th>\n",
       "      <td>0.809804</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.039265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.25</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.034018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.01-0.25</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.026956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.01-0.001-0.25</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.041176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0.25</th>\n",
       "      <td>0.864706</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.025490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.001-0.25</th>\n",
       "      <td>0.790196</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.049643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.005-0.25</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.026013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.005-0.25</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.030440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.5</th>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.035349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0.5</th>\n",
       "      <td>0.825490</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.041548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.5</th>\n",
       "      <td>0.845098</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.030941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.030376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0</th>\n",
       "      <td>0.817647</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.026380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0</th>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.043888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.005-0.5</th>\n",
       "      <td>0.831373</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.044019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0.5</th>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.052796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.001-0.5</th>\n",
       "      <td>0.701961</td>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.051729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750-0.005-0.005-0.25</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.029672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750-0.005-0.001-0.25</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.037255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750-0.001-0.001-0.25</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.050412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean accs       med       std\n",
       "200-0.005-0.0001-0.25   0.794118  0.823529  0.059635\n",
       "200-0.005-0.0005-0.25   0.831373  0.833333  0.036367\n",
       "200-0.01-0.001-0.25     0.831373  0.843137  0.031859\n",
       "200-0.005-0.001-0.25    0.856863  0.843137  0.032869\n",
       "200-0.005-0.001-0.5     0.807843  0.823529  0.033735\n",
       "200-0.001-0.001-0.25    0.698039  0.676471  0.065737\n",
       "200-0.05-0.005-0.25     0.817647  0.823529  0.031677\n",
       "200-0.01-0.005-0.25     0.866667  0.862745  0.032575\n",
       "200-0.005-0.005-0.25    0.860784  0.862745  0.026956\n",
       "200-0.001-0.005-0.25    0.809804  0.803922  0.039265\n",
       "200-0.05-0.01-0.25      0.849020  0.862745  0.034018\n",
       "200-0.005-0.01-0.25     0.860784  0.862745  0.026956\n",
       "500-0.01-0.001-0.25     0.837255  0.833333  0.041176\n",
       "500-0.005-0.001-0.25    0.864706  0.862745  0.025490\n",
       "500-0.001-0.001-0.25    0.790196  0.774510  0.049643\n",
       "500-0.005-0.005-0.25    0.858824  0.862745  0.026013\n",
       "500-0.001-0.005-0.25    0.837255  0.843137  0.030440\n",
       "200-0.05-0.01-0.5       0.813725  0.813725  0.035349\n",
       "200-0.05-0.005-0.5      0.825490  0.823529  0.041548\n",
       "200-0.01-0.005-0.5      0.845098  0.852941  0.030941\n",
       "200-0.05-0.01-0         0.843137  0.843137  0.030376\n",
       "200-0.05-0.005-0        0.817647  0.803922  0.026380\n",
       "200-0.01-0.005-0        0.829412  0.823529  0.043888\n",
       "500-0.005-0.005-0.5     0.831373  0.843137  0.044019\n",
       "500-0.005-0.001-0.5     0.794118  0.794118  0.052796\n",
       "500-0.001-0.001-0.5     0.701961  0.696078  0.051729\n",
       "750-0.005-0.005-0.25    0.860784  0.862745  0.029672\n",
       "750-0.005-0.001-0.25    0.856863  0.852941  0.037255\n",
       "750-0.001-0.001-0.25    0.837255  0.833333  0.050412"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 0.01-1-1-True: 0.471 (0.569)\n",
      "-1: 0.1-1-1-True: 0.490 (0.549)\n",
      "-1: 1-1-1-True: 0.608 (0.647)\n",
      "-1: 1-1-1-True: 0.843 (0.843)\n",
      "-1: 1-1-1-True: 0.725 (0.745)\n",
      "-1: 1-1-1-False: 0.588 (0.667)\n",
      "-1: 1-5-1-True: 0.765 (0.784)\n",
      "-1: 1-10-1-True: 0.765 (0.804)\n",
      "-1: 1-10-5-True: 0.824 (0.863)\n",
      "-1: 1-1-5-True: 0.627 (0.686)\n",
      "-1: 1-1-10-True: 0.588 (0.745)\n",
      "-1: 1-5-10-True: 0.745 (0.824)\n",
      "-1: 0.1-10-10-True: 0.784 (0.863)\n",
      "-1: 1-10-10-True: 0.863 (0.863)\n",
      "-1: 1-10-10-True: 0.804 (0.863)\n",
      "-1: 1-25-25-True: 0.804 (0.863)\n",
      "-1: 1-25-25-True: 0.843 (0.863)\n",
      "-1: 0.1-25-25-True: 0.824 (0.882)\n",
      "-1: 1-25-25-True: 0.843 (0.882)\n",
      "-1: 1-50-50-True: 0.843 (0.863)\n",
      "-2: 0.01-1-1-True: 0.569 (0.686)\n",
      "-2: 0.1-1-1-True: 0.706 (0.784)\n",
      "-2: 1-1-1-True: 0.627 (0.745)\n",
      "-2: 1-1-1-True: 0.843 (0.902)\n",
      "-2: 1-1-1-True: 0.843 (0.882)\n",
      "-2: 1-1-1-False: 0.804 (0.882)\n",
      "-2: 1-5-1-True: 0.863 (0.882)\n",
      "-2: 1-10-1-True: 0.765 (0.922)\n",
      "-2: 1-10-5-True: 0.843 (0.902)\n",
      "-2: 1-1-5-True: 0.784 (0.843)\n",
      "-2: 1-1-10-True: 0.725 (0.804)\n",
      "-2: 1-5-10-True: 0.784 (0.922)\n",
      "-2: 0.1-10-10-True: 0.863 (0.902)\n",
      "-2: 1-10-10-True: 0.863 (0.902)\n",
      "-2: 1-10-10-True: 0.804 (0.902)\n",
      "-2: 1-25-25-True: 0.843 (0.922)\n",
      "-2: 1-25-25-True: 0.882 (0.922)\n",
      "-2: 0.1-25-25-True: 0.824 (0.902)\n",
      "-2: 1-25-25-True: 0.863 (0.902)\n",
      "-2: 1-50-50-True: 0.863 (0.922)\n",
      "-3: 0.01-1-1-True: 0.529 (0.627)\n",
      "-3: 0.1-1-1-True: 0.725 (0.725)\n",
      "-3: 1-1-1-True: 0.725 (0.725)\n",
      "-3: 1-1-1-True: 0.824 (0.843)\n",
      "-3: 1-1-1-True: 0.843 (0.863)\n",
      "-3: 1-1-1-False: 0.843 (0.843)\n",
      "-3: 1-5-1-True: 0.765 (0.863)\n",
      "-3: 1-10-1-True: 0.824 (0.824)\n",
      "-3: 1-10-5-True: 0.882 (0.882)\n",
      "-3: 1-1-5-True: 0.686 (0.706)\n",
      "-3: 1-1-10-True: 0.627 (0.725)\n",
      "-3: 1-5-10-True: 0.843 (0.922)\n",
      "-3: 0.1-10-10-True: 0.843 (0.922)\n",
      "-3: 1-10-10-True: 0.843 (0.922)\n",
      "-3: 1-10-10-True: 0.863 (0.922)\n",
      "-3: 1-25-25-True: 0.824 (0.902)\n",
      "-3: 1-25-25-True: 0.902 (0.922)\n",
      "-3: 0.1-25-25-True: 0.882 (0.941)\n",
      "-3: 1-25-25-True: 0.863 (0.941)\n",
      "-3: 1-50-50-True: 0.863 (0.941)\n",
      "-4: 0.01-1-1-True: 0.667 (0.706)\n",
      "-4: 0.1-1-1-True: 0.608 (0.627)\n",
      "-4: 1-1-1-True: 0.667 (0.725)\n",
      "-4: 1-1-1-True: 0.549 (0.647)\n",
      "-4: 1-1-1-True: 0.765 (0.804)\n",
      "-4: 1-1-1-False: 0.804 (0.902)\n",
      "-4: 1-5-1-True: 0.843 (0.882)\n",
      "-4: 1-10-1-True: 0.745 (0.765)\n",
      "-4: 1-10-5-True: 0.902 (0.941)\n",
      "-4: 1-1-5-True: 0.706 (0.765)\n",
      "-4: 1-1-10-True: 0.784 (0.824)\n",
      "-4: 1-5-10-True: 0.922 (0.961)\n",
      "-4: 0.1-10-10-True: 0.922 (0.941)\n",
      "-4: 1-10-10-True: 0.902 (0.941)\n",
      "-4: 1-10-10-True: 0.902 (0.961)\n",
      "-4: 1-25-25-True: 0.922 (0.922)\n",
      "-4: 1-25-25-True: 0.843 (0.941)\n",
      "-4: 0.1-25-25-True: 0.922 (0.941)\n",
      "-4: 1-25-25-True: 0.882 (0.922)\n",
      "-4: 1-50-50-True: 0.863 (0.941)\n",
      "-5: 0.01-1-1-True: 0.471 (0.490)\n",
      "-5: 0.1-1-1-True: 0.412 (0.529)\n",
      "-5: 1-1-1-True: 0.627 (0.647)\n",
      "-5: 1-1-1-True: 0.725 (0.784)\n",
      "-5: 1-1-1-True: 0.725 (0.843)\n",
      "-5: 1-1-1-False: 0.686 (0.824)\n",
      "-5: 1-5-1-True: 0.804 (0.824)\n",
      "-5: 1-10-1-True: 0.804 (0.804)\n",
      "-5: 1-10-5-True: 0.765 (0.902)\n",
      "-5: 1-1-5-True: 0.706 (0.765)\n",
      "-5: 1-1-10-True: 0.686 (0.784)\n",
      "-5: 1-5-10-True: 0.824 (0.882)\n",
      "-5: 0.1-10-10-True: 0.843 (0.922)\n",
      "-5: 1-10-10-True: 0.882 (0.922)\n",
      "-5: 1-10-10-True: 0.882 (0.922)\n",
      "-5: 1-25-25-True: 0.784 (0.882)\n",
      "-5: 1-25-25-True: 0.843 (0.922)\n",
      "-5: 0.1-25-25-True: 0.902 (0.922)\n",
      "-5: 1-25-25-True: 0.941 (0.941)\n",
      "-5: 1-50-50-True: 0.902 (0.941)\n",
      "-6: 0.01-1-1-True: 0.588 (0.647)\n",
      "-6: 0.1-1-1-True: 0.627 (0.667)\n",
      "-6: 1-1-1-True: 0.706 (0.725)\n",
      "-6: 1-1-1-True: 0.804 (0.863)\n",
      "-6: 1-1-1-True: 0.725 (0.784)\n",
      "-6: 1-1-1-False: 0.608 (0.686)\n",
      "-6: 1-5-1-True: 0.784 (0.804)\n",
      "-6: 1-10-1-True: 0.784 (0.784)\n",
      "-6: 1-10-5-True: 0.824 (0.902)\n",
      "-6: 1-1-5-True: 0.667 (0.706)\n",
      "-6: 1-1-10-True: 0.627 (0.725)\n",
      "-6: 1-5-10-True: 0.882 (0.902)\n",
      "-6: 0.1-10-10-True: 0.863 (0.902)\n",
      "-6: 1-10-10-True: 0.863 (0.902)\n",
      "-6: 1-10-10-True: 0.843 (0.902)\n",
      "-6: 1-25-25-True: 0.843 (0.882)\n",
      "-6: 1-25-25-True: 0.863 (0.882)\n",
      "-6: 0.1-25-25-True: 0.882 (0.902)\n",
      "-6: 1-25-25-True: 0.882 (0.922)\n",
      "-6: 1-50-50-True: 0.882 (0.922)\n",
      "-7: 0.01-1-1-True: 0.627 (0.667)\n",
      "-7: 0.1-1-1-True: 0.569 (0.588)\n",
      "-7: 1-1-1-True: 0.706 (0.706)\n",
      "-7: 1-1-1-True: 0.843 (0.882)\n",
      "-7: 1-1-1-True: 0.804 (0.804)\n",
      "-7: 1-1-1-False: 0.765 (0.843)\n",
      "-7: 1-5-1-True: 0.804 (0.843)\n",
      "-7: 1-10-1-True: 0.745 (0.824)\n",
      "-7: 1-10-5-True: 0.863 (0.902)\n",
      "-7: 1-1-5-True: 0.745 (0.784)\n",
      "-7: 1-1-10-True: 0.824 (0.824)\n",
      "-7: 1-5-10-True: 0.863 (0.922)\n",
      "-7: 0.1-10-10-True: 0.863 (0.902)\n",
      "-7: 1-10-10-True: 0.882 (0.902)\n",
      "-7: 1-10-10-True: 0.863 (0.902)\n",
      "-7: 1-25-25-True: 0.824 (0.863)\n",
      "-7: 1-25-25-True: 0.902 (0.902)\n",
      "-7: 0.1-25-25-True: 0.843 (0.902)\n",
      "-7: 1-25-25-True: 0.824 (0.902)\n",
      "-7: 1-50-50-True: 0.882 (0.902)\n",
      "-8: 0.01-1-1-True: 0.529 (0.588)\n",
      "-8: 0.1-1-1-True: 0.529 (0.569)\n",
      "-8: 1-1-1-True: 0.471 (0.510)\n",
      "-8: 1-1-1-True: 0.608 (0.706)\n",
      "-8: 1-1-1-True: 0.804 (0.863)\n",
      "-8: 1-1-1-False: 0.706 (0.784)\n",
      "-8: 1-5-1-True: 0.647 (0.725)\n",
      "-8: 1-10-1-True: 0.706 (0.725)\n",
      "-8: 1-10-5-True: 0.745 (0.804)\n",
      "-8: 1-1-5-True: 0.608 (0.667)\n",
      "-8: 1-1-10-True: 0.667 (0.725)\n",
      "-8: 1-5-10-True: 0.824 (0.882)\n",
      "-8: 0.1-10-10-True: 0.824 (0.882)\n",
      "-8: 1-10-10-True: 0.843 (0.902)\n",
      "-8: 1-10-10-True: 0.882 (0.902)\n",
      "-8: 1-25-25-True: 0.745 (0.824)\n",
      "-8: 1-25-25-True: 0.824 (0.843)\n",
      "-8: 0.1-25-25-True: 0.863 (0.882)\n",
      "-8: 1-25-25-True: 0.804 (0.882)\n",
      "-8: 1-50-50-True: 0.843 (0.882)\n",
      "-9: 0.01-1-1-True: 0.647 (0.686)\n",
      "-9: 0.1-1-1-True: 0.549 (0.569)\n",
      "-9: 1-1-1-True: 0.608 (0.608)\n",
      "-9: 1-1-1-True: 0.725 (0.784)\n",
      "-9: 1-1-1-True: 0.745 (0.804)\n",
      "-9: 1-1-1-False: 0.784 (0.824)\n",
      "-9: 1-5-1-True: 0.784 (0.863)\n",
      "-9: 1-10-1-True: 0.725 (0.765)\n",
      "-9: 1-10-5-True: 0.843 (0.882)\n",
      "-9: 1-1-5-True: 0.647 (0.667)\n",
      "-9: 1-1-10-True: 0.647 (0.686)\n",
      "-9: 1-5-10-True: 0.843 (0.882)\n",
      "-9: 0.1-10-10-True: 0.863 (0.882)\n",
      "-9: 1-10-10-True: 0.843 (0.882)\n",
      "-9: 1-10-10-True: 0.824 (0.882)\n",
      "-9: 1-25-25-True: 0.765 (0.863)\n",
      "-9: 1-25-25-True: 0.824 (0.863)\n",
      "-9: 0.1-25-25-True: 0.843 (0.882)\n",
      "-9: 1-25-25-True: 0.843 (0.902)\n",
      "-9: 1-50-50-True: 0.882 (0.902)\n",
      "-10: 0.01-1-1-True: 0.686 (0.784)\n",
      "-10: 0.1-1-1-True: 0.569 (0.627)\n",
      "-10: 1-1-1-True: 0.588 (0.627)\n",
      "-10: 1-1-1-True: 0.745 (0.804)\n",
      "-10: 1-1-1-True: 0.824 (0.882)\n",
      "-10: 1-1-1-False: 0.725 (0.745)\n",
      "-10: 1-5-1-True: 0.745 (0.804)\n",
      "-10: 1-10-1-True: 0.804 (0.824)\n",
      "-10: 1-10-5-True: 0.843 (0.863)\n",
      "-10: 1-1-5-True: 0.588 (0.706)\n",
      "-10: 1-1-10-True: 0.667 (0.745)\n",
      "-10: 1-5-10-True: 0.882 (0.902)\n",
      "-10: 0.1-10-10-True: 0.863 (0.882)\n",
      "-10: 1-10-10-True: 0.882 (0.902)\n",
      "-10: 1-10-10-True: 0.843 (0.902)\n",
      "-10: 1-25-25-True: 0.902 (0.902)\n",
      "-10: 1-25-25-True: 0.882 (0.902)\n",
      "-10: 0.1-25-25-True: 0.882 (0.902)\n",
      "-10: 1-25-25-True: 0.863 (0.902)\n",
      "-10: 1-50-50-True: 0.843 (0.902)\n",
      "----- 30.66 mins -----\n"
     ]
    }
   ],
   "source": [
    "# h0, norm (norm, not norm, norm H), sep vs joint training, optimizers iters (W vs h)\n",
    "EXPS = [\n",
    "        # {'h0': .001, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': .01, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': .1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 1000, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 1500, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "\n",
    "        # {'h0': .1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        {'h0': 1, 'epochs': 1000, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 5, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 1, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 5, 'alt': True},\n",
    "\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 5, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 10, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 5, 'epochs_W': 10, 'alt': True},\n",
    "        \n",
    "        {'h0': .1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 500, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 50, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 100, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': .1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 50, 'epochs_W': 50, 'alt': True},\n",
    "    ]\n",
    "\n",
    "\n",
    "best_accs6 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs6 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, l_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=exp['h0'])\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        if not exp['alt']:\n",
    "            model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "            _, acc = model.train(feat, labels, exp['epochs'], LR, WD)\n",
    "        else:\n",
    "            model = GF_NodeClassModel(arch, S,  K, masks, LOSS_FN, device=device)\n",
    "            _, acc = model.train(feat, labels, exp['epochs'], LR, WD, epochs_h=exp['epochs_h'],\n",
    "                                 epochs_W=exp['epochs_W'])\n",
    "\n",
    "        best_accs6[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs6[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"h0\"]}-{exp[\"epochs_h\"]}-{exp[\"epochs_W\"]}-{exp[\"alt\"]}: {best_val_accs6[j,i]:.3f} ({best_accs6[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"h0\"]}-{exp[\"epochs_h\"]}-{exp[\"epochs_W\"]}-{exp[\"alt\"]}' for exp in EXPS]\n",
    "table_over6 = summary_table(best_accs6, index_name)\n",
    "table6 = summary_table(best_val_accs6, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.01-1-1-True</th>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.074018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-1-1-True</th>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.089533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-1-True</th>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.070724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000-1-1-1-True</th>\n",
       "      <td>0.750980</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.098059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500-1-1-1-True</th>\n",
       "      <td>0.780392</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.046235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000-1-1-1-False</th>\n",
       "      <td>0.731373</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.080869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-5-1-True</th>\n",
       "      <td>0.780392</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.056011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-1-True</th>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.035565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-5-True</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.045775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-5-True</th>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.058331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-10-True</th>\n",
       "      <td>0.684314</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.069849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-5-10-True</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.048388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-10-10-True</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.033102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-10-True</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.019212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-1-10-10-True</th>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.031859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50-1-25-25-True</th>\n",
       "      <td>0.825490</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.052941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100-1-25-25-True</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.028347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-25-25-True</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.031373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-25-25-True</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.035565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-50-50-True</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.019212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mean accs       med       std\n",
       "200-0.01-1-1-True    0.578431  0.578431  0.074018\n",
       "200-0.1-1-1-True     0.578431  0.568627  0.089533\n",
       "200-1-1-1-True       0.633333  0.627451  0.070724\n",
       "1000-1-1-1-True      0.750980  0.774510  0.098059\n",
       "1500-1-1-1-True      0.780392  0.784314  0.046235\n",
       "1000-1-1-1-False     0.731373  0.745098  0.080869\n",
       "200-1-5-1-True       0.780392  0.784314  0.056011\n",
       "200-1-10-1-True      0.766667  0.764706  0.035565\n",
       "200-1-10-5-True      0.833333  0.843137  0.045775\n",
       "200-1-1-5-True       0.676471  0.676471  0.058331\n",
       "200-1-1-10-True      0.684314  0.666667  0.069849\n",
       "200-1-5-10-True      0.841176  0.843137  0.048388\n",
       "200-0.1-10-10-True   0.852941  0.862745  0.033102\n",
       "200-1-10-10-True     0.866667  0.862745  0.019212\n",
       "500-1-10-10-True     0.850980  0.852941  0.031859\n",
       "50-1-25-25-True      0.825490  0.823529  0.052941\n",
       "100-1-25-25-True     0.860784  0.852941  0.028347\n",
       "200-0.1-25-25-True   0.866667  0.872549  0.031373\n",
       "200-1-25-25-True     0.860784  0.862745  0.035565\n",
       "200-1-50-50-True     0.866667  0.862745  0.019212"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 2-2-16: 0.765 (0.843)\n",
      "-1: 2-3-16: 0.784 (0.882)\n",
      "-1: 2-3-32: 0.745 (0.863)\n",
      "-1: 2-4-16: 0.824 (0.863)\n",
      "-1: 3-2-16: 0.804 (0.843)\n",
      "-1: 4-2-16: 0.725 (0.843)\n",
      "-1: 3-3-16: 0.804 (0.843)\n",
      "-1: 4-3-16: 0.725 (0.804)\n",
      "-1: 2-2-32: 0.824 (0.863)\n",
      "-1: 2-2-50: 0.745 (0.843)\n",
      "-1: 2-2-75: 0.784 (0.863)\n",
      "-1: 2-2-100: 0.745 (0.843)\n",
      "-1: 2-3-50: 0.725 (0.843)\n",
      "-1: 2-3-75: 0.804 (0.863)\n",
      "-1: 2-3-100: 0.824 (0.843)\n",
      "-1: 3-2-50: 0.843 (0.863)\n",
      "-1: 3-2-64: 0.804 (0.863)\n",
      "-1: 3-2-128: 0.804 (0.863)\n",
      "-2: 2-2-16: 0.863 (0.902)\n",
      "-2: 2-3-16: 0.843 (0.922)\n",
      "-2: 2-3-32: 0.863 (0.941)\n",
      "-2: 2-4-16: 0.843 (0.922)\n",
      "-2: 3-2-16: 0.863 (0.902)\n",
      "-2: 4-2-16: 0.824 (0.922)\n",
      "-2: 3-3-16: 0.824 (0.902)\n",
      "-2: 4-3-16: 0.843 (0.882)\n",
      "-2: 2-2-32: 0.804 (0.941)\n",
      "-2: 2-2-50: 0.784 (0.922)\n",
      "-2: 2-2-75: 0.824 (0.902)\n",
      "-2: 2-2-100: 0.824 (0.922)\n",
      "-2: 2-3-50: 0.824 (0.922)\n",
      "-2: 2-3-75: 0.863 (0.922)\n",
      "-2: 2-3-100: 0.765 (0.882)\n",
      "-2: 3-2-50: 0.804 (0.941)\n",
      "-2: 3-2-64: 0.804 (0.941)\n",
      "-2: 3-2-128: 0.902 (0.941)\n",
      "-3: 2-2-16: 0.882 (0.922)\n",
      "-3: 2-3-16: 0.902 (0.902)\n",
      "-3: 2-3-32: 0.843 (0.922)\n",
      "-3: 2-4-16: 0.804 (0.902)\n",
      "-3: 3-2-16: 0.863 (0.941)\n",
      "-3: 4-2-16: 0.745 (0.843)\n",
      "-3: 3-3-16: 0.863 (0.922)\n",
      "-3: 4-3-16: 0.843 (0.863)\n",
      "-3: 2-2-32: 0.843 (0.922)\n",
      "-3: 2-2-50: 0.863 (0.922)\n",
      "-3: 2-2-75: 0.863 (0.922)\n",
      "-3: 2-2-100: 0.863 (0.922)\n",
      "-3: 2-3-50: 0.882 (0.922)\n",
      "-3: 2-3-75: 0.882 (0.922)\n",
      "-3: 2-3-100: 0.824 (0.902)\n",
      "-3: 3-2-50: 0.882 (0.941)\n",
      "-3: 3-2-64: 0.882 (0.941)\n",
      "-3: 3-2-128: 0.824 (0.941)\n",
      "-4: 2-2-16: 0.863 (0.941)\n",
      "-4: 2-3-16: 0.882 (0.941)\n",
      "-4: 2-3-32: 0.922 (0.961)\n",
      "-4: 2-4-16: 0.824 (0.922)\n",
      "-4: 3-2-16: 0.863 (0.941)\n",
      "-4: 4-2-16: 0.804 (0.863)\n",
      "-4: 3-3-16: 0.882 (0.902)\n",
      "-4: 4-3-16: 0.843 (0.882)\n",
      "-4: 2-2-32: 0.922 (0.941)\n",
      "-4: 2-2-50: 0.882 (0.980)\n",
      "-4: 2-2-75: 0.922 (0.961)\n",
      "-4: 2-2-100: 0.882 (0.941)\n",
      "-4: 2-3-50: 0.882 (0.961)\n",
      "-4: 2-3-75: 0.843 (0.961)\n",
      "-4: 2-3-100: 0.922 (0.941)\n",
      "-4: 3-2-50: 0.922 (0.961)\n",
      "-4: 3-2-64: 0.961 (0.961)\n",
      "-4: 3-2-128: 0.902 (0.941)\n",
      "-5: 2-2-16: 0.863 (0.922)\n",
      "-5: 2-3-16: 0.863 (0.902)\n",
      "-5: 2-3-32: 0.843 (0.922)\n",
      "-5: 2-4-16: 0.824 (0.902)\n",
      "-5: 3-2-16: 0.745 (0.882)\n",
      "-5: 4-2-16: 0.804 (0.902)\n",
      "-5: 3-3-16: 0.765 (0.882)\n",
      "-5: 4-3-16: 0.725 (0.902)\n",
      "-5: 2-2-32: 0.863 (0.922)\n",
      "-5: 2-2-50: 0.863 (0.941)\n",
      "-5: 2-2-75: 0.882 (0.922)\n",
      "-5: 2-2-100: 0.804 (0.922)\n",
      "-5: 2-3-50: 0.784 (0.922)\n",
      "-5: 2-3-75: 0.824 (0.922)\n",
      "-5: 2-3-100: 0.824 (0.922)\n",
      "-5: 3-2-50: 0.882 (0.922)\n",
      "-5: 3-2-64: 0.882 (0.941)\n",
      "-5: 3-2-128: 0.863 (0.941)\n",
      "-6: 2-2-16: 0.843 (0.922)\n",
      "-6: 2-3-16: 0.843 (0.863)\n",
      "-6: 2-3-32: 0.863 (0.902)\n",
      "-6: 2-4-16: 0.882 (0.902)\n",
      "-6: 3-2-16: 0.863 (0.882)\n",
      "-6: 4-2-16: 0.882 (0.882)\n",
      "-6: 3-3-16: 0.824 (0.863)\n",
      "-6: 4-3-16: 0.824 (0.843)\n",
      "-6: 2-2-32: 0.843 (0.902)\n",
      "-6: 2-2-50: 0.843 (0.922)\n",
      "-6: 2-2-75: 0.843 (0.902)\n",
      "-6: 2-2-100: 0.843 (0.882)\n",
      "-6: 2-3-50: 0.882 (0.902)\n",
      "-6: 2-3-75: 0.843 (0.882)\n",
      "-6: 2-3-100: 0.843 (0.882)\n",
      "-6: 3-2-50: 0.843 (0.922)\n",
      "-6: 3-2-64: 0.863 (0.922)\n",
      "-6: 3-2-128: 0.863 (0.922)\n",
      "-7: 2-2-16: 0.863 (0.902)\n",
      "-7: 2-3-16: 0.882 (0.902)\n",
      "-7: 2-3-32: 0.902 (0.902)\n",
      "-7: 2-4-16: 0.882 (0.902)\n",
      "-7: 3-2-16: 0.882 (0.922)\n",
      "-7: 4-2-16: 0.882 (0.882)\n",
      "-7: 3-3-16: 0.843 (0.941)\n",
      "-7: 4-3-16: 0.843 (0.902)\n",
      "-7: 2-2-32: 0.824 (0.922)\n",
      "-7: 2-2-50: 0.863 (0.902)\n",
      "-7: 2-2-75: 0.843 (0.902)\n",
      "-7: 2-2-100: 0.843 (0.882)\n",
      "-7: 2-3-50: 0.843 (0.902)\n",
      "-7: 2-3-75: 0.882 (0.882)\n",
      "-7: 2-3-100: 0.843 (0.882)\n",
      "-7: 3-2-50: 0.902 (0.922)\n",
      "-7: 3-2-64: 0.882 (0.902)\n",
      "-7: 3-2-128: 0.882 (0.902)\n",
      "-8: 2-2-16: 0.843 (0.902)\n",
      "-8: 2-3-16: 0.824 (0.902)\n",
      "-8: 2-3-32: 0.902 (0.922)\n",
      "-8: 2-4-16: 0.882 (0.902)\n",
      "-8: 3-2-16: 0.824 (0.882)\n",
      "-8: 4-2-16: 0.824 (0.882)\n",
      "-8: 3-3-16: 0.824 (0.863)\n",
      "-8: 4-3-16: 0.765 (0.824)\n",
      "-8: 2-2-32: 0.843 (0.902)\n",
      "-8: 2-2-50: 0.863 (0.882)\n",
      "-8: 2-2-75: 0.784 (0.863)\n",
      "-8: 2-2-100: 0.784 (0.902)\n",
      "-8: 2-3-50: 0.843 (0.902)\n",
      "-8: 2-3-75: 0.824 (0.863)\n",
      "-8: 2-3-100: 0.765 (0.902)\n",
      "-8: 3-2-50: 0.843 (0.922)\n",
      "-8: 3-2-64: 0.843 (0.922)\n",
      "-8: 3-2-128: 0.804 (0.922)\n",
      "-9: 2-2-16: 0.863 (0.902)\n",
      "-9: 2-3-16: 0.824 (0.902)\n",
      "-9: 2-3-32: 0.843 (0.882)\n",
      "-9: 2-4-16: 0.784 (0.882)\n",
      "-9: 3-2-16: 0.843 (0.882)\n",
      "-9: 4-2-16: 0.804 (0.902)\n",
      "-9: 3-3-16: 0.824 (0.843)\n",
      "-9: 4-3-16: 0.784 (0.863)\n",
      "-9: 2-2-32: 0.843 (0.882)\n",
      "-9: 2-2-50: 0.824 (0.882)\n",
      "-9: 2-2-75: 0.843 (0.863)\n",
      "-9: 2-2-100: 0.843 (0.863)\n",
      "-9: 2-3-50: 0.863 (0.882)\n",
      "-9: 2-3-75: 0.843 (0.882)\n",
      "-9: 2-3-100: 0.784 (0.863)\n",
      "-9: 3-2-50: 0.902 (0.902)\n",
      "-9: 3-2-64: 0.863 (0.882)\n",
      "-9: 3-2-128: 0.824 (0.882)\n",
      "-10: 2-2-16: 0.961 (0.961)\n",
      "-10: 2-3-16: 0.882 (0.882)\n",
      "-10: 2-3-32: 0.843 (0.902)\n",
      "-10: 2-4-16: 0.863 (0.882)\n",
      "-10: 3-2-16: 0.882 (0.922)\n",
      "-10: 4-2-16: 0.882 (0.941)\n",
      "-10: 3-3-16: 0.843 (0.882)\n",
      "-10: 4-3-16: 0.843 (0.902)\n",
      "-10: 2-2-32: 0.882 (0.941)\n",
      "-10: 2-2-50: 0.902 (0.941)\n",
      "-10: 2-2-75: 0.882 (0.941)\n",
      "-10: 2-2-100: 0.902 (0.941)\n",
      "-10: 2-3-50: 0.863 (0.902)\n",
      "-10: 2-3-75: 0.882 (0.882)\n",
      "-10: 2-3-100: 0.863 (0.902)\n",
      "-10: 3-2-50: 0.902 (0.941)\n",
      "-10: 3-2-64: 0.902 (0.941)\n",
      "-10: 3-2-128: 0.922 (0.961)\n",
      "----- 28.23 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'L': 2, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 50},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 75},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 100},\n",
    "\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 50},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 75},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 100},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 50},\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 128},\n",
    "        ]\n",
    "\n",
    "best_accs7 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs7 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, exp['hid_dim'], OUT_DIM, exp['L'], exp['K'], act=ACT, l_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  exp['K'], masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs7[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs7[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}: {best_val_accs7[j,i]:.3f} ({best_accs7[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}' for exp in EXPS]\n",
    "table_over7 = summary_table(best_accs7, index_name)\n",
    "table7 = summary_table(best_val_accs7, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.045098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.034244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.046442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-16</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.040184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-16</th>\n",
       "      <td>0.817647</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.051915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-16</th>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.030440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-16</th>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.047222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.031677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-50</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.044713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-75</th>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.040942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-100</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.044063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-50</th>\n",
       "      <td>0.839216</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.047869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-75</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.026380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-100</th>\n",
       "      <td>0.825490</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.045098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-50</th>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.035349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-64</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.043888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-128</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.040942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean accs       med       std\n",
       "2-2-16    0.860784  0.862745  0.045098\n",
       "2-3-16    0.852941  0.852941  0.034244\n",
       "2-3-32    0.856863  0.852941  0.046442\n",
       "2-4-16    0.841176  0.833333  0.033333\n",
       "3-2-16    0.843137  0.862745  0.040184\n",
       "4-2-16    0.817647  0.813725  0.051915\n",
       "3-3-16    0.829412  0.823529  0.030440\n",
       "4-3-16    0.803922  0.833333  0.047222\n",
       "2-2-32    0.849020  0.843137  0.031677\n",
       "2-2-50    0.843137  0.862745  0.044713\n",
       "2-2-75    0.847059  0.843137  0.040942\n",
       "2-2-100   0.833333  0.843137  0.044063\n",
       "2-3-50    0.839216  0.852941  0.047869\n",
       "2-3-75    0.849020  0.843137  0.026380\n",
       "2-3-100   0.825490  0.823529  0.045098\n",
       "3-2-50    0.872549  0.882353  0.035349\n",
       "3-2-64    0.868627  0.872549  0.043888\n",
       "3-2-128   0.858824  0.862745  0.040942"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSO, bias, and batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1: orig-True-False: 0.804 (0.882)\n",
      "- 1: trans-True-False: 0.784 (0.863)\n",
      "- 1: sym-True-False: 0.529 (0.529)\n",
      "- 1: orig-True-True: 0.784 (0.843)\n",
      "- 1: trans-True-True: 0.784 (0.882)\n",
      "- 1: sym-True-True: 0.863 (0.882)\n",
      "- 1: orig-False-False: 0.824 (0.863)\n",
      "- 1: sym-False-False: 0.863 (0.863)\n",
      "- 2: orig-True-False: 0.843 (0.941)\n",
      "- 2: trans-True-False: 0.843 (0.882)\n",
      "- 2: sym-True-False: 0.627 (0.627)\n",
      "- 2: orig-True-True: 0.804 (0.843)\n",
      "- 2: trans-True-True: 0.765 (0.804)\n",
      "- 2: sym-True-True: 0.745 (0.863)\n",
      "- 2: orig-False-False: 0.863 (0.922)\n",
      "- 2: sym-False-False: 0.627 (0.627)\n",
      "- 3: orig-True-False: 0.882 (0.922)\n",
      "- 3: trans-True-False: 0.843 (0.882)\n",
      "- 3: sym-True-False: 0.804 (0.882)\n",
      "- 3: orig-True-True: 0.804 (0.863)\n",
      "- 3: trans-True-True: 0.765 (0.863)\n",
      "- 3: sym-True-True: 0.843 (0.922)\n",
      "- 3: orig-False-False: 0.902 (0.922)\n",
      "- 3: sym-False-False: 0.412 (0.412)\n",
      "- 4: orig-True-False: 0.941 (0.961)\n",
      "- 4: trans-True-False: 0.902 (0.961)\n",
      "- 4: sym-True-False: 0.882 (0.902)\n",
      "- 4: orig-True-True: 0.843 (0.902)\n",
      "- 4: trans-True-True: 0.941 (0.941)\n",
      "- 4: sym-True-True: 0.941 (0.961)\n",
      "- 4: orig-False-False: 0.941 (0.961)\n",
      "- 4: sym-False-False: 0.431 (0.431)\n",
      "- 5: orig-True-False: 0.902 (0.941)\n",
      "- 5: trans-True-False: 0.882 (0.882)\n",
      "- 5: sym-True-False: 0.843 (0.902)\n",
      "- 5: orig-True-True: 0.882 (0.922)\n",
      "- 5: trans-True-True: 0.824 (0.902)\n",
      "- 5: sym-True-True: 0.882 (0.941)\n",
      "- 5: orig-False-False: 0.882 (0.922)\n",
      "- 5: sym-False-False: 0.392 (0.392)\n",
      "- 6: orig-True-False: 0.902 (0.902)\n",
      "- 6: trans-True-False: 0.882 (0.902)\n",
      "- 6: sym-True-False: 0.510 (0.529)\n",
      "- 6: orig-True-True: 0.824 (0.843)\n",
      "- 6: trans-True-True: 0.863 (0.863)\n",
      "- 6: sym-True-True: 0.824 (0.882)\n",
      "- 6: orig-False-False: 0.843 (0.902)\n",
      "- 6: sym-False-False: 0.804 (0.843)\n",
      "- 7: orig-True-False: 0.882 (0.941)\n",
      "- 7: trans-True-False: 0.824 (0.882)\n",
      "- 7: sym-True-False: 0.471 (0.471)\n",
      "- 7: orig-True-True: 0.843 (0.882)\n",
      "- 7: trans-True-True: 0.843 (0.863)\n",
      "- 7: sym-True-True: 0.843 (0.882)\n",
      "- 7: orig-False-False: 0.882 (0.902)\n",
      "- 7: sym-False-False: 0.471 (0.471)\n",
      "- 8: orig-True-False: 0.882 (0.902)\n",
      "- 8: trans-True-False: 0.765 (0.882)\n",
      "- 8: sym-True-False: 0.510 (0.510)\n",
      "- 8: orig-True-True: 0.784 (0.882)\n",
      "- 8: trans-True-True: 0.843 (0.882)\n",
      "- 8: sym-True-True: 0.882 (0.882)\n",
      "- 8: orig-False-False: 0.843 (0.902)\n",
      "- 8: sym-False-False: 0.765 (0.804)\n",
      "- 9: orig-True-False: 0.863 (0.902)\n",
      "- 9: trans-True-False: 0.804 (0.843)\n",
      "- 9: sym-True-False: 0.451 (0.451)\n",
      "- 9: orig-True-True: 0.843 (0.902)\n",
      "- 9: trans-True-True: 0.824 (0.882)\n",
      "- 9: sym-True-True: 0.824 (0.863)\n",
      "- 9: orig-False-False: 0.843 (0.882)\n",
      "- 9: sym-False-False: 0.451 (0.451)\n",
      "- 10: orig-True-False: 0.902 (0.922)\n",
      "- 10: trans-True-False: 0.824 (0.882)\n",
      "- 10: sym-True-False: 0.451 (0.451)\n",
      "- 10: orig-True-True: 0.863 (0.922)\n",
      "- 10: trans-True-True: 0.843 (0.902)\n",
      "- 10: sym-True-True: 0.843 (0.882)\n",
      "- 10: orig-False-False: 0.882 (0.902)\n",
      "- 10: sym-False-False: 0.451 (0.451)\n",
      "----- 26.17 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [{'GSO': 'orig', 'bias': True, 'bn': False},\n",
    "        {'GSO': 'trans', 'bias': True, 'bn': False},\n",
    "        {'GSO': 'sym', 'bias': True, 'bn': False},\n",
    "\n",
    "        {'GSO': 'orig', 'bias': True, 'bn': True},\n",
    "        {'GSO': 'trans', 'bias': True, 'bn': True},\n",
    "        {'GSO': 'sym', 'bias': True, 'bn': True},\n",
    "\n",
    "        {'GSO': 'orig', 'bias': False, 'bn': False},\n",
    "        {'GSO': 'sym', 'bias': False, 'bn': False},\n",
    "        ]\n",
    "\n",
    "best_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, l_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0, bias=False, batch_norm=exp['bn'])\n",
    "        \n",
    "        if exp['GSO'] == 'sym':\n",
    "            A_aux = (A + A.T)/2\n",
    "            A_aux = np.where(A_aux > 0, 1, 0)\n",
    "        elif exp['GSO'] == 'trans':\n",
    "            A_aux = A.T\n",
    "        else:\n",
    "            A_aux = A\n",
    "\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A_aux, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A_aux).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  K, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs3b[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs3b[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'- {i+1}: {exp[\"GSO\"]}-{exp[\"bias\"]}-{exp[\"bn\"]}: {best_val_accs3b[j,i]:.3f} ({best_accs3b[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"GSO\"]}-{exp[\"bias\"]}-{exp[\"bn\"]}' for exp in EXPS]\n",
    "table_over7b = summary_table(best_accs3b, index_name)\n",
    "table7b = summary_table(best_val_accs3b, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>orig-True-False</th>\n",
       "      <td>0.880392</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.035565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trans-True-False</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.042237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sym-True-False</th>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.162165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig-True-True</th>\n",
       "      <td>0.827451</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.031373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trans-True-True</th>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.049643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sym-True-True</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.048069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig-False-False</th>\n",
       "      <td>0.870588</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.033044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sym-False-False</th>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.460784</td>\n",
       "      <td>0.171935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean accs       med       std\n",
       "orig-True-False    0.880392  0.882353  0.035565\n",
       "trans-True-False   0.835294  0.833333  0.042237\n",
       "sym-True-False     0.607843  0.519608  0.162165\n",
       "orig-True-True     0.827451  0.833333  0.031373\n",
       "trans-True-True    0.829412  0.833333  0.049643\n",
       "sym-True-True      0.849020  0.843137  0.048069\n",
       "orig-False-False   0.870588  0.872549  0.033044\n",
       "sym-False-False    0.566667  0.460784  0.171935"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearities and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.863)\n",
      "-1: ReLU()-Softmax(dim=1)-NLLLoss(): 0.804 (0.882)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.647 (0.706)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.667 (0.725)\n",
      "-1: ReLU()-Identity()-CrossEntropyLoss(): 0.627 (0.647)\n",
      "-1: ReLU()-Identity()-NLLLoss(): 0.275 (0.294)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.843)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.784 (0.843)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.863)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.784 (0.863)\n",
      "-1: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.843)\n",
      "-1: Identity()-Softmax(dim=1)-NLLLoss(): 0.804 (0.863)\n",
      "-1: Identity()-Identity()-CrossEntropyLoss(): 0.686 (0.725)\n",
      "-2: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-2: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.784 (0.784)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.745 (0.765)\n",
      "-2: ReLU()-Identity()-CrossEntropyLoss(): 0.608 (0.824)\n",
      "-2: ReLU()-Identity()-NLLLoss(): 0.333 (0.392)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.843 (0.941)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.902)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.804 (0.902)\n",
      "-2: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-2: Identity()-Softmax(dim=1)-NLLLoss(): 0.824 (0.922)\n",
      "-2: Identity()-Identity()-CrossEntropyLoss(): 0.745 (0.765)\n",
      "-3: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.941)\n",
      "-3: ReLU()-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.549 (0.569)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.588 (0.588)\n",
      "-3: ReLU()-Identity()-CrossEntropyLoss(): 0.627 (0.667)\n",
      "-3: ReLU()-Identity()-NLLLoss(): 0.510 (0.510)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.922)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.922 (0.922)\n",
      "-3: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-3: Identity()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-3: Identity()-Identity()-CrossEntropyLoss(): 0.725 (0.725)\n",
      "-4: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.941)\n",
      "-4: ReLU()-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.608 (0.627)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.608 (0.647)\n",
      "-4: ReLU()-Identity()-CrossEntropyLoss(): 0.608 (0.627)\n",
      "-4: ReLU()-Identity()-NLLLoss(): 0.431 (0.431)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.922 (0.961)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.902)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.882 (0.941)\n",
      "-4: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.941 (0.980)\n",
      "-4: Identity()-Softmax(dim=1)-NLLLoss(): 0.922 (0.922)\n",
      "-4: Identity()-Identity()-CrossEntropyLoss(): 0.706 (0.784)\n",
      "-5: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-5: ReLU()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.569 (0.627)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.608 (0.627)\n",
      "-5: ReLU()-Identity()-CrossEntropyLoss(): 0.569 (0.608)\n",
      "-5: ReLU()-Identity()-NLLLoss(): 0.451 (0.471)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.784 (0.902)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-5: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.922)\n",
      "-5: Identity()-Softmax(dim=1)-NLLLoss(): 0.804 (0.922)\n",
      "-5: Identity()-Identity()-CrossEntropyLoss(): 0.608 (0.627)\n",
      "-6: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.843)\n",
      "-6: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.882)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.549 (0.588)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.588 (0.608)\n",
      "-6: ReLU()-Identity()-CrossEntropyLoss(): 0.686 (0.706)\n",
      "-6: ReLU()-Identity()-NLLLoss(): 0.569 (0.569)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.882)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-6: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-6: Identity()-Softmax(dim=1)-NLLLoss(): 0.882 (0.902)\n",
      "-6: Identity()-Identity()-CrossEntropyLoss(): 0.706 (0.765)\n",
      "-7: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-7: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.549 (0.569)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.647 (0.765)\n",
      "-7: ReLU()-Identity()-CrossEntropyLoss(): 0.529 (0.569)\n",
      "-7: ReLU()-Identity()-NLLLoss(): 0.314 (0.314)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.902)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-7: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.902)\n",
      "-7: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.882)\n",
      "-7: Identity()-Identity()-CrossEntropyLoss(): 0.627 (0.647)\n",
      "-8: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.882)\n",
      "-8: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.882)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.510 (0.588)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.529 (0.569)\n",
      "-8: ReLU()-Identity()-CrossEntropyLoss(): 0.549 (0.549)\n",
      "-8: ReLU()-Identity()-NLLLoss(): 0.510 (0.510)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.882)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.882)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.804 (0.882)\n",
      "-8: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-8: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.882)\n",
      "-8: Identity()-Identity()-CrossEntropyLoss(): 0.588 (0.627)\n",
      "-9: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-9: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.882)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.490 (0.549)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.490 (0.529)\n",
      "-9: ReLU()-Identity()-CrossEntropyLoss(): 0.667 (0.667)\n",
      "-9: ReLU()-Identity()-NLLLoss(): 0.471 (0.471)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.902)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.804 (0.882)\n",
      "-9: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-9: Identity()-Softmax(dim=1)-NLLLoss(): 0.863 (0.882)\n",
      "-9: Identity()-Identity()-CrossEntropyLoss(): 0.588 (0.647)\n",
      "-10: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-10: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.882)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.529 (0.549)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.608 (0.627)\n",
      "-10: ReLU()-Identity()-CrossEntropyLoss(): 0.686 (0.725)\n",
      "-10: ReLU()-Identity()-NLLLoss(): 0.549 (0.627)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.882)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.902)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-10: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.882)\n",
      "-10: Identity()-Softmax(dim=1)-NLLLoss(): 0.882 (0.902)\n",
      "-10: Identity()-Identity()-CrossEntropyLoss(): 0.706 (0.706)\n",
      "----- 17.36 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [{'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},]\n",
    "\n",
    "best_accs8 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs8 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=exp['act'], l_act=exp['lact'],\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  K, masks, exp['loss'], device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs8[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs8[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}: {best_val_accs8[j,i]:.3f} ({best_accs8[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}' for exp in EXPS]\n",
    "table_over8 = summary_table(best_accs8, index_name)\n",
    "table8 = summary_table(best_val_accs8, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.043003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.028347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.080964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.066782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.615686</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.052025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Identity()-NLLLoss()</th>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.460784</td>\n",
       "      <td>0.096955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.031677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.839216</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.038021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.023934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.045395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.034018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.036367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.668627</td>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.056456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean accs       med  \\\n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()             0.856863  0.872549   \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                      0.860784  0.862745   \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()          0.578431  0.549020   \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                   0.607843  0.607843   \n",
       "ReLU()-Identity()-CrossEntropyLoss()                 0.615686  0.617647   \n",
       "ReLU()-Identity()-NLLLoss()                          0.441176  0.460784   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()     0.856863  0.852941   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()              0.839216  0.833333   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...   0.841176  0.833333   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...   0.847059  0.843137   \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()         0.868627  0.843137   \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                  0.854902  0.852941   \n",
       "Identity()-Identity()-CrossEntropyLoss()             0.668627  0.696078   \n",
       "\n",
       "                                                         std  \n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()            0.043003  \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                     0.028347  \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()         0.080964  \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                  0.066782  \n",
       "ReLU()-Identity()-CrossEntropyLoss()                0.052025  \n",
       "ReLU()-Identity()-NLLLoss()                         0.096955  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()    0.031677  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()             0.038021  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...  0.023934  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...  0.045395  \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()        0.034018  \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                 0.036367  \n",
       "Identity()-Identity()-CrossEntropyLoss()            0.056456  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPS = [\n",
    "        {'name': 'Kipf', 'norm': 'none'},\n",
    "        {'name': 'Kipf', 'norm': 'both'},\n",
    "\n",
    "        {'name': 'A-GCNN', 'norm': False},\n",
    "        {'name': 'A-GCNN', 'norm': True},\n",
    "\n",
    "        {'name': 'H-GCNN', 'norm': False}, # This should be the same as A-GCNN not norm\n",
    "        {'name': 'H-GCNN', 'norm': True},\n",
    "\n",
    "        {'name': 'W-GCN-A', 'norm': False},\n",
    "        {'name': 'W-GCN-A', 'norm': True},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- RUN: 1\n",
      "\tKipf-none: acc = 0.549  -  acc2 = 0.549  -  acc (over) = 0.608\n",
      "\tKipf-both: acc = 0.588  -  acc2 = 0.588  -  acc (over) = 0.608\n",
      "\tA-GCNN-False: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tA-GCNN-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.863\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.804  -  acc (over) = 0.863\n",
      "\tH-GCNN-True: acc = 0.804  -  acc2 = 0.804  -  acc (over) = 0.843\n",
      "\tW-GCN-A-False: acc = 0.725  -  acc2 = 0.725  -  acc (over) = 0.725\n",
      "\tW-GCN-A-True: acc = 0.725  -  acc2 = 0.784  -  acc (over) = 0.824\n",
      "- RUN: 2\n",
      "\tKipf-none: acc = 0.588  -  acc2 = 0.588  -  acc (over) = 0.725\n",
      "\tKipf-both: acc = 0.647  -  acc2 = 0.686  -  acc (over) = 0.686\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.863  -  acc (over) = 0.882\n",
      "\tA-GCNN-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.922\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.863  -  acc2 = 0.902  -  acc (over) = 0.922\n",
      "\tW-GCN-A-False: acc = 0.804  -  acc2 = 0.784  -  acc (over) = 0.804\n",
      "\tW-GCN-A-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.902\n",
      "- RUN: 3\n",
      "\tKipf-none: acc = 0.490  -  acc2 = 0.490  -  acc (over) = 0.588\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.569  -  acc (over) = 0.647\n",
      "\tA-GCNN-False: acc = 0.863  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.941\n",
      "\tH-GCNN-False: acc = 0.745  -  acc2 = 0.765  -  acc (over) = 0.843\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.902  -  acc (over) = 0.922\n",
      "\tW-GCN-A-False: acc = 0.647  -  acc2 = 0.647  -  acc (over) = 0.686\n",
      "\tW-GCN-A-True: acc = 0.824  -  acc2 = 0.843  -  acc (over) = 0.863\n",
      "- RUN: 4\n",
      "\tKipf-none: acc = 0.529  -  acc2 = 0.529  -  acc (over) = 0.647\n",
      "\tKipf-both: acc = 0.529  -  acc2 = 0.529  -  acc (over) = 0.569\n",
      "\tA-GCNN-False: acc = 0.941  -  acc2 = 0.922  -  acc (over) = 0.961\n",
      "\tA-GCNN-True: acc = 0.804  -  acc2 = 0.804  -  acc (over) = 0.922\n",
      "\tH-GCNN-False: acc = 0.941  -  acc2 = 0.843  -  acc (over) = 0.941\n",
      "\tH-GCNN-True: acc = 0.902  -  acc2 = 0.902  -  acc (over) = 0.961\n",
      "\tW-GCN-A-False: acc = 0.667  -  acc2 = 0.647  -  acc (over) = 0.745\n",
      "\tW-GCN-A-True: acc = 0.765  -  acc2 = 0.824  -  acc (over) = 0.882\n",
      "- RUN: 5\n",
      "\tKipf-none: acc = 0.373  -  acc2 = 0.392  -  acc (over) = 0.392\n",
      "\tKipf-both: acc = 0.392  -  acc2 = 0.490  -  acc (over) = 0.569\n",
      "\tA-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.922\n",
      "\tA-GCNN-True: acc = 0.922  -  acc2 = 0.843  -  acc (over) = 0.941\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tH-GCNN-True: acc = 0.765  -  acc2 = 0.804  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.608  -  acc2 = 0.647  -  acc (over) = 0.706\n",
      "\tW-GCN-A-True: acc = 0.843  -  acc2 = 0.804  -  acc (over) = 0.843\n",
      "- RUN: 6\n",
      "\tKipf-none: acc = 0.471  -  acc2 = 0.451  -  acc (over) = 0.529\n",
      "\tKipf-both: acc = 0.549  -  acc2 = 0.549  -  acc (over) = 0.569\n",
      "\tA-GCNN-False: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.804\n",
      "\tA-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.863\n",
      "\tH-GCNN-True: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.843\n",
      "\tW-GCN-A-False: acc = 0.686  -  acc2 = 0.706  -  acc (over) = 0.725\n",
      "\tW-GCN-A-True: acc = 0.745  -  acc2 = 0.745  -  acc (over) = 0.784\n",
      "- RUN: 7\n",
      "\tKipf-none: acc = 0.471  -  acc2 = 0.471  -  acc (over) = 0.471\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.549  -  acc (over) = 0.627\n",
      "\tA-GCNN-False: acc = 0.843  -  acc2 = 0.784  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.843  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.882\n",
      "\tW-GCN-A-False: acc = 0.725  -  acc2 = 0.745  -  acc (over) = 0.804\n",
      "\tW-GCN-A-True: acc = 0.824  -  acc2 = 0.804  -  acc (over) = 0.843\n",
      "- RUN: 8\n",
      "\tKipf-none: acc = 0.510  -  acc2 = 0.510  -  acc (over) = 0.510\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.529  -  acc (over) = 0.627\n",
      "\tA-GCNN-False: acc = 0.804  -  acc2 = 0.824  -  acc (over) = 0.843\n",
      "\tA-GCNN-True: acc = 0.824  -  acc2 = 0.804  -  acc (over) = 0.882\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.902  -  acc (over) = 0.941\n",
      "\tH-GCNN-True: acc = 0.745  -  acc2 = 0.765  -  acc (over) = 0.784\n",
      "\tW-GCN-A-False: acc = 0.627  -  acc2 = 0.627  -  acc (over) = 0.706\n",
      "\tW-GCN-A-True: acc = 0.765  -  acc2 = 0.765  -  acc (over) = 0.804\n",
      "- RUN: 9\n",
      "\tKipf-none: acc = 0.569  -  acc2 = 0.569  -  acc (over) = 0.588\n",
      "\tKipf-both: acc = 0.490  -  acc2 = 0.490  -  acc (over) = 0.588\n",
      "\tA-GCNN-False: acc = 0.784  -  acc2 = 0.804  -  acc (over) = 0.882\n",
      "\tA-GCNN-True: acc = 0.863  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tH-GCNN-True: acc = 0.804  -  acc2 = 0.765  -  acc (over) = 0.824\n",
      "\tW-GCN-A-False: acc = 0.686  -  acc2 = 0.627  -  acc (over) = 0.706\n",
      "\tW-GCN-A-True: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.784\n",
      "- RUN: 10\n",
      "\tKipf-none: acc = 0.510  -  acc2 = 0.471  -  acc (over) = 0.569\n",
      "\tKipf-both: acc = 0.529  -  acc2 = 0.529  -  acc (over) = 0.549\n",
      "\tA-GCNN-False: acc = 0.784  -  acc2 = 0.804  -  acc (over) = 0.863\n",
      "\tA-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tH-GCNN-False: acc = 0.824  -  acc2 = 0.804  -  acc (over) = 0.863\n",
      "\tH-GCNN-True: acc = 0.784  -  acc2 = 0.765  -  acc (over) = 0.804\n",
      "\tW-GCN-A-False: acc = 0.765  -  acc2 = 0.725  -  acc (over) = 0.784\n",
      "\tW-GCN-A-True: acc = 0.765  -  acc2 = 0.745  -  acc (over) = 0.784\n",
      "- RUN: 11\n",
      "\tKipf-none: acc = 0.549  -  acc2 = 0.549  -  acc (over) = 0.608\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.529  -  acc (over) = 0.647\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.843\n",
      "\tA-GCNN-True: acc = 0.824  -  acc2 = 0.804  -  acc (over) = 0.882\n",
      "\tH-GCNN-False: acc = 0.765  -  acc2 = 0.745  -  acc (over) = 0.804\n",
      "\tH-GCNN-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.843\n",
      "\tW-GCN-A-False: acc = 0.667  -  acc2 = 0.647  -  acc (over) = 0.667\n",
      "\tW-GCN-A-True: acc = 0.784  -  acc2 = 0.843  -  acc (over) = 0.843\n",
      "- RUN: 12\n",
      "\tKipf-none: acc = 0.667  -  acc2 = 0.686  -  acc (over) = 0.765\n",
      "\tKipf-both: acc = 0.588  -  acc2 = 0.647  -  acc (over) = 0.686\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.863  -  acc2 = 0.824  -  acc (over) = 0.902\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.843  -  acc (over) = 0.922\n",
      "\tW-GCN-A-False: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.804\n",
      "\tW-GCN-A-True: acc = 0.843  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "- RUN: 13\n",
      "\tKipf-none: acc = 0.412  -  acc2 = 0.412  -  acc (over) = 0.412\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.569  -  acc (over) = 0.647\n",
      "\tA-GCNN-False: acc = 0.882  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.941\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.627  -  acc2 = 0.627  -  acc (over) = 0.667\n",
      "\tW-GCN-A-True: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.863\n",
      "- RUN: 14\n",
      "\tKipf-none: acc = 0.588  -  acc2 = 0.588  -  acc (over) = 0.608\n",
      "\tKipf-both: acc = 0.510  -  acc2 = 0.510  -  acc (over) = 0.569\n",
      "\tA-GCNN-False: acc = 0.922  -  acc2 = 0.922  -  acc (over) = 0.961\n",
      "\tA-GCNN-True: acc = 0.922  -  acc2 = 0.922  -  acc (over) = 0.941\n",
      "\tH-GCNN-False: acc = 0.902  -  acc2 = 0.902  -  acc (over) = 0.941\n",
      "\tH-GCNN-True: acc = 0.863  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.725  -  acc2 = 0.725  -  acc (over) = 0.765\n",
      "\tW-GCN-A-True: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.863\n",
      "- RUN: 15\n",
      "\tKipf-none: acc = 0.333  -  acc2 = 0.333  -  acc (over) = 0.392\n",
      "\tKipf-both: acc = 0.588  -  acc2 = 0.588  -  acc (over) = 0.667\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.882\n",
      "\tA-GCNN-True: acc = 0.863  -  acc2 = 0.843  -  acc (over) = 0.922\n",
      "\tH-GCNN-False: acc = 0.804  -  acc2 = 0.804  -  acc (over) = 0.882\n",
      "\tH-GCNN-True: acc = 0.843  -  acc2 = 0.804  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.608  -  acc2 = 0.608  -  acc (over) = 0.686\n",
      "\tW-GCN-A-True: acc = 0.804  -  acc2 = 0.725  -  acc (over) = 0.863\n",
      "- RUN: 16\n",
      "\tKipf-none: acc = 0.608  -  acc2 = 0.608  -  acc (over) = 0.667\n",
      "\tKipf-both: acc = 0.549  -  acc2 = 0.588  -  acc (over) = 0.588\n",
      "\tA-GCNN-False: acc = 0.804  -  acc2 = 0.784  -  acc (over) = 0.863\n",
      "\tA-GCNN-True: acc = 0.843  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tH-GCNN-False: acc = 0.804  -  acc2 = 0.804  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.824  -  acc2 = 0.804  -  acc (over) = 0.863\n",
      "\tW-GCN-A-False: acc = 0.667  -  acc2 = 0.667  -  acc (over) = 0.745\n",
      "\tW-GCN-A-True: acc = 0.784  -  acc2 = 0.765  -  acc (over) = 0.804\n",
      "- RUN: 17\n",
      "\tKipf-none: acc = 0.627  -  acc2 = 0.627  -  acc (over) = 0.647\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.549  -  acc (over) = 0.667\n",
      "\tA-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.863  -  acc2 = 0.843  -  acc (over) = 0.922\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.882\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.765  -  acc2 = 0.765  -  acc (over) = 0.765\n",
      "\tW-GCN-A-True: acc = 0.804  -  acc2 = 0.784  -  acc (over) = 0.824\n",
      "- RUN: 18\n",
      "\tKipf-none: acc = 0.686  -  acc2 = 0.686  -  acc (over) = 0.725\n",
      "\tKipf-both: acc = 0.647  -  acc2 = 0.667  -  acc (over) = 0.686\n",
      "\tA-GCNN-False: acc = 0.902  -  acc2 = 0.902  -  acc (over) = 0.922\n",
      "\tA-GCNN-True: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tH-GCNN-False: acc = 0.804  -  acc2 = 0.804  -  acc (over) = 0.863\n",
      "\tH-GCNN-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.863\n",
      "\tW-GCN-A-False: acc = 0.588  -  acc2 = 0.588  -  acc (over) = 0.725\n",
      "\tW-GCN-A-True: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.824\n",
      "- RUN: 19\n",
      "\tKipf-none: acc = 0.471  -  acc2 = 0.471  -  acc (over) = 0.471\n",
      "\tKipf-both: acc = 0.451  -  acc2 = 0.451  -  acc (over) = 0.451\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.882\n",
      "\tA-GCNN-True: acc = 0.824  -  acc2 = 0.843  -  acc (over) = 0.863\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.627  -  acc2 = 0.647  -  acc (over) = 0.686\n",
      "\tW-GCN-A-True: acc = 0.765  -  acc2 = 0.765  -  acc (over) = 0.784\n",
      "- RUN: 20\n",
      "\tKipf-none: acc = 0.314  -  acc2 = 0.314  -  acc (over) = 0.314\n",
      "\tKipf-both: acc = 0.510  -  acc2 = 0.529  -  acc (over) = 0.549\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.882\n",
      "\tA-GCNN-True: acc = 0.863  -  acc2 = 0.902  -  acc (over) = 0.902\n",
      "\tH-GCNN-False: acc = 0.824  -  acc2 = 0.804  -  acc (over) = 0.843\n",
      "\tH-GCNN-True: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.843\n",
      "\tW-GCN-A-False: acc = 0.725  -  acc2 = 0.725  -  acc (over) = 0.765\n",
      "\tW-GCN-A-True: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.784\n"
     ]
    }
   ],
   "source": [
    "N_RUNS = 20\n",
    "\n",
    "best_accs = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs2 = np.zeros((len(EXPS), N_RUNS))\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    print(f'- RUN: {i+1}')\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        # t_i = time.time()\n",
    "        if exp['name'] == 'Kipf':\n",
    "            arch = GCNN_2L(IN_DIM, HID_DIM, OUT_DIM, act=ACT, l_act=LAST_ACT,\n",
    "                           dropout=DROPOUT, norm=exp['norm'])\n",
    "            S = dgl.from_networkx(nx.from_numpy_array(A)).add_self_loop().to(device)\n",
    "            \n",
    "        elif exp['name'] == 'A-GCNN':\n",
    "            arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, l_act=LAST_ACT,\n",
    "            dropout=DROPOUT, diff_layer=GFGCNLayer, init_h0=h0)\n",
    "            if exp['norm']:\n",
    "                S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "            else:\n",
    "                S = torch.Tensor(A).to(device)\n",
    "\n",
    "        elif exp['name'] == 'H-GCNN':\n",
    "            arch = GFGCN_Spows(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, l_act=LAST_ACT,\n",
    "                               dropout=DROPOUT, norm=exp['norm'], dev=device)\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        elif exp['name'] == 'W-GCN-A':\n",
    "            arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, l_act=LAST_ACT,\n",
    "                         dropout=DROPOUT, diff_layer=GFGCN_noh_Layer)\n",
    "            if exp['norm']:\n",
    "                S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "            else:\n",
    "                S = torch.Tensor(A).to(device)  \n",
    "            \n",
    "        else:\n",
    "            raise Exception(f'ERROR: Unknown architecture: {exp[\"name\"]}')\n",
    "\n",
    "        if exp['name'] in ['Kipf', 'W-GCN-A']:\n",
    "            model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "            loss, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "        else:\n",
    "            model = GF_NodeClassModel(arch, S, K, masks, LOSS_FN, device=device)\n",
    "            loss, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                                    epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs[j,i] = model.test(feat, model.S, labels, masks['test'])\n",
    "        best_val_accs2[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'\\t{exp[\"name\"]}-{exp[\"norm\"]}: acc = {best_val_accs[j,i]:.3f}  -  acc2 = {best_val_accs2[j,i]:.3f}  -  acc (over) = {best_accs[j,i]:.3f}')\n",
    "\n",
    "\n",
    "# Print results\n",
    "index_name = [f'{exp[\"name\"]}-{exp[\"norm\"]}' for exp in EXPS]\n",
    "table_comp_over = summary_table(best_accs, index_name)\n",
    "table_comp = summary_table(best_val_accs, index_name)\n",
    "table_comp2 = summary_table(best_val_accs2, index_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kipf-none</th>\n",
       "      <td>0.515686</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.100192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kipf-both</th>\n",
       "      <td>0.560784</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.064557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-False</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.044756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-True</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.031677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-False</th>\n",
       "      <td>0.840196</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.042633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-True</th>\n",
       "      <td>0.834314</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.045342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-False</th>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.062005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-True</th>\n",
       "      <td>0.795098</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.034230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean accs       med       std\n",
       "Kipf-none       0.515686  0.519608  0.100192\n",
       "Kipf-both       0.560784  0.588235  0.064557\n",
       "A-GCNN-False    0.837255  0.823529  0.044756\n",
       "A-GCNN-True     0.856863  0.862745  0.031677\n",
       "H-GCNN-False    0.840196  0.843137  0.042633\n",
       "H-GCNN-True     0.834314  0.833333  0.045342\n",
       "W-GCN-A-False   0.686275  0.676471  0.062005\n",
       "W-GCN-A-True    0.795098  0.784314  0.034230"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kipf-none</th>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.101484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kipf-both</th>\n",
       "      <td>0.556863</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.057635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-False</th>\n",
       "      <td>0.834314</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.041351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-True</th>\n",
       "      <td>0.851961</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.033087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-False</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.041316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-True</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.048626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-False</th>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.656863</td>\n",
       "      <td>0.058913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-True</th>\n",
       "      <td>0.796078</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.037920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean accs       med       std\n",
       "Kipf-none       0.514706  0.519608  0.101484\n",
       "Kipf-both       0.556863  0.549020  0.057635\n",
       "A-GCNN-False    0.834314  0.823529  0.041351\n",
       "A-GCNN-True     0.851961  0.843137  0.033087\n",
       "H-GCNN-False    0.835294  0.843137  0.041316\n",
       "H-GCNN-True     0.833333  0.823529  0.048626\n",
       "W-GCN-A-False   0.683333  0.656863  0.058913\n",
       "W-GCN-A-True    0.796078  0.784314  0.037920"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_comp2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
