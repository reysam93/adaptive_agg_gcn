{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f9db661e7d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import utils\n",
    "from gsp_utils.baselines_archs import GCNN_2L\n",
    "from gsp_utils.baselines_models import NodeClassModel, GF_NodeClassModel\n",
    "from gsp_utils.data import normalize_gso\n",
    "from src.arch import GFGCN, GFGCNLayer, GFGCN_noh_Layer, GFGCN_Spows\n",
    "\n",
    "SEED = 15\n",
    "# PATH = 'results/diff_filters/'\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTAS:\n",
    "- Se consiguen resultado prácticamente iguales con y sin bias\n",
    "- Clara diferencia usando optimización alterna vs única pasada (la única iba mejor con LR=0.05 y WD=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "def summary_table(acc, index_name):\n",
    "    mean_accs = acc.mean(axis=1)\n",
    "    med_accs = np.median(acc, axis=1)\n",
    "    std_accs = acc.std(axis=1)\n",
    "    return DataFrame(np.vstack((mean_accs, med_accs, std_accs)).T, columns=['mean accs', 'med', 'std'], index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: WisconsinDataset\n",
      "Number of nodes: 251\n",
      "Number of features: 1703\n",
      "Shape of signals: torch.Size([251, 1703])\n",
      "Number of classes: 5\n",
      "Norm of A: 22.69361114501953\n",
      "Max value of A: 1.0\n",
      "Proportion of validation data: 0.32\n",
      "Proportion of test data: 0.20\n",
      "Node homophily: 0.13\n",
      "Edge homophily: 0.20\n"
     ]
    }
   ],
   "source": [
    "# Dataset must be from DGL\n",
    "dataset_name = 'WisconsinDataset'\n",
    "\n",
    "A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device,\n",
    "                                                     verb=True)\n",
    "N = A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without normalizing the GSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PARAMETERS  -  0.874\n",
    "## Training params\n",
    "N_RUNS = 10\n",
    "N_EPOCHS = 200  # 100\n",
    "EPOCHS_h = 25\n",
    "EPOCHS_W = 25\n",
    "LR = .005\n",
    "WD = .001\n",
    "DROPOUT = .25\n",
    "\n",
    "# BEST PARAMETERS\n",
    "## Architecture params\n",
    "N_LAYERS = 2\n",
    "K = 3  # 2\n",
    "HID_DIM = 32  # 32\n",
    "\n",
    "## Model params\n",
    "h0 = 1  # 1\n",
    "NORM = False\n",
    "\n",
    "IN_DIM = feat.shape[1]\n",
    "OUT_DIM = n_class\n",
    "\n",
    "ACT = nn.ReLU()   # nn.LeakyReLU() \n",
    "LAST_ACT = nn.Softmax(dim=1)\n",
    "LOSS_FN = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting eval/test acc/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val acc: 0.825  -  Best test acc: 0.804\n",
      "Test acc at best val: 0.745\n",
      "Val acc: 0.800  -  Test acc: 0.765\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = N_EPOCHS\n",
    "epochs_h = EPOCHS_h\n",
    "epochs_W = EPOCHS_W\n",
    "lr =  .005 #LR\n",
    "wd = .005  #WD\n",
    "drop = DROPOUT\n",
    "L = N_LAYERS\n",
    "K_aux = K\n",
    "hid_dim = HID_DIM\n",
    "h0_aux = 1\n",
    "norm = False\n",
    "act = ACT\n",
    "lact = LAST_ACT\n",
    "loss_fn = LOSS_FN\n",
    "patience = 200\n",
    "bias = True\n",
    "\n",
    "iters_aux = 1\n",
    "\n",
    "err1 = np.zeros(iters_aux)\n",
    "err2 = np.zeros(iters_aux)\n",
    "for i in range(iters_aux):\n",
    "\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "\n",
    "    # Create model\n",
    "    arch = GFGCN(IN_DIM, hid_dim, OUT_DIM, L, K_aux, act=act, last_act=lact,\n",
    "                dropout=drop, init_h0=h0_aux, bias=bias)\n",
    "    S = torch.Tensor(A).to(device)\n",
    "    model = GF_NodeClassModel(arch, S, K, masks, loss_fn, device=device)\n",
    "    loss, acc = model.train(feat, labels, epochs, lr, wd, epochs_h=epochs_h, epochs_W=epochs_W,\n",
    "                            patience=patience)\n",
    "\n",
    "    idx_max_acc = np.argmax(acc[\"val\"])\n",
    "    print(f'Best val acc: {acc[\"val\"][idx_max_acc]:.3f}  -  Best test acc: {np.max(acc[\"test\"]):.3f}')\n",
    "    print(f'Test acc at best val: {acc[\"test\"][idx_max_acc]:.3f}')\n",
    "\n",
    "    acc_val = model.test(feat, S, labels, masks['val'])\n",
    "    acc_test = model.test(feat, S, labels, masks['test'])\n",
    "    print(f'Val acc: {acc_val:.3f}  -  Test acc: {acc_test:.3f}')\n",
    "\n",
    "    err1[i] = acc[\"test\"][idx_max_acc]\n",
    "    err2[i] = acc_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc at best val acc: 0.745 +- 0.000\n",
      "Acc at test: 0.765 +- 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f9e4c563d00>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAFfCAYAAAAxo9Q/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAACQ9UlEQVR4nO2dd3gUVReH3930hCSUkJDQO0iJ9F6UjoBgBxQQRFGwgYooItgVUZQPGwqogF1Q6V2q9NA7CaEk9HSSbLLz/XEzu1mSQMommw3nfZ59ZnbquTPJzm/OOfdcg6ZpGoIgCIIg3NYYHW2AIAiCIAiORwSBIAiCIAgiCARBEARBEEEgCIIgCAIiCARBEARBQASBIAiCIAiIIBAEQRAEAXB1tAG5wWw2c/78eXx9fTEYDI42RxAEQRCcBk3TiI+PJyQkBKMxZz+AUwiC8+fPU7lyZUebIQiCIAhOy5kzZ6hUqVKO651CEPj6+gKqMX5+fnY5pslkYuXKlXTv3h03Nze7HNPRSJucA2mT81AS2yVtcg7s2aa4uDgqV65seZbmhFMIAj1M4OfnZ1dB4O3tjZ+fX4n6A5I2FX+kTc5DSWyXtMk5KIw23SrkLkmFgiAIgiCIIBAEQRAEQQSBIAiCIAg4SQ6BIAiCUDikp6djMpkcbUaBMJlMuLq6kpycTHp6uqPNsQt5aZObmxsuLi4FPqcIAkEQhNsQTdOIiooiJibG0aYUGE3TqFChAmfOnCkxtWry2qbSpUtToUKFArVfBIEgCMJtyMWLF4mPjycwMBBvb2+nfpCazWYSEhIoVarUTQvvOBO5bZOmaSQlJXHx4kUAgoOD831OEQSCIAi3GQaDgbi4OIKCgihXrpyjzSkwZrOZ1NRUPD09S5QgyG2bvLy8ACXyAgMD8x0+KBlXThAEQcg1+gPD29vbwZYI9kK/lwXJBxFBIAiCcJvizGECwRZ73EsRBIIgCIIg3KY5BFev4vLAA3Q8exZ69XK0NYIgCILgcG5PD4G7O8Z16yhz/Dhcv+5oawRBEITbmPXr12MwGBzeBTTPgmDDhg307duXkJAQDAYDixYtuun2ekNv/ERHR+fX5oLj44OmZ2GWgD64giAItwPDhg2jf//+jjaDiIgIDAYDYWFhdjle27ZtiYqKwt/f3y7Hyy95FgSJiYmEhoYyc+bMPO139OhRoqKiLJ/AwMC8ntpupJpNpPn6qC8iCARBEIRCIDU1NVfbubu7F7iokD3IsyDo1asX77zzDgMGDMjTfoGBgVSoUMHycWRf0ZjkGCINcQCYr111mB2CIAjFAU2DxETHfDTNfu34999/admyJR4eHgQHB/Pqq6+SlpZmWf/777/TqFEjvLy8KFeuHF27diUxMRFQ3uyWLVvi4+ND6dKladeuHadPn872PNWrVwegSZMmGAwGOnfuDFg9GO+++y4hISHUrVsXgB9//JHmzZvj6+tLhQoVGDRokKWQkH7uzCGDuXPnUrZsWdasWUODBg0oVaoUPXv2JCoqyn4XKxuKLKnwzjvvJCUlhYYNGzJ58mTatWuX47YpKSmkpKRYvsfFqYe3yWSyS81tHxcfznhmHDs6ktJOXsdbR782zl6XPDPSJuegJLYJSma79LZomobZbMZsNpOYCH5+jnlJi4sz4+OTu201TbPYfePy8+fP06dPH4YOHcrcuXM5cuQITz31FB4eHrz55ptERUUxcOBAPvzwQ/r37098fDybNm0iPT2d1NRU+vfvzxNPPMH8+fNJTU1l+/bt2Z4L4L///qN169asXLmSBg0a4O7ujtlsRtM01qxZg6+vLytWrABUgaGUlBSmTJlC3bp1uXjxIi+99BJDhw5lyZIllm30qf5JSkrif//7H3PnzsXFxYUhQ4Ywbtw45s2bl+210c9vMpmyFCbK7d9voQuC4OBgvvrqK5o3b05KSgrffvstnTt3Ztu2bTRt2jTbfd5//32mTJmSZfnKlSvtVkjDz9MAaOzd8i+JXqXtcsziwqpVqxxtgt2RNjkHJbFNUPLapQ+ak5CQQGpqKuolubRDbImLiyO34xGZTCbS0tIsL4mZ+e6776hYsSLvvvsuBoOBkJAQxo8fz5QpU3j++ec5ceIEaWlpdO3albJly1K2bFmqVq2K2Wzm3LlzxMbGctddd1G+fHkAixc8u3PplQE9PT0tz6S4uDhMJhPe3t5MmzYNd3d3y/IHHnjAsm9AQADvvvsud999N+fPn6dUqVIkJSUBEB8fj9FoJDk5GZPJxCeffGLxRgwfPpypU6dmaw+o8MT169fZsGGDjVcEsBz/VhS6IKhbt67FbQIqeeLkyZN8+umn/Pjjj9nuM2HCBMaOHWv5HhcXR+XKlenevTt+fn52sWvlGx5AMtUCylC1d2+7HNPRmEwmVq1aRbdu3XBzc3O0OXZB2uQclMQ2Qclsl8lkYt26dXh6elKqVCk8PT3x9VVv6o7A29uP3IbO3dzccHV1zfIc0DSNY8eO0bZtW5vEvC5duvDyyy8TFxdH27Zt6dKlC+3bt6d79+5069aNBx54gDJlyuDn58fQoUO5//776dq1K127duXBBx/McVyAUqVKAeDj42Nji5ubG40aNSIgIMBm+127djFlyhT27dvHtWvXLB6BmJgYQkJCLKLC19cXPz8/i9CoXr06vr6+GAwGqlevzqVLl3J8BiYnJ+Pl5UXHjh3x9PS0WZeTiLgRh9QhaNmyJZs2bcpxvYeHBx4eHlmWu7m52e2fMqWUJ5CMOeZKiflH17HndSouSJucg5LYJiiZ7TIYDBiNRks+l6+vgw3KBXovtRtz0DK79TOv0+eNRiNubm6sWrWKLVu2sHLlSmbOnMkbb7zBtm3bqF69OnPnzuX5559n+fLl/Prrr7zxxhusWrWK1q1bZ7Ej83Ezn89gMGQZjCgxMZFevXrRo0cP5s+fT/ny5YmMjKRHjx6kpaXZHEOf1+3Vj2k0GnFxcUHTtBzz74xGIwaDIdu/1dz+7TokaBQWFlagEZnsQWoppcjSr11xqB2CIAhCwalTpw7//fcfWqYsxc2bN+Pr60ulSpUA9XBt164dU6ZMYc+ePbi7u7Nw4ULL9k2aNGHChAls2bKFhg0bsmDBgmzPpYcD0nMR6zhy5AhXrlzhgw8+oEOHDtSrV88mobA4kWcPQUJCAidOnLB8Dw8PJywsjLJly1KlShUmTJjAuXPn+OGHHwCYPn061atXp0GDBiQnJ/Ptt9+ydu1aVq5cab9W5IM0f+Xy0a7FONQOQRAEIffExsZm6f9fpkwZRowYwVdffcWzzz7LmDFjOHr0KG+++SZjx47FaDSybds21qxZQ/fu3QkMDGTbtm1cunSJ+vXrEx4ezjfffEO/fv0ICQnh6NGjHD9+nCFDhmRrQ2BgIF5eXixfvpxKlSrh6emZYw2BKlWq4O7uzowZMxg1ahQHDhzg7bfftvdlsQt5FgQ7d+7krrvusnzXY/16ZmdUVBSRkZGW9ampqYwbN45z587h7e1N48aNWb16tc0xHEF6RhzGEBvjUDsEQRCE3LN+/XqaNGlis2z48OFMmzaNxYsXM378eEJDQylbtiwjRoxg4sSJAPj5+bFhwwamT59OXFwcVatWZdq0afTq1YsLFy5w5MgRvv/+e65cuUJwcDCjR4/mqaeeytYGV1dXPv/8c9566y0mTZpEhw4dWL9+fbbbli9fnrlz5/Laa6/x+eef07RpUz7++GP69etn1+tiDwyaZs9eoIVDXFwc/v7+xMbG2i2p8Idx3RjyyWqONa1GnV3hdjmmozGZTCxdupTevXuXmHintMk5KIltgpLZLpPJxMqVK6levTo1atTIkoDmjJjNZuLi4vDz83NojRt7ktc2JScnEx4eTvXq1bNNKszNM7RkXLl8YChTBgC3eFWUYnPkZt7d8C7p5lz2fxEEQRCEEsRtKQguXICfFgcB4JGgBjd6bvlzTFw3kVWnSlZ/Y0EQBEHIDbelIPDzgzMxaiwFrwRVEfFM7BkADl486DC7BEEQBMFR3JaCwMsLko2q22OppDTS0k1cTroMwNErRx1pmiAIgiA4hNtSEABQqiIAbukal66cQUPlVoogEARBEG5HbltB4OVXgfSMcplXok5alh+5fMRBFgmCIAiC47htBUF5v7LEZPTMuBYVzltr4cRn0H7HRWKSYxxqmyAIgiAUNbetIAgpU9oiCC6eO8bQMKh5Df74FUyPDYYSNNypIAiCINyK21YQBAf4EJsxftK1iMNUyhgMygyU/30pzJ/vMNsEQRCEks369esxGAzExMQ42hQLt60gCCxvIMZdVW723H8YIxDnDp/pA1vt2eMw27IlOhpGj4Z9+xxtiSAIgkMYNmwY/fv3d7QZJRaHDH9cHChfXiPG1QNII+RoFACny7txIDAjVHD4sOOMy4558+CLL+DqVfjpJ0dbIwiCIJQwbl8PQSDEuKokggZnkgG4XiWYwwEZGxQ3QRClRAsnT958O0EQhNuUf//9l5YtW+Lh4UFwcDCvvvoqaWlplvW///47jRo1wsvLi3LlytG1a1cSE1X5+vXr19OyZUt8fHwoXbo07dq14/Tp09mep23btowfP95m2aVLl3Bzc2PDhg0A/PjjjzRv3hxfX18qVKjAoEGDiu2wxzq3rSAICNCIcfEBIEj9PeBetwGHy2dscPYsxMU5xrjsuHRJTcNLxkBMgiAUHzRNIzE10SEfe42vd/78efr06UOLFi3Yu3cvX375Jd999x3vvPMOAFFRUQwcOJDhw4dz+PBh1q9fz3333YemaaSlpdG/f386derEvn372Lp1K08++SQGgyHbcw0ePJiff/7ZxvZffvmFkJAQOnToAKhBpN5++2327t3LokWLiIiIYNiwYXZpa2Fx24YMAgMhxuBrsyygUUuuJ60l2ieFConAkSPQsqVdz3s56TL+Hv64ueRx5DRdWV6+DAkJUKqUXe0SBOH2JcmURKn3HfObkjAhAR93nwIf57vvvqNy5cr873//w2AwUK9ePc6fP8/48eOZNGkSUVFRpKWlcd9991G1alUAGjVqBMDVq1eJjY2lT58+1KxZE4D69evneK6HHnqIF154gU2bNlkEwIIFCxg4cKBFRAwfPtyyfY0aNfj8889p0aIFCQkJlCqmv9+3sYcAYgz+NstK1b+TOuXqcEj3Etg5bLA5cjMh00IYtXhU3nfWPQQAERF2s0kQBKEkcOzYMVq3bm3zVt+uXTsSEhI4e/YsoaGhdOnShUaNGvHggw8ya9Ysrl27BkDZsmUZNmwYPXr0oG/fvnz22WdE6WHabChfvjzdu3dnfkZvtPDwcLZu3crgwYMt2+zatYu+fftSpUoVfH196dSpEwCRkZGF0Xy7cNt6CNzdIcHFVhD4NWhCvYR6HC6/n7sjuKUgOB9/HheDC0GlgnJ1zvc2vYfJbOKPw38wq98sjIY86LHMgiA8HBo2zP2+giAIN8HbzZuECQkOO3dR4OLiwqpVq9iyZQsrV65kxowZvP7662zbto3q1aszZ84cnnvuOZYvX84vv/zCxIkTWbVqFa1bt872eIMHD+a5555jxowZLFiwgEaNGlk8DomJifTo0YMePXowf/58ypcvT2RkJD169CA1NbVI2psfblsPAUCiq1UQJLuCsVJlGgc1zlVi4f4L+6n7v7o0/LIh0QnRtzzX4UuHWXp8KQCxKbF5G1VR06whA5A8AkEQ7IrBYMDH3cchn5zi9HmlTp06/PfffzZx/c2bN+Pr60ulSpUs7WzXrh1Tpkxhz549uLu7s3DhQsv2TZo0YcKECWzZsoWGDRuyYMGCHM937733kpyczPLly1mwYIGNd+DIkSNcuXKFDz74gA4dOlCvXr1in1AIt7kgSPUoY5k/V94TjEYlCG4RMriSdIV7f76XhNQELiddZuyKsVm2MaXbVjr89L9Pbb5vPrM594YmJEBKivW7CAJBEG5TYmNjCQsLs/mcOXOGESNGcObMGZ599lmOHDnCX3/9xZtvvsnYsWMxGo1s27aN9957j507dxIZGcmff/7JpUuXqF+/PuHh4UyYMIGtW7dy+vRpVq5cyfHjx2+aR+Dj40P//v154403OHz4MAMHDrSsq1KlCu7u7syYMYNTp07x999/8/bbbxfF5SkQt7UgMHlaBcGlCirBMDQo1OIh0E6etH0QA2nmNB7+/WHCY8Kp6FsRo8HITwd+YsWJFQAkpyUz8I+B+H/gz7e7vwXgQsIFftj7AwBdqncB8igIMocLQASBIAi3LevXr6dJkyY2n7feeouQkBAWL17M9u3bCQ0NZdSoUYwYMYKJEycC4Ofnx4YNG+jduzd16tRh4sSJTJs2jV69euHt7c2RI0e4//77qVOnDk8++SSjR4/mqaeeuqktgwcPZu/evXTo0IEqVapYlpcvX565c+fy22+/cccdd/DBBx/w8ccfF+p1sQe3bQ4BgOYdYJmPqaTcAlX8q5AU4EesRxz+KWY4ftwmXj9+1XjWhK/Bx82HZYOXMXvPbKZvm87jfz3Oc62eY+nxpWyM3AjAyH9GcuDiARYdWURKegotQlrwSrtXWBO+hk2Rm3Jv6I2uJhEEgiDchsydO5e5c+dmWW42m4mLi6NTp05s3749233r16/P8uXLs10XFBRkEzrILb169cqx2+TAgQNtvAaAzbadO3e2W5dLe3FbewhcfMta5pOrVgRUjKlxhdBs8wh+3Psjn/z3CQDf9/+eRkGNeOuut6hRpgZRCVFMWDOBjZEb8fPwY3AjFU/6bNtnnI49TVX/qnzV5yvaVGqD0WAkIiaCU9dO8cjvj/DUP0/d/A9D9xD4ZnSTDA9XeQWCIAiCYCdua0Hg5V+a9Ix8FnON6pbljYMaW7seLlsGwLErxxj5z0gAJnaYyP133A+Ar4cvO0bu4Mt7vqR7ze40C27Gxsc38uOAH3nnrnfw8/Dj+VbPc+CZAzQNboqvhy+hQaEA9P2pL78c/IVvdn/DnuibjJ2gC4LmzdU0Pl6VMBYEQRAEO3FbhwwCfL2JKA1VY8DYqLFleWhQKPMaw/AwYM4c6NuXOW4bGBCWQmeXmjxp0sB9E7RvD0BZr7KMaj6KUc1t6wu83vF1XuvwWpYs2naV27Eneg+HLh2yLJu/bz5Ng5tmb6geMqhSBSpUUAMdhYdDuXIqx6F9e+U9yMEdJgiCIAi34rb2EJT3dafXYOj0OPjVtGaTNg5qzLoaMLOzqp6lDR/OC4M+56c/4KlfT2J4913o00e9qd+C7LrUtK/SPsv8Twd+It2cnv1BdA9B+fJQPcOToRcn+vdf2LkT1q0rfuMvCIIgCE7DbS0IypZO57hPabZUgRDfEMvyhoENMWDgxfaJmJrdiSEmhqB4M2f9DaQ/OhgqVYLYWJg9O1/n7V6zO9VLV6dvnb6seHQFZb3KEpUQxbqIddnvoAuCwECrINATCxcvtmxmzBhUw7BgATz2mCq9LAiCIAi54LYWBP7+KbDsc/h3EjX961qW+7j7UKtsLUyu8N/0l9nYtQ4j+sE73w3D5cd5kNGNhenTIdNIWrmljFcZTj53kr8e+QtvN28evONBAObvn5/9DnrIILOHQE8szCQIDP/+izE1FZdnn1XDJTdtCt98IwmIgiAIwi25rQWBr28qhv2PwropXL1q69pvHKRyCt48+S19u1xgdlN4pNkQtfKxx1T8PiICFi3K17kNBoMlnKD3SPjj0B/Ep2QThsguZBAWpkIEmbogGjZuJGj3bgx6KOP6dXjqKcimm44gCIIgZOa2FgQuLsoLD2q048zcV/8+ANZFrCM2JZZKfpXoWLWjWuntDU8/reanTSuwHe2qtKN22drEp8Yzad2krBtkDhncdZcaiGHrVhipej1w113g6Ynh4kXq/PqrWvbCC/DKK2p+/HiIiSmwnYIgCELJ5bYWBAA1aih3+smTtssHNRrEgacP8GjjR/F28+bVdq/aDkY0ejS4ucF//8GuXQWywWgw8nmvzwH4fPvn7Dqf6XiZxzEoXx5q1IA33lDft2xR0/vvh7ZtASh96pRa9sgj8M47UL++EhRvvlkgGwVBEISSzW0vCDKGvubEiazrGgQ24McBP5L4WiKjW462XVmhAjzwgJr/+usC29Hzoh8fJXfArJl5cvGTpJkzchMyj2NQPqM4wvjx0NjaTZJ77oHOnS1ftWrVoGVLJVhmzFALZ86EAwcKbKcgCIJQMrntBUGtWspDcPx4PnYelVF3YP581esgv/z8M3TsyMsfbKRFjA+7o3az9cxWtU4PF3h5gY/qBombm+rh4O0NHTtCtWo2gsB8//2gd3fs0gX69oX0dNDDCYIgCE7IsGHD6N+/v6PNICIiAoPBQFhYmF2PazAYWJTPvDR7cNsLgpo1lSDIzkNwSzp0gDvugKQkldWfH376CQYPVg9sYORpVTM5KiFKrdfDBXqyg06zZnD6tLUYUcuWaKVKAWB+6CHbbfUKh9G3HqZZEARBuD257QVBrVpqmi9BYDBYvQT/+x/o8fsbSUqCb7+FyEjb5QcOwJAhYDZDvXoAdN6rPA1Xkq6o/IHMPQxuJCBAeQ4APDxI//NPdrz0EjRpYrudvu+NoyYKgiCUIP79919atmyJh4cHwcHBvPrqq6Rl6hr++++/06hRI7y8vChXrhxdu3YlMTERUKMotmzZEh8fH0qXLk27du04ffp0tuepntHbq0mTJhgMBjpn8tB+++231K9fH09PT+rVq8cXX3xhWZeamsqYMWMIDg7G09OTqlWr8v777wNQrVo1AAYMGIDBYKBGjRr2vDS54rYuXQxWD0F0tCo8qI8flGseewwmTFBFgGrWhF694LffrO59TYPHH1fu+qAgWLECQkOVCHj6aVXHoG9flYdQsSK1T8VQKRYafPEb/PU6tGqljpOdILgBrXNnziclceeNK0QQCIJwMzRNvbg4Am9va4izAJw/f54+ffowbNgwfvjhB44cOcLIkSPx9PRk8uTJREVFMXDgQD766CMGDBhAfHw8GzduRNM00tLS6N+/PyNHjuSnn34iNTWV7du3Z1tpFmD79u20bNmS1atX06BBA9zd3QGYP38+kyZN4n//+x9NmjRhz549jBw5Eh8fH4YOHcrnn3/O33//za+//kqVKlU4c+YMZ86cAWDHjh0EBgYyZ84cevbsmeO5C5PbXhCULq1etC9fVj0N7rwzHwdYtgzefhvWrFHz8+ap/v8A339vjd1fuACdOsFnn6lugJs2KeEwcyYEB6ueAps3894aaL9/HWhYQwI3hgzyQnaCwGwG423vIBIEAZQYyAg5FjkJCdYXqALw3XffUblyZf73v/9hMBioV68e58+fZ/z48UyaNImoqCjS0tK47777qFq1KgCNGjUC4OrVq8TGxtKnTx9qZmSa169fP8dzlc/4TS1XrhwVKlSwLH/zzTeZNm0a992nuq1Xr16dQ4cO8fXXXzN06FAiIyOpXbs27du3x2AwWOzIfMzSpUtToUIFy5DORYk8EbCGDfKVWAgql2DlSnj3XfX9zz/V9MQJGDNGzU+YoAYhio2FYcNUnQCAyZOhcmU1n5Es89g+MGpAXWv1xNx4CHLkRkFw7JgqrPT66/k/piAIQjHi2LFjtG7d2ubNul27diQkJHD27FlCQ0Pp0qULjRo14sEHH2TWrFlcu3YNgLJlyzJs2DB69OhB3759+eyzz4iKisrT+RMTEzl58iQjRoygVKlSls8777zDyYx+7cOGDSMsLIy6devy3HPPsXLlSvtdADuQZ0GwYcMG+vbtS0hISJ4zIjdv3oyrqyt35vk1vHCpXVtN85VHkJkMVcjatXDtGrz0EiQmqh4Ab7+twgWTJlm7DLZoAc8/b90/U/ZsrK87bN6sqgy2aAE3JgrmBV0QXLsGJpMaECkmBhYsyP8xBUEoOXh7qzd1R3y8vYukiS4uLqxatYply5Zxxx13MGPGDOrWrUt4RrXXOXPmsHXrVtq2bcsvv/xCnTp1+O+//3J9/ISEBABmzZpFWFiY5XPgwAHLcZo2bUp4eDhvv/02169f56GHHuIBvft6MSDPgiAxMZHQ0FBmzpyZp/1iYmIYMmQIXbp0yespC50CJRZmpk4daNhQ5QW8+y789ZeKjX35pSqL6O0NU6bA3r3qbX3jRtWFMJMhMQ2Uu2raQ5XUW/zQobB9uxIF+aVsWWuM7soVOH9ezUdEwNWr+T+uIAglA4NBue0d8bFTrFx/gGuZxm7ZvHkzvr6+VKpUKaOZBtq1a8eUKVPYs2cP7u7uLFy40LJ9kyZNmDBhAlu2bKFhw4YsyOGlSc8ZSE+3jlAbFBRESEgIp06dolatWjYfPQkRwM/Pj4cffphZs2bxyy+/8Mcff3A143fYzc3N5phFTZ5zCHr16kWvXr3yfKJRo0YxaNAgXFxcHNrPMjsKHDLIzH33qd4Deknjhx6y9CCwISAg292PffcRL311P+cbG3jLDuYASoyUK6cSJS5dgnPnrOv27FG1CgRBEJyA2NjYLP3/y5Qpw4gRI/jqq6949tlnGTNmDEePHuXNN99k7NixGI1Gtm3bxpo1a+jevTuBgYFs27aNS5cuUb9+fcLDw/nmm2/o168fISEhHD16lOPHjzNkyJBsbQgMDMTLy4vly5dTqVIlPD098ff3Z8qUKTz33HP4+/vTs2dPUlJS2LlzJ9euXWPs2LF88sknBAcH06RJE4xGI7/99hsVKlSgdOnSgOppsGbNGtq1a4ebmxsuLi6FfDVtKZKkwjlz5nDq1CnmzZvHO++8c8vtU1JSSNGr84ElscJkMmEymexik34ck8lEtWoGwJUTJzRMpryPXmhDv364vWV9lJteeUW56XOJZ5VabKwGpa9fyXNbM7fpRlwDAjBcvkxaVBTGs2ctrqH0HTswd+yY9WAXLmA4fRqtZcs82WBvbtYmZ0Xa5DyUxHbpbdE0DbPZjNlsdrBFuUfTNNavX0+TG7pWDx8+nGnTpvHPP//w6quvEhoaStmyZRk+fDivvfYaZrOZUqVK8e+//zJ9+nTi4uKoWrUqH3/8MT169ODChQscPnyY77//nitXrhAcHMwzzzzDyJEjs70+RqOR6dOn88477zBp0iQ6dOjA2rVrGT58OJ6enkybNo2XX34ZHx8fGjVqxHPPPYfZbMbHx4ePPvqI48eP4+LiQosWLVicMWKt2Wxm6tSpvPTSS8yaNYuKFSsSFhZmuU+3wmw2o2kaJpMpi5DI7d+vQdPyPzauwWBg4cKFN60cdfz4cdq3b8/GjRupU6cOkydPZtGiRTet8DR58mSmTJmSZfmCBQvwLoR4U0KCG48+2huAn39ejKdnAVw2mkaXp5+mVHQ051u3Zserr+Zp9xhTDMMODgPgj9A/cDHYRyG2e/11Ag4eZMdLL1Hn99/xj4gA4GyHDuwaNy7L9u0nTKDc4cOsnT6d+Iz+sYIglAxcXV2pUKEClStXtri/BecmNTWVM2fOEB0dbVN7ASApKYlBgwYRGxuLn59fjscoVA9Beno6gwYNYsqUKdSpUyfX+02YMIGxY8davsfFxVG5cmW6d+9+08bkBZPJxKpVq+jWrRtubm4895zG1asGatbsQWhowY5t+PBDzP/7H+VnzaJ35p4CuSDNnGYRBK07t6a8T+57F9zYpsy4fP89HDxI00qVMGYU4gCoGB1NUO/etgdKScE1I37S0ccH7cb1RcjN2uSsSJuch5LYLpPJxLp16/D09KRUqVJ4eno62qQCo2ka8fHx+Pr6OqT/fmGQ1zYlJyfj5eVFx44ds9zT3HZfLFRBEB8fz86dO9mzZw9jMrrf6W4NV1dXVq5cyd13351lPw8PDzw8PLIsd3Nzs/s/pX7MWrVU7t7p026WSr/5ZsgQGDIkX3063XDD38Of2JRY4tLiCHELyfsxsrtOQUEAuERH29QjMBw/jtv165BZaB08qBIjAdeTJ20THx1EYdx7RyNtch5KYrsMBgNGoxFjCahHorvU9TaVBPLaJqPRiMFgyPZvNbd/u4V65fz8/Ni/f79NF4xRo0ZRt25dwsLCaKVX4SsG6FUic6o+XJSU8y4HZJQvthd618N9+9TUwwMyMm/Zu9d228zfjx2znw2CIAhCsSXPHoKEhAROZOqfFx4eTlhYGGXLlqVKlSpMmDCBc+fO8cMPP2A0GmnYsKHN/oGBgXh6emZZ7mj0YZCLhSDwKsepa6e4cr0QBIGeuxESouohnD0Lu3ap4ko6mfM7RBAIgiDcFuTZQ7Bz506aNGliyfIcO3YsTZo0YdKkSQBERUUReeMgPk6ALggyCko5lEL1EOg1CCpWhKZN1fyKFaq6ov7wz+whOH7cMhKjIAgliwLklAvFDHvcyzx7CDp37nzTE8+dO/em+0+ePJnJkyfn9bSFTrEKGXhlCILC8BDoZBYEy5erT+nSakTGzIIgJQXOnAHpaSAIJQa9+E1SUhJe+oipglOTlDE4VUFyXW77wY10dA/B6dMqn87VgVfGIggKw0OgExICd98N3bqpsMH586qc8bRpqnqhqytUrapcJseOiSAQhBKEpmn4+flx8eJFALy9vZ06O99sNpOamkpycnKJSirMTZs0TSMpKYmLFy9SunTpAhUzEkGQQUiIyrPTX4gzVZosciwhg8L2EHh7q0GZAD76CMaPV1NQ1RVr1bIKgu7dc38uTVOjKRZxlS1BEHJPYGAgLi4uFlHgzGiaxvXr1/Hy8nJqYZOZvLZJHyWxIIggyMBoVCLgyBH1DHSkIAjwVmWN7SoIbiyVXLGi7fchQ+C11+D6dfU9NNS6TV4TC7t1U+MkHDgAJaCPsyCURAwGA8HBwQQGBjp9JUaTycSGDRvo2LFjiekempc22avMsQiCTNSooQSBo/MI9JDB5aTL9juom5vKEYiJUd9vFAQVKkCfPmpAJoA774QyZdR8XgRBfDysWWPdTx/ZURCEYomLi0uR18y3Ny4uLqSlpeHp6VliBIEj2lQygi12orj0NCiUXgZgGza4URAAjBhhnQ8NVaM3Ahw9mvtzZFZTmQdREgRBEIo1IggyUVx6GhRKLwOwFQTBwVnX9+oF9eurkRFbtLAKgtOnITk5d+cQQSAIguCUiCDIRHH0ENi1n7AuCMqWhey6Grm6wn//KVd/6dIQGAj+/ipJMLcXJfN2IggEQRCcBhEEmdA9BCdPqmego9A9BCaziYTUBPsdWBcE2YULdPz8lGAAMBisXoLDh3N3DhEEgiAITokIgkzogiAuTnXFdxTebt54uKjBnQql6+HNBMGNtGyppsuW5W57CRkIgiA4JSIIMuHlpeoRgGPDBgaDoXASCxs0UNO8jO/8wANqunAh5KZrkngIBEEQnBIRBDegewkcPaZPoSQWDhwI27bBlCm536dDBzV08rVr1u6EOZGWphIQdUQQCIIgOA0iCG6gdWs1nT/fsXYUiofAaFQhAA+P3O/j4gL33afmf/vt5tueOaNEgV5V6/JlVfpREARBKPaIILiBUaPU82z58rx1v7c3waVUt8Bz8cXgLfvBB9V00aKbhw30cEHdulbRERVVqKYJgiAI9kEEwQ3UrKkK9gHMnOk4O2qUUbGLU9eKwfCLHTuqLohXr948bKALgpo1rckYEjYQBEFwCkQQZMOzz6rpnDmqx4EjKFaCwMUFHnpIzb/+ugoLbN6syh23bAkzZqiSyHoPg5o1rT0ZRBAIgiA4BSIIsqFrV1WwLyEBBg9WowMXNbogOHnNwVWSdF5/XY1tsHs3vPAC9O8PFy7Ajh3w3HMq+SIsTG1bo4YIAkEQBCdDBEE2GAzwzjvqxXjxYiUOliwpWht0QRARE0G6OT3LelO6iaGLhvLZf58VjUEVKsD06Wp+5kyVMNismVoWHKwSLvShlMVDIAiC4HSIIMiB++6DXbugTRvlKXjmGUhNLbrzV/StiLuLO2nmNM7GZXVR/Hv6X37Y+wPjVo7jfPz5ojHqscfUeAegcgT++guefx5+/12pJx0RBIIgCE6HCIKbEBqqcugqVIDISPj++6I7t4vRhWqlqwHZ5xEcunQIgHQtnVm7ZhWNUQYD/Pijcp+sX2996LdtC+++q+ZdXKB6dREEgiAIToYIglvg5QXjx6v5997LXbE+e3GzxMKDFw9a5r/e9TWm9CIyrFw5lU9Qu7bt8pdfhrfegi++AE9PEQSCIAhOhgiCXPDkk6pYX0SEekEuKmqUzlkQHLp8yDIflRDFX0f/KjK7ssVohDfeUBcLbAWBI0eKEgRBEHKFCIJc4O2tXoBBvQAXFTn1NNA0zeIhuKf2PQDM3OHAognZEawKK5GSosoeC4IgCMUaEQS5ZNAgNd29u+hGQqxZtiaQ1UNwIfEC15KvYTQYmdZ9GgDrI9aTmJpYNIblBk9PFV4ACRsIgiA4ASIIcklwMNxxh/J+r1tXNOfMKYdA9w7UKFODugF1KetVVm0XUwyKGGUmKEhNL150rB2CIAjCLRFBkAe6dFHTWw36Zy+ql64OqBEPY5NjLcv1HgYNyqvhjGuWUZ6EYlPESMfXV00Ti5HnQhAEQcgWEQR5QBcEq1cXzfl8PXwp710egPCYcMvyg5eUh+CO8ncAUKtsLaCYlDnOTKlSapqQ4Fg7BEEQhFsigiAPdO6skumPH1cj/RYFeh7ByavWt3+n8RDogiA+3nb5qVPQty/06AEPPAAbNhS9bYIgCIINIgjygL8/tGih5osqbHBjTwNN05zHQ6CHDG70EHz6qaoJvXIl/PEHjBlT9LYJgiAINoggyCNFnUdQr1w9APZf3A/AxcSLXL1+FQMG6gWodRYvQibRoBWHvv85hQyWLVPTceOUy2X//pxdLlev4nv6dOHZKAiCIAAiCPKMLgjWry+a8zULaQbAzvM7AWv+QI0yNfBy8wKsHoLIuEhMZhOjlo4i8ONAouKjisbInMhOEBw/DidPgpsbvPmmGiURYOnSbA/h8tBD3PXCC3DsWOHaKgiCcJsjgiCPNG+upmfPwpUrhX++ZsFKEBy9fJT4lHi2ntkKQJPgJpZtgnyC8HHzwayZiUyOZN7+eVxOusy6iCLqH5kT2QkC/cHfoYMKKfTubbs8M5qGYedODJqGYd++wrVVEAThNkcEQR7x81Nj94DydBc2QaWCqORXCQ2NPdF7WH96PQCdq3a2bGMwGCxhg1VXVmEyq3ENDl86XPgG3ozsBIEeLtBHTbxHVVpk9WpITrbdPzoaQ1ISAAY9pGA2Q3R0IRksCIJw+yKCIB+Ehqrp3r1Fcz7dS7D1zFa2nNkCQKdqnWy20cMG66+ttyw7cuVI0RiYEzcKgqQka6xFFwShoWoo5aSkrL0NTmVKkoyMVNPPP1dVoopy6ElBEITbABEE+aBxYzUtKi928xAVp5i1exZJpiQCvAMsPQx09K6HyWbrW3ax8xCsX6/GNqhSRZV9BDWksh42WLLEdv+T1m6UBl0Q6EUg3n1XeQsEQRAEuyCCIB8UtSDQPQR6L4JOVTthNNjeOt1DkJnjV4+TZk675fHjUuL4cseX/H30bztYm4kbBYHeNaNnTyUEdPSwwbx5EG4twGQjCPSQwfHj1unKlfa1VxAE4TYmz4Jgw4YN9O3bl5CQEAwGA4sWLbrp9ps2baJdu3aUK1cOLy8v6tWrx6effppfe4sFesjgwAFIu/XztsDoPQ10OlXtlGUb3UMA0LZSWzxdPUlNTyX8WniWbXU0TePtf9+m8qeVeWbpM/T/uT/rI9bbze4shYmiMno91K1ru12vXtC0qRo1qk8fiM0o05w5ZHDmjLrYmQXDjBm5s2PmTBgxAkymvLdBEAThNiHPgiAxMZHQ0FBmzszdcLs+Pj6MGTOGDRs2cPjwYSZOnMjEiRP55ptv8mxscaFGDTUkcnIynDhR+OcL9Amksl9ly/fO1Tpn2Sazh6BHzR7ULaceukcu55xHsPT4Uiatn0RcShx+Hn5oaAxZOISY5Bj7GH6jhyAm47hlythu5+EBf/+tcgkOHYIhQ9TyzB6CK1fgyBH1UHd1VQuXLcv+Bvz+O8yZA9evw9tvq8JHs2fDxo32aZcgCEIJJM+CoFevXrzzzjsMGDAgV9s3adKEgQMH0qBBA6pVq8ajjz5Kjx492OjEP85GIzRqpOaLLGyQ4SUo51WOBoENsqyv5FcJL1dVl6B7je7UL18fgMOXc84jmBM2B4CRTUdy5sUz1CxTkzNxZxi9dLR9jL6xUqEuCEqXzrptxYrwzz/q4v79txoy+eQNpZjXrlXTOnWUV0HT4OuvbbfZvx8efBCGD1fJh5MmWdcVVb1pQRAEJ8S1qE+4Z88etmzZwjvvvJPjNikpKaSkpFi+x8XFAWAymTDZye2rHye/x2vUyMi2bS7s3p3OgAGFn9zWIrgFi44solPVTqSnpZNOepZtvun9Det2rqNRuUbULlMbgEMXD2XbxitJVyw5A081fQovoxdz+82l8w+dWbB/AY82fJSu1bsWzGgPD9wALSGBNJMJ12vXMABppUqhZXfdGzXCpWlTjDt3kr5oES4ZwyYnBQTgffky5lWrMALmGjUwDxmC67JlaL/+Stq771pyEoyzZ+MCaEYjhozQg1ahAoboaNIjIjAXg7BBQf/2iiMlsU1QMtslbXIO7Nmm3B6jyARBpUqVuHTpEmlpaUyePJknnngix23ff/99pkyZkmX5ypUr8fb2tqtdq1atytd+RmN1oDFr1lyiTZttNutSUlw4c8aXmjVjbHLnCkLN9JoMDh5MZ2NnluZQ1c8XX/oF9mP16tVcv3YdgP9O/Jft9ksuLcFkNlHDqwZnd57lLGcB6FWuF4svL+bphU/zSd1PcDG45Ntmt/h4egOGlBSW/f033S9cwBPYdOAAsRn1BW7kjsqVqb1zJ/GffkppINXXl7hq1fC+fJn0tWsxAqdcXDisafTy9MQ1MpItn31GTJ06GNLS6D5nDi7A9ldewQBogH94OPV++YXIrVvZl8O1cwT5/dsrzpTENkHJbJe0yTmwR5uScvi9vZEiEwQbN24kISGB//77j1dffZVatWoxcODAbLedMGECY8eOtXyPi4ujcuXKdO/eHT8/P7vYYzKZWLVqFd26dcPNzS3P+/v5GfjmGzh7NojmzXsTGKiWnz4Nffq4cvSogSefTOfzz80Y7dSX4z7uu+n6zG2qdK0SH3/7MRfMF+jWoxsHLh2gcWBjXIzqAf/W7LcAGNN+DL1b9LYco/X11tT7sh6nk09zueJlHr/z8fwbnJpqme3VsSOu15VIadenD1Srlu0uBldXWLiQ0hnhApc6dbgeEACAW8YfdfVu3ag6YADGP/6AX3+lfXQ05hdewPDPP7jGxqIFBdH0jTdUeWTAMHs2/PILVV1cqNS7d7bnLUoK+rdXHCmJbYKS2S5pk3NgzzbpXvZbUWSCoHpGeb9GjRpx4cIFJk+enKMg8PDwwMPDI8tyNzc3u9/s/B6zWTMVIo+ONlCnjhuPPAKVK8M331iT6b/5xgVNc2H8eAgIUKMlFgVubm7cEXgHBgzEJMfQZm4b9l3Yx8zeM3mmxTMcuHiA3dG7cTO68didj9m0P8gtiDc6vsG4leN4c8ObDAodRCn3Uvk1RH1MJtxiYy2VCN3Kl7c8rLPQubNKGszovmGoUYPrnp42m7jUq4eLmxs89BD8+isuf/6Jy8cfq26LgOHRR3HL7EmqWhUA47lzGIvRj0Vh/D07mpLYJiiZ7ZI2OQf2aFNu93dIHQKz2WyTI+CM+Pmp8vstWqgie7Nnw5QpSgw0agSffKLy42bNglq1VB5ds2Zq+eXLhW+fl5sX1csoEbbvgsp8XH1KFfVZeVL13+9WsxsB3gFZ9h3dYjQ1ytQgOiGaqZunFswQvafBuXNqajCoi5cTPj7WAY8ArUYNkgJusLFWRo+KXr1Ud4+ICHjnHZWUCDBsmO32lSqpaUlKKkxJse2WKQiCUEDyLAgSEhIICwsjLCwMgPDwcMLCwojMqCQ3YcIEhujdxoCZM2fyzz//cPz4cY4fP853333Hxx9/zKOPPmqfFjiQ9u1h2zYlDF5/HZ58EsaPVxV4X3wRfvoJatdWzziA3bvViL+1asGnn9p41AuFJhXUAEgVfSsCsP3cdgC2nVM5D20rtc12Pw9XDz7s+iEAU7dM5VycepgnpCawOXIzC/YvyH3XRF0Q6A9jf39uGUO5+27LrFazJtfLl7euc3e3PuC9va1FjSZNUl6FLl2gYUPb41XO6LJ59apSbyWBp56CmjXVH6AgCIIdyHPIYOfOndx1112W73qsf+jQocydO5eoqCiLOADlDZgwYQLh4eG4urpSs2ZNPvzwQ5566ik7mO94DAb1oqqX5s/MQw+pD8ClS/Dbb6qX3L59MHYs/PILrFhReKGEz3p+Rv96/elRswdBHwdxLv4cUfFRFmHQulLrHPe9v/79tKvcjs1nNjNm2Ri8XL345eAvmDXVo2JMizHM6J2LwkAZgmDdhh+4C7Lvcngjd98Nb6kcB6pX53rmzMyaNcElU6Lj0KHqwpYqBS+/rC7sjfj5qfUJCcpTUbv2rW0o7uzeraaHDkGrVo61RRCEEkGeBUHnzp3RNC3H9XPnzrX5/uyzz/Lss8/m2bCSRvny8Mwz6sVuzhx45RX1cnfPPUoU6F4Ee1LRryKPNlaemDvK38HBSwdZcnwJETERGDDQomKLHPc1GAxM6z6N1t+1ZtGRRZblfh5+xKXEsf389twZkSEIDoWtyr0gaN1abZeYiFavHslxcWguLhjS063hAp177lEPx8qVVaJG9o1RXoUjR5SnoiQIgvPn1TSXyUKCIAi3QsYyKGJcXOCJJ1RZf39/2LwZ7rtP1dgpTPSH//+2/w+A+uXr4+dx8x4brSq1YmTTkQD0q9uP3U/uZuuIrQAcvHjQ4i24KRmCoJL+3MqNIPDwUEWIVq+GoCA0FxdVuAiyf5g3aZKzGNDRwwZnz976/MWdlBS4ckXNiyAQBMFOiCBwEE2aKM+At7cao+cWQ0IUmBYhShDsvaDGbG5VMXdu5q/6fMWVV67w1yN/0SS4CbXL1sbN6EaiKZHI2MhbHyCjWqFFENxYtjgnmjSBjh0tXzX9gX6jhyC36HkHJUEQREdb5/VxHwRBEAqICAIH0qqVNeQ9cSKkZy0+aDd0QWA5dy4FgdFgpKxXWct3Nxc36gaocRIOXjx4y/1N3qrLYGX9uZUbD0E2mF98UYUH7r8/X/uXKEGghwtAPASCINgNEQQOZtw49dJ86BAsWFB452kc1Bg3o7UvaqtK+U9Ea1BejaVw8NKtBUGiu5oG6sn9+RQEWr9+sHgxlgpQeUX3MJSErod6oQsQD4EgCHZDBIGDKV1adVUEa8+5wsDD1YM7K9wJgLebNw0DG958h5uQF0EQ53ZDnkFuQwb2RjwEgiAIN0UEQTHg2WdVqD0iAg7nPDhhgdHDBs2Cm+FqzH+RSn20xdyEDK652Cocc1GVa7wREQSCIAg3RQRBMcDb25orF5mLPL38MiR0COW9y1t6DuQX3btw6NKhW/Y0uOJiW5EywTv/gyUVCD1kcPkyZIypYBdOnICePYu2QJCEDARBKASKfPhjIXuqVoU9ewpXELSq1IqLL18s8HFqlqmJh4sH19OuE34tnJpla+a47SWDbWXAy+5p5GV4KlO6idg0Ozz0/P1VsYfERFWcKL+9FW7kiy9UdxFXV5XjUBSIh0AQhEJAPATFhCpV1LQwBYG9cDG6UC+gHnDrPIJoEmy+X3DLW73mB35/gBEHRxARE5Gn/bKgFycC+4YN9qlxItiwwZoAsnu3bddAe5PZQyCCQBAEOyGCoJigC4LTpx1rR27JbR7BOc327f68S+7HEkg3p7Pu9DrStDS2nN2SdyNvpEYNNZ01y36VoPbvV9P4eNi1S4UOmjeHvn3tc/zsuNFDYM5FgShBEIRbIIKgmOBMHgKAhuVVHsHyk8u5ev0qAGdiz7Dt7Db+jfiX0zFK2Zw1x9jsF2nI/Rvt6djTJKepIZOPXD5ScKNfe02VilywAL78suDHu3ABLmYKwaxdC999p8TGzp0qS9TeZK5SCOpciYn2P48gCLcdkkNQTHA2QdClRhcMaw1sOL2Bmp/XxM/Dz6ZyoY+bDyefO0lk+lWb/cINMbk+x6FLhyzzR67YQRC0bw8ffaSKP7zwAly7Bo8/DiEh+TueHi7QWbbMdtmyZfD00/k2N1v0UIS7u/IMpKUpL0FGRUhBEIT8Ih6CYkLVqmp67lzh1SKwJy0rtmTp4KU0CmxETHIMkbGRuBhcqFa6Gj5uPiSaEpm/fz4xmeoQmIwQkZr7pEYbQWAPDwGocakffBBMJlUeskqV/HsL9HBB/fpqunGjbdb/0qUFszUzhw6p3AQ9XBASokZxBOlpIAiCXRBBUEwIDLS+9J0752hrckfPWj3Z89Qelg9ezopHVxDzagzhz4fzTItnAJgTNocEd+v2MZ5wPiEqh6NlJbMgOHHtBKZ0U8GNNhhg/nyYOxfatlX1op99Vj3M84ruDXj4YdsKivpY2GvXQnJygU0mLQ06dYI2bWCrGlyKkBDruNmSWCgIgh0QQVBMMBqtXeWdJWwAqsdBj1o96F6zO6Xc1ciG3Wt2B+DAxQNZBUH8+ewOky2ZBUGaOY2T107ax2g3Nxg6FDZtgkGDlCh46CE1uuKGDSqUcCPp6ZB6Qw8JXRCEhsJdd1mXf/wxBAdDUpI6XkE5fVrVT0hNhalT1bLgYPEQCIJgV0QQFCOcLY8gJ9pXaY+nqxrUKLMguOYJFxIvkGa+dUxE0zSLIPBx8QHg8CU7l3E0GOCbb6BBAxWb79ZNvYlXqaLCCbow0DTo3x+CgqzdQNLSlBsfoHFjtS+oHgZ33AG9e6vvy5YV3M7jx63zeg5B5pCBeAgEQbADIgiKESVFEHi6etKhSgcAUlwh3WgAINYLzJqZi4m3ziM4E3eGRFMibkY3mvo2BeDw5UKo6+zjAwsXQpcuUK+ectMkJMC770LLlmp+3TpVdCgmRgkIUA/plBS1f7VqyuMwfboKR4A1bLB4ccG7OGYWBDrBwRIyEATBroggKEboiYXOUovgZnSrkfHGbACTtwcAyaW8gNyFDXTvQO2ytanipZTS4cuHWXp8KU2/bkpYdJj9jK1dW4ULDh9WF//PP6FiRVWWeNIkePtt67Zz56rwgR4uaNRIxXtcXeH556FOHbW8WzclFk6cKHhyoS4I3DO5WySpUBAEOyOCoBhRUjwEAN1qdrPMp3srIWDyUzkGeREE9QPqU9lDJVfsjtrNyH9Gsid6D1/usEMdgewwGGDAAPj2W/V9+nRYv149jEuXVln+K1ZYk/saN87+OH5+1i6Hb7+dOy9BTIzyNKxZY7tcFwSjR1uXiYdAEAQ7I4KgGFGSBEHjoMYE+QQBYCilhIBWWj3AzsVZu1GYNTNHLh9h3r55rI9YbxksSRcE9QLqUcmzkmWZLiY2ndlUuA3o2RMeecT6IH/8cfWwBtV18fPP1fzdd+d8jHHjwNNTVS9cswYOHIDly1WCYHZ8+y388IOqkZAZXRD066eSIKtUgRYtJIdAEAS7IoWJihGZyxdrmnpZdVaMBiM/DviR/87+h9c/fwFn8AqoAJzg460fM6D+ALaf286oxaOIytQVsZJfJfrW6cv6iPWA8hB4xnvianS1SUY8dOkQV5KuUM67XOE1Yvp0WLVK9RZ49VXlmv/sMzh2TK1/+mnVOyEnKlSAJ59U4qFvX0sXRDfg7kqVcOneXQmKhx5SPR/+/Vftd+CAUoVVqqh6CXrFw9q1Yd486x+GhAwEQbAj4iEoRujdDhMSlPfY2elWsxtvdHrD4iFo37gPNcrU4NS1UzT7phn3/nwvUQlReLl60bZyW/w9/Dkbd5Yvd35p6WJYP6A+rgZXapVRoxM2CmxE7bK1Adhyxg7jG9yMoCDYu1c9oKtVU90LmzdX6+6/H2bMuLVqe/llFW5ITlbT2sp237NnMc6eDY8+qgRDerptF8UlS9Q0PFyt8/ZWeQOZzychA0EQ7IgIgmKEtzeUL6/mS0LYwELLluDigl+7u1n56EqCfIIsrv8XWr3AtfHX2Dx8M9EvRbPw4YW81OYlulTvwsimIy1jJvSu1Rt3F3c+7fEpHat2BGBTZCGHDUAlF+qDIoEaB+Hrr9WbuovLrfevVEmFC374QXUZPHYMU1QU/732GmbduzBvnhIemR/s+lDKerigVq2s4kNCBoIg2BEJGRQzKlaES5dUtcLQUEdbYyc+/BBefx38/akJrHpsFW9veJtHGj7CffXvs2zm6epJ/3r96V+vv2WZyaSqE75/9/tMvmsyvh6+nIk7w3d7vmPzmc1F3BDUG37GW36uad9efXTKleNCy5akjxmD8c8/ISzM2p2xRg04dUpVOUxKsgqC7M6pewgkZCAIgh0QD0ExIyBATTMPaOf0GAzWhxfQKKgRvz74q40YuPUhDPh6qAF82ldRD9cd53dYRkN0SgICrAWNdEHw1FMqdpScrOof3EwQiIdAEAQ7IoKgmFEiBYGdqVmmJkE+QaSmp7Lz/E5Hm1MwHn5YTfXeDJ06wT33qPnFi3MnCMRDIAiCHRBBUMwol5E0n1PPNEF5C3QvwfPLn2fcinFExEQ41qj80r+/teCQjw80bap6JADMmgWbM8IiNwsZxMWpro3168Pffxe6yYIglExEEBQzxEOQO3rVUqWBd0ft5pP/PuHu7+8myZTkYKvygb+/tcxx+/aq+2HPnjBsmOpdkJTRppt5CBIS4H//gyNH4Jln7DPCoiAItx0iCIoZ4iHIHcObDGfT45v46p6vqORXifCYcN7Z8I6jzcofr72mBljSCxIZjTB7tkrGNBhUT4WgoKz76YIArOWRz52z5iMIgiDkAREExQzxEOQOg8FAuyrteKr5U8zoNQOAqVumcvDiQQdblg9atlS1Dnr2tC4zGOCVV1R3xI0bs6934OGhPgBXr1qXv/ceJCYWrs2CIJQ4RBAUM8RDkHf61+tPv7r9SDOnMW7lOEebY18aNVJFkXIis5egeXOoXh0uXIAvvih00wRBKFmIIChmiIcgf3zQ5QMA1kWsIzU91cHWFCGZunPStasKPwD89JNj7BEEwWkRQVDMyOwhyM0AeYKiXkA9SnuWJjU9lQMXDzjanKIjs4egc2drl8WwMLh2zREWCYLgpIggKGboHoLUVAkD5wWDwUCz4GYA7Dq/y8HWFCG6IHBxgbZt1bDIdesqNblxo2NtEwTBqRBBUMzw9rbmiUkeQd5oHqIGHnL6YkV5QQ8ZNG8OvqqSI506qak+eqIgCEIuEEFQzDAYJI8gv1gEQdRtJAjKllVTXQSACh0ArF9f1NYIguDE5FkQbNiwgb59+xISEoLBYGDRokU33f7PP/+kW7dulC9fHj8/P9q0acOKFSvya+9tgfQ0yB+6INh/YT8paSlomoZW0hMxnnsOhg+H55+3LtPFwZ49JWMcbUEQioQ8C4LExERCQ0OZOXNmrrbfsGED3bp1Y+nSpezatYu77rqLvn37smfPnjwbe7sgHoL8UdW/KuW8ymEym9gVtYv2c9rT4IsGWSoYXky8yDe7vnHOyoY3cued8N13EBJiXRYSoiobahpsKoIhogVBKBHkefjjXr160UsvtZoLpk+fbvP9vffe46+//uKff/6hSZMm2e6TkpJCSkqK5XtcxmhuJpPJMhxuQdGPY6/j2ZOyZV0AIxcupGMymXO9X3FuU37Ja5uaVmjKqvBVjPx7JIcuHwJg1YlV9K7VG4DLSZfp/ENnjl09xrHLx3j/7vcLx/CbUBT3yaVDB4zHj5O+di3mHj0K7Tw6JfFvD0pmu6RNzoE925TbY+RZEBQUs9lMfHw8ZfXYZza8//77TJkyJcvylStX4u3tbVd7Vq1aZdfj2YOEhMZAdbZtO06NGkfzvH9xbFNByW2b/JNUkp0uBgC+WfsNHIPr6deZdHISx5PUCILf7PiGlkkt8TB62N/gXFCY96mSvz/NgLRvv2WXnx+XchDf9qYk/u1ByWyXtMk5sEebkpJy5w0tckHw8ccfk5CQwEMPPZTjNhMmTGDs2LGW73FxcVSuXJnu3bvjl7nfdQEwmUysWrWKbt264ebmZpdj2ovt240sXw5lytSmd++aud6vOLcpv+S1TalHU/n9j98BCPAK4PL1yxxNP0rv3r0Z9vcwjicdp5xXOTxdPTkXf474KvEMaDygsJthQ5Hcp44d0VavxmP/ftpOmUL6W29hfvXV7LdNTMQQFobWtm32JZJzQUn824OS2S5pk3NgzzbpXvZbUaSCYMGCBUyZMoW//vqLwMDAHLfz8PDAwyPrW5ubm5vdb3ZhHLOg6Jfm2jUX3Nxc8rx/cWxTQcltm9pWaYuLwQWDwcCSwUtoN7sdJ6+dZF3kOn4++DMAfw/8m42nN/Lqmlf5avdXjGg2orDNz5ZCvU9lyqghkceNgy+/xGXyZFxGjLDNNdB56y349FN4+WX46KMCnbYk/u1ByWyXtMk5sEebcrt/kXU7/Pnnn3niiSf49ddf6dq1a1Gd1inRkwqll0HeqehXkX8G/sOaIWtoWbEl7au0B+CxhY9h1sz0rNWTtpXbMqLpCDxcPNh5fifbz213sNWFhJeXGtOgbVswm+Hnn7PfbvVqNf34Y+mqKAi3MUUiCH766Scef/xxfvrpJ+7RS6sKOaJ3O5ReBvmjV+1edKzaUc3XUgmwFxMvAvBy25cBCPAO4OGGDwMwbuU4TOklJxkpC48+qqY//ph1XWIiHMwYIVLTYOhQiI0tOtsEQSg25FkQJCQkEBYWRlhYGADh4eGEhYURGRkJqPj/kCFDLNsvWLCAIUOGMG3aNFq1akV0dDTR0dHEyo9OjoiHwH70rGUdUrhZcDPuqnaX5fsbHd/Az8OPTZGbGLdyHClpKWw7u40rSSVMiT30ELi5qfENDtwwzsPu3cp7EBQENWtCZKTyFAiCcNuRZ0Gwc+dOmjRpYukyOHbsWJo0acKkSZMAiIqKsogDgG+++Ya0tDRGjx5NcHCw5fN85kIqgg3iIbAfjQIbUdmvMqC8A4ZMSXO1ytbixwHqrXnG9hmU+bAMrb9rTYc5HW45YuJfR/6i4RcN2RPlBPU0ypWD3qrbJfPn267bnhEuadMG9KTDrVuLzjZBEIoNeRYEnTt3tlSAy/yZO3cuAHPnzmV9pjjk+vXrb7q9kBXdQ3D9OuSyt4iQAwaDgT8e+oNv+37LQw2y9mzpV7cfb3R8A4DradcBOHz5MDO351x4KzU9lTHLxnDw0kG+3vV14Rhub/Swwfz5yiOgowuCli1VkSOAvXtlqE1BuA2RsQyKIaVKKQ8viJfAHrSo2IIRTUfYeAcyM6XzFFY8uoK9o/Yyq+8stezfKVxOusy5uHNcTrKN3fy490fOxp0FYPOZzYVrvL3o00f9YZ05ox74Ojt2qGnLltCgARiNKlYVFeUYOwVBcBgiCIohMsBR0WIwGOheszuNgxrz+J2Pc2eFO4lNieWOmXdQ6dNKBE8LZuLaiSSnJZNmTuODzR9Y9j1w8QDXrl9zoPW5xNPTOsbBmjVqeukShIer+ebNVa+EunXV98yiQRCE2wIRBMUUGeDIMbgYXZjeYzoAl5IuYcBAmjmNdze+S63Pa9Htx26cuHqCcl7lqFa6GgBbzmxxnMF5oUsXNdUFge4dqFfPOoxyaKia7ttXtLYJguBwRBAUU6SngePoVK0Tfz3yF9/3/54LL13gj4f+ILhUMOfiz7E+Yj0AL7R+gbur3Q04UdhAFwQbNkBqqm3+gI4uCMRDIAi3HUVeuljIHRUqqOm5c46143alX91+lvn76t9Hj5o92HxmM3uj93I97TovtX2JBfsXMDtsNpsinWREwYYNoXx5FSrYtg30GumZBUHjxmoqgkAQbjvEQ1BMqVFDTU+edKwdgsLH3YfuNbvzcruXmdRpEp6unpYqiNvPbSclLeUWRygclh1fxgvLX+BCwoVbb2w0wt3Kq8GLL8KWLSq3oE8f6za6h+DoUUhOzr9h58+rcy1cmP9jCIJQpIggKKbUzBjTSARB8aV22dqU9y5PSnoKu6J2WZZfN10n3ZxeqOe+kHCBB359gN4LevPZts94bvlzudtRDxvsyrB3wgSoWtW6PiREJbCkp1srGGbHV1/BvffCm2/Cv/9m7ab4ww+wbh28X/TDSwuCkD9EEBRTdEFw6pRj7RByxmAwWLwEG05vAGD/hf1UmFaBe3++F60Q+/I/veRp/jj8By4GNfjVrwd/ZW90Ltz8uiAA9Uf2yiu26w2GW+cRhIfDs8/C33+rgZE6d8b49tu222RUMmXvXpWvcDuRkACHDsGePVLPQXAqRBAUU3RBEBEBaWkONUW4Cd1qdAPg4y0fE50Qzaglo4hLiWPJ8SWsi1hn2U7TNKZunsqDvz1oqWGgcyHhAi8uf5GHf3+Yexbcw9LjS7OcZ8mxJfx68FcAzJqZteFrAVg2eBmPNHwEgEnrJ93a4Bo1VL0BgBkzVMjgRnRB8PbbamCk2bNt13/wgfqjbNoUHlbjQbi88w41/vnHuo0uCFJTs5ZLLkzOn1fei+wqem3ZAmPHqgJML71kXR4VBStWwIkTBf9nGzsWfH3VNW7aFL78Ui1PS4NFiyRLWCjWiCAopoSEgIeH+h05c8bR1gg5MaLpCEKDQrly/Qptvmtj0wVx8vrJFi/Bp/99yiurX+H3Q7/Tfm57TiVZXT/jV49n+rbp/HrwV5YeX8qQhUO4ev2qZf25uHPc+/O9PPz7w4RfC+fE1RPEpsTi6epJ52qdmdxpMkaDkb+P/s22s9tubfSyZfDff9CrV/br27ZV04gIVcZ4/Hjrm+6ZMzBnjpqfPl2NoPjWWwA0+u47DKtXqzfkY8esx9u5U1VHfPVV9bC+GdevZ788OjrndZl54w14+mm4sRLq7t3Qrp0a5nnvXmW77rno2VN9atcGHx+VfDlwoNonL5w8CZ99puZ9fNT0tddUEufo0TBggLq2UlxEKKaIICimGI1QvbqalzyC4ou7iztz+8/F1ehKREwEAOPajMPdxZ2NkRv56+hffLHjC8atHAdAkE8Q5xPO89qJ19h5ficRMRHM2zcPgLfvepv6AfW5cv0KU9ZPsZxjTtgc0jWVk7A+Yj07zqn6AXdWuBM3FzfqBtRlaOhQAEb+M5LktFskA1auDK1a5bz+vvvgjz9UHoCnp3qrPXJErfvoIzCZVJGjDh3UsokTMT/2GADG+fNh/35bV/nOnbB2LXz4oQo15PRgnzYNvL2VfQ8/DMePq+X79qk8h/vuu3m7wCpEbsx/2JYhlOrXVw/r9HT1jxUfb6254OmpRMLBg0rotGiB8YUX8Lx06dbnBdU+s1mJi9hY5YmIjYVu3eCbb9Q2x48rYZDimCTUArFjB3TtenOhdOiQSkgVnBIRBMUYySNwDu6scCcTO0wEoHFQYz7o+gFPNn0SgAG/DGD00tEAPNfyOY6MOULnqp1JNiczaOEgJq6dSLqWTrca3ZjYcSIzes0AYOaOmRy6dIh0czrf7v7Wcq71p9ez47wSBC1CWliWv9/lfQJ9Atl/cT8vrczkDs8PRqN6+D72GLRurZZt3AhxcfBthi2TMoUnDAbMGSOcGlautD4wPDzUdMcO+FWFO0hLU7H17JilykZz9qzafvx49X3OHPWgXr7cGopITMw+LKC7006csF1+6JCa9umjCjGBEjm60AkMVMcMD1celIEDwWzG5Ysv6DFyJC53320t5JQd585ZvRKvvw4uLiokA9ZcjMcfBz8/dS1ffDHnY+WH+PjCH7Z6zhxV1ConL8+SJarbauPGhdNtdepUJeb0vwHB7oggKMZITwPn4Y1Ob/DHQ3+w4tEVuBpdebX9q/h7qOp/tcrWYmKHiXzS4xNKe5bmt/t/I8g9iIjYCObvV6MPvt7hdQC61OjCvXXvJV1LZ9iiYfx04CdOx562nGd9RPaCIKhUEN/3/x5QYuLvo3/naOt/Z//j+7Dvc9cw3QuwcaP6wU9Ohjp14K67bDbT2rbF5OWF4dIla87BAw+o6YEDyuOgoxdEysypU+rN0sUF5imPCYsXw8WL8Ntv1u2++EK54Bs0UGWWExKs68xma+GOGwXB4cNqescdVkFw9Kh1ef36SghVq6be8BcsgDVrMHfsCIBx0yaYMoUc+fhj5Tnp2BHaq0RT2reHwYPVfJcuSvD8/LP6Pns2XLNTyWuTSXkj7ryzYF1Fb8XZjNyX7B7227erYbbT05V4GzzY/rZ8/bUSgX/9leMmhm+/VYW3hHwhgqAYI4LAeTAajNxX/z4qlFIVpSr6VeTYs8c4++JZjj97nLfvfhsXo+oR4O/pz8vVXsbdxR2AtpXb0rFqR8uxpnWfhr+HPzvO7+CxhcoV/0STJ3AzuhEZG2nJE2hRsUVmE+hZqydjW48FVP5CdpjSTfT9qS/D/hrG1jO5GOY4syD4/Xc1/8ADqjdCZtzcuKQnI+oegr59VSGktDS4as2JyFYQLFumpu3aqYdJ06bqQTdmjHrIu2bUUJs/H4YMgdOn1QNKtwngwgVrUmBEhG3vBt1DUL++dbyGGwXBjdx9N+mrV/PfxInW7bND0+D7DIE1YYLtuq+/VqGXhQuV2OnZExo1UiEDXRzomM3565UQHq4EVUSECs0UFrog2L9fPfh14uOV5yUpSQmfoCAVdrnxWhSEM2esP4T6vbwBv4gIXJ95xirChDwjgqAYI8WJnJtAn0Aq+lXMdl0t71p82ftLapetzbTu02xGYqxZtiZbRmyxjJUAqlRyy4qqomC6lo6fhx91ytXJctxX27+K0WBkT/QeTl3LGmtaG77WMnrjxsiNt25EmzbqQXb6NOi9CPQ3/xu42LSp7YImTaBFJtGiP4hvJgj0RMdhw9RU9w4MHqyS/ZKSVOhA57vvrPOZs2/NZmUzQEyMdfTG+vWz9xDccUe2bQKIrVZNzYSHK5FyI+Hh6m3f3d1a+EnHx0eFXnx91XeDQYUOwBpi2LtXJR0GBUGlSnnviZDZG5K5p4e90b0v16/bnnP3buW1CQ5WPSl0D9H06RAZaZ9zr7P22MlJEJTS7Tt7tvDDJyUUEQTFmMw5BNKdueTxWKPHOPbsMVpXap1l3R3l7+C/Ef8xoN4AXmrzEg0CG3BXNaubvllwM4yGrP++5X3K07laZwB+P/R7lvW/HPzFMr/1bC48BKVKqbd1UA/DGjWUazobLmQWBD4+6g+4eXPrsg8yRok8edI20z452fpm27u3mg4caB0DHFSS4ejR1u/PP69c/Js2WRMJb+yOoytp/aFfsaKK4evC5MgRW89BDiSXLYvm7a3eivXRITOje0QaNVKi4FYMHqw8Htu3w7vvKtH0xRdKCJw/b823yC168iUoQVDQH4vUVJVPkZmUFBW+0ckcx9evScOG6u+ld29rCWx7DZKVWRAcPZpt91DvC5mqdWa+JkKuEUFQjKleXb1QxMVJT6XbkaBSQfz58J9M7T4VwPKgB9v8gRt58I4HgayCIDU9lYVHrKWEt5zZkrviSXrYALIPF2SQHBCAptc4aNxYeRb0cRIqVFAhhDoZXo3MCXr//qveOitWVA9VUKN79e2r5suWVdntjz6qPBYPPqh6JOjeBL0b5I2CQH+LvTEsULu2asO1a9YHx00EAUYj1Kql5jN3p9TRBcGNHpKcCAy0Cp+JE5XQ6tkThg9Xy24lCK5cgffes3paMr+tnzuXc9JmbunZUwm/zD8658/bbpM5jyAiQk31blFgG5axB5kFgcmUbaa1jSDI7j4Jt0QEQTHG01P9RoKEDQRoU7kNbkb11nxj/kBmBtQbgNFgZMf5HZyOsSYkrjq5ipjkGIJ8gnB3cedi4kXCY7J5472Rjtb8hpzCBTpm/SHepo2a9uqlyhf/8outQMgcNsgcLsgsNsaOVW/czz6rvAWlSqniQr/+qo6lP0C//169MZ61LfhkeVDqXgA9LODlZVuu2ddXFf64CVrt2momuzfPvAoCsIYNAEaMUAmUb76pvm/YkPUBDOrN/5tvlKh6/XV4UvVksbRT96gUJGyQlKQevhcvqrwRnRtHWcssCHQPgR5aAfsKgvBwFf5xdbUeN5uwgU90tPWLCIJ8IYKgmCN5BIKOt5s3z7Z8lmbBzSwVErMjqFSQJUlx3MpxPPTbQwz+czDvbXoPUB6EpsHq4ZW5kFKOdO6s4sMtW9qGALLB/NprKq7/xhtqgdGoChLpouJGQXD9Ovz0k5rX35p12rVTrmv9QXkjffqocReiolSdAd1DoD80bhQEmb0A+jaghEIOXg8dLScPgablTxD06QPPPKPe9GfNUgKnShUlpDTNtleGzh9/wFNPWRM09+9X10dv56BBavp3zj1MbklmwbNzp3VeF1ve3mp6Kw9B5jyN/LJ+PcycafWYtGxpzUnJRhB4Zw5pFFQQpKdbu97eRvFaEQTFHKlFIGRmWo9p7HxyJ/6e/jfd7oH66k3+j8N/8Nuh31iwf4Hl4f9ww4dpU0m9weeqp4G/v/oD3LDhlg9OPD3Vm3vp0tmv1wXBtm0q8W/2bPU2WrWq7aiLOq6uOZ/T3d3axW/7dqsg0LtE3hgyyJw4qD+w4Obhggxy9BCcPasS6lxcrHHz3ODqqh52EybYtu+hh9T0l1+y7qN3t3v8cSXQzGYVetHf0J9/Xh1r9+6s3pLckvkBnjmso3sI9LEwzp2zhhRu5iHQ6zzklchIFboYM0YJSlDCVL+HNwqC9HS8MxeQKmgOwdatqmfIvHk3H+SroMybp0RtQUScHRFBUMzRPZmZw2OCcCsGNx5M1xpd6VK9C+/e/S5TOk+hZ62ePN38adpWbmsVBLlJLAT1oNcLDRWE0FDl+r98Wf3QT1X5Ebz0km0SYW7J7HG4URCcOqW6xOlvsDl5CHIhCCy5Dze+eeregQYNsh8XIq88qPI/2LzZ9oGsaSrXApQnQH9T/uMP9Tbr5aWubbt21uX5IbMg2LnT+nasC4x69axuS33gKn1dZg+Bfr0uXlS9PPLK66+rREa9dwbAPffkLAjOncOYOdHw2LGCvdkvXmydX7Ik/8e5GXFxqkDV1aswapT67mBEEBRzypdX09xWTxUEgNKepVn12CpWD1nNax1eY1KnSSwbvIwv7vkCo8FIm8pKEOy9sJfHFj5Gk6+bMHXz1FuXPc7gxNUT/HP0H5JM2VQLvBmeniqjHpQYOH1a/ZHr+QB5RS/BvGWLNe7etq0SFyYTrF6tlgUEWP+ZIM+CwBIyOHPGtkJifsIFN6NiRejeXc23a2cthxwRoc7t6qrCCnroRnen16ypwjN6jkfmYk6ZSUvD+OWXtHrnHVwrVVIP3sxkFgRXr1rf/vWHfqVKtqNhnjmjHrxeXipZUsfPT3kxbjxmbti1y1qcat069Ya+ebO6r7ogOHJEXZcMDBl2apUrq+sQF2fbKyKvZBYES7MONmYXPv7Y2sU0KsoyJogjEUFQzAkIUFMRBII9qeRXicp+lTFrZubtm0dYdBivrH6F+jPrs/1cNnUCMnHt+jXazW5Hv5/7UeHjCjy9+GkSUhNuuo8Njz1m+yB64QVrbDqv6A/GyEj1gHB1VQ8i/S32o4/U9MaHfh5DBpQrB2XKqPnMCT32FgSgHob9+ytB8+qrqriR7h1o0UJ16dQ9BPpDTxcs99+vpps3W938md+cX38dl+efp8LOnRguXlSCI3NXyhs9ILqXQj9WZkGwe7dtuODG0E5+8gg0zToS5aOPQrNmSgToA25Vr648VdevW+tMgMUOrV49a8JoPvMIzKdOKhGit2fzZvtVldSJjlY9ZUB5B0ANjJVDjYWiQgRBMUc8BEJhMbrFaEJ8Q3iiyRN81vMzQnxDiIiJ4P5f77cZbRFg5/mdHLuifmBfX/s6FxMvYjQYiU+N56tdXzF62egcuzBqmpZ13VtvqboCd91lW18gr/j72z7cQ0LAxYX0mhmC4L//1HTkSNv9goNV0ljv3lbxcDMMBtuwgaapMIV+fHsKgvLl4c8/YfJk9X3qVGudBj0588bkTj3HoVIl68Pz999VDL5MGVUt8ehRNdojcPTBBzF36KDCDbpo0jTrw1s/j55YqHsIKla0hiXWrcs+f0AnP3kE27erZEJ3d1Wj4UZy6Glg8RBUq5ZzeCcXRMRE8Ma4jHvZsaMSi+npsGpVno91UyZOVJ6m1q2Vx6xfPyXc9HvhIEQQFHNEEAiFxfj24zk39hyz+s3iuVbPcXj0YWqXrc3ZuLM8tfgpNE0jOS2ZZ5c+S4tZLWjwRQNGLR7FVzvV4DZrhqxh4cMLMRqM/HTwJ9ZdW5flHMeuHKPcR+V45I9HLKJg3r55TN/+OUmffqQedP43T5C8JXoeAXA9WP3DbPJQSTdprkb48UfllciMwaDi7EuWKBdzbtAfNCtWqIduq1bK5evtbX1rthcGA7z8svJMhIersRVAjTIJynWY+SGsewjAmocwYYJKXExIUImIAwaAyYS5Vy+ODB6MWRccs2ercMuFC8rVbjTCI4+odTt2qAeiHo6pVEkJAg8POHeO5MUZiY6Z8wd0MnU9TDenc+LqCcxahpt//XolVD75xHafL79U00ceUb0uskMPG2zebL1cujCpXr1AgmDcynF03K9i+ZfvbmPt+XJDHkFsciwfb/mY+365j+92f4cpPZsKljmxapW1wubUqepeP/us+r52rUN7NYggKOboguDy5duq94vgAPw8/Fhw/wJcja78fuh3mn3TjDoz6vC/Hf8DIM2cxte7vkZD47HGj9G5Wmf61+vPlM5q0J9vzn5j8SLoTF4/mWvJ1/j14K/MCZvDT/t/4rGFj/HiihepM6MOP+3/qcB2x4VaPQR73a+SkJrAuGpH+aUBdB1sZkunXHgAcoElj2DWLPjvPzQPDzWuwubNKlHyVvtrGvP2zaP2jNo8+c+Tt36IeHtbvSfp6epBrb+dg62XILMg0PMI9GGm27dX4ZTDh8HdnfSPP1b2dOig1qWmKve17h2oVs3ae2PXLs4f3w3p6WhGoyqv7OVlWe+yWD0ozdUy1XXQyfDcpB0+RKe5nag9ozaVP63M6L9HkTRqBMTEoE2YwLZ18zh+5bjqtaCP7/DMMzlfx4yCVNrUqao3AFgSR7VbCQJNs3m70jSNfRf2kZiayOpTq1kR9ied1aH4o0aySmQElZNRrRpao0b8/N2LVP60Mi+vepmFRxbyxD9PUG9mPcsYIzclLk7VnQASnhxmvc5t2qi8lzNnrEmwDkAEQTFHFwRpaVKeWyh8moc05927lat2T/QezsSdobx3eZYOWsrP9/9MWa+yVPStyNRuUy37TGg/wTKk8/Mrnrd4Ag5fOszPB6wD+Ly44kWe+OcJAHzdfTkXf45Bfw6yjLy44sQKBv0xiAMXD+TJ5p+8rDH9zebTfLDpA3b5J/LIg/BvdXhh+QvWN9MC8A/WWPiWStD61QDCp0/OsZQzQPi1cEYtHsWwRcPoMKcDjy18jBNXTzBr9ywG/DLAkpSpaRp/HfmLr3Z+xXXTdesBxoyx9l5o2lQl6+lkFgR6yADUW7zePXDqVPU2rtcoeO012231XI6vvlKJmaAeqPXrK0GSkMCHk1XNi6ulPdBc1ABdUa1URUq3jMu6KDnMckhTuok3173J44dVqWrzsSNsPa3e5s/Hnydx9td4H1X9qA2pqVwb+Rh1ZtTh/RF1ISWFI1V9eCX292xDUGbNzDDf1fx6BxjS0kh74H72HlhD4tH96jpWr25t342CIDFRFb8KDFRjZVy9yrsb3yX0q1AqTKvAo38+ysMHwSMdjpWFmXGrlQArV86Ss2A4cIA+T0+n5eF46gfU5+W2LxPoE8ipa6d4bOFjtiJP01RooEsX9cAPDVUi6cwZogK9CAqYy+JjGcmLPj4WT1fEou85cfWG0TqLCs0JiI2N1QAtNjbWbsdMTU3VFi1apKWmptrtmIWFr6+mgaYdO3bz7ZypTblF2lT0mM1mbe2ptdpfR/7Slh9frl27fs2yLiUtRUtISciyz5ELRzS3KW4ak9F+O/ibpmmaNvD3gRqT0fr91E9r/W1rjcloTEbr+kNXLSElQXt+2fMak9E83/HU3lr/lub6lqvGZLSyH5bVdp7bqS05tkR79M9HtTfWvqGFRYVpZrM5y3mvm65rFd8L0JJd0DTQxvTCcp4Jqydovu/5akxGm717drbtTDen3/Ra6Pdq+dHlWtlXDdq/VdB+7BakBb9XTmMyWtVPq2oR1yKy3TfZlKzd+dWdFnv0tj6z+BnN6x0vjclo1aZX08YsGaO1nNXSsk2VT6to8/bOs9hmfuYZTQMt+dWXbU+wZo36YfDw0LR023ZERuzT1i/6zHrNzGZNi4iwaVNqaqqmmc2auVlTdZxSpdT0+ec1TdO0xM7tNA20U6XVtd1aEe2DjR9oa0+t1fqODVbbZnxajnLR1oWv087Hnde6fN9FYzKacRLa9Yz70vG1EG3X+V3asn1/apfLeWsaaF80x3Lfvmlu1CL91Pzwfuo6rAtfl+WavrLyFY3JaL6voh0pp7bfVcFqx/Wo85p26pT67uKiaR99pGmpqdrUReO1XbV9bWxOCSirdXrcYL0/b6LtDXHRNNDGdzdqTEY7cOGAdmzt79r3L96t9XnKT1tRQ+2b5uaipW/fpmmapl2LidbG3ldKq/Us2hfbv9BirsdoY5eP1dbOGGdzPv2T5mLUOgxT52z0RSPLfb42Vt3nuaFo/X/ub9ffidw+Q10dI0OEvBAQoLpTX7pkK+4FoTAwGAzcVf2ubNe5u7hbhm3OTI0yNbgv8D5+ufALY1eMZU/UHot3YErnKXi6etJiVgsCvANYcN8CfNx9+KTHJ5y8dpLFxxYzaf0kAMp4luHq9au0/LalzVv92xveppxXOe6scCcP3PEATzV7CoPBwPx98zmXepmwqh60OpXC8bJqex83H15p9wr+Hv68uuZVRi8dTf3y9WldqTVJpiRm7ZrFx1s/xsvVixWPrqB6mepsitzE/H3ziUqIwmQ28USTJ7in5j3sjd/LjEUzuOqpMfuzocy5dw53J0TReW5njl89TpcfurDnqT34evjaXJPX175OWHQY5bzK8XJbNdx1v7r9qFm2JgMbDeTen+8lIibCEpLxcfPB39OfyNhIHl34KNO3TWdY6DDm3bGVig/CEpepeH44i6r+VankV4mkpFhebOLDuWplObt+Eg81eIjGQY2JS4mj/cI+RMZG8nvtitx/x/1gMGCuUpl90WFsP7OdC/EXaJHYgrn753K4/hF+3IXKNQCoW5eY5BhG3hnBb+uheoxafM4PXl2jigQZS0GMt5HSSeoenfBL567vrX8zPm4+TGg/gQvzp1P15GVWrayAe99UeHc2XEkivVJFKnwzldgf1hE4fRYjd6rjpPh6k/JAdzi1iPc2vmcZvyMxNZHJ6yfz8VYV7njtnvd5x/tXPvlgD00zKhbHeMCGazvoV6+vShj980945RW0iRN5KWMo7EQvV7w//R/pn07D/ehxlv4IM1/rQqvhkwhb9BWNz/8EHh6cvb8zRK1g6KKh7L2wlzT/NPCHw6MqsX1pRcqu3wZTP4Zff6X0Ox8z7c8E3naFKWdfoVO3r9h7YR+Dvsm4GI8+yqom/my5upcrhussStjF2dIGvFw92X9xPwsPLyQ5LZkFV2axBOgcAavcfPKWl2AnDJpW/CPTcXFx+Pv7Exsbi19ml1kBMJlMLF26lN69e+OWn4IoRUirVir5dtEiuPfenLdzpjblFmmTc2AymVi4eCHjT48nIjbCsnxgw4EsuF8lxF1MvIinqyd+Htb/4ZjkGJp/05yT104ysulIPur2EX1/6sumyE14u3kz/M7hnI0/y/ITy21qJPSr24+7qt3F2xve5ur1q8yq9wojkupxZ9In7Lt8gFHNRvFlny9JM6dx78/3svT4UgK8A7i37r38efhPriVbu5FV8a/CoIaD+GjLR1lCC8GlgolKUEMnN6nQhM3DN+Pl5gXAubhztJ3dlsjYSMa0GMOM3jMA5f6fv38+jy1UiYx/PfIX/er2y3LN4lPiWRO+hlUnV+Hv6c/zrZ7Hz8OPT//7lPc3vW/TldNoMN4y7OFmdOOXB35hyfElfLdHJa21qdSGLSO2sPrUagb/OZiLiVn75hvMcOALuCOjS/zVxb8zNE65sxf97cO9u9XIh6v7NKBb84OU9izNA/UfYPrs8/j8tRSzbym6fN6MLWe3kpqeSs0yNfnjoT8IrRCqxkO4917bbnsuLiomP2CAGuny/ffVei8v6NuXiIaVqPV5LdK1dLY/sZ2ImAjGrhzL2TjV0+Hdu9/ltQ6vkW5OZ9e6+TQb9houZ8+xuwK88XFvlgxeonIm5s5ViZkZpZ5PloGHH4B7Bk7ir7CfeefLY/Q5Dpq7O4aPPlL5CL/8AkOH8tO4Hgz6c5DF5L51+jK6xWi61OiC64FDyv3v4qJqMbRubRVTwLJa8Hdd+HIJJHoYmfHzi0zYO83mmn/c7WPiUuJ4a8NbBPkEcTHxIl6pGrEfGnBN1yA8HFPFinb7ncjtM1QEgRP8KPfpo5JcZ82CJ57IeTtnalNukTY5B3qbvO7wYszyMYRWCKV/3f481OAh3Fxu3sar16+y78I+OlXthMFg4LrpOitOrqBd5XaU91FJNMlpyRy8eJCVJ1cy+d/JpKanWvYPDQplw+Mb8PPwY0/UHr7a+RVv3/02gT6qUE5CagKd53ZmV9Quyz7VSldjbOuxzNwxk6NXrLkBAxsOpGPVjkTGRvLZts9IMiXhZnDjyWZPMvmuyQR4B9jYvvrUarr92A0DBtYNXUdqeirvbnyXf0+rugG6MMkrFxIuMHn9ZFaeWsngRoN5rtVzeLh4EBkbSWRsJGfizuDr7ktl/8qcuHqC+fvns/rUalwMLqRr6Rgw4GJ0Ic2cxvLByxn21zCiE6LxcfOhWXAzDkYd5IrpCtVLV+eNjm9wfc43PDNDdaGsOtZIpJ8ZT1dPtnf/nUadHoTr1zF/+CH7h/SgXkA9PFw9VH2EUaNUyea9e9E0jYTUBHzcfWyH5j55UtVVOHBA9ZL43//UUMk3YdiiYXy/93t83X2JT4233LNPun/CgPoDbDc+c4aYF5/hMffFLKlr4NTzp6hWuhoAaQlx3PVuHfYYLtCkdns2RW6y7FbNK4SwTQ3x/2el7fG2bSPhzjsI/SqUa9ev8cU9X/BIw0dst+nYUYmdKlVUDYxGjdjftxW1PvoWr0xlH97pAG9kpHO82PpF6gfUp065OnSs2pGY5BiqfVaNuBTVq+Hp5k8z8729GLZsgTlzMA0eXOSCQHIIimkcNzPDhqnw03vv3Xw7Z2pTbpE2OQdF2aad53ZqdWbU0UKmhWhf7/xaS0279Tmj46O1+3+5X3vy7ye1NafWaGnpaZqmadr5uPNag5kNNK93vLLkGUTHR2vf7PhG++7X727aruGLhtvkCei5Aq+veV1LSUspWGNzSVp6mjZk4RDL+V9Y9oI2dOFQiy1MRqv3v3paUmqSlpqaqi1cuFA7H3Pech00k0mLuruVtrJ5GY031THm7pmr1n39tabVqKFphw/bnjQhQf04LVp0awOTkzUtLEzlMuSCw5cOa4bJBov9k9dN1pJSk3LcPjU1VQudFqoxGW34ouHaubhzmqZp2u8Hf9eYjFb+o/JaQkqC1uKbFhqT0TrM7qBFx0drWlqapn35paZVrKh+ZFu1shwzMTVRM6Wbsj/hL7/Y5gb8+KOmaZoWsXmplh4aqmmgpZby1sq8oq7l+xvfz/YwH2z8QGMy2rgV41S+x4QJ6nhDhzokh0AEgRP8KL/8svobefHFm2/nTG3KLdIm56Co25RuTs82yTA/mNJNWnxKfLbrctOuq0lXtZBpIRqT0QI+CtBG/TNKOx1z2i625YV0c7o2ed1kbfii4VpiaqK2+/xuG5Gy8fRGTdNu3iaz2aytD1+vLT22tKjNz8LM7TO1pxc/rYVfC7/ltqmpqdqrc1+1aa//+/6az7s+GpPRXlv9mqZpmhZzPUZbfHRxVhF5/bqm/fOPpkVF5c641FRNC85IrKxSRX3XSUnRtNmzNW37dm3+vvmWJNvsMJvN2uXEy9YFK1aoY9auLUmFQvZIcSJBsMXGJV1AXI2ulHK/dR2BnCjjVYYdI3dwOuY0LSq2wNXomJ9Vo8HIm52tQ0U3CW5C52qdWR+xnqebP037Ku1veQyDwUCnap0K08xc80yLnGsRZEdL/5ZMaDeBZSeXse/CPmJTVD9tbzdvRjVX5YH9Pf25p849WXf29Mx+tM2ccHNThZ+ee05V3czs0nd3V4WggEG0uOlhDAYD5bzLWRe0b6/G32jTJve22BERBE6ACAJBKN6E+IYQ4hviaDOy8OOAH1lybAlD7xzqaFMKHaPByJROU3iv63vEp8RzPv488anxVChVgUp+lex/wmefVYWpClppMzPe3tYaEqai72WQZ5m9YcMG+vbtS0hICAaDgUWLFt10+6ioKAYNGkSdOnUwGo288MIL+TT19kUf4EgfGEsQBCE3VPKrxFPNn8LT1Q5DMzsRvh6+1A2oS/OQ5oUjBnTsKQaKAXkWBImJiYSGhjJz5sxcbZ+SkkL58uWZOHEiofau932bIB4CQRAEobDJc8igV69e9MqoJZ0bqlWrxmeffQbA7Nmz83o6AREEgiAIQuFTLHMIUlJSSElJsXyPi1P9NE0mEyY7xVX049jreIVJ6dIAbly/DjExJnx8st/OmdqUW6RNzkFJbBOUzHZJm5wDe7Ypt8coUGEig8HAwoUL6d+/f66279y5M3feeSfTp0+/6XaTJ09mypQpWZYvWLAAb2/vfFjq3GgaPPRQH0wmF77+eiVBQddvvZMgCIIgAElJSQwaNOiWhYmKpYdgwoQJjB071vI9Li6OypUr0717d7tWKly1ahXdunVzimpxQUFGzp6FRo3upnnz7DWcs7UpN0ibnIOS2CYome2SNjkH9myT7mW/FcVSEHh4eODh4ZFluZubm91vdmEcszAICICzZ+HaNVduZa6ztCkvSJucg5LYJiiZ7ZI2OQf2aFNu97dfdQ+hUJHEQkEQBKEwybOHICEhgRMnTli+h4eHExYWRtmyZalSpQoTJkzg3Llz/PDDD5ZtwsLCLPteunSJsLAw3N3dueOOOwregtsEXRBILQJBEAShMMizINi5cyd33WUd91qP9Q8dOpS5c+cSFRVFZGSkzT5NmjSxzO/atYsFCxZQtWpVIiIi8mn27Yd4CARBEITCJM+CoHPnztysY8LcuXOzLCtARwYhg0A1kiuffgo7d8LkyarstSAIgiDYA8khcBL694fatSE1FdasgfHjHW2RIAiCUJIQQeAk3HEHHD0Ky5er78ePO9YeQRAEoWQhgsCJMBiso2JeugTx8Y61RxAEQSg5iCBwMvz8rKMfnjzpWFsEQRCEkoMIAiekZk01FUEgCIIg2AsRBE6ILghOnXKsHYIgCELJQQSBE1KjhpqKh0AQBEGwFyIInBAJGQiCIAj2RgSBEyKCQBAEQbA3IgicEF0QREaCyeRYWwRBEISSgQgCJyQ4GDw9IT1diQJBEARBKCgiCJwQg8E2sfDcOdi3D5KTHWuXIAiC4LyIIHBS9LDBqlVQvz6EhkLp0q5MnNiWlBTH2iYIgiA4HyIInBRdEHz8sSph7OICZrOBAwfKs2mTwbHGCYIgCE6HCAInRRcEAN7eauCjwYPNAKxZI4JAEARByBsiCJyUzILgvffU965ddUEgt1UQBEHIG66ONkDIH61bq94GTZrAmDFq2d13awCEhcHly9ZBkARBEAThVsirpJNSpgycPQv//KPyB0AJhCpV4tA0A+vWOdY+QRAEwbkQQeDEGI3qk5nQ0EuA6n0gCIIgCLlFBEEJQxcEq1c72BBBEATBqRBBUMJo0OAyrq4a4eEy1oEgCIKQe0QQlDC8vNJp00YlF65Y4WBjBEEQBKdBBEEJpEcPJQiWLnWwIYIgCILTIIKgBNKjh6pHsHatjG8gCIIg5A4RBCWQxo0hJASuX4cNGxxtjSAIguAMiCAogRgM0KuXmpewgSAIgpAbRBCUUHr3VtNlyxxrhyAIguAciCAooXTtCq6ucOxY1u6HqakwZQrs2OEY2wRBEITihwiCEoqfH7Rvr+a/+8523Q8/wOTJcO+9kJh48+NcuaLCDmZzoZgpCIIgFBNEEJRgXnhBTadPh/Pnrcv1MEJUFHz6ac77X7umBlG65x549dXCslIQBEEoDoggKMH06wdt26reBlOmqGUmk21Z448+gosXs+6blgaPPAInTqjvU6dKPoIgCEJJRoY/LsEYDPDBB9CxowobjB0Lly5BXByUKwfVqsGuXVbhUKYMuLmpMMGWLerj7Q3du8OiRTBkCOzdq7o05oXly+HHH5U3IjCwMFoqCIIgFBQRBCWcDh2gTx9YvBjGjYM771TLu3eHkSPh7rth2zb1uRGDAb7/Xu3fpg2EhanQwQ8/5P78SUkwdKjyQnh7w6xZ9miVIAiCYG9EENwGTJ2qxjVYsgQ2b1bLevSAu+6C9ethzx44d055DlJToVQpaNhQeRbq11fbz5oFLVrAvHlKFNxxR+7O/eWX1pDEnDnwyitQu7bdmygIgiAUEBEEtwH16sGLL6p8gZgYtax7dzXt1El9bkXz5nDfffDnnzBpEvz++633SUxU5wQoX16FKyZPhvnz89MKQRAEoTDJsyDYsGEDU6dOZdeuXURFRbFw4UL69+9/033Wr1/P2LFjOXjwIJUrV2bixIkMGzYsnyYL+eGNN9SD+Nw5FTYIDs77Md56CxYuhD/+gCeeUOLCYAAvL+snMVH1aEhPVwmMFy9CjRrw00/QqpWa1qunBEb16iofoVQpdZyYGDh9Wh3DaFT77t9vJDy8Bj16qPwGQRAEoXDIsyBITEwkNDSU4cOHc999991y+/DwcO655x5GjRrF/PnzWbNmDU888QTBwcH06NEjX0YLeadUKfj6axg4EJ5+On/HaNAAHn1UJQjeWNvgZkycCC1bwkMPwa+/Kg/DjRiNOdU6cAEa0bJlGk89lT+7BUEQhFuTZ0HQq1cveumF8nPBV199RfXq1Zk2bRoA9evXZ9OmTXz66aciCIqYe+6B2Fj1Np5fPvlEuf89PZWXwWBQ3Rr1j6cnVKyoHvBnzoCvr0oqBJg7V3kJduyA/fvV+rg4tU4XA+XLg78/aJoqruTtbWbzZiPvvOPC0KHq+IIgCIL9KfQcgq1bt9K1a1ebZT169OAFvWpONqSkpJCSkmL5Hpfx1DCZTJhMJrvYpR/HXscrDhRFm/z9VVfGvJCerj6urvDss7brEhOVkEhJgdKlwcfHdn18vIk6dQycPevFF1+k8+yzzl8yUf72nIeS2C5pk3Ngzzbl9hiFLgiio6MJCgqyWRYUFERcXBzXr1/Hy8sryz7vv/8+U/RKOplYuXIl3t7edrVv1apVdj1ecaCktenhh6vyxRd38tZbaYSErMLLK93RJtmFknafoGS2CUpmu6RNzoE92pSUlJSr7YplL4MJEyYwduxYy/e4uDgqV65M9+7d8fPzs8s5TCYTq1atolu3briVkGy1ktqmtLTVrFjRmJMnPTh2rBcTJji3l6Ck3qeS1iYome2SNjkH9myT7mW/FYUuCCpUqMCFCxdsll24cAE/P79svQMAHh4eeHh4ZFnu5uZm95tdGMd0NCWtTa6uGm++aWbIECOffOLCmDEulC3raKsKTkm7T1Ay2wQls13SJufAHm3K7f6FPpZBmzZtWLNmjc2yVatW0aZNm8I+tVCCeOghjUaNVFLk1KmOtkYQBKHkkWdBkJCQQFhYGGFhYYDqVhgWFkZkZCSg3P1DhgyxbD9q1ChOnTrFK6+8wpEjR/jiiy/49ddfefHFF+3TAuG2wGiEd99V8599BtHRjrVHEAShpJFnQbBz506aNGlCkyZNABg7dixNmjRhUkbn8qioKIs4AKhevTpLlixh1apVhIaGMm3aNL799lvpcijkmT591HDMmUdvFARBEOxDnnMIOnfujKZpOa6fO3dutvvs2bMnr6cSBBsMBvjwQ1VqedYs1YUxt2MqCIIgCDen0HMIBMGedOwI996r6hq88oqjrREEQSg5FMtuh4JwMz76SI3cuGQJ9O2rqhzqxY0GDoQxYxxtoSAIgvMhgkBwOurUUeMxzJgBixfbrvvvPxVSaNTIMbYJgiA4KyIIBKfkww/VYEtpadaRFn/4AZYvh+efhzVrCjZmgyAIwu2GCALBKfHyIsvoh23aQP36sG6dGo2xVSs1AFNAgGNsFARBcCYkqVAoMVSrBi+/rOZHjoTGjaF2bTh3zqFmCYIgOAUiCIQSxfjxqlaBn5/yIsTEwMSJjrZKEASh+COCQChR+PjA1q2qxPG6dWrZ999DRmFNQRAEIQckh0AosbRqBY88Aj//DM88A088oeoXxMdDYiJomko8NBhUaWQfH+VZ8PdXn9RUuHgRkpLAxUXlI/TqBa7yXyMIQglEftqEEs3778PChcprsHVrwY9Xo4YKSzzxhBIRgiAIJQURBEKJplo1mDMHfvpJFTAyGpUXwMdHeQY0TX3MZuU1iI21ftzcIDAQSpVSnoXNm+HUKdW7YfNm1ZNBvAWCIJQU5OdMKPEMHKg+BSUpCb74Al59VdU8uH4dhg9X4qJUKVuR4emphIefn9WTkJYGycng4WE9psmkhIdOSgq4u+dcQ0EfRkRqLAiCYG9EEAhCLvH2hpdegpo14eGH4bff1Odm+PlBkyZKFGzfrrwQwcGueHp2YvhwV65etYqH+HglMlxdoWxZlbdgMqlPaqqapqWp5eXKQfnyqsZCtWoqNBIcXCSXQRCEEooIAkHIIwMGqHEUPvgArl5VD3n9oycqJiert/24OPj3X9v9o6IMQGnL9+Rk9dFJS1PJjDmRnq7W69v8+y9UrSpDQguCUDBEEAhCPujWTX1uRkoKHD0Ku3aph3jr1lCxIhw+nMayZTsZMKAZVau6kZCgchZ8fVXvhqQkJTTMZhU+cHOz/ZhMcPmy+vz2G3z9NezeXTTtFgSh5CKCQBAKCQ8PVS2xcWPb5S1aaFy6dIFGjdQDvlw52/Vly0KlSjc/dsWKaururgSB1FkQBKGgSMcpQXBiQkPV9OxZ5TEQBEHILyIIBMGJ8fODWrXU/J49jrVFEATnRgSBIDg5TZqoqQgCQRAKgggCQXByRBAIgmAPRBAIgpMjgkAQBHsggkAQnBxdEBw7pmohCIIg5AcRBILg5AQFqSqFmgb79jnaGkEQnBURBIJQArjzTjWVsIEgCPlFBIEglACaNVPThQsda4cgCM6LCAJBKAE8/rgaFGn1ali3ztHWCILgjIggEIQSQI0a8NRTan7CBOswyYIgCLlFBIEglBAmTlRDNG/bBn/95WhrBEFwNkQQCEIJoUIFeOEFNf/ZZw41RRAEJ0QEgSCUIB5/XE03b4aEBMfaIgiCcyGCQBBKEDVrQvXqYDLB+vWOtkYQBGdCBIEglCAMBujRQ82vXOlYWwRBcC5EEAhCCaN7dzVdscKxdgiC4FyIIBCEEsbdd4OLixrbICLC0dYIguAsiCAQhBKGvz+0bq3mV63K/X4pKRAdDWlphWOXINgbsxlSUyEpyX61N9LT1XFvR1wdbYAgCPane3fV02DCBNUF0WxWP3TZfcxmJQbi4tS+rq5QrZqqaWAwWD+aprbN7gOuJCZ2wcfH+pOi/0BrmvVzK3KzjYsLuLtbP6BGeUxJsdpqNNpOM9uqtznzvD7VNNV+68eVpCTbdmWHwZD7tt2sjZmPc+Mxc7N/TrbYLrPeq+zOkd3fh/6AzPz3oH9ysi0/D+icrqOmKaGalqYSZvV56znccHHpS1CQAQ8P6/q0NGW70Wj7cXFReyUlQXKy+lv39VX/A5cvKzvKls36P5D5Gtx4PfTv+t9TWpqaZm5bTsfK/D+iaUrQz5+f9+tXUPIlCGbOnMnUqVOJjo4mNDSUGTNm0LJly2y3NZlMvP/++3z//fecO3eOunXr8uGHH9KzZ88CGS4IQs707w+TJ8OVK+qTF9LS4MSJvJ7RAJTK605OQElsV0lsE6SnGzl/Pn/7JiTAxYvW75qmhIGjqF7dMefNsyD45ZdfGDt2LF999RWtWrVi+vTp9OjRg6NHjxIYGJhl+4kTJzJv3jxmzZpFvXr1WLFiBQMGDGDLli000QdyFwTBrjRurIZCPn9evQ1l99HflPQ37vLlwc9P7RMert649TeW7N6y9I/BAOnpaWzdupU2bdrg5mb9WcnpTepm3Gqb9HTlJk5Ntdro4wMeHmq9/qZ/o+16m3Oad3GxviHrb5fXr6exdesW2rRpi6tr/hyqmmZ7HXJq483ervVj5Hb/m31PS1P3qnXrNjZt0s+R3d9Kdp4A/XOztuXmfudk7424uVk9N5nnXV3BbDbx559radjwbsANV1dlt6trzt4tTVMeAE9P5WGKi1N//0FB6m/gyhXlPcjs3cpu/sbv+t+SbkN2HoDsrmHmj69v7q+bPcnzX/gnn3zCyJEjeTyjAspXX33FkiVLmD17Nq+++mqW7X/88Udef/11evfuDcDTTz/N6tWrmTZtGvPmzcv2HCkpKaSkpFi+x2X4Mk0mEyaTKa8mZ4t+HHsdrzggbXIOiqpNdeuqT14wm1XFwwoV8rafyWQiJuYqLVqk4uZWcgZSMJlMXLt2jebNS0679HvVsmXJalNAQDKNGplwc7PPMcuXt89x8os9fydyewyDpuU+0pOamoq3tze///47/fv3tywfOnQoMTEx/JVNAfVy5crx0UcfMWLECMuyRx99lE2bNhGRQwr05MmTmTJlSpblCxYswNvbO7fmCoIgCMJtT1JSEoMGDSI2NhY/P78ct8uTh+Dy5cukp6cTFBRkszwoKIgjR45ku0+PHj345JNP6NixIzVr1mTNmjX8+eefpGfOtriBCRMmMHbsWMv3uLg4KleuTPfu3W/amLxgMplYtWoV3bp1w81ektLBSJucA2mT81AS2yVtcg7s2Sbdy34rCr2XwWeffcbIkSOpV68eBoOBmjVr8vjjjzN79uwc9/Hw8MBDDwhmws3Nze43uzCO6WikTc6BtMl5KIntkjY5B/ZoU273z1MdgoCAAFxcXLhw4YLN8gsXLlAhh6Bj+fLlWbRoEYmJiZw+fZojR45QqlQpatSokZdTC4IgCIJQiORJELi7u9OsWTPWrFljWWY2m1mzZg1t2rS56b6enp5UrFiRtLQ0/vjjD+699978WSwIgiAIgt3Jc8hg7NixDB06lObNm9OyZUumT59OYmKipdfBkCFDqFixIu+//z4A27Zt49y5c9x5552cO3eOyZMnYzabeeWVV+zbEkEQBEEQ8k2eBcHDDz/MpUuXmDRpEtHR0dx5550sX77ckmgYGRmJ0Wh1PCQnJzNx4kROnTpFqVKl6N27Nz/++COlS5e2WyMEQRAEQSgY+UoqHDNmDGPGjMl23fobBmHv1KkThw4dys9pBEEQBEEoImRwI0EQBEEQRBAIgiAIguAkox3qxRRzW1whN5hMJpKSkoiLiysx/ValTc6BtMl5KIntkjY5B/Zsk/7svFVhYqcQBPHx8QBUrlzZwZYIgiAIgnMSHx+Pv79/juvzNJaBozCbzZw/fx5fX18MeRk+6ybo5ZDPnDljt3LIjkba5BxIm5yHktguaZNzYM82aZpGfHw8ISEhNr0Ab8QpPARGo5FKlSoVyrH9/PxKzB+QjrTJOZA2OQ8lsV3SJufAXm26mWdAR5IKBUEQBEEQQSAIgiAIwm0sCDw8PHjzzTezHVXRWZE2OQfSJuehJLZL2uQcOKJNTpFUKAiCIAhC4XLbeggEQRAEQbAigkAQBEEQBBEEgiAIgiCIIBAEQRAEAREEgiAIgiBwmwqCmTNnUq1aNTw9PWnVqhXbt293tEm55v3336dFixb4+voSGBhI//79OXr0qM02nTt3xmAw2HxGjRrlIItzx+TJk7PYXK9ePcv65ORkRo8eTbly5ShVqhT3338/Fy5ccKDFt6ZatWpZ2mQwGBg9ejTgHPdpw4YN9O3bl5CQEAwGA4sWLbJZr2kakyZNIjg4GC8vL7p27crx48dttrl69SqDBw/Gz8+P0qVLM2LECBISEoqwFbbcrE0mk4nx48fTqFEjfHx8CAkJYciQIZw/f97mGNnd2w8++KCIW2LlVvdp2LBhWezt2bOnzTbOdJ+AbP+3DAYDU6dOtWxT3O5Tbn6/c/NbFxkZyT333IO3tzeBgYG8/PLLpKWlFdi+204Q/PLLL4wdO5Y333yT3bt3ExoaSo8ePbh48aKjTcsV//77L6NHj+a///5j1apVmEwmunfvTmJios12I0eOJCoqyvL56KOPHGRx7mnQoIGNzZs2bbKse/HFF/nnn3/47bff+Pfffzl//jz33XefA629NTt27LBpz6pVqwB48MEHLdsU9/uUmJhIaGgoM2fOzHb9Rx99xOeff85XX33Ftm3b8PHxoUePHiQnJ1u2GTx4MAcPHmTVqlUsXryYDRs28OSTTxZVE7JwszYlJSWxe/du3njjDXbv3s2ff/7J0aNH6devX5Zt33rrLZt79+yzzxaF+dlyq/sE0LNnTxt7f/rpJ5v1znSfAJu2REVFMXv2bAwGA/fff7/NdsXpPuXm9/tWv3Xp6encc889pKamsmXLFr7//nvmzp3LpEmTCm6gdpvRsmVLbfTo0Zbv6enpWkhIiPb+++870Kr8c/HiRQ3Q/v33X8uyTp06ac8//7zjjMoHb775phYaGprtupiYGM3NzU377bffLMsOHz6sAdrWrVuLyMKC8/zzz2s1a9bUzGazpmnOd58AbeHChZbvZrNZq1ChgjZ16lTLspiYGM3Dw0P76aefNE3TtEOHDmmAtmPHDss2y5Yt0wwGg3bu3Lkisz0nbmxTdmzfvl0DtNOnT1uWVa1aVfv0008L17h8kl2bhg4dqt1777057lMS7tO9996r3X333TbLivN90rSsv9+5+a1bunSpZjQatejoaMs2X375pebn56elpKQUyJ7bykOQmprKrl276Nq1q2WZ0Wika9eubN261YGW5Z/Y2FgAypYta7N8/vz5BAQE0LBhQyZMmEBSUpIjzMsTx48fJyQkhBo1ajB48GAiIyMB2LVrFyaTyea+1atXjypVqjjNfUtNTWXevHkMHz7cZsROZ7xPOuHh4URHR9vcF39/f1q1amW5L1u3bqV06dI0b97csk3Xrl0xGo1s27atyG3OD7GxsRgMBkqXLm2z/IMPPqBcuXI0adKEqVOn2sVlW5isX7+ewMBA6taty9NPP82VK1cs65z9Pl24cIElS5YwYsSILOuK83268fc7N791W7dupVGjRgQFBVm26dGjB3FxcRw8eLBA9jjFaIf24vLly6Snp9tcSICgoCCOHDniIKvyj9ls5oUXXqBdu3Y0bNjQsnzQoEFUrVqVkJAQ9u3bx/jx4zl69Ch//vmnA629Oa1atWLu3LnUrVuXqKgopkyZQocOHThw4ADR0dG4u7tn+UEOCgoiOjraMQbnkUWLFhETE8OwYcMsy5zxPmVGv/bZ/T/p66KjowkMDLRZ7+rqStmyZZ3i3iUnJzN+/HgGDhxoM+Lcc889R9OmTSlbtixbtmxhwoQJREVF8cknnzjQ2pzp2bMn9913H9WrV+fkyZO89tpr9OrVi61bt+Li4uL09+n777/H19c3SxixON+n7H6/c/NbFx0dne3/nL6uINxWgqCkMXr0aA4cOGATawds4n6NGjUiODiYLl26cPLkSWrWrFnUZuaKXr16WeYbN25Mq1atqFq1Kr/++iteXl4OtMw+fPfdd/Tq1YuQkBDLMme8T7cTJpOJhx56CE3T+PLLL23WjR071jLfuHFj3N3deeqpp3j//feLZT39Rx55xDLfqFEjGjduTM2aNVm/fj1dunRxoGX2Yfbs2QwePBhPT0+b5cX5PuX0++1IbquQQUBAAC4uLlkyNi9cuECFChUcZFX+GDNmDIsXL2bdunVUqlTpptu2atUKgBMnThSFaXahdOnS1KlThxMnTlChQgVSU1OJiYmx2cZZ7tvp06dZvXo1TzzxxE23c7b7pF/7m/0/VahQIUvCblpaGlevXi3W904XA6dPn2bVqlW3HI++VatWpKWlERERUTQGFpAaNWoQEBBg+Vtz1vsEsHHjRo4ePXrL/y8oPvcpp9/v3PzWVahQIdv/OX1dQbitBIG7uzvNmjVjzZo1lmVms5k1a9bQpk0bB1qWezRNY8yYMSxcuJC1a9dSvXr1W+4TFhYGQHBwcCFbZz8SEhI4efIkwcHBNGvWDDc3N5v7dvToUSIjI53ivs2ZM4fAwEDuueeem27nbPepevXqVKhQwea+xMXFsW3bNst9adOmDTExMezatcuyzdq1azGbzRYBVNzQxcDx48dZvXo15cqVu+U+YWFhGI3GLG734srZs2e5cuWK5W/NGe+TznfffUezZs0IDQ295baOvk+3+v3OzW9dmzZt2L9/v42A00XrHXfcUWADbyt+/vlnzcPDQ5s7d6526NAh7cknn9RKly5tk7FZnHn66ac1f39/bf369VpUVJTlk5SUpGmapp04cUJ76623tJ07d2rh4eHaX3/9pdWoUUPr2LGjgy2/OePGjdPWr1+vhYeHa5s3b9a6du2qBQQEaBcvXtQ0TdNGjRqlValSRVu7dq22c+dOrU2bNlqbNm0cbPWtSU9P16pUqaKNHz/eZrmz3Kf4+Hhtz5492p49ezRA++STT7Q9e/ZYMu4/+OADrXTp0tpff/2l7du3T7v33nu16tWra9evX7cco2fPnlqTJk20bdu2aZs2bdJq166tDRw40FFNummbUlNTtX79+mmVKlXSwsLCbP7H9AzuLVu2aJ9++qkWFhamnTx5Ups3b55Wvnx5bciQIcWyTfHx8dpLL72kbd26VQsPD9dWr16tNW3aVKtdu7aWnJxsOYYz3Sed2NhYzdvbW/vyyy+z7F8c79Otfr817da/dWlpaVrDhg217t27a2FhYdry5cu18uXLaxMmTCiwfbedINA0TZsxY4ZWpUoVzd3dXWvZsqX233//OdqkXANk+5kzZ46maZoWGRmpdezYUStbtqzm4eGh1apVS3v55Ze12NhYxxp+Cx5++GEtODhYc3d31ypWrKg9/PDD2okTJyzrr1+/rj3zzDNamTJlNG9vb23AgAFaVFSUAy3OHStWrNAA7ejRozbLneU+rVu3Ltu/t6FDh2qaproevvHGG1pQUJDm4eGhdenSJUtbr1y5og0cOFArVaqU5ufnpz3++ONafHy8A1qjuFmbwsPDc/wfW7dunaZpmrZr1y6tVatWmr+/v+bp6anVr19fe++992wersWpTUlJSVr37t218uXLa25ublrVqlW1kSNHZnkJcqb7pPP1119rXl5eWkxMTJb9i+N9utXvt6bl7rcuIiJC69Wrl+bl5aUFBARo48aN00wmU4HtM2QYKQiCIAjCbcxtlUMgCIIgCEL2iCAQBEEQBEEEgSAIgiAIIggEQRAEQUAEgSAIgiAIiCAQBEEQBAERBIIgCIIgIIJAEARBEAREEAiCIAiCgAgCQRAEQRAQQSAIgiAIAvB/TM0THWV4CN4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'Acc at best val acc: {err1.mean():.3f} +- {err1.std():.3f}')\n",
    "print(f'Acc at test: {err2.mean():.3f} +- {err1.std():.3f}')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=[6,4])\n",
    "plt.plot(acc['train'], 'b-', label='Acc train')\n",
    "plt.plot(acc['val'], 'g-', label='Acc val')\n",
    "plt.plot(acc['test'], 'r-', label='Acc test')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=[6,4])\n",
    "plt.plot(loss['train'], 'b-', label='Loss train')\n",
    "plt.plot(loss['val'], 'g-', label='Loss val')\n",
    "plt.plot(loss['test'], 'r-', label='Loss test')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training paramerters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 200-0.005-0.0001-0.25: 0.863 (0.882)\n",
      "-1: 200-0.005-0.0005-0.25: 0.863 (0.882)\n",
      "-1: 200-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-1: 200-0.005-0.001-0.25: 0.824 (0.882)\n",
      "-1: 200-0.005-0.001-0.5: 0.804 (0.882)\n",
      "-1: 200-0.001-0.001-0.25: 0.745 (0.765)\n",
      "-1: 200-0.05-0.005-0.25: 0.804 (0.882)\n",
      "-1: 200-0.01-0.005-0.25: 0.863 (0.863)\n",
      "-1: 200-0.005-0.005-0.25: 0.824 (0.863)\n",
      "-1: 200-0.001-0.005-0.25: 0.784 (0.804)\n",
      "-1: 200-0.05-0.01-0.25: 0.843 (0.882)\n",
      "-1: 200-0.005-0.01-0.25: 0.804 (0.824)\n",
      "-1: 200-0.05-0.01-0.5: 0.784 (0.882)\n",
      "-1: 200-0.05-0.005-0.5: 0.804 (0.882)\n",
      "-1: 200-0.01-0.005-0.5: 0.882 (0.882)\n",
      "-1: 200-0.05-0.01-0: 0.804 (0.843)\n",
      "-1: 200-0.05-0.005-0: 0.804 (0.863)\n",
      "-1: 200-0.01-0.005-0: 0.784 (0.784)\n",
      "-1: 500-0.005-0.001-0.5: 0.824 (0.882)\n",
      "-2: 200-0.005-0.0001-0.25: 0.902 (0.922)\n",
      "-2: 200-0.005-0.0005-0.25: 0.824 (0.922)\n",
      "-2: 200-0.01-0.001-0.25: 0.725 (0.941)\n",
      "-2: 200-0.005-0.001-0.25: 0.784 (0.922)\n",
      "-2: 200-0.005-0.001-0.5: 0.843 (0.922)\n",
      "-2: 200-0.001-0.001-0.25: 0.824 (0.882)\n",
      "-2: 200-0.05-0.005-0.25: 0.863 (0.941)\n",
      "-2: 200-0.01-0.005-0.25: 0.882 (0.902)\n",
      "-2: 200-0.005-0.005-0.25: 0.882 (0.922)\n",
      "-2: 200-0.001-0.005-0.25: 0.902 (0.922)\n",
      "-2: 200-0.05-0.01-0.25: 0.882 (0.922)\n",
      "-2: 200-0.005-0.01-0.25: 0.843 (0.922)\n",
      "-2: 200-0.05-0.01-0.5: 0.882 (0.941)\n",
      "-2: 200-0.05-0.005-0.5: 0.843 (0.902)\n",
      "-2: 200-0.01-0.005-0.5: 0.843 (0.941)\n",
      "-2: 200-0.05-0.01-0: 0.843 (0.922)\n",
      "-2: 200-0.05-0.005-0: 0.824 (0.902)\n",
      "-2: 200-0.01-0.005-0: 0.882 (0.882)\n",
      "-2: 500-0.005-0.001-0.5: 0.824 (0.902)\n",
      "-3: 200-0.005-0.0001-0.25: 0.863 (0.902)\n",
      "-3: 200-0.005-0.0005-0.25: 0.863 (0.922)\n",
      "-3: 200-0.01-0.001-0.25: 0.843 (0.922)\n",
      "-3: 200-0.005-0.001-0.25: 0.922 (0.922)\n",
      "-3: 200-0.005-0.001-0.5: 0.902 (0.941)\n",
      "-3: 200-0.001-0.001-0.25: 0.745 (0.765)\n",
      "-3: 200-0.05-0.005-0.25: 0.843 (0.882)\n",
      "-3: 200-0.01-0.005-0.25: 0.882 (0.922)\n",
      "-3: 200-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-3: 200-0.001-0.005-0.25: 0.863 (0.882)\n",
      "-3: 200-0.05-0.01-0.25: 0.902 (0.922)\n",
      "-3: 200-0.005-0.01-0.25: 0.863 (0.922)\n",
      "-3: 200-0.05-0.01-0.5: 0.824 (0.902)\n",
      "-3: 200-0.05-0.005-0.5: 0.824 (0.902)\n",
      "-3: 200-0.01-0.005-0.5: 0.882 (0.922)\n",
      "-3: 200-0.05-0.01-0: 0.863 (0.902)\n",
      "-3: 200-0.05-0.005-0: 0.863 (0.902)\n",
      "-3: 200-0.01-0.005-0: 0.882 (0.902)\n",
      "-3: 500-0.005-0.001-0.5: 0.882 (0.922)\n",
      "-4: 200-0.005-0.0001-0.25: 0.922 (0.922)\n",
      "-4: 200-0.005-0.0005-0.25: 0.902 (0.941)\n",
      "-4: 200-0.01-0.001-0.25: 0.863 (0.961)\n",
      "-4: 200-0.005-0.001-0.25: 0.941 (0.961)\n",
      "-4: 200-0.005-0.001-0.5: 0.902 (0.961)\n",
      "-4: 200-0.001-0.001-0.25: 0.725 (0.745)\n",
      "-4: 200-0.05-0.005-0.25: 0.824 (0.922)\n",
      "-4: 200-0.01-0.005-0.25: 0.922 (0.961)\n",
      "-4: 200-0.005-0.005-0.25: 0.902 (0.961)\n",
      "-4: 200-0.001-0.005-0.25: 0.843 (0.922)\n",
      "-4: 200-0.05-0.01-0.25: 0.843 (0.922)\n",
      "-4: 200-0.005-0.01-0.25: 0.941 (0.961)\n",
      "-4: 200-0.05-0.01-0.5: 0.863 (0.882)\n",
      "-4: 200-0.05-0.005-0.5: 0.824 (0.863)\n",
      "-4: 200-0.01-0.005-0.5: 0.922 (0.961)\n",
      "-4: 200-0.05-0.01-0: 0.824 (0.922)\n",
      "-4: 200-0.05-0.005-0: 0.824 (0.922)\n",
      "-4: 200-0.01-0.005-0: 0.922 (0.922)\n",
      "-4: 500-0.005-0.001-0.5: 0.902 (0.941)\n",
      "-5: 200-0.005-0.0001-0.25: 0.843 (0.961)\n",
      "-5: 200-0.005-0.0005-0.25: 0.882 (0.941)\n",
      "-5: 200-0.01-0.001-0.25: 0.882 (0.941)\n",
      "-5: 200-0.005-0.001-0.25: 0.882 (0.922)\n",
      "-5: 200-0.005-0.001-0.5: 0.902 (0.922)\n",
      "-5: 200-0.001-0.001-0.25: 0.725 (0.745)\n",
      "-5: 200-0.05-0.005-0.25: 0.882 (0.922)\n",
      "-5: 200-0.01-0.005-0.25: 0.922 (0.922)\n",
      "-5: 200-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-5: 200-0.001-0.005-0.25: 0.843 (0.882)\n",
      "-5: 200-0.05-0.01-0.25: 0.882 (0.922)\n",
      "-5: 200-0.005-0.01-0.25: 0.922 (0.922)\n",
      "-5: 200-0.05-0.01-0.5: 0.882 (0.922)\n",
      "-5: 200-0.05-0.005-0.5: 0.765 (0.922)\n",
      "-5: 200-0.01-0.005-0.5: 0.843 (0.941)\n",
      "-5: 200-0.05-0.01-0: 0.843 (0.922)\n",
      "-5: 200-0.05-0.005-0: 0.882 (0.922)\n",
      "-5: 200-0.01-0.005-0: 0.804 (0.863)\n",
      "-5: 500-0.005-0.001-0.5: 0.863 (0.961)\n",
      "-6: 200-0.005-0.0001-0.25: 0.824 (0.863)\n",
      "-6: 200-0.005-0.0005-0.25: 0.843 (0.902)\n",
      "-6: 200-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-6: 200-0.005-0.001-0.25: 0.882 (0.902)\n",
      "-6: 200-0.005-0.001-0.5: 0.765 (0.843)\n",
      "-6: 200-0.001-0.001-0.25: 0.686 (0.745)\n",
      "-6: 200-0.05-0.005-0.25: 0.804 (0.843)\n",
      "-6: 200-0.01-0.005-0.25: 0.824 (0.902)\n",
      "-6: 200-0.005-0.005-0.25: 0.843 (0.902)\n",
      "-6: 200-0.001-0.005-0.25: 0.863 (0.882)\n",
      "-6: 200-0.05-0.01-0.25: 0.804 (0.863)\n",
      "-6: 200-0.005-0.01-0.25: 0.882 (0.882)\n",
      "-6: 200-0.05-0.01-0.5: 0.784 (0.843)\n",
      "-6: 200-0.05-0.005-0.5: 0.784 (0.843)\n",
      "-6: 200-0.01-0.005-0.5: 0.824 (0.882)\n",
      "-6: 200-0.05-0.01-0: 0.863 (0.863)\n",
      "-6: 200-0.05-0.005-0: 0.863 (0.863)\n",
      "-6: 200-0.01-0.005-0: 0.843 (0.863)\n",
      "-6: 500-0.005-0.001-0.5: 0.824 (0.843)\n",
      "-7: 200-0.005-0.0001-0.25: 0.863 (0.902)\n",
      "-7: 200-0.005-0.0005-0.25: 0.882 (0.922)\n",
      "-7: 200-0.01-0.001-0.25: 0.843 (0.922)\n",
      "-7: 200-0.005-0.001-0.25: 0.882 (0.902)\n",
      "-7: 200-0.005-0.001-0.5: 0.902 (0.941)\n",
      "-7: 200-0.001-0.001-0.25: 0.843 (0.863)\n",
      "-7: 200-0.05-0.005-0.25: 0.843 (0.902)\n",
      "-7: 200-0.01-0.005-0.25: 0.843 (0.902)\n",
      "-7: 200-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-7: 200-0.001-0.005-0.25: 0.843 (0.843)\n",
      "-7: 200-0.05-0.01-0.25: 0.843 (0.902)\n",
      "-7: 200-0.005-0.01-0.25: 0.902 (0.902)\n",
      "-7: 200-0.05-0.01-0.5: 0.863 (0.902)\n",
      "-7: 200-0.05-0.005-0.5: 0.882 (0.902)\n",
      "-7: 200-0.01-0.005-0.5: 0.863 (0.922)\n",
      "-7: 200-0.05-0.01-0: 0.824 (0.882)\n",
      "-7: 200-0.05-0.005-0: 0.824 (0.863)\n",
      "-7: 200-0.01-0.005-0: 0.824 (0.882)\n",
      "-7: 500-0.005-0.001-0.5: 0.863 (0.922)\n",
      "-8: 200-0.005-0.0001-0.25: 0.843 (0.882)\n",
      "-8: 200-0.005-0.0005-0.25: 0.882 (0.941)\n",
      "-8: 200-0.01-0.001-0.25: 0.824 (0.922)\n",
      "-8: 200-0.005-0.001-0.25: 0.843 (0.882)\n",
      "-8: 200-0.005-0.001-0.5: 0.843 (0.922)\n",
      "-8: 200-0.001-0.001-0.25: 0.784 (0.804)\n",
      "-8: 200-0.05-0.005-0.25: 0.824 (0.882)\n",
      "-8: 200-0.01-0.005-0.25: 0.843 (0.882)\n",
      "-8: 200-0.005-0.005-0.25: 0.824 (0.922)\n",
      "-8: 200-0.001-0.005-0.25: 0.784 (0.804)\n",
      "-8: 200-0.05-0.01-0.25: 0.804 (0.863)\n",
      "-8: 200-0.005-0.01-0.25: 0.843 (0.863)\n",
      "-8: 200-0.05-0.01-0.5: 0.824 (0.843)\n",
      "-8: 200-0.05-0.005-0.5: 0.784 (0.863)\n",
      "-8: 200-0.01-0.005-0.5: 0.824 (0.902)\n",
      "-8: 200-0.05-0.01-0: 0.784 (0.882)\n",
      "-8: 200-0.05-0.005-0: 0.784 (0.902)\n",
      "-8: 200-0.01-0.005-0: 0.804 (0.863)\n",
      "-8: 500-0.005-0.001-0.5: 0.863 (0.922)\n",
      "-9: 200-0.005-0.0001-0.25: 0.863 (0.882)\n",
      "-9: 200-0.005-0.0005-0.25: 0.824 (0.902)\n",
      "-9: 200-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-9: 200-0.005-0.001-0.25: 0.843 (0.902)\n",
      "-9: 200-0.005-0.001-0.5: 0.804 (0.882)\n",
      "-9: 200-0.001-0.001-0.25: 0.784 (0.824)\n",
      "-9: 200-0.05-0.005-0.25: 0.824 (0.882)\n",
      "-9: 200-0.01-0.005-0.25: 0.843 (0.922)\n",
      "-9: 200-0.005-0.005-0.25: 0.804 (0.882)\n",
      "-9: 200-0.001-0.005-0.25: 0.824 (0.824)\n",
      "-9: 200-0.05-0.01-0.25: 0.843 (0.863)\n",
      "-9: 200-0.005-0.01-0.25: 0.784 (0.863)\n",
      "-9: 200-0.05-0.01-0.5: 0.824 (0.882)\n",
      "-9: 200-0.05-0.005-0.5: 0.784 (0.863)\n",
      "-9: 200-0.01-0.005-0.5: 0.824 (0.902)\n",
      "-9: 200-0.05-0.01-0: 0.804 (0.882)\n",
      "-9: 200-0.05-0.005-0: 0.824 (0.863)\n",
      "-9: 200-0.01-0.005-0: 0.824 (0.843)\n",
      "-9: 500-0.005-0.001-0.5: 0.843 (0.902)\n",
      "-10: 200-0.005-0.0001-0.25: 0.902 (0.922)\n",
      "-10: 200-0.005-0.0005-0.25: 0.902 (0.922)\n",
      "-10: 200-0.01-0.001-0.25: 0.922 (0.922)\n",
      "-10: 200-0.005-0.001-0.25: 0.922 (0.922)\n",
      "-10: 200-0.005-0.001-0.5: 0.882 (0.922)\n",
      "-10: 200-0.001-0.001-0.25: 0.804 (0.804)\n",
      "-10: 200-0.05-0.005-0.25: 0.922 (0.922)\n",
      "-10: 200-0.01-0.005-0.25: 0.824 (0.922)\n",
      "-10: 200-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-10: 200-0.001-0.005-0.25: 0.824 (0.824)\n",
      "-10: 200-0.05-0.01-0.25: 0.902 (0.922)\n",
      "-10: 200-0.005-0.01-0.25: 0.882 (0.882)\n",
      "-10: 200-0.05-0.01-0.5: 0.824 (0.902)\n",
      "-10: 200-0.05-0.005-0.5: 0.843 (0.902)\n",
      "-10: 200-0.01-0.005-0.5: 0.863 (0.922)\n",
      "-10: 200-0.05-0.01-0: 0.902 (0.902)\n",
      "-10: 200-0.05-0.005-0: 0.902 (0.922)\n",
      "-10: 200-0.01-0.005-0: 0.843 (0.863)\n",
      "-10: 500-0.005-0.001-0.5: 0.882 (0.922)\n",
      "----- 62.25 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-4, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-4, 'drop': .25},\n",
    "\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "        \n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-2, 'drop': .25},\n",
    "\n",
    "        # {'epochs': 500, 'lr': .01, 'wd': 1e-3, 'drop': .25},\n",
    "        # {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        # {'epochs': 500, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "\n",
    "        # {'epochs': 500, 'lr': .01, 'wd': 5e-3, 'drop': .25},\n",
    "        # {'epochs': 500, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        # {'epochs': 500, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "\n",
    "\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': 0},\n",
    "\n",
    "        \n",
    "        # {'epochs': 500, 'lr': .005, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        # {'epochs': 500, 'lr': .001, 'wd': 1e-3, 'drop': .5},\n",
    "\n",
    "        # {'epochs': 750, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        # {'epochs': 750, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "        # {'epochs': 750, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "        ]\n",
    "\n",
    "best_accs1 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs1 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=exp['drop'], init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S, K, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, exp['epochs'], exp['lr'], exp['wd'],\n",
    "                             epochs_h=EPOCHS_h, epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs1[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs1[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}: {best_val_accs1[j,i]:.3f} ({best_accs1[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}' for exp in EXPS]\n",
    "table_over1 = summary_table(best_accs1, index_name)\n",
    "table1 = summary_table(best_val_accs1, index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0001-0.25</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.029149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0005-0.25</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.027451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.25</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.047222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.25</th>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.046607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.5</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.048189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.001-0.25</th>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.046772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0.25</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.035076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.25</th>\n",
       "      <td>0.864706</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.034467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.005-0.25</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.028074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.25</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.034018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.25</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.034187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.01-0.25</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.047059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.5</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.034187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0.5</th>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.034244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.5</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.030440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.033044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0</th>\n",
       "      <td>0.839216</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.034856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.040612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0.5</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.026380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean accs       med       std\n",
       "200-0.005-0.0001-0.25   0.868627  0.862745  0.029149\n",
       "200-0.005-0.0005-0.25   0.866667  0.872549  0.027451\n",
       "200-0.01-0.001-0.25     0.843137  0.843137  0.047222\n",
       "200-0.005-0.001-0.25    0.872549  0.882353  0.046607\n",
       "200-0.005-0.001-0.5     0.854902  0.862745  0.048189\n",
       "200-0.001-0.001-0.25    0.766667  0.764706  0.046772\n",
       "200-0.05-0.005-0.25     0.843137  0.833333  0.035076\n",
       "200-0.01-0.005-0.25     0.864706  0.852941  0.034467\n",
       "200-0.005-0.005-0.25    0.852941  0.862745  0.028074\n",
       "200-0.001-0.005-0.25    0.837255  0.843137  0.034018\n",
       "200-0.05-0.01-0.25      0.854902  0.843137  0.034187\n",
       "200-0.005-0.01-0.25     0.866667  0.872549  0.047059\n",
       "200-0.05-0.01-0.5       0.835294  0.823529  0.034187\n",
       "200-0.05-0.005-0.5      0.813725  0.813725  0.034244\n",
       "200-0.01-0.005-0.5      0.856863  0.852941  0.030440\n",
       "200-0.05-0.01-0         0.835294  0.833333  0.033044\n",
       "200-0.05-0.005-0        0.839216  0.823529  0.034856\n",
       "200-0.01-0.005-0        0.841176  0.833333  0.040612\n",
       "500-0.005-0.001-0.5     0.856863  0.862745  0.026380"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 1-1-1-False: 0.490 (0.569)\n",
      "-1: 1-1-1-False: 0.588 (0.588)\n",
      "-1: 1-1-1-False: 0.608 (0.647)\n",
      "-1: 1-1-1-False: 0.804 (0.804)\n",
      "-1: 1-1-1-True: 0.451 (0.569)\n",
      "-1: 1-10-10-True: 0.863 (0.882)\n",
      "-1: 1-25-25-True: 0.824 (0.863)\n",
      "-1: 1-50-50-True: 0.804 (0.882)\n",
      "-1: 1-100-100-True: 0.824 (0.902)\n",
      "-1: 1-25-25-True: 0.882 (0.902)\n",
      "-1: 1-10-5-True: 0.784 (0.824)\n",
      "-1: 1-25-5-True: 0.804 (0.843)\n",
      "-1: 1-5-10-True: 0.804 (0.843)\n",
      "-1: 1-5-25-True: 0.765 (0.804)\n",
      "-2: 1-1-1-False: 0.647 (0.706)\n",
      "-2: 1-1-1-False: 0.784 (0.882)\n",
      "-2: 1-1-1-False: 0.608 (0.706)\n",
      "-2: 1-1-1-False: 0.529 (0.667)\n",
      "-2: 1-1-1-True: 0.510 (0.667)\n",
      "-2: 1-10-10-True: 0.843 (0.941)\n",
      "-2: 1-25-25-True: 0.882 (0.941)\n",
      "-2: 1-50-50-True: 0.843 (0.922)\n",
      "-2: 1-100-100-True: 0.824 (0.941)\n",
      "-2: 1-25-25-True: 0.804 (0.922)\n",
      "-2: 1-10-5-True: 0.824 (0.902)\n",
      "-2: 1-25-5-True: 0.882 (0.882)\n",
      "-2: 1-5-10-True: 0.902 (0.922)\n",
      "-2: 1-5-25-True: 0.843 (0.882)\n",
      "-3: 1-1-1-False: 0.471 (0.510)\n",
      "-3: 1-1-1-False: 0.510 (0.569)\n",
      "-3: 1-1-1-False: 0.843 (0.882)\n",
      "-3: 1-1-1-False: 0.765 (0.824)\n",
      "-3: 1-1-1-True: 0.549 (0.569)\n",
      "-3: 1-10-10-True: 0.863 (0.902)\n",
      "-3: 1-25-25-True: 0.882 (0.922)\n",
      "-3: 1-50-50-True: 0.902 (0.922)\n",
      "-3: 1-100-100-True: 0.863 (0.922)\n",
      "-3: 1-25-25-True: 0.863 (0.922)\n",
      "-3: 1-10-5-True: 0.863 (0.922)\n",
      "-3: 1-25-5-True: 0.902 (0.941)\n",
      "-3: 1-5-10-True: 0.804 (0.882)\n",
      "-3: 1-5-25-True: 0.902 (0.902)\n",
      "-4: 1-1-1-False: 0.412 (0.529)\n",
      "-4: 1-1-1-False: 0.451 (0.510)\n",
      "-4: 1-1-1-False: 0.392 (0.412)\n",
      "-4: 1-1-1-False: 0.510 (0.569)\n",
      "-4: 1-1-1-True: 0.529 (0.529)\n",
      "-4: 1-10-10-True: 0.902 (0.922)\n",
      "-4: 1-25-25-True: 0.902 (0.961)\n",
      "-4: 1-50-50-True: 0.922 (0.961)\n",
      "-4: 1-100-100-True: 0.902 (0.961)\n",
      "-4: 1-25-25-True: 0.922 (0.941)\n",
      "-4: 1-10-5-True: 0.824 (0.941)\n",
      "-4: 1-25-5-True: 0.902 (0.961)\n",
      "-4: 1-5-10-True: 0.902 (0.922)\n",
      "-4: 1-5-25-True: 0.922 (0.941)\n",
      "-5: 1-1-1-False: 0.431 (0.529)\n",
      "-5: 1-1-1-False: 0.804 (0.863)\n",
      "-5: 1-1-1-False: 0.412 (0.588)\n",
      "-5: 1-1-1-False: 0.392 (0.412)\n",
      "-5: 1-1-1-True: 0.392 (0.392)\n",
      "-5: 1-10-10-True: 0.882 (0.922)\n",
      "-5: 1-25-25-True: 0.843 (0.922)\n",
      "-5: 1-50-50-True: 0.843 (0.941)\n",
      "-5: 1-100-100-True: 0.882 (0.941)\n",
      "-5: 1-25-25-True: 0.863 (0.941)\n",
      "-5: 1-10-5-True: 0.765 (0.843)\n",
      "-5: 1-25-5-True: 0.784 (0.882)\n",
      "-5: 1-5-10-True: 0.784 (0.843)\n",
      "-5: 1-5-25-True: 0.843 (0.902)\n",
      "-6: 1-1-1-False: 0.569 (0.627)\n",
      "-6: 1-1-1-False: 0.765 (0.765)\n",
      "-6: 1-1-1-False: 0.627 (0.647)\n",
      "-6: 1-1-1-False: 0.569 (0.608)\n",
      "-6: 1-1-1-True: 0.588 (0.647)\n",
      "-6: 1-10-10-True: 0.843 (0.863)\n",
      "-6: 1-25-25-True: 0.804 (0.902)\n",
      "-6: 1-50-50-True: 0.882 (0.902)\n",
      "-6: 1-100-100-True: 0.882 (0.902)\n",
      "-6: 1-25-25-True: 0.902 (0.902)\n",
      "-6: 1-10-5-True: 0.745 (0.824)\n",
      "-6: 1-25-5-True: 0.745 (0.804)\n",
      "-6: 1-5-10-True: 0.706 (0.745)\n",
      "-6: 1-5-25-True: 0.784 (0.882)\n",
      "-7: 1-1-1-False: 0.549 (0.588)\n",
      "-7: 1-1-1-False: 0.471 (0.529)\n",
      "-7: 1-1-1-False: 0.529 (0.588)\n",
      "-7: 1-1-1-False: 0.608 (0.667)\n",
      "-7: 1-1-1-True: 0.569 (0.588)\n",
      "-7: 1-10-10-True: 0.824 (0.882)\n",
      "-7: 1-25-25-True: 0.863 (0.902)\n",
      "-7: 1-50-50-True: 0.863 (0.922)\n",
      "-7: 1-100-100-True: 0.863 (0.902)\n",
      "-7: 1-25-25-True: 0.882 (0.922)\n",
      "-7: 1-10-5-True: 0.863 (0.902)\n",
      "-7: 1-25-5-True: 0.843 (0.902)\n",
      "-7: 1-5-10-True: 0.529 (0.569)\n",
      "-7: 1-5-25-True: 0.804 (0.902)\n",
      "-8: 1-1-1-False: 0.490 (0.608)\n",
      "-8: 1-1-1-False: 0.765 (0.784)\n",
      "-8: 1-1-1-False: 0.588 (0.588)\n",
      "-8: 1-1-1-False: 0.529 (0.569)\n",
      "-8: 1-1-1-True: 0.490 (0.569)\n",
      "-8: 1-10-10-True: 0.863 (0.902)\n",
      "-8: 1-25-25-True: 0.824 (0.922)\n",
      "-8: 1-50-50-True: 0.882 (0.902)\n",
      "-8: 1-100-100-True: 0.863 (0.922)\n",
      "-8: 1-25-25-True: 0.804 (0.902)\n",
      "-8: 1-10-5-True: 0.824 (0.824)\n",
      "-8: 1-25-5-True: 0.863 (0.902)\n",
      "-8: 1-5-10-True: 0.784 (0.843)\n",
      "-8: 1-5-25-True: 0.843 (0.882)\n",
      "-9: 1-1-1-False: 0.431 (0.451)\n",
      "-9: 1-1-1-False: 0.765 (0.784)\n",
      "-9: 1-1-1-False: 0.510 (0.549)\n",
      "-9: 1-1-1-False: 0.451 (0.451)\n",
      "-9: 1-1-1-True: 0.471 (0.549)\n",
      "-9: 1-10-10-True: 0.843 (0.882)\n",
      "-9: 1-25-25-True: 0.824 (0.902)\n",
      "-9: 1-50-50-True: 0.863 (0.902)\n",
      "-9: 1-100-100-True: 0.843 (0.882)\n",
      "-9: 1-25-25-True: 0.824 (0.902)\n",
      "-9: 1-10-5-True: 0.804 (0.882)\n",
      "-9: 1-25-5-True: 0.804 (0.902)\n",
      "-9: 1-5-10-True: 0.824 (0.843)\n",
      "-9: 1-5-25-True: 0.824 (0.882)\n",
      "-10: 1-1-1-False: 0.510 (0.529)\n",
      "-10: 1-1-1-False: 0.510 (0.608)\n",
      "-10: 1-1-1-False: 0.804 (0.824)\n",
      "-10: 1-1-1-False: 0.294 (0.353)\n",
      "-10: 1-1-1-True: 0.549 (0.588)\n",
      "-10: 1-10-10-True: 0.863 (0.922)\n",
      "-10: 1-25-25-True: 0.843 (0.922)\n",
      "-10: 1-50-50-True: 0.882 (0.922)\n",
      "-10: 1-100-100-True: 0.882 (0.922)\n",
      "-10: 1-25-25-True: 0.902 (0.922)\n",
      "-10: 1-10-5-True: 0.843 (0.882)\n",
      "-10: 1-25-5-True: 0.902 (0.922)\n",
      "-10: 1-5-10-True: 0.804 (0.863)\n",
      "-10: 1-5-25-True: 0.882 (0.922)\n",
      "----- 34.14 mins -----\n"
     ]
    }
   ],
   "source": [
    "# h0, norm (norm, not norm, norm H), sep vs joint training, optimizers iters (W vs h)\n",
    "EXPS = [\n",
    "        # {'h0': .1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        {'h0': 1, 'epochs': 1000, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        {'h0': 1, 'epochs': 5000, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        {'h0': 1, 'epochs': 10000, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},        \n",
    "\n",
    "        # {'h0': .1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},   # Current best\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 50, 'epochs_W': 50, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 100, 'epochs_W': 100, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 400, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 5, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 5, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 5, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 5, 'epochs_W': 25, 'alt': True},\n",
    "    ]\n",
    "\n",
    "\n",
    "best_accs2 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs2 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=exp['h0'])\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        if not exp['alt']:\n",
    "            model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "            _, acc = model.train(feat, labels, exp['epochs'], LR, WD)\n",
    "        else:\n",
    "            model = GF_NodeClassModel(arch, S,  K, masks, LOSS_FN, device=device)\n",
    "            _, acc = model.train(feat, labels, exp['epochs'], LR, WD, epochs_h=exp['epochs_h'],\n",
    "                                 epochs_W=exp['epochs_W'])\n",
    "\n",
    "        best_accs2[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs2[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"h0\"]}-{exp[\"epochs_h\"]}-{exp[\"epochs_W\"]}-{exp[\"alt\"]}: {best_val_accs2[j,i]:.3f} ({best_accs2[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"h0\"]}-{exp[\"epochs_h\"]}-{exp[\"epochs_W\"]}-{exp[\"alt\"]}' for exp in EXPS]\n",
    "table_over2 = summary_table(best_accs2, index_name)\n",
    "table2 = summary_table(best_val_accs2, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-1-1-1-False</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.068627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000-1-1-1-False</th>\n",
       "      <td>0.641176</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.139767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000-1-1-1-False</th>\n",
       "      <td>0.592157</td>\n",
       "      <td>0.598039</td>\n",
       "      <td>0.138870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10000-1-1-1-False</th>\n",
       "      <td>0.545098</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.147203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-1-True</th>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.056829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-10-True</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.021118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-25-25-True</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.030440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-50-50-True</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.031677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-100-100-True</th>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.024802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400-1-25-25-True</th>\n",
       "      <td>0.864706</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.039654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-5-True</th>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.037461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-25-5-True</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.053339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-5-10-True</th>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.100747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-5-25-True</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.047587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mean accs       med       std\n",
       "200-1-1-1-False      0.500000  0.490196  0.068627\n",
       "1000-1-1-1-False     0.641176  0.676471  0.139767\n",
       "5000-1-1-1-False     0.592157  0.598039  0.138870\n",
       "10000-1-1-1-False    0.545098  0.529412  0.147203\n",
       "200-1-1-1-True       0.509804  0.519608  0.056829\n",
       "200-1-10-10-True     0.858824  0.862745  0.021118\n",
       "200-1-25-25-True     0.849020  0.843137  0.030440\n",
       "200-1-50-50-True     0.868627  0.872549  0.031677\n",
       "200-1-100-100-True   0.862745  0.862745  0.024802\n",
       "400-1-25-25-True     0.864706  0.872549  0.039654\n",
       "200-1-10-5-True      0.813725  0.823529  0.037461\n",
       "200-1-25-5-True      0.843137  0.852941  0.053339\n",
       "200-1-5-10-True      0.784314  0.803922  0.100747\n",
       "200-1-5-25-True      0.841176  0.843137  0.047587"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design - Bias True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 2-2-32: 0.804 (0.882)\n",
      "-1: 2-3-32: 0.843 (0.863)\n",
      "-1: 2-4-32: 0.843 (0.882)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 3-2-32: 0.863 (0.882)\n",
      "-1: 4-2-32: 0.804 (0.882)\n",
      "-1: 5-2-32: 0.804 (0.882)\n",
      "-1: 6-2-32: 0.451 (0.549)\n",
      "-1: 3-3-32: 0.843 (0.863)\n",
      "-1: 4-3-32: 0.824 (0.863)\n",
      "-1: 2-2-8: 0.706 (0.725)\n",
      "-1: 2-2-32: 0.882 (0.902)\n",
      "-1: 2-2-50: 0.804 (0.882)\n",
      "-1: 2-2-75: 0.824 (0.882)\n",
      "-1: 2-2-100: 0.843 (0.882)\n",
      "-1: 2-3-50: 0.824 (0.882)\n",
      "-1: 2-3-75: 0.804 (0.863)\n",
      "-1: 2-3-100: 0.843 (0.882)\n",
      "-1: 3-2-32: 0.843 (0.882)\n",
      "-1: 3-2-50: 0.824 (0.882)\n",
      "-1: 3-3-50: 0.882 (0.882)\n",
      "-2: 2-2-32: 0.882 (0.922)\n",
      "-2: 2-3-32: 0.882 (0.922)\n",
      "-2: 2-4-32: 0.843 (0.902)\n",
      "-2: 3-2-32: 0.882 (0.941)\n",
      "-2: 4-2-32: 0.882 (0.922)\n",
      "-2: 5-2-32: 0.843 (0.941)\n",
      "-2: 6-2-32: 0.824 (0.843)\n",
      "-2: 3-3-32: 0.843 (0.922)\n",
      "-2: 4-3-32: 0.843 (0.922)\n",
      "-2: 2-2-8: 0.824 (0.902)\n",
      "-2: 2-2-32: 0.863 (0.922)\n",
      "-2: 2-2-50: 0.863 (0.922)\n",
      "-2: 2-2-75: 0.843 (0.902)\n",
      "-2: 2-2-100: 0.902 (0.941)\n",
      "-2: 2-3-50: 0.882 (0.922)\n",
      "-2: 2-3-75: 0.863 (0.941)\n",
      "-2: 2-3-100: 0.882 (0.941)\n",
      "-2: 3-2-32: 0.863 (0.922)\n",
      "-2: 3-2-50: 0.882 (0.941)\n",
      "-2: 3-3-50: 0.824 (0.922)\n",
      "-3: 2-2-32: 0.863 (0.922)\n",
      "-3: 2-3-32: 0.863 (0.941)\n",
      "-3: 2-4-32: 0.902 (0.902)\n",
      "-3: 3-2-32: 0.882 (0.941)\n",
      "-3: 4-2-32: 0.863 (0.922)\n",
      "-3: 5-2-32: 0.843 (0.922)\n",
      "-3: 6-2-32: 0.843 (0.902)\n",
      "-3: 3-3-32: 0.824 (0.902)\n",
      "-3: 4-3-32: 0.490 (0.490)\n",
      "-3: 2-2-8: 0.667 (0.784)\n",
      "-3: 2-2-32: 0.882 (0.941)\n",
      "-3: 2-2-50: 0.863 (0.922)\n",
      "-3: 2-2-75: 0.902 (0.922)\n",
      "-3: 2-2-100: 0.882 (0.922)\n",
      "-3: 2-3-50: 0.882 (0.922)\n",
      "-3: 2-3-75: 0.804 (0.922)\n",
      "-3: 2-3-100: 0.804 (0.922)\n",
      "-3: 3-2-32: 0.843 (0.922)\n",
      "-3: 3-2-50: 0.882 (0.922)\n",
      "-3: 3-3-50: 0.863 (0.902)\n",
      "-4: 2-2-32: 0.902 (0.961)\n",
      "-4: 2-3-32: 0.922 (0.961)\n",
      "-4: 2-4-32: 0.882 (0.941)\n",
      "-4: 3-2-32: 0.882 (0.941)\n",
      "-4: 4-2-32: 0.882 (0.961)\n",
      "-4: 5-2-32: 0.902 (0.941)\n",
      "-4: 6-2-32: 0.490 (0.510)\n",
      "-4: 3-3-32: 0.843 (0.882)\n",
      "-4: 4-3-32: 0.451 (0.490)\n",
      "-4: 2-2-8: 0.804 (0.882)\n",
      "-4: 2-2-32: 0.843 (0.961)\n",
      "-4: 2-2-50: 0.902 (0.961)\n",
      "-4: 2-2-75: 0.922 (0.961)\n",
      "-4: 2-2-100: 0.941 (0.961)\n",
      "-4: 2-3-50: 0.843 (0.941)\n",
      "-4: 2-3-75: 0.902 (0.941)\n",
      "-4: 2-3-100: 0.922 (0.961)\n",
      "-4: 3-2-32: 0.882 (0.941)\n",
      "-4: 3-2-50: 0.922 (0.961)\n",
      "-4: 3-3-50: 0.882 (0.941)\n",
      "-5: 2-2-32: 0.882 (0.922)\n",
      "-5: 2-3-32: 0.922 (0.941)\n",
      "-5: 2-4-32: 0.902 (0.941)\n",
      "-5: 3-2-32: 0.882 (0.922)\n",
      "-5: 4-2-32: 0.922 (0.941)\n",
      "-5: 5-2-32: 0.902 (0.922)\n",
      "-5: 6-2-32: 0.490 (0.510)\n",
      "-5: 3-3-32: 0.882 (0.941)\n",
      "-5: 4-3-32: 0.863 (0.902)\n",
      "-5: 2-2-8: 0.725 (0.745)\n",
      "-5: 2-2-32: 0.922 (0.941)\n",
      "-5: 2-2-50: 0.843 (0.922)\n",
      "-5: 2-2-75: 0.843 (0.941)\n",
      "-5: 2-2-100: 0.922 (0.922)\n",
      "-5: 2-3-50: 0.882 (0.922)\n",
      "-5: 2-3-75: 0.902 (0.922)\n",
      "-5: 2-3-100: 0.824 (0.902)\n",
      "-5: 3-2-32: 0.882 (0.922)\n",
      "-5: 3-2-50: 0.902 (0.941)\n",
      "-5: 3-3-50: 0.843 (0.941)\n",
      "-6: 2-2-32: 0.882 (0.922)\n",
      "-6: 2-3-32: 0.824 (0.902)\n",
      "-6: 2-4-32: 0.882 (0.902)\n",
      "-6: 3-2-32: 0.863 (0.922)\n",
      "-6: 4-2-32: 0.765 (0.843)\n",
      "-6: 5-2-32: 0.804 (0.863)\n",
      "-6: 6-2-32: 0.804 (0.863)\n",
      "-6: 3-3-32: 0.804 (0.863)\n",
      "-6: 4-3-32: 0.784 (0.863)\n",
      "-6: 2-2-8: 0.627 (0.745)\n",
      "-6: 2-2-32: 0.843 (0.922)\n",
      "-6: 2-2-50: 0.843 (0.922)\n",
      "-6: 2-2-75: 0.863 (0.922)\n",
      "-6: 2-2-100: 0.863 (0.922)\n",
      "-6: 2-3-50: 0.863 (0.922)\n",
      "-6: 2-3-75: 0.843 (0.902)\n",
      "-6: 2-3-100: 0.843 (0.902)\n",
      "-6: 3-2-32: 0.863 (0.922)\n",
      "-6: 3-2-50: 0.863 (0.902)\n",
      "-6: 3-3-50: 0.588 (0.588)\n",
      "-7: 2-2-32: 0.863 (0.902)\n",
      "-7: 2-3-32: 0.863 (0.902)\n",
      "-7: 2-4-32: 0.863 (0.902)\n",
      "-7: 3-2-32: 0.882 (0.902)\n",
      "-7: 4-2-32: 0.882 (0.902)\n",
      "-7: 5-2-32: 0.863 (0.922)\n",
      "-7: 6-2-32: 0.784 (0.902)\n",
      "-7: 3-3-32: 0.902 (0.902)\n",
      "-7: 4-3-32: 0.529 (0.529)\n",
      "-7: 2-2-8: 0.843 (0.863)\n",
      "-7: 2-2-32: 0.882 (0.922)\n",
      "-7: 2-2-50: 0.882 (0.902)\n",
      "-7: 2-2-75: 0.843 (0.863)\n",
      "-7: 2-2-100: 0.882 (0.882)\n",
      "-7: 2-3-50: 0.863 (0.922)\n",
      "-7: 2-3-75: 0.824 (0.882)\n",
      "-7: 2-3-100: 0.843 (0.902)\n",
      "-7: 3-2-32: 0.843 (0.902)\n",
      "-7: 3-2-50: 0.863 (0.922)\n",
      "-7: 3-3-50: 0.843 (0.902)\n",
      "-8: 2-2-32: 0.843 (0.902)\n",
      "-8: 2-3-32: 0.863 (0.882)\n",
      "-8: 2-4-32: 0.843 (0.922)\n",
      "-8: 3-2-32: 0.882 (0.902)\n",
      "-8: 4-2-32: 0.804 (0.863)\n",
      "-8: 5-2-32: 0.863 (0.922)\n",
      "-8: 6-2-32: 0.843 (0.882)\n",
      "-8: 3-3-32: 0.804 (0.941)\n",
      "-8: 4-3-32: 0.804 (0.902)\n",
      "-8: 2-2-8: 0.882 (0.902)\n",
      "-8: 2-2-32: 0.824 (0.902)\n",
      "-8: 2-2-50: 0.902 (0.941)\n",
      "-8: 2-2-75: 0.863 (0.882)\n",
      "-8: 2-2-100: 0.824 (0.902)\n",
      "-8: 2-3-50: 0.863 (0.902)\n",
      "-8: 2-3-75: 0.863 (0.902)\n",
      "-8: 2-3-100: 0.843 (0.902)\n",
      "-8: 3-2-32: 0.843 (0.922)\n",
      "-8: 3-2-50: 0.824 (0.922)\n",
      "-8: 3-3-50: 0.745 (0.863)\n",
      "-9: 2-2-32: 0.824 (0.902)\n",
      "-9: 2-3-32: 0.882 (0.882)\n",
      "-9: 2-4-32: 0.824 (0.902)\n",
      "-9: 3-2-32: 0.843 (0.902)\n",
      "-9: 4-2-32: 0.784 (0.882)\n",
      "-9: 5-2-32: 0.824 (0.922)\n",
      "-9: 6-2-32: 0.804 (0.882)\n",
      "-9: 3-3-32: 0.824 (0.902)\n",
      "-9: 4-3-32: 0.843 (0.902)\n",
      "-9: 2-2-8: 0.765 (0.824)\n",
      "-9: 2-2-32: 0.843 (0.922)\n",
      "-9: 2-2-50: 0.843 (0.882)\n",
      "-9: 2-2-75: 0.843 (0.922)\n",
      "-9: 2-2-100: 0.824 (0.902)\n",
      "-9: 2-3-50: 0.804 (0.902)\n",
      "-9: 2-3-75: 0.843 (0.882)\n",
      "-9: 2-3-100: 0.863 (0.902)\n",
      "-9: 3-2-32: 0.843 (0.902)\n",
      "-9: 3-2-50: 0.843 (0.882)\n",
      "-9: 3-3-50: 0.824 (0.902)\n",
      "-10: 2-2-32: 0.922 (0.922)\n",
      "-10: 2-3-32: 0.902 (0.922)\n",
      "-10: 2-4-32: 0.902 (0.922)\n",
      "-10: 3-2-32: 0.902 (0.922)\n",
      "-10: 4-2-32: 0.529 (0.569)\n",
      "-10: 5-2-32: 0.863 (0.941)\n",
      "-10: 6-2-32: 0.863 (0.922)\n",
      "-10: 3-3-32: 0.902 (0.922)\n",
      "-10: 4-3-32: 0.902 (0.922)\n",
      "-10: 2-2-8: 0.882 (0.922)\n",
      "-10: 2-2-32: 0.922 (0.922)\n",
      "-10: 2-2-50: 0.882 (0.922)\n",
      "-10: 2-2-75: 0.922 (0.922)\n",
      "-10: 2-2-100: 0.922 (0.922)\n",
      "-10: 2-3-50: 0.882 (0.922)\n",
      "-10: 2-3-75: 0.902 (0.922)\n",
      "-10: 2-3-100: 0.902 (0.922)\n",
      "-10: 3-2-32: 0.882 (0.922)\n",
      "-10: 3-2-50: 0.882 (0.922)\n",
      "-10: 3-3-50: 0.804 (0.824)\n",
      "----- 75.62 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [\n",
    "        # {'L': 2, 'K': 3, 'hid_dim': 8},\n",
    "        # {'L': 2, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 32},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 5, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 6, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 32},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 8},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 50},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 75},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 100},\n",
    "\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 50},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 75},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 100},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 50},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 50},\n",
    "        ]\n",
    "\n",
    "best_accs3 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs3 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, exp['hid_dim'], OUT_DIM, exp['L'], exp['K'], act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  exp['K'], masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs3[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs3[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}: {best_val_accs3[j,i]:.3f} ({best_accs3[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}' for exp in EXPS]\n",
    "table_over3 = summary_table(best_accs3, index_name)\n",
    "table3 = summary_table(best_val_accs3, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.033735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.876471</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.030440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-32</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.027799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-32</th>\n",
       "      <td>0.876471</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.015314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-32</th>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.106027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5-2-32</th>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.033044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6-2-32</th>\n",
       "      <td>0.719608</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.160509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-32</th>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.034856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-32</th>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.162922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-8</th>\n",
       "      <td>0.772549</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.084655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.870588</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.031859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-50</th>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.029083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-75</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.033735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-100</th>\n",
       "      <td>0.880392</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.039654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-50</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.026013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-75</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.036367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-100</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.034018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-32</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.017094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-50</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.030440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-50</th>\n",
       "      <td>0.809804</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.083212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean accs       med       std\n",
       "2-2-32    0.866667  0.872549  0.033735\n",
       "2-3-32    0.876471  0.872549  0.030440\n",
       "2-4-32    0.868627  0.872549  0.027799\n",
       "3-2-32    0.876471  0.882353  0.015314\n",
       "4-2-32    0.811765  0.833333  0.106027\n",
       "5-2-32    0.850980  0.852941  0.033044\n",
       "6-2-32    0.719608  0.803922  0.160509\n",
       "3-3-32    0.847059  0.843137  0.034856\n",
       "4-3-32    0.733333  0.813725  0.162922\n",
       "2-2-8     0.772549  0.784314  0.084655\n",
       "2-2-32    0.870588  0.872549  0.031859\n",
       "2-2-50    0.862745  0.862745  0.029083\n",
       "2-2-75    0.866667  0.852941  0.033735\n",
       "2-2-100   0.880392  0.882353  0.039654\n",
       "2-3-50    0.858824  0.862745  0.026013\n",
       "2-3-75    0.854902  0.852941  0.036367\n",
       "2-3-100   0.856863  0.843137  0.034018\n",
       "3-2-32    0.858824  0.852941  0.017094\n",
       "3-2-50    0.868627  0.872549  0.030440\n",
       "3-3-50    0.809804  0.833333  0.083212"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSO, bias, and batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1: orig-True-False: 0.824 (0.882)\n",
      "- 1: trans-True-False: 0.824 (0.882)\n",
      "- 1: sym-True-False: 0.549 (0.549)\n",
      "- 1: orig-True-True: 0.804 (0.824)\n",
      "- 1: trans-True-True: 0.804 (0.863)\n",
      "- 1: sym-True-True: 0.843 (0.863)\n",
      "- 1: orig-False-False: 0.824 (0.882)\n",
      "- 1: sym-False-False: 0.824 (0.863)\n",
      "- 2: orig-True-False: 0.882 (0.902)\n",
      "- 2: trans-True-False: 0.843 (0.882)\n",
      "- 2: sym-True-False: 0.863 (0.902)\n",
      "- 2: orig-True-True: 0.725 (0.863)\n",
      "- 2: trans-True-True: 0.725 (0.824)\n",
      "- 2: sym-True-True: 0.745 (0.863)\n",
      "- 2: orig-False-False: 0.804 (0.941)\n",
      "- 2: sym-False-False: 0.863 (0.902)\n",
      "- 3: orig-True-False: 0.882 (0.922)\n",
      "- 3: trans-True-False: 0.882 (0.941)\n",
      "- 3: sym-True-False: 0.412 (0.412)\n",
      "- 3: orig-True-True: 0.804 (0.882)\n",
      "- 3: trans-True-True: 0.784 (0.863)\n",
      "- 3: sym-True-True: 0.843 (0.902)\n",
      "- 3: orig-False-False: 0.902 (0.922)\n",
      "- 3: sym-False-False: 0.902 (0.922)\n",
      "- 4: orig-True-False: 0.882 (0.961)\n",
      "- 4: trans-True-False: 0.902 (0.941)\n",
      "- 4: sym-True-False: 0.431 (0.431)\n",
      "- 4: orig-True-True: 0.902 (0.941)\n",
      "- 4: trans-True-True: 0.706 (0.922)\n",
      "- 4: sym-True-True: 0.784 (0.961)\n",
      "- 4: orig-False-False: 0.902 (0.941)\n",
      "- 4: sym-False-False: 0.431 (0.431)\n",
      "- 5: orig-True-False: 0.902 (0.922)\n",
      "- 5: trans-True-False: 0.843 (0.843)\n",
      "- 5: sym-True-False: 0.392 (0.392)\n",
      "- 5: orig-True-True: 0.882 (0.941)\n",
      "- 5: trans-True-True: 0.863 (0.863)\n",
      "- 5: sym-True-True: 0.882 (0.941)\n",
      "- 5: orig-False-False: 0.902 (0.941)\n",
      "- 5: sym-False-False: 0.765 (0.863)\n",
      "- 6: orig-True-False: 0.863 (0.902)\n",
      "- 6: trans-True-False: 0.863 (0.902)\n",
      "- 6: sym-True-False: 0.824 (0.863)\n",
      "- 6: orig-True-True: 0.784 (0.882)\n",
      "- 6: trans-True-True: 0.765 (0.824)\n",
      "- 6: sym-True-True: 0.745 (0.843)\n",
      "- 6: orig-False-False: 0.804 (0.902)\n",
      "- 6: sym-False-False: 0.510 (0.529)\n",
      "- 7: orig-True-False: 0.882 (0.902)\n",
      "- 7: trans-True-False: 0.843 (0.882)\n",
      "- 7: sym-True-False: 0.882 (0.882)\n",
      "- 7: orig-True-True: 0.784 (0.902)\n",
      "- 7: trans-True-True: 0.765 (0.843)\n",
      "- 7: sym-True-True: 0.784 (0.843)\n",
      "- 7: orig-False-False: 0.882 (0.922)\n",
      "- 7: sym-False-False: 0.471 (0.471)\n",
      "- 8: orig-True-False: 0.863 (0.902)\n",
      "- 8: trans-True-False: 0.843 (0.922)\n",
      "- 8: sym-True-False: 0.824 (0.922)\n",
      "- 8: orig-True-True: 0.784 (0.843)\n",
      "- 8: trans-True-True: 0.706 (0.824)\n",
      "- 8: sym-True-True: 0.804 (0.902)\n",
      "- 8: orig-False-False: 0.843 (0.902)\n",
      "- 8: sym-False-False: 0.510 (0.510)\n",
      "- 9: orig-True-False: 0.824 (0.902)\n",
      "- 9: trans-True-False: 0.765 (0.843)\n",
      "- 9: sym-True-False: 0.451 (0.471)\n",
      "- 9: orig-True-True: 0.824 (0.882)\n",
      "- 9: trans-True-True: 0.784 (0.882)\n",
      "- 9: sym-True-True: 0.765 (0.863)\n",
      "- 9: orig-False-False: 0.843 (0.902)\n",
      "- 9: sym-False-False: 0.451 (0.451)\n",
      "- 10: orig-True-False: 0.922 (0.922)\n",
      "- 10: trans-True-False: 0.843 (0.882)\n",
      "- 10: sym-True-False: 0.451 (0.451)\n",
      "- 10: orig-True-True: 0.863 (0.882)\n",
      "- 10: trans-True-True: 0.882 (0.922)\n",
      "- 10: sym-True-True: 0.686 (0.686)\n",
      "- 10: orig-False-False: 0.922 (0.922)\n",
      "- 10: sym-False-False: 0.451 (0.451)\n",
      "----- 23.54 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [{'GSO': 'orig', 'bias': True, 'bn': False},\n",
    "        {'GSO': 'trans', 'bias': True, 'bn': False},\n",
    "        {'GSO': 'sym', 'bias': True, 'bn': False},\n",
    "\n",
    "        {'GSO': 'orig', 'bias': True, 'bn': True},\n",
    "        {'GSO': 'trans', 'bias': True, 'bn': True},\n",
    "        {'GSO': 'sym', 'bias': True, 'bn': True},\n",
    "\n",
    "        {'GSO': 'orig', 'bias': False, 'bn': False},\n",
    "        {'GSO': 'sym', 'bias': False, 'bn': False},\n",
    "        ]\n",
    "\n",
    "best_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0, bias=False, batch_norm=exp['bn'])\n",
    "        \n",
    "        if exp['GSO'] == 'sym':\n",
    "            A_aux = (A + A.T)/2\n",
    "            A_aux = np.where(A_aux > 0, 1, 0)\n",
    "        elif exp['GSO'] == 'trans':\n",
    "            A_aux = A.T\n",
    "        else:\n",
    "            A_aux = A\n",
    "\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A_aux, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A_aux).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  K, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs3b[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs3b[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'- {i+1}: {exp[\"GSO\"]}-{exp[\"bias\"]}-{exp[\"bn\"]}: {best_val_accs3b[j,i]:.3f} ({best_accs3b[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"GSO\"]}-{exp[\"bias\"]}-{exp[\"bn\"]}' for exp in EXPS]\n",
    "table_over3b = summary_table(best_accs3b, index_name)\n",
    "table3b = summary_table(best_val_accs3b, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>orig-True-False</th>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.029412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trans-True-False</th>\n",
       "      <td>0.845098</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.034467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sym-True-False</th>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig-True-True</th>\n",
       "      <td>0.815686</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.050526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trans-True-True</th>\n",
       "      <td>0.778431</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.056863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sym-True-True</th>\n",
       "      <td>0.788235</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.054621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig-False-False</th>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.042054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sym-False-False</th>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.184408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean accs       med       std\n",
       "orig-True-False    0.872549  0.882353  0.029412\n",
       "trans-True-False   0.845098  0.843137  0.034467\n",
       "sym-True-False     0.607843  0.500000  0.200538\n",
       "orig-True-True     0.815686  0.803922  0.050526\n",
       "trans-True-True    0.778431  0.774510  0.056863\n",
       "sym-True-True      0.788235  0.784314  0.054621\n",
       "orig-False-False   0.862745  0.862745  0.042054\n",
       "sym-False-False    0.617647  0.509804  0.184408"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearities and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.863)\n",
      "-1: ReLU()-Softmax(dim=1)-NLLLoss(): 0.882 (0.882)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.412 (0.549)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.431 (0.471)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.843)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.804 (0.863)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.804 (0.882)\n",
      "-1: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.725 (0.824)\n",
      "-1: Identity()-Softmax(dim=1)-NLLLoss(): 0.824 (0.843)\n",
      "-1: Identity()-Identity()-CrossEntropyLoss(): 0.588 (0.588)\n",
      "-2: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.922)\n",
      "-2: ReLU()-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.608 (0.667)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.529 (0.588)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.941)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.941)\n",
      "-2: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.922)\n",
      "-2: Identity()-Softmax(dim=1)-NLLLoss(): 0.824 (0.941)\n",
      "-2: Identity()-Identity()-CrossEntropyLoss(): 0.706 (0.725)\n",
      "-3: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-3: ReLU()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.431 (0.608)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.471 (0.549)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.804 (0.922)\n",
      "-3: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-3: Identity()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-3: Identity()-Identity()-CrossEntropyLoss(): 0.706 (0.706)\n",
      "-4: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.922 (0.961)\n",
      "-4: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.392 (0.549)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.451 (0.627)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.922 (0.961)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.922 (0.961)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.922 (0.961)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-4: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.961)\n",
      "-4: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-4: Identity()-Identity()-CrossEntropyLoss(): 0.784 (0.784)\n",
      "-5: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.882)\n",
      "-5: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.667 (0.706)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.431 (0.510)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.902)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.961)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-5: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-5: Identity()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-5: Identity()-Identity()-CrossEntropyLoss(): 0.608 (0.627)\n",
      "-6: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.902)\n",
      "-6: ReLU()-Softmax(dim=1)-NLLLoss(): 0.882 (0.902)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.294 (0.569)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.569 (0.667)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.902)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-6: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.882)\n",
      "-6: Identity()-Softmax(dim=1)-NLLLoss(): 0.765 (0.843)\n",
      "-6: Identity()-Identity()-CrossEntropyLoss(): 0.588 (0.608)\n",
      "-7: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-7: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.471 (0.529)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.431 (0.490)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.804 (0.922)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.941)\n",
      "-7: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-7: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.863)\n",
      "-7: Identity()-Identity()-CrossEntropyLoss(): 0.824 (0.824)\n",
      "-8: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.902)\n",
      "-8: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.412 (0.471)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.510 (0.569)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.882)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.902)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.902 (0.902)\n",
      "-8: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.863)\n",
      "-8: Identity()-Softmax(dim=1)-NLLLoss(): 0.804 (0.863)\n",
      "-8: Identity()-Identity()-CrossEntropyLoss(): 0.647 (0.667)\n",
      "-9: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-9: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.412 (0.569)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.431 (0.647)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.902)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-9: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.902)\n",
      "-9: Identity()-Softmax(dim=1)-NLLLoss(): 0.804 (0.863)\n",
      "-9: Identity()-Identity()-CrossEntropyLoss(): 0.686 (0.706)\n",
      "-10: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-10: ReLU()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.451 (0.569)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.471 (0.490)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.863 (0.882)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-10: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.902)\n",
      "-10: Identity()-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-10: Identity()-Identity()-CrossEntropyLoss(): 0.706 (0.706)\n",
      "----- 35.75 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        # {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},]\n",
    "\n",
    "best_accs4 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs4 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=exp['act'], last_act=exp['lact'],\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  K, masks, exp['loss'], device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs4[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs4[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}: {best_val_accs4[j,i]:.3f} ({best_accs4[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}' for exp in EXPS]\n",
    "table_over4 = summary_table(best_accs4, index_name)\n",
    "table4 = summary_table(best_val_accs4, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.044063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.019706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.454902</td>\n",
       "      <td>0.421569</td>\n",
       "      <td>0.102187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.472549</td>\n",
       "      <td>0.460784</td>\n",
       "      <td>0.045943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.037255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.038273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.019608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.029346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.051281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.040232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.684314</td>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.075152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean accs       med  \\\n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()             0.852941  0.843137   \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                      0.868627  0.872549   \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()          0.454902  0.421569   \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                   0.472549  0.460784   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()     0.856863  0.852941   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()              0.856863  0.872549   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...   0.882353  0.882353   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...   0.850980  0.852941   \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()         0.835294  0.843137   \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                  0.837255  0.833333   \n",
       "Identity()-Identity()-CrossEntropyLoss()             0.684314  0.696078   \n",
       "\n",
       "                                                         std  \n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()            0.044063  \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                     0.019706  \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()         0.102187  \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                  0.045943  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()    0.037255  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()             0.038273  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...  0.019608  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...  0.029346  \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()        0.051281  \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                 0.040232  \n",
       "Identity()-Identity()-CrossEntropyLoss()            0.075152  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the GSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PARAMETERS  -  0.874\n",
    "## Training params\n",
    "N_RUNS = 10\n",
    "NORM = True\n",
    "N_EPOCHS = 200  # 750? --> repeat training params section\n",
    "EPOCHS_h = 10 # 10\n",
    "EPOCHS_W = 10 # 5\n",
    "LR = .005\n",
    "WD = .001\n",
    "DROPOUT = .25\n",
    "\n",
    "# BEST PARAMETERS\n",
    "## Architecture params\n",
    "N_LAYERS = 3\n",
    "K = 2\n",
    "HID_DIM = 50  # 32\n",
    "\n",
    "## Model params\n",
    "h0 = 1  # 1\n",
    "\n",
    "IN_DIM = feat.shape[1]\n",
    "OUT_DIM = n_class\n",
    "\n",
    "ACT = nn.ReLU() \n",
    "LAST_ACT = nn.Softmax(dim=1)\n",
    "LOSS_FN = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training paramerters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 200-0.005-0.0001-0.25: 0.647 (0.725)\n",
      "-1: 200-0.005-0.0005-0.25: 0.804 (0.863)\n",
      "-1: 200-0.01-0.001-0.25: 0.804 (0.863)\n",
      "-1: 200-0.005-0.001-0.25: 0.824 (0.863)\n",
      "-1: 200-0.005-0.001-0.5: 0.843 (0.882)\n",
      "-1: 200-0.001-0.001-0.25: 0.647 (0.686)\n",
      "-1: 200-0.05-0.005-0.25: 0.804 (0.902)\n",
      "-1: 200-0.01-0.005-0.25: 0.843 (0.882)\n",
      "-1: 200-0.005-0.005-0.25: 0.824 (0.863)\n",
      "-1: 200-0.001-0.005-0.25: 0.765 (0.804)\n",
      "-1: 200-0.05-0.01-0.25: 0.804 (0.882)\n",
      "-1: 200-0.005-0.01-0.25: 0.824 (0.863)\n",
      "-1: 500-0.01-0.001-0.25: 0.784 (0.863)\n",
      "-1: 500-0.005-0.001-0.25: 0.843 (0.863)\n",
      "-1: 500-0.001-0.001-0.25: 0.745 (0.804)\n",
      "-1: 500-0.005-0.005-0.25: 0.843 (0.863)\n",
      "-1: 500-0.001-0.005-0.25: 0.804 (0.843)\n",
      "-1: 200-0.05-0.01-0.5: 0.824 (0.843)\n",
      "-1: 200-0.05-0.005-0.5: 0.824 (0.863)\n",
      "-1: 200-0.01-0.005-0.5: 0.804 (0.882)\n",
      "-1: 200-0.05-0.01-0: 0.784 (0.863)\n",
      "-1: 200-0.05-0.005-0: 0.804 (0.863)\n",
      "-1: 200-0.01-0.005-0: 0.784 (0.824)\n",
      "-1: 500-0.005-0.005-0.5: 0.784 (0.843)\n",
      "-1: 500-0.005-0.001-0.5: 0.745 (0.784)\n",
      "-1: 500-0.001-0.001-0.5: 0.667 (0.667)\n",
      "-1: 750-0.005-0.005-0.25: 0.804 (0.863)\n",
      "-1: 750-0.005-0.001-0.25: 0.843 (0.882)\n",
      "-1: 750-0.001-0.001-0.25: 0.784 (0.843)\n",
      "-2: 200-0.005-0.0001-0.25: 0.863 (0.902)\n",
      "-2: 200-0.005-0.0005-0.25: 0.882 (0.922)\n",
      "-2: 200-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-2: 200-0.005-0.001-0.25: 0.863 (0.922)\n",
      "-2: 200-0.005-0.001-0.5: 0.824 (0.882)\n",
      "-2: 200-0.001-0.001-0.25: 0.804 (0.824)\n",
      "-2: 200-0.05-0.005-0.25: 0.824 (0.902)\n",
      "-2: 200-0.01-0.005-0.25: 0.824 (0.922)\n",
      "-2: 200-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-2: 200-0.001-0.005-0.25: 0.863 (0.882)\n",
      "-2: 200-0.05-0.01-0.25: 0.882 (0.922)\n",
      "-2: 200-0.005-0.01-0.25: 0.863 (0.922)\n",
      "-2: 500-0.01-0.001-0.25: 0.824 (0.902)\n",
      "-2: 500-0.005-0.001-0.25: 0.863 (0.922)\n",
      "-2: 500-0.001-0.001-0.25: 0.843 (0.882)\n",
      "-2: 500-0.005-0.005-0.25: 0.882 (0.922)\n",
      "-2: 500-0.001-0.005-0.25: 0.843 (0.902)\n",
      "-2: 200-0.05-0.01-0.5: 0.863 (0.922)\n",
      "-2: 200-0.05-0.005-0.5: 0.863 (0.922)\n",
      "-2: 200-0.01-0.005-0.5: 0.863 (0.902)\n",
      "-2: 200-0.05-0.01-0: 0.824 (0.902)\n",
      "-2: 200-0.05-0.005-0: 0.804 (0.922)\n",
      "-2: 200-0.01-0.005-0: 0.863 (0.882)\n",
      "-2: 500-0.005-0.005-0.5: 0.843 (0.902)\n",
      "-2: 500-0.005-0.001-0.5: 0.843 (0.902)\n",
      "-2: 500-0.001-0.001-0.5: 0.706 (0.725)\n",
      "-2: 750-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-2: 750-0.005-0.001-0.25: 0.824 (0.902)\n",
      "-2: 750-0.001-0.001-0.25: 0.863 (0.922)\n",
      "-3: 200-0.005-0.0001-0.25: 0.725 (0.784)\n",
      "-3: 200-0.005-0.0005-0.25: 0.804 (0.882)\n",
      "-3: 200-0.01-0.001-0.25: 0.843 (0.941)\n",
      "-3: 200-0.005-0.001-0.25: 0.843 (0.902)\n",
      "-3: 200-0.005-0.001-0.5: 0.824 (0.843)\n",
      "-3: 200-0.001-0.001-0.25: 0.686 (0.686)\n",
      "-3: 200-0.05-0.005-0.25: 0.824 (0.922)\n",
      "-3: 200-0.01-0.005-0.25: 0.922 (0.922)\n",
      "-3: 200-0.005-0.005-0.25: 0.902 (0.941)\n",
      "-3: 200-0.001-0.005-0.25: 0.843 (0.863)\n",
      "-3: 200-0.05-0.01-0.25: 0.863 (0.941)\n",
      "-3: 200-0.005-0.01-0.25: 0.863 (0.922)\n",
      "-3: 500-0.01-0.001-0.25: 0.922 (0.922)\n",
      "-3: 500-0.005-0.001-0.25: 0.882 (0.941)\n",
      "-3: 500-0.001-0.001-0.25: 0.784 (0.843)\n",
      "-3: 500-0.005-0.005-0.25: 0.882 (0.941)\n",
      "-3: 500-0.001-0.005-0.25: 0.863 (0.922)\n",
      "-3: 200-0.05-0.01-0.5: 0.804 (0.902)\n",
      "-3: 200-0.05-0.005-0.5: 0.863 (0.882)\n",
      "-3: 200-0.01-0.005-0.5: 0.882 (0.922)\n",
      "-3: 200-0.05-0.01-0: 0.882 (0.941)\n",
      "-3: 200-0.05-0.005-0: 0.804 (0.922)\n",
      "-3: 200-0.01-0.005-0: 0.863 (0.882)\n",
      "-3: 500-0.005-0.005-0.5: 0.882 (0.922)\n",
      "-3: 500-0.005-0.001-0.5: 0.706 (0.804)\n",
      "-3: 500-0.001-0.001-0.5: 0.667 (0.667)\n",
      "-3: 750-0.005-0.005-0.25: 0.882 (0.922)\n",
      "-3: 750-0.005-0.001-0.25: 0.863 (0.922)\n",
      "-3: 750-0.001-0.001-0.25: 0.863 (0.882)\n",
      "-4: 200-0.005-0.0001-0.25: 0.824 (0.863)\n",
      "-4: 200-0.005-0.0005-0.25: 0.882 (0.922)\n",
      "-4: 200-0.01-0.001-0.25: 0.863 (0.922)\n",
      "-4: 200-0.005-0.001-0.25: 0.882 (0.961)\n",
      "-4: 200-0.005-0.001-0.5: 0.784 (0.882)\n",
      "-4: 200-0.001-0.001-0.25: 0.745 (0.765)\n",
      "-4: 200-0.05-0.005-0.25: 0.745 (0.902)\n",
      "-4: 200-0.01-0.005-0.25: 0.902 (0.961)\n",
      "-4: 200-0.005-0.005-0.25: 0.902 (0.961)\n",
      "-4: 200-0.001-0.005-0.25: 0.824 (0.882)\n",
      "-4: 200-0.05-0.01-0.25: 0.882 (0.922)\n",
      "-4: 200-0.005-0.01-0.25: 0.922 (0.922)\n",
      "-4: 500-0.01-0.001-0.25: 0.804 (0.922)\n",
      "-4: 500-0.005-0.001-0.25: 0.882 (0.941)\n",
      "-4: 500-0.001-0.001-0.25: 0.765 (0.961)\n",
      "-4: 500-0.005-0.005-0.25: 0.902 (0.941)\n",
      "-4: 500-0.001-0.005-0.25: 0.863 (0.922)\n",
      "-4: 200-0.05-0.01-0.5: 0.843 (0.863)\n",
      "-4: 200-0.05-0.005-0.5: 0.824 (0.843)\n",
      "-4: 200-0.01-0.005-0.5: 0.882 (0.902)\n",
      "-4: 200-0.05-0.01-0: 0.902 (0.922)\n",
      "-4: 200-0.05-0.005-0: 0.804 (0.941)\n",
      "-4: 200-0.01-0.005-0: 0.922 (0.941)\n",
      "-4: 500-0.005-0.005-0.5: 0.882 (0.941)\n",
      "-4: 500-0.005-0.001-0.5: 0.784 (0.882)\n",
      "-4: 500-0.001-0.001-0.5: 0.725 (0.745)\n",
      "-4: 750-0.005-0.005-0.25: 0.922 (0.961)\n",
      "-4: 750-0.005-0.001-0.25: 0.922 (0.922)\n",
      "-4: 750-0.001-0.001-0.25: 0.941 (0.961)\n",
      "-5: 200-0.005-0.0001-0.25: 0.824 (0.843)\n",
      "-5: 200-0.005-0.0005-0.25: 0.804 (0.902)\n",
      "-5: 200-0.01-0.001-0.25: 0.882 (0.941)\n",
      "-5: 200-0.005-0.001-0.25: 0.922 (0.922)\n",
      "-5: 200-0.005-0.001-0.5: 0.804 (0.882)\n",
      "-5: 200-0.001-0.001-0.25: 0.608 (0.725)\n",
      "-5: 200-0.05-0.005-0.25: 0.824 (0.922)\n",
      "-5: 200-0.01-0.005-0.25: 0.863 (0.922)\n",
      "-5: 200-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-5: 200-0.001-0.005-0.25: 0.784 (0.784)\n",
      "-5: 200-0.05-0.01-0.25: 0.804 (0.961)\n",
      "-5: 200-0.005-0.01-0.25: 0.863 (0.902)\n",
      "-5: 500-0.01-0.001-0.25: 0.843 (0.941)\n",
      "-5: 500-0.005-0.001-0.25: 0.843 (0.922)\n",
      "-5: 500-0.001-0.001-0.25: 0.765 (0.843)\n",
      "-5: 500-0.005-0.005-0.25: 0.804 (0.902)\n",
      "-5: 500-0.001-0.005-0.25: 0.784 (0.882)\n",
      "-5: 200-0.05-0.01-0.5: 0.804 (0.922)\n",
      "-5: 200-0.05-0.005-0.5: 0.725 (0.882)\n",
      "-5: 200-0.01-0.005-0.5: 0.784 (0.922)\n",
      "-5: 200-0.05-0.01-0: 0.843 (0.922)\n",
      "-5: 200-0.05-0.005-0: 0.843 (0.882)\n",
      "-5: 200-0.01-0.005-0: 0.784 (0.843)\n",
      "-5: 500-0.005-0.005-0.5: 0.745 (0.882)\n",
      "-5: 500-0.005-0.001-0.5: 0.824 (0.882)\n",
      "-5: 500-0.001-0.001-0.5: 0.706 (0.725)\n",
      "-5: 750-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-5: 750-0.005-0.001-0.25: 0.922 (0.941)\n",
      "-5: 750-0.001-0.001-0.25: 0.784 (0.804)\n",
      "-6: 200-0.005-0.0001-0.25: 0.784 (0.824)\n",
      "-6: 200-0.005-0.0005-0.25: 0.824 (0.863)\n",
      "-6: 200-0.01-0.001-0.25: 0.824 (0.882)\n",
      "-6: 200-0.005-0.001-0.25: 0.824 (0.882)\n",
      "-6: 200-0.005-0.001-0.5: 0.784 (0.804)\n",
      "-6: 200-0.001-0.001-0.25: 0.667 (0.725)\n",
      "-6: 200-0.05-0.005-0.25: 0.804 (0.882)\n",
      "-6: 200-0.01-0.005-0.25: 0.843 (0.922)\n",
      "-6: 200-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-6: 200-0.001-0.005-0.25: 0.863 (0.863)\n",
      "-6: 200-0.05-0.01-0.25: 0.824 (0.882)\n",
      "-6: 200-0.005-0.01-0.25: 0.863 (0.882)\n",
      "-6: 500-0.01-0.001-0.25: 0.863 (0.882)\n",
      "-6: 500-0.005-0.001-0.25: 0.863 (0.922)\n",
      "-6: 500-0.001-0.001-0.25: 0.765 (0.843)\n",
      "-6: 500-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-6: 500-0.001-0.005-0.25: 0.882 (0.882)\n",
      "-6: 200-0.05-0.01-0.5: 0.745 (0.843)\n",
      "-6: 200-0.05-0.005-0.5: 0.804 (0.824)\n",
      "-6: 200-0.01-0.005-0.5: 0.863 (0.882)\n",
      "-6: 200-0.05-0.01-0: 0.843 (0.902)\n",
      "-6: 200-0.05-0.005-0: 0.843 (0.882)\n",
      "-6: 200-0.01-0.005-0: 0.824 (0.843)\n",
      "-6: 500-0.005-0.005-0.5: 0.863 (0.882)\n",
      "-6: 500-0.005-0.001-0.5: 0.725 (0.843)\n",
      "-6: 500-0.001-0.001-0.5: 0.667 (0.706)\n",
      "-6: 750-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-6: 750-0.005-0.001-0.25: 0.824 (0.902)\n",
      "-6: 750-0.001-0.001-0.25: 0.824 (0.843)\n",
      "-7: 200-0.005-0.0001-0.25: 0.824 (0.902)\n",
      "-7: 200-0.005-0.0005-0.25: 0.843 (0.902)\n",
      "-7: 200-0.01-0.001-0.25: 0.843 (0.922)\n",
      "-7: 200-0.005-0.001-0.25: 0.843 (0.902)\n",
      "-7: 200-0.005-0.001-0.5: 0.843 (0.882)\n",
      "-7: 200-0.001-0.001-0.25: 0.804 (0.843)\n",
      "-7: 200-0.05-0.005-0.25: 0.863 (0.902)\n",
      "-7: 200-0.01-0.005-0.25: 0.882 (0.902)\n",
      "-7: 200-0.005-0.005-0.25: 0.843 (0.902)\n",
      "-7: 200-0.001-0.005-0.25: 0.843 (0.863)\n",
      "-7: 200-0.05-0.01-0.25: 0.863 (0.902)\n",
      "-7: 200-0.005-0.01-0.25: 0.863 (0.902)\n",
      "-7: 500-0.01-0.001-0.25: 0.843 (0.922)\n",
      "-7: 500-0.005-0.001-0.25: 0.922 (0.922)\n",
      "-7: 500-0.001-0.001-0.25: 0.863 (0.863)\n",
      "-7: 500-0.005-0.005-0.25: 0.843 (0.902)\n",
      "-7: 500-0.001-0.005-0.25: 0.863 (0.882)\n",
      "-7: 200-0.05-0.01-0.5: 0.843 (0.902)\n",
      "-7: 200-0.05-0.005-0.5: 0.882 (0.922)\n",
      "-7: 200-0.01-0.005-0.5: 0.843 (0.902)\n",
      "-7: 200-0.05-0.01-0: 0.843 (0.882)\n",
      "-7: 200-0.05-0.005-0: 0.863 (0.882)\n",
      "-7: 200-0.01-0.005-0: 0.824 (0.824)\n",
      "-7: 500-0.005-0.005-0.5: 0.863 (0.902)\n",
      "-7: 500-0.005-0.001-0.5: 0.863 (0.902)\n",
      "-7: 500-0.001-0.001-0.5: 0.824 (0.824)\n",
      "-7: 750-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-7: 750-0.005-0.001-0.25: 0.863 (0.902)\n",
      "-7: 750-0.001-0.001-0.25: 0.882 (0.882)\n",
      "-8: 200-0.005-0.0001-0.25: 0.804 (0.843)\n",
      "-8: 200-0.005-0.0005-0.25: 0.765 (0.843)\n",
      "-8: 200-0.01-0.001-0.25: 0.765 (0.882)\n",
      "-8: 200-0.005-0.001-0.25: 0.843 (0.902)\n",
      "-8: 200-0.005-0.001-0.5: 0.725 (0.784)\n",
      "-8: 200-0.001-0.001-0.25: 0.627 (0.647)\n",
      "-8: 200-0.05-0.005-0.25: 0.804 (0.882)\n",
      "-8: 200-0.01-0.005-0.25: 0.863 (0.922)\n",
      "-8: 200-0.005-0.005-0.25: 0.824 (0.882)\n",
      "-8: 200-0.001-0.005-0.25: 0.784 (0.804)\n",
      "-8: 200-0.05-0.01-0.25: 0.882 (0.882)\n",
      "-8: 200-0.005-0.01-0.25: 0.843 (0.882)\n",
      "-8: 500-0.01-0.001-0.25: 0.824 (0.902)\n",
      "-8: 500-0.005-0.001-0.25: 0.863 (0.902)\n",
      "-8: 500-0.001-0.001-0.25: 0.804 (0.804)\n",
      "-8: 500-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-8: 500-0.001-0.005-0.25: 0.804 (0.843)\n",
      "-8: 200-0.05-0.01-0.5: 0.765 (0.882)\n",
      "-8: 200-0.05-0.005-0.5: 0.843 (0.882)\n",
      "-8: 200-0.01-0.005-0.5: 0.824 (0.843)\n",
      "-8: 200-0.05-0.01-0: 0.843 (0.902)\n",
      "-8: 200-0.05-0.005-0: 0.784 (0.863)\n",
      "-8: 200-0.01-0.005-0: 0.765 (0.784)\n",
      "-8: 500-0.005-0.005-0.5: 0.843 (0.863)\n",
      "-8: 500-0.005-0.001-0.5: 0.784 (0.863)\n",
      "-8: 500-0.001-0.001-0.5: 0.627 (0.706)\n",
      "-8: 750-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-8: 750-0.005-0.001-0.25: 0.804 (0.882)\n",
      "-8: 750-0.001-0.001-0.25: 0.765 (0.784)\n",
      "-9: 200-0.005-0.0001-0.25: 0.824 (0.843)\n",
      "-9: 200-0.005-0.0005-0.25: 0.843 (0.902)\n",
      "-9: 200-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-9: 200-0.005-0.001-0.25: 0.824 (0.902)\n",
      "-9: 200-0.005-0.001-0.5: 0.824 (0.863)\n",
      "-9: 200-0.001-0.001-0.25: 0.667 (0.725)\n",
      "-9: 200-0.05-0.005-0.25: 0.824 (0.863)\n",
      "-9: 200-0.01-0.005-0.25: 0.824 (0.882)\n",
      "-9: 200-0.005-0.005-0.25: 0.843 (0.882)\n",
      "-9: 200-0.001-0.005-0.25: 0.765 (0.824)\n",
      "-9: 200-0.05-0.01-0.25: 0.804 (0.882)\n",
      "-9: 200-0.005-0.01-0.25: 0.824 (0.863)\n",
      "-9: 500-0.01-0.001-0.25: 0.784 (0.882)\n",
      "-9: 500-0.005-0.001-0.25: 0.863 (0.902)\n",
      "-9: 500-0.001-0.001-0.25: 0.706 (0.784)\n",
      "-9: 500-0.005-0.005-0.25: 0.843 (0.882)\n",
      "-9: 500-0.001-0.005-0.25: 0.824 (0.824)\n",
      "-9: 200-0.05-0.01-0.5: 0.804 (0.882)\n",
      "-9: 200-0.05-0.005-0.5: 0.824 (0.863)\n",
      "-9: 200-0.01-0.005-0.5: 0.863 (0.882)\n",
      "-9: 200-0.05-0.01-0: 0.824 (0.843)\n",
      "-9: 200-0.05-0.005-0: 0.784 (0.843)\n",
      "-9: 200-0.01-0.005-0: 0.824 (0.843)\n",
      "-9: 500-0.005-0.005-0.5: 0.784 (0.843)\n",
      "-9: 500-0.005-0.001-0.5: 0.863 (0.882)\n",
      "-9: 500-0.001-0.001-0.5: 0.686 (0.725)\n",
      "-9: 750-0.005-0.005-0.25: 0.824 (0.882)\n",
      "-9: 750-0.005-0.001-0.25: 0.843 (0.882)\n",
      "-9: 750-0.001-0.001-0.25: 0.824 (0.863)\n",
      "-10: 200-0.005-0.0001-0.25: 0.824 (0.882)\n",
      "-10: 200-0.005-0.0005-0.25: 0.863 (0.902)\n",
      "-10: 200-0.01-0.001-0.25: 0.804 (0.922)\n",
      "-10: 200-0.005-0.001-0.25: 0.902 (0.902)\n",
      "-10: 200-0.005-0.001-0.5: 0.824 (0.882)\n",
      "-10: 200-0.001-0.001-0.25: 0.725 (0.765)\n",
      "-10: 200-0.05-0.005-0.25: 0.863 (0.902)\n",
      "-10: 200-0.01-0.005-0.25: 0.902 (0.902)\n",
      "-10: 200-0.005-0.005-0.25: 0.882 (0.902)\n",
      "-10: 200-0.001-0.005-0.25: 0.765 (0.804)\n",
      "-10: 200-0.05-0.01-0.25: 0.882 (0.941)\n",
      "-10: 200-0.005-0.01-0.25: 0.882 (0.882)\n",
      "-10: 500-0.01-0.001-0.25: 0.882 (0.922)\n",
      "-10: 500-0.005-0.001-0.25: 0.824 (0.902)\n",
      "-10: 500-0.001-0.001-0.25: 0.863 (0.882)\n",
      "-10: 500-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-10: 500-0.001-0.005-0.25: 0.843 (0.882)\n",
      "-10: 200-0.05-0.01-0.5: 0.843 (0.902)\n",
      "-10: 200-0.05-0.005-0.5: 0.804 (0.882)\n",
      "-10: 200-0.01-0.005-0.5: 0.843 (0.902)\n",
      "-10: 200-0.05-0.01-0: 0.843 (0.902)\n",
      "-10: 200-0.05-0.005-0: 0.843 (0.922)\n",
      "-10: 200-0.01-0.005-0: 0.843 (0.843)\n",
      "-10: 500-0.005-0.005-0.5: 0.824 (0.863)\n",
      "-10: 500-0.005-0.001-0.5: 0.804 (0.863)\n",
      "-10: 500-0.001-0.001-0.5: 0.745 (0.804)\n",
      "-10: 750-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-10: 750-0.005-0.001-0.25: 0.863 (0.902)\n",
      "-10: 750-0.001-0.001-0.25: 0.843 (0.882)\n",
      "----- 55.59 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-4, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-4, 'drop': .25},\n",
    "\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "        \n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-2, 'drop': .25},\n",
    "\n",
    "        {'epochs': 500, 'lr': .01, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "\n",
    "\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': 0},\n",
    "\n",
    "        \n",
    "        {'epochs': 500, 'lr': .005, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 500, 'lr': .001, 'wd': 1e-3, 'drop': .5},\n",
    "\n",
    "        {'epochs': 750, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 750, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 750, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "        ]\n",
    "\n",
    "best_accs5 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs5 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=exp['drop'], init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S, K, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, exp['epochs'], exp['lr'], exp['wd'], epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs5[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs5[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}: {best_val_accs5[j,i]:.3f} ({best_accs5[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}' for exp in EXPS]\n",
    "table_over5 = summary_table(best_accs5, index_name)\n",
    "table5 = summary_table(best_val_accs5, index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0001-0.25</th>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.059635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0005-0.25</th>\n",
       "      <td>0.831373</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.036367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.25</th>\n",
       "      <td>0.831373</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.031859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.25</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.032869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.5</th>\n",
       "      <td>0.807843</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.033735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.001-0.25</th>\n",
       "      <td>0.698039</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.065737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0.25</th>\n",
       "      <td>0.817647</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.031677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.25</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.032575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.005-0.25</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.026956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.25</th>\n",
       "      <td>0.809804</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.039265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.25</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.034018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.01-0.25</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.026956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.01-0.001-0.25</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.041176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0.25</th>\n",
       "      <td>0.864706</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.025490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.001-0.25</th>\n",
       "      <td>0.790196</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.049643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.005-0.25</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.026013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.005-0.25</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.030440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.5</th>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.035349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0.5</th>\n",
       "      <td>0.825490</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.041548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.5</th>\n",
       "      <td>0.845098</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.030941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.030376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0</th>\n",
       "      <td>0.817647</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.026380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0</th>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.043888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.005-0.5</th>\n",
       "      <td>0.831373</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.044019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0.5</th>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.052796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.001-0.5</th>\n",
       "      <td>0.701961</td>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.051729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750-0.005-0.005-0.25</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.029672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750-0.005-0.001-0.25</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.037255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750-0.001-0.001-0.25</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.050412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean accs       med       std\n",
       "200-0.005-0.0001-0.25   0.794118  0.823529  0.059635\n",
       "200-0.005-0.0005-0.25   0.831373  0.833333  0.036367\n",
       "200-0.01-0.001-0.25     0.831373  0.843137  0.031859\n",
       "200-0.005-0.001-0.25    0.856863  0.843137  0.032869\n",
       "200-0.005-0.001-0.5     0.807843  0.823529  0.033735\n",
       "200-0.001-0.001-0.25    0.698039  0.676471  0.065737\n",
       "200-0.05-0.005-0.25     0.817647  0.823529  0.031677\n",
       "200-0.01-0.005-0.25     0.866667  0.862745  0.032575\n",
       "200-0.005-0.005-0.25    0.860784  0.862745  0.026956\n",
       "200-0.001-0.005-0.25    0.809804  0.803922  0.039265\n",
       "200-0.05-0.01-0.25      0.849020  0.862745  0.034018\n",
       "200-0.005-0.01-0.25     0.860784  0.862745  0.026956\n",
       "500-0.01-0.001-0.25     0.837255  0.833333  0.041176\n",
       "500-0.005-0.001-0.25    0.864706  0.862745  0.025490\n",
       "500-0.001-0.001-0.25    0.790196  0.774510  0.049643\n",
       "500-0.005-0.005-0.25    0.858824  0.862745  0.026013\n",
       "500-0.001-0.005-0.25    0.837255  0.843137  0.030440\n",
       "200-0.05-0.01-0.5       0.813725  0.813725  0.035349\n",
       "200-0.05-0.005-0.5      0.825490  0.823529  0.041548\n",
       "200-0.01-0.005-0.5      0.845098  0.852941  0.030941\n",
       "200-0.05-0.01-0         0.843137  0.843137  0.030376\n",
       "200-0.05-0.005-0        0.817647  0.803922  0.026380\n",
       "200-0.01-0.005-0        0.829412  0.823529  0.043888\n",
       "500-0.005-0.005-0.5     0.831373  0.843137  0.044019\n",
       "500-0.005-0.001-0.5     0.794118  0.794118  0.052796\n",
       "500-0.001-0.001-0.5     0.701961  0.696078  0.051729\n",
       "750-0.005-0.005-0.25    0.860784  0.862745  0.029672\n",
       "750-0.005-0.001-0.25    0.856863  0.852941  0.037255\n",
       "750-0.001-0.001-0.25    0.837255  0.833333  0.050412"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 0.01-1-1-True: 0.471 (0.569)\n",
      "-1: 0.1-1-1-True: 0.490 (0.549)\n",
      "-1: 1-1-1-True: 0.608 (0.647)\n",
      "-1: 1-1-1-True: 0.843 (0.843)\n",
      "-1: 1-1-1-True: 0.725 (0.745)\n",
      "-1: 1-1-1-False: 0.588 (0.667)\n",
      "-1: 1-5-1-True: 0.765 (0.784)\n",
      "-1: 1-10-1-True: 0.765 (0.804)\n",
      "-1: 1-10-5-True: 0.824 (0.863)\n",
      "-1: 1-1-5-True: 0.627 (0.686)\n",
      "-1: 1-1-10-True: 0.588 (0.745)\n",
      "-1: 1-5-10-True: 0.745 (0.824)\n",
      "-1: 0.1-10-10-True: 0.784 (0.863)\n",
      "-1: 1-10-10-True: 0.863 (0.863)\n",
      "-1: 1-10-10-True: 0.804 (0.863)\n",
      "-1: 1-25-25-True: 0.804 (0.863)\n",
      "-1: 1-25-25-True: 0.843 (0.863)\n",
      "-1: 0.1-25-25-True: 0.824 (0.882)\n",
      "-1: 1-25-25-True: 0.843 (0.882)\n",
      "-1: 1-50-50-True: 0.843 (0.863)\n",
      "-2: 0.01-1-1-True: 0.569 (0.686)\n",
      "-2: 0.1-1-1-True: 0.706 (0.784)\n",
      "-2: 1-1-1-True: 0.627 (0.745)\n",
      "-2: 1-1-1-True: 0.843 (0.902)\n",
      "-2: 1-1-1-True: 0.843 (0.882)\n",
      "-2: 1-1-1-False: 0.804 (0.882)\n",
      "-2: 1-5-1-True: 0.863 (0.882)\n",
      "-2: 1-10-1-True: 0.765 (0.922)\n",
      "-2: 1-10-5-True: 0.843 (0.902)\n",
      "-2: 1-1-5-True: 0.784 (0.843)\n",
      "-2: 1-1-10-True: 0.725 (0.804)\n",
      "-2: 1-5-10-True: 0.784 (0.922)\n",
      "-2: 0.1-10-10-True: 0.863 (0.902)\n",
      "-2: 1-10-10-True: 0.863 (0.902)\n",
      "-2: 1-10-10-True: 0.804 (0.902)\n",
      "-2: 1-25-25-True: 0.843 (0.922)\n",
      "-2: 1-25-25-True: 0.882 (0.922)\n",
      "-2: 0.1-25-25-True: 0.824 (0.902)\n",
      "-2: 1-25-25-True: 0.863 (0.902)\n",
      "-2: 1-50-50-True: 0.863 (0.922)\n",
      "-3: 0.01-1-1-True: 0.529 (0.627)\n",
      "-3: 0.1-1-1-True: 0.725 (0.725)\n",
      "-3: 1-1-1-True: 0.725 (0.725)\n",
      "-3: 1-1-1-True: 0.824 (0.843)\n",
      "-3: 1-1-1-True: 0.843 (0.863)\n",
      "-3: 1-1-1-False: 0.843 (0.843)\n",
      "-3: 1-5-1-True: 0.765 (0.863)\n",
      "-3: 1-10-1-True: 0.824 (0.824)\n",
      "-3: 1-10-5-True: 0.882 (0.882)\n",
      "-3: 1-1-5-True: 0.686 (0.706)\n",
      "-3: 1-1-10-True: 0.627 (0.725)\n",
      "-3: 1-5-10-True: 0.843 (0.922)\n",
      "-3: 0.1-10-10-True: 0.843 (0.922)\n",
      "-3: 1-10-10-True: 0.843 (0.922)\n",
      "-3: 1-10-10-True: 0.863 (0.922)\n",
      "-3: 1-25-25-True: 0.824 (0.902)\n",
      "-3: 1-25-25-True: 0.902 (0.922)\n",
      "-3: 0.1-25-25-True: 0.882 (0.941)\n",
      "-3: 1-25-25-True: 0.863 (0.941)\n",
      "-3: 1-50-50-True: 0.863 (0.941)\n",
      "-4: 0.01-1-1-True: 0.667 (0.706)\n",
      "-4: 0.1-1-1-True: 0.608 (0.627)\n",
      "-4: 1-1-1-True: 0.667 (0.725)\n",
      "-4: 1-1-1-True: 0.549 (0.647)\n",
      "-4: 1-1-1-True: 0.765 (0.804)\n",
      "-4: 1-1-1-False: 0.804 (0.902)\n",
      "-4: 1-5-1-True: 0.843 (0.882)\n",
      "-4: 1-10-1-True: 0.745 (0.765)\n",
      "-4: 1-10-5-True: 0.902 (0.941)\n",
      "-4: 1-1-5-True: 0.706 (0.765)\n",
      "-4: 1-1-10-True: 0.784 (0.824)\n",
      "-4: 1-5-10-True: 0.922 (0.961)\n",
      "-4: 0.1-10-10-True: 0.922 (0.941)\n",
      "-4: 1-10-10-True: 0.902 (0.941)\n",
      "-4: 1-10-10-True: 0.902 (0.961)\n",
      "-4: 1-25-25-True: 0.922 (0.922)\n",
      "-4: 1-25-25-True: 0.843 (0.941)\n",
      "-4: 0.1-25-25-True: 0.922 (0.941)\n",
      "-4: 1-25-25-True: 0.882 (0.922)\n",
      "-4: 1-50-50-True: 0.863 (0.941)\n",
      "-5: 0.01-1-1-True: 0.471 (0.490)\n",
      "-5: 0.1-1-1-True: 0.412 (0.529)\n",
      "-5: 1-1-1-True: 0.627 (0.647)\n",
      "-5: 1-1-1-True: 0.725 (0.784)\n",
      "-5: 1-1-1-True: 0.725 (0.843)\n",
      "-5: 1-1-1-False: 0.686 (0.824)\n",
      "-5: 1-5-1-True: 0.804 (0.824)\n",
      "-5: 1-10-1-True: 0.804 (0.804)\n",
      "-5: 1-10-5-True: 0.765 (0.902)\n",
      "-5: 1-1-5-True: 0.706 (0.765)\n",
      "-5: 1-1-10-True: 0.686 (0.784)\n",
      "-5: 1-5-10-True: 0.824 (0.882)\n",
      "-5: 0.1-10-10-True: 0.843 (0.922)\n",
      "-5: 1-10-10-True: 0.882 (0.922)\n",
      "-5: 1-10-10-True: 0.882 (0.922)\n",
      "-5: 1-25-25-True: 0.784 (0.882)\n",
      "-5: 1-25-25-True: 0.843 (0.922)\n",
      "-5: 0.1-25-25-True: 0.902 (0.922)\n",
      "-5: 1-25-25-True: 0.941 (0.941)\n",
      "-5: 1-50-50-True: 0.902 (0.941)\n",
      "-6: 0.01-1-1-True: 0.588 (0.647)\n",
      "-6: 0.1-1-1-True: 0.627 (0.667)\n",
      "-6: 1-1-1-True: 0.706 (0.725)\n",
      "-6: 1-1-1-True: 0.804 (0.863)\n",
      "-6: 1-1-1-True: 0.725 (0.784)\n",
      "-6: 1-1-1-False: 0.608 (0.686)\n",
      "-6: 1-5-1-True: 0.784 (0.804)\n",
      "-6: 1-10-1-True: 0.784 (0.784)\n",
      "-6: 1-10-5-True: 0.824 (0.902)\n",
      "-6: 1-1-5-True: 0.667 (0.706)\n",
      "-6: 1-1-10-True: 0.627 (0.725)\n",
      "-6: 1-5-10-True: 0.882 (0.902)\n",
      "-6: 0.1-10-10-True: 0.863 (0.902)\n",
      "-6: 1-10-10-True: 0.863 (0.902)\n",
      "-6: 1-10-10-True: 0.843 (0.902)\n",
      "-6: 1-25-25-True: 0.843 (0.882)\n",
      "-6: 1-25-25-True: 0.863 (0.882)\n",
      "-6: 0.1-25-25-True: 0.882 (0.902)\n",
      "-6: 1-25-25-True: 0.882 (0.922)\n",
      "-6: 1-50-50-True: 0.882 (0.922)\n",
      "-7: 0.01-1-1-True: 0.627 (0.667)\n",
      "-7: 0.1-1-1-True: 0.569 (0.588)\n",
      "-7: 1-1-1-True: 0.706 (0.706)\n",
      "-7: 1-1-1-True: 0.843 (0.882)\n",
      "-7: 1-1-1-True: 0.804 (0.804)\n",
      "-7: 1-1-1-False: 0.765 (0.843)\n",
      "-7: 1-5-1-True: 0.804 (0.843)\n",
      "-7: 1-10-1-True: 0.745 (0.824)\n",
      "-7: 1-10-5-True: 0.863 (0.902)\n",
      "-7: 1-1-5-True: 0.745 (0.784)\n",
      "-7: 1-1-10-True: 0.824 (0.824)\n",
      "-7: 1-5-10-True: 0.863 (0.922)\n",
      "-7: 0.1-10-10-True: 0.863 (0.902)\n",
      "-7: 1-10-10-True: 0.882 (0.902)\n",
      "-7: 1-10-10-True: 0.863 (0.902)\n",
      "-7: 1-25-25-True: 0.824 (0.863)\n",
      "-7: 1-25-25-True: 0.902 (0.902)\n",
      "-7: 0.1-25-25-True: 0.843 (0.902)\n",
      "-7: 1-25-25-True: 0.824 (0.902)\n",
      "-7: 1-50-50-True: 0.882 (0.902)\n",
      "-8: 0.01-1-1-True: 0.529 (0.588)\n",
      "-8: 0.1-1-1-True: 0.529 (0.569)\n",
      "-8: 1-1-1-True: 0.471 (0.510)\n",
      "-8: 1-1-1-True: 0.608 (0.706)\n",
      "-8: 1-1-1-True: 0.804 (0.863)\n",
      "-8: 1-1-1-False: 0.706 (0.784)\n",
      "-8: 1-5-1-True: 0.647 (0.725)\n",
      "-8: 1-10-1-True: 0.706 (0.725)\n",
      "-8: 1-10-5-True: 0.745 (0.804)\n",
      "-8: 1-1-5-True: 0.608 (0.667)\n",
      "-8: 1-1-10-True: 0.667 (0.725)\n",
      "-8: 1-5-10-True: 0.824 (0.882)\n",
      "-8: 0.1-10-10-True: 0.824 (0.882)\n",
      "-8: 1-10-10-True: 0.843 (0.902)\n",
      "-8: 1-10-10-True: 0.882 (0.902)\n",
      "-8: 1-25-25-True: 0.745 (0.824)\n",
      "-8: 1-25-25-True: 0.824 (0.843)\n",
      "-8: 0.1-25-25-True: 0.863 (0.882)\n",
      "-8: 1-25-25-True: 0.804 (0.882)\n",
      "-8: 1-50-50-True: 0.843 (0.882)\n",
      "-9: 0.01-1-1-True: 0.647 (0.686)\n",
      "-9: 0.1-1-1-True: 0.549 (0.569)\n",
      "-9: 1-1-1-True: 0.608 (0.608)\n",
      "-9: 1-1-1-True: 0.725 (0.784)\n",
      "-9: 1-1-1-True: 0.745 (0.804)\n",
      "-9: 1-1-1-False: 0.784 (0.824)\n",
      "-9: 1-5-1-True: 0.784 (0.863)\n",
      "-9: 1-10-1-True: 0.725 (0.765)\n",
      "-9: 1-10-5-True: 0.843 (0.882)\n",
      "-9: 1-1-5-True: 0.647 (0.667)\n",
      "-9: 1-1-10-True: 0.647 (0.686)\n",
      "-9: 1-5-10-True: 0.843 (0.882)\n",
      "-9: 0.1-10-10-True: 0.863 (0.882)\n",
      "-9: 1-10-10-True: 0.843 (0.882)\n",
      "-9: 1-10-10-True: 0.824 (0.882)\n",
      "-9: 1-25-25-True: 0.765 (0.863)\n",
      "-9: 1-25-25-True: 0.824 (0.863)\n",
      "-9: 0.1-25-25-True: 0.843 (0.882)\n",
      "-9: 1-25-25-True: 0.843 (0.902)\n",
      "-9: 1-50-50-True: 0.882 (0.902)\n",
      "-10: 0.01-1-1-True: 0.686 (0.784)\n",
      "-10: 0.1-1-1-True: 0.569 (0.627)\n",
      "-10: 1-1-1-True: 0.588 (0.627)\n",
      "-10: 1-1-1-True: 0.745 (0.804)\n",
      "-10: 1-1-1-True: 0.824 (0.882)\n",
      "-10: 1-1-1-False: 0.725 (0.745)\n",
      "-10: 1-5-1-True: 0.745 (0.804)\n",
      "-10: 1-10-1-True: 0.804 (0.824)\n",
      "-10: 1-10-5-True: 0.843 (0.863)\n",
      "-10: 1-1-5-True: 0.588 (0.706)\n",
      "-10: 1-1-10-True: 0.667 (0.745)\n",
      "-10: 1-5-10-True: 0.882 (0.902)\n",
      "-10: 0.1-10-10-True: 0.863 (0.882)\n",
      "-10: 1-10-10-True: 0.882 (0.902)\n",
      "-10: 1-10-10-True: 0.843 (0.902)\n",
      "-10: 1-25-25-True: 0.902 (0.902)\n",
      "-10: 1-25-25-True: 0.882 (0.902)\n",
      "-10: 0.1-25-25-True: 0.882 (0.902)\n",
      "-10: 1-25-25-True: 0.863 (0.902)\n",
      "-10: 1-50-50-True: 0.843 (0.902)\n",
      "----- 30.66 mins -----\n"
     ]
    }
   ],
   "source": [
    "# h0, norm (norm, not norm, norm H), sep vs joint training, optimizers iters (W vs h)\n",
    "EXPS = [\n",
    "        # {'h0': .001, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': .01, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': .1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 1000, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 1500, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "\n",
    "        # {'h0': .1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        {'h0': 1, 'epochs': 1000, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 5, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 1, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 5, 'alt': True},\n",
    "\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 5, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 10, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 5, 'epochs_W': 10, 'alt': True},\n",
    "        \n",
    "        {'h0': .1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 500, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 50, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 100, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': .1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 50, 'epochs_W': 50, 'alt': True},\n",
    "    ]\n",
    "\n",
    "\n",
    "best_accs6 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs6 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=exp['h0'])\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        if not exp['alt']:\n",
    "            model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "            _, acc = model.train(feat, labels, exp['epochs'], LR, WD)\n",
    "        else:\n",
    "            model = GF_NodeClassModel(arch, S,  K, masks, LOSS_FN, device=device)\n",
    "            _, acc = model.train(feat, labels, exp['epochs'], LR, WD, epochs_h=exp['epochs_h'],\n",
    "                                 epochs_W=exp['epochs_W'])\n",
    "\n",
    "        best_accs6[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs6[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"h0\"]}-{exp[\"epochs_h\"]}-{exp[\"epochs_W\"]}-{exp[\"alt\"]}: {best_val_accs6[j,i]:.3f} ({best_accs6[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"h0\"]}-{exp[\"epochs_h\"]}-{exp[\"epochs_W\"]}-{exp[\"alt\"]}' for exp in EXPS]\n",
    "table_over6 = summary_table(best_accs6, index_name)\n",
    "table6 = summary_table(best_val_accs6, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.01-1-1-True</th>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.074018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-1-1-True</th>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.089533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-1-True</th>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.070724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000-1-1-1-True</th>\n",
       "      <td>0.750980</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.098059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500-1-1-1-True</th>\n",
       "      <td>0.780392</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.046235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000-1-1-1-False</th>\n",
       "      <td>0.731373</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.080869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-5-1-True</th>\n",
       "      <td>0.780392</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.056011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-1-True</th>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.035565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-5-True</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.045775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-5-True</th>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.058331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-10-True</th>\n",
       "      <td>0.684314</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.069849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-5-10-True</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.048388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-10-10-True</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.033102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-10-True</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.019212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-1-10-10-True</th>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.031859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50-1-25-25-True</th>\n",
       "      <td>0.825490</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.052941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100-1-25-25-True</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.028347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-25-25-True</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.031373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-25-25-True</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.035565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-50-50-True</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.019212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mean accs       med       std\n",
       "200-0.01-1-1-True    0.578431  0.578431  0.074018\n",
       "200-0.1-1-1-True     0.578431  0.568627  0.089533\n",
       "200-1-1-1-True       0.633333  0.627451  0.070724\n",
       "1000-1-1-1-True      0.750980  0.774510  0.098059\n",
       "1500-1-1-1-True      0.780392  0.784314  0.046235\n",
       "1000-1-1-1-False     0.731373  0.745098  0.080869\n",
       "200-1-5-1-True       0.780392  0.784314  0.056011\n",
       "200-1-10-1-True      0.766667  0.764706  0.035565\n",
       "200-1-10-5-True      0.833333  0.843137  0.045775\n",
       "200-1-1-5-True       0.676471  0.676471  0.058331\n",
       "200-1-1-10-True      0.684314  0.666667  0.069849\n",
       "200-1-5-10-True      0.841176  0.843137  0.048388\n",
       "200-0.1-10-10-True   0.852941  0.862745  0.033102\n",
       "200-1-10-10-True     0.866667  0.862745  0.019212\n",
       "500-1-10-10-True     0.850980  0.852941  0.031859\n",
       "50-1-25-25-True      0.825490  0.823529  0.052941\n",
       "100-1-25-25-True     0.860784  0.852941  0.028347\n",
       "200-0.1-25-25-True   0.866667  0.872549  0.031373\n",
       "200-1-25-25-True     0.860784  0.862745  0.035565\n",
       "200-1-50-50-True     0.866667  0.862745  0.019212"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 2-2-16: 0.765 (0.843)\n",
      "-1: 2-3-16: 0.784 (0.882)\n",
      "-1: 2-3-32: 0.745 (0.863)\n",
      "-1: 2-4-16: 0.824 (0.863)\n",
      "-1: 3-2-16: 0.804 (0.843)\n",
      "-1: 4-2-16: 0.725 (0.843)\n",
      "-1: 3-3-16: 0.804 (0.843)\n",
      "-1: 4-3-16: 0.725 (0.804)\n",
      "-1: 2-2-32: 0.824 (0.863)\n",
      "-1: 2-2-50: 0.745 (0.843)\n",
      "-1: 2-2-75: 0.784 (0.863)\n",
      "-1: 2-2-100: 0.745 (0.843)\n",
      "-1: 2-3-50: 0.725 (0.843)\n",
      "-1: 2-3-75: 0.804 (0.863)\n",
      "-1: 2-3-100: 0.824 (0.843)\n",
      "-1: 3-2-50: 0.843 (0.863)\n",
      "-1: 3-2-64: 0.804 (0.863)\n",
      "-1: 3-2-128: 0.804 (0.863)\n",
      "-2: 2-2-16: 0.863 (0.902)\n",
      "-2: 2-3-16: 0.843 (0.922)\n",
      "-2: 2-3-32: 0.863 (0.941)\n",
      "-2: 2-4-16: 0.843 (0.922)\n",
      "-2: 3-2-16: 0.863 (0.902)\n",
      "-2: 4-2-16: 0.824 (0.922)\n",
      "-2: 3-3-16: 0.824 (0.902)\n",
      "-2: 4-3-16: 0.843 (0.882)\n",
      "-2: 2-2-32: 0.804 (0.941)\n",
      "-2: 2-2-50: 0.784 (0.922)\n",
      "-2: 2-2-75: 0.824 (0.902)\n",
      "-2: 2-2-100: 0.824 (0.922)\n",
      "-2: 2-3-50: 0.824 (0.922)\n",
      "-2: 2-3-75: 0.863 (0.922)\n",
      "-2: 2-3-100: 0.765 (0.882)\n",
      "-2: 3-2-50: 0.804 (0.941)\n",
      "-2: 3-2-64: 0.804 (0.941)\n",
      "-2: 3-2-128: 0.902 (0.941)\n",
      "-3: 2-2-16: 0.882 (0.922)\n",
      "-3: 2-3-16: 0.902 (0.902)\n",
      "-3: 2-3-32: 0.843 (0.922)\n",
      "-3: 2-4-16: 0.804 (0.902)\n",
      "-3: 3-2-16: 0.863 (0.941)\n",
      "-3: 4-2-16: 0.745 (0.843)\n",
      "-3: 3-3-16: 0.863 (0.922)\n",
      "-3: 4-3-16: 0.843 (0.863)\n",
      "-3: 2-2-32: 0.843 (0.922)\n",
      "-3: 2-2-50: 0.863 (0.922)\n",
      "-3: 2-2-75: 0.863 (0.922)\n",
      "-3: 2-2-100: 0.863 (0.922)\n",
      "-3: 2-3-50: 0.882 (0.922)\n",
      "-3: 2-3-75: 0.882 (0.922)\n",
      "-3: 2-3-100: 0.824 (0.902)\n",
      "-3: 3-2-50: 0.882 (0.941)\n",
      "-3: 3-2-64: 0.882 (0.941)\n",
      "-3: 3-2-128: 0.824 (0.941)\n",
      "-4: 2-2-16: 0.863 (0.941)\n",
      "-4: 2-3-16: 0.882 (0.941)\n",
      "-4: 2-3-32: 0.922 (0.961)\n",
      "-4: 2-4-16: 0.824 (0.922)\n",
      "-4: 3-2-16: 0.863 (0.941)\n",
      "-4: 4-2-16: 0.804 (0.863)\n",
      "-4: 3-3-16: 0.882 (0.902)\n",
      "-4: 4-3-16: 0.843 (0.882)\n",
      "-4: 2-2-32: 0.922 (0.941)\n",
      "-4: 2-2-50: 0.882 (0.980)\n",
      "-4: 2-2-75: 0.922 (0.961)\n",
      "-4: 2-2-100: 0.882 (0.941)\n",
      "-4: 2-3-50: 0.882 (0.961)\n",
      "-4: 2-3-75: 0.843 (0.961)\n",
      "-4: 2-3-100: 0.922 (0.941)\n",
      "-4: 3-2-50: 0.922 (0.961)\n",
      "-4: 3-2-64: 0.961 (0.961)\n",
      "-4: 3-2-128: 0.902 (0.941)\n",
      "-5: 2-2-16: 0.863 (0.922)\n",
      "-5: 2-3-16: 0.863 (0.902)\n",
      "-5: 2-3-32: 0.843 (0.922)\n",
      "-5: 2-4-16: 0.824 (0.902)\n",
      "-5: 3-2-16: 0.745 (0.882)\n",
      "-5: 4-2-16: 0.804 (0.902)\n",
      "-5: 3-3-16: 0.765 (0.882)\n",
      "-5: 4-3-16: 0.725 (0.902)\n",
      "-5: 2-2-32: 0.863 (0.922)\n",
      "-5: 2-2-50: 0.863 (0.941)\n",
      "-5: 2-2-75: 0.882 (0.922)\n",
      "-5: 2-2-100: 0.804 (0.922)\n",
      "-5: 2-3-50: 0.784 (0.922)\n",
      "-5: 2-3-75: 0.824 (0.922)\n",
      "-5: 2-3-100: 0.824 (0.922)\n",
      "-5: 3-2-50: 0.882 (0.922)\n",
      "-5: 3-2-64: 0.882 (0.941)\n",
      "-5: 3-2-128: 0.863 (0.941)\n",
      "-6: 2-2-16: 0.843 (0.922)\n",
      "-6: 2-3-16: 0.843 (0.863)\n",
      "-6: 2-3-32: 0.863 (0.902)\n",
      "-6: 2-4-16: 0.882 (0.902)\n",
      "-6: 3-2-16: 0.863 (0.882)\n",
      "-6: 4-2-16: 0.882 (0.882)\n",
      "-6: 3-3-16: 0.824 (0.863)\n",
      "-6: 4-3-16: 0.824 (0.843)\n",
      "-6: 2-2-32: 0.843 (0.902)\n",
      "-6: 2-2-50: 0.843 (0.922)\n",
      "-6: 2-2-75: 0.843 (0.902)\n",
      "-6: 2-2-100: 0.843 (0.882)\n",
      "-6: 2-3-50: 0.882 (0.902)\n",
      "-6: 2-3-75: 0.843 (0.882)\n",
      "-6: 2-3-100: 0.843 (0.882)\n",
      "-6: 3-2-50: 0.843 (0.922)\n",
      "-6: 3-2-64: 0.863 (0.922)\n",
      "-6: 3-2-128: 0.863 (0.922)\n",
      "-7: 2-2-16: 0.863 (0.902)\n",
      "-7: 2-3-16: 0.882 (0.902)\n",
      "-7: 2-3-32: 0.902 (0.902)\n",
      "-7: 2-4-16: 0.882 (0.902)\n",
      "-7: 3-2-16: 0.882 (0.922)\n",
      "-7: 4-2-16: 0.882 (0.882)\n",
      "-7: 3-3-16: 0.843 (0.941)\n",
      "-7: 4-3-16: 0.843 (0.902)\n",
      "-7: 2-2-32: 0.824 (0.922)\n",
      "-7: 2-2-50: 0.863 (0.902)\n",
      "-7: 2-2-75: 0.843 (0.902)\n",
      "-7: 2-2-100: 0.843 (0.882)\n",
      "-7: 2-3-50: 0.843 (0.902)\n",
      "-7: 2-3-75: 0.882 (0.882)\n",
      "-7: 2-3-100: 0.843 (0.882)\n",
      "-7: 3-2-50: 0.902 (0.922)\n",
      "-7: 3-2-64: 0.882 (0.902)\n",
      "-7: 3-2-128: 0.882 (0.902)\n",
      "-8: 2-2-16: 0.843 (0.902)\n",
      "-8: 2-3-16: 0.824 (0.902)\n",
      "-8: 2-3-32: 0.902 (0.922)\n",
      "-8: 2-4-16: 0.882 (0.902)\n",
      "-8: 3-2-16: 0.824 (0.882)\n",
      "-8: 4-2-16: 0.824 (0.882)\n",
      "-8: 3-3-16: 0.824 (0.863)\n",
      "-8: 4-3-16: 0.765 (0.824)\n",
      "-8: 2-2-32: 0.843 (0.902)\n",
      "-8: 2-2-50: 0.863 (0.882)\n",
      "-8: 2-2-75: 0.784 (0.863)\n",
      "-8: 2-2-100: 0.784 (0.902)\n",
      "-8: 2-3-50: 0.843 (0.902)\n",
      "-8: 2-3-75: 0.824 (0.863)\n",
      "-8: 2-3-100: 0.765 (0.902)\n",
      "-8: 3-2-50: 0.843 (0.922)\n",
      "-8: 3-2-64: 0.843 (0.922)\n",
      "-8: 3-2-128: 0.804 (0.922)\n",
      "-9: 2-2-16: 0.863 (0.902)\n",
      "-9: 2-3-16: 0.824 (0.902)\n",
      "-9: 2-3-32: 0.843 (0.882)\n",
      "-9: 2-4-16: 0.784 (0.882)\n",
      "-9: 3-2-16: 0.843 (0.882)\n",
      "-9: 4-2-16: 0.804 (0.902)\n",
      "-9: 3-3-16: 0.824 (0.843)\n",
      "-9: 4-3-16: 0.784 (0.863)\n",
      "-9: 2-2-32: 0.843 (0.882)\n",
      "-9: 2-2-50: 0.824 (0.882)\n",
      "-9: 2-2-75: 0.843 (0.863)\n",
      "-9: 2-2-100: 0.843 (0.863)\n",
      "-9: 2-3-50: 0.863 (0.882)\n",
      "-9: 2-3-75: 0.843 (0.882)\n",
      "-9: 2-3-100: 0.784 (0.863)\n",
      "-9: 3-2-50: 0.902 (0.902)\n",
      "-9: 3-2-64: 0.863 (0.882)\n",
      "-9: 3-2-128: 0.824 (0.882)\n",
      "-10: 2-2-16: 0.961 (0.961)\n",
      "-10: 2-3-16: 0.882 (0.882)\n",
      "-10: 2-3-32: 0.843 (0.902)\n",
      "-10: 2-4-16: 0.863 (0.882)\n",
      "-10: 3-2-16: 0.882 (0.922)\n",
      "-10: 4-2-16: 0.882 (0.941)\n",
      "-10: 3-3-16: 0.843 (0.882)\n",
      "-10: 4-3-16: 0.843 (0.902)\n",
      "-10: 2-2-32: 0.882 (0.941)\n",
      "-10: 2-2-50: 0.902 (0.941)\n",
      "-10: 2-2-75: 0.882 (0.941)\n",
      "-10: 2-2-100: 0.902 (0.941)\n",
      "-10: 2-3-50: 0.863 (0.902)\n",
      "-10: 2-3-75: 0.882 (0.882)\n",
      "-10: 2-3-100: 0.863 (0.902)\n",
      "-10: 3-2-50: 0.902 (0.941)\n",
      "-10: 3-2-64: 0.902 (0.941)\n",
      "-10: 3-2-128: 0.922 (0.961)\n",
      "----- 28.23 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'L': 2, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 50},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 75},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 100},\n",
    "\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 50},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 75},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 100},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 50},\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 128},\n",
    "        ]\n",
    "\n",
    "best_accs7 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs7 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, exp['hid_dim'], OUT_DIM, exp['L'], exp['K'], act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  exp['K'], masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs7[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs7[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}: {best_val_accs7[j,i]:.3f} ({best_accs7[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}' for exp in EXPS]\n",
    "table_over7 = summary_table(best_accs7, index_name)\n",
    "table7 = summary_table(best_val_accs7, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.045098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.034244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.046442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-16</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.040184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-16</th>\n",
       "      <td>0.817647</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.051915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-16</th>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.030440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-16</th>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.047222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.031677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-50</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.044713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-75</th>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.040942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-100</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.044063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-50</th>\n",
       "      <td>0.839216</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.047869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-75</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.026380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-100</th>\n",
       "      <td>0.825490</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.045098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-50</th>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.035349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-64</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.043888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-128</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.040942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean accs       med       std\n",
       "2-2-16    0.860784  0.862745  0.045098\n",
       "2-3-16    0.852941  0.852941  0.034244\n",
       "2-3-32    0.856863  0.852941  0.046442\n",
       "2-4-16    0.841176  0.833333  0.033333\n",
       "3-2-16    0.843137  0.862745  0.040184\n",
       "4-2-16    0.817647  0.813725  0.051915\n",
       "3-3-16    0.829412  0.823529  0.030440\n",
       "4-3-16    0.803922  0.833333  0.047222\n",
       "2-2-32    0.849020  0.843137  0.031677\n",
       "2-2-50    0.843137  0.862745  0.044713\n",
       "2-2-75    0.847059  0.843137  0.040942\n",
       "2-2-100   0.833333  0.843137  0.044063\n",
       "2-3-50    0.839216  0.852941  0.047869\n",
       "2-3-75    0.849020  0.843137  0.026380\n",
       "2-3-100   0.825490  0.823529  0.045098\n",
       "3-2-50    0.872549  0.882353  0.035349\n",
       "3-2-64    0.868627  0.872549  0.043888\n",
       "3-2-128   0.858824  0.862745  0.040942"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSO, bias, and batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1: orig-True-False: 0.804 (0.882)\n",
      "- 1: trans-True-False: 0.784 (0.863)\n",
      "- 1: sym-True-False: 0.529 (0.529)\n",
      "- 1: orig-True-True: 0.784 (0.843)\n",
      "- 1: trans-True-True: 0.784 (0.882)\n",
      "- 1: sym-True-True: 0.863 (0.882)\n",
      "- 1: orig-False-False: 0.824 (0.863)\n",
      "- 1: sym-False-False: 0.863 (0.863)\n",
      "- 2: orig-True-False: 0.843 (0.941)\n",
      "- 2: trans-True-False: 0.843 (0.882)\n",
      "- 2: sym-True-False: 0.627 (0.627)\n",
      "- 2: orig-True-True: 0.804 (0.843)\n",
      "- 2: trans-True-True: 0.765 (0.804)\n",
      "- 2: sym-True-True: 0.745 (0.863)\n",
      "- 2: orig-False-False: 0.863 (0.922)\n",
      "- 2: sym-False-False: 0.627 (0.627)\n",
      "- 3: orig-True-False: 0.882 (0.922)\n",
      "- 3: trans-True-False: 0.843 (0.882)\n",
      "- 3: sym-True-False: 0.804 (0.882)\n",
      "- 3: orig-True-True: 0.804 (0.863)\n",
      "- 3: trans-True-True: 0.765 (0.863)\n",
      "- 3: sym-True-True: 0.843 (0.922)\n",
      "- 3: orig-False-False: 0.902 (0.922)\n",
      "- 3: sym-False-False: 0.412 (0.412)\n",
      "- 4: orig-True-False: 0.941 (0.961)\n",
      "- 4: trans-True-False: 0.902 (0.961)\n",
      "- 4: sym-True-False: 0.882 (0.902)\n",
      "- 4: orig-True-True: 0.843 (0.902)\n",
      "- 4: trans-True-True: 0.941 (0.941)\n",
      "- 4: sym-True-True: 0.941 (0.961)\n",
      "- 4: orig-False-False: 0.941 (0.961)\n",
      "- 4: sym-False-False: 0.431 (0.431)\n",
      "- 5: orig-True-False: 0.902 (0.941)\n",
      "- 5: trans-True-False: 0.882 (0.882)\n",
      "- 5: sym-True-False: 0.843 (0.902)\n",
      "- 5: orig-True-True: 0.882 (0.922)\n",
      "- 5: trans-True-True: 0.824 (0.902)\n",
      "- 5: sym-True-True: 0.882 (0.941)\n",
      "- 5: orig-False-False: 0.882 (0.922)\n",
      "- 5: sym-False-False: 0.392 (0.392)\n",
      "- 6: orig-True-False: 0.902 (0.902)\n",
      "- 6: trans-True-False: 0.882 (0.902)\n",
      "- 6: sym-True-False: 0.510 (0.529)\n",
      "- 6: orig-True-True: 0.824 (0.843)\n",
      "- 6: trans-True-True: 0.863 (0.863)\n",
      "- 6: sym-True-True: 0.824 (0.882)\n",
      "- 6: orig-False-False: 0.843 (0.902)\n",
      "- 6: sym-False-False: 0.804 (0.843)\n",
      "- 7: orig-True-False: 0.882 (0.941)\n",
      "- 7: trans-True-False: 0.824 (0.882)\n",
      "- 7: sym-True-False: 0.471 (0.471)\n",
      "- 7: orig-True-True: 0.843 (0.882)\n",
      "- 7: trans-True-True: 0.843 (0.863)\n",
      "- 7: sym-True-True: 0.843 (0.882)\n",
      "- 7: orig-False-False: 0.882 (0.902)\n",
      "- 7: sym-False-False: 0.471 (0.471)\n",
      "- 8: orig-True-False: 0.882 (0.902)\n",
      "- 8: trans-True-False: 0.765 (0.882)\n",
      "- 8: sym-True-False: 0.510 (0.510)\n",
      "- 8: orig-True-True: 0.784 (0.882)\n",
      "- 8: trans-True-True: 0.843 (0.882)\n",
      "- 8: sym-True-True: 0.882 (0.882)\n",
      "- 8: orig-False-False: 0.843 (0.902)\n",
      "- 8: sym-False-False: 0.765 (0.804)\n",
      "- 9: orig-True-False: 0.863 (0.902)\n",
      "- 9: trans-True-False: 0.804 (0.843)\n",
      "- 9: sym-True-False: 0.451 (0.451)\n",
      "- 9: orig-True-True: 0.843 (0.902)\n",
      "- 9: trans-True-True: 0.824 (0.882)\n",
      "- 9: sym-True-True: 0.824 (0.863)\n",
      "- 9: orig-False-False: 0.843 (0.882)\n",
      "- 9: sym-False-False: 0.451 (0.451)\n",
      "- 10: orig-True-False: 0.902 (0.922)\n",
      "- 10: trans-True-False: 0.824 (0.882)\n",
      "- 10: sym-True-False: 0.451 (0.451)\n",
      "- 10: orig-True-True: 0.863 (0.922)\n",
      "- 10: trans-True-True: 0.843 (0.902)\n",
      "- 10: sym-True-True: 0.843 (0.882)\n",
      "- 10: orig-False-False: 0.882 (0.902)\n",
      "- 10: sym-False-False: 0.451 (0.451)\n",
      "----- 26.17 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [{'GSO': 'orig', 'bias': True, 'bn': False},\n",
    "        {'GSO': 'trans', 'bias': True, 'bn': False},\n",
    "        {'GSO': 'sym', 'bias': True, 'bn': False},\n",
    "\n",
    "        {'GSO': 'orig', 'bias': True, 'bn': True},\n",
    "        {'GSO': 'trans', 'bias': True, 'bn': True},\n",
    "        {'GSO': 'sym', 'bias': True, 'bn': True},\n",
    "\n",
    "        {'GSO': 'orig', 'bias': False, 'bn': False},\n",
    "        {'GSO': 'sym', 'bias': False, 'bn': False},\n",
    "        ]\n",
    "\n",
    "best_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0, bias=False, batch_norm=exp['bn'])\n",
    "        \n",
    "        if exp['GSO'] == 'sym':\n",
    "            A_aux = (A + A.T)/2\n",
    "            A_aux = np.where(A_aux > 0, 1, 0)\n",
    "        elif exp['GSO'] == 'trans':\n",
    "            A_aux = A.T\n",
    "        else:\n",
    "            A_aux = A\n",
    "\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A_aux, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A_aux).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  K, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs3b[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs3b[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'- {i+1}: {exp[\"GSO\"]}-{exp[\"bias\"]}-{exp[\"bn\"]}: {best_val_accs3b[j,i]:.3f} ({best_accs3b[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"GSO\"]}-{exp[\"bias\"]}-{exp[\"bn\"]}' for exp in EXPS]\n",
    "table_over7b = summary_table(best_accs3b, index_name)\n",
    "table7b = summary_table(best_val_accs3b, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>orig-True-False</th>\n",
       "      <td>0.880392</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.035565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trans-True-False</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.042237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sym-True-False</th>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.162165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig-True-True</th>\n",
       "      <td>0.827451</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.031373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trans-True-True</th>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.049643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sym-True-True</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.048069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig-False-False</th>\n",
       "      <td>0.870588</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.033044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sym-False-False</th>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.460784</td>\n",
       "      <td>0.171935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean accs       med       std\n",
       "orig-True-False    0.880392  0.882353  0.035565\n",
       "trans-True-False   0.835294  0.833333  0.042237\n",
       "sym-True-False     0.607843  0.519608  0.162165\n",
       "orig-True-True     0.827451  0.833333  0.031373\n",
       "trans-True-True    0.829412  0.833333  0.049643\n",
       "sym-True-True      0.849020  0.843137  0.048069\n",
       "orig-False-False   0.870588  0.872549  0.033044\n",
       "sym-False-False    0.566667  0.460784  0.171935"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearities and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.863)\n",
      "-1: ReLU()-Softmax(dim=1)-NLLLoss(): 0.804 (0.882)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.647 (0.706)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.667 (0.725)\n",
      "-1: ReLU()-Identity()-CrossEntropyLoss(): 0.627 (0.647)\n",
      "-1: ReLU()-Identity()-NLLLoss(): 0.275 (0.294)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.843)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.784 (0.843)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.863)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.784 (0.863)\n",
      "-1: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.843)\n",
      "-1: Identity()-Softmax(dim=1)-NLLLoss(): 0.804 (0.863)\n",
      "-1: Identity()-Identity()-CrossEntropyLoss(): 0.686 (0.725)\n",
      "-2: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-2: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.784 (0.784)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.745 (0.765)\n",
      "-2: ReLU()-Identity()-CrossEntropyLoss(): 0.608 (0.824)\n",
      "-2: ReLU()-Identity()-NLLLoss(): 0.333 (0.392)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.843 (0.941)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.902)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.804 (0.902)\n",
      "-2: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-2: Identity()-Softmax(dim=1)-NLLLoss(): 0.824 (0.922)\n",
      "-2: Identity()-Identity()-CrossEntropyLoss(): 0.745 (0.765)\n",
      "-3: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.941)\n",
      "-3: ReLU()-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.549 (0.569)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.588 (0.588)\n",
      "-3: ReLU()-Identity()-CrossEntropyLoss(): 0.627 (0.667)\n",
      "-3: ReLU()-Identity()-NLLLoss(): 0.510 (0.510)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.922)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.922 (0.922)\n",
      "-3: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-3: Identity()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-3: Identity()-Identity()-CrossEntropyLoss(): 0.725 (0.725)\n",
      "-4: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.941)\n",
      "-4: ReLU()-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.608 (0.627)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.608 (0.647)\n",
      "-4: ReLU()-Identity()-CrossEntropyLoss(): 0.608 (0.627)\n",
      "-4: ReLU()-Identity()-NLLLoss(): 0.431 (0.431)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.922 (0.961)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.902)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.882 (0.941)\n",
      "-4: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.941 (0.980)\n",
      "-4: Identity()-Softmax(dim=1)-NLLLoss(): 0.922 (0.922)\n",
      "-4: Identity()-Identity()-CrossEntropyLoss(): 0.706 (0.784)\n",
      "-5: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-5: ReLU()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.569 (0.627)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.608 (0.627)\n",
      "-5: ReLU()-Identity()-CrossEntropyLoss(): 0.569 (0.608)\n",
      "-5: ReLU()-Identity()-NLLLoss(): 0.451 (0.471)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.784 (0.902)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-5: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.922)\n",
      "-5: Identity()-Softmax(dim=1)-NLLLoss(): 0.804 (0.922)\n",
      "-5: Identity()-Identity()-CrossEntropyLoss(): 0.608 (0.627)\n",
      "-6: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.843)\n",
      "-6: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.882)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.549 (0.588)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.588 (0.608)\n",
      "-6: ReLU()-Identity()-CrossEntropyLoss(): 0.686 (0.706)\n",
      "-6: ReLU()-Identity()-NLLLoss(): 0.569 (0.569)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.882)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-6: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-6: Identity()-Softmax(dim=1)-NLLLoss(): 0.882 (0.902)\n",
      "-6: Identity()-Identity()-CrossEntropyLoss(): 0.706 (0.765)\n",
      "-7: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-7: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.549 (0.569)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.647 (0.765)\n",
      "-7: ReLU()-Identity()-CrossEntropyLoss(): 0.529 (0.569)\n",
      "-7: ReLU()-Identity()-NLLLoss(): 0.314 (0.314)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.902)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-7: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.902)\n",
      "-7: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.882)\n",
      "-7: Identity()-Identity()-CrossEntropyLoss(): 0.627 (0.647)\n",
      "-8: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.882)\n",
      "-8: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.882)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.510 (0.588)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.529 (0.569)\n",
      "-8: ReLU()-Identity()-CrossEntropyLoss(): 0.549 (0.549)\n",
      "-8: ReLU()-Identity()-NLLLoss(): 0.510 (0.510)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.882)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.882)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.804 (0.882)\n",
      "-8: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-8: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.882)\n",
      "-8: Identity()-Identity()-CrossEntropyLoss(): 0.588 (0.627)\n",
      "-9: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-9: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.882)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.490 (0.549)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.490 (0.529)\n",
      "-9: ReLU()-Identity()-CrossEntropyLoss(): 0.667 (0.667)\n",
      "-9: ReLU()-Identity()-NLLLoss(): 0.471 (0.471)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.902)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.804 (0.882)\n",
      "-9: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-9: Identity()-Softmax(dim=1)-NLLLoss(): 0.863 (0.882)\n",
      "-9: Identity()-Identity()-CrossEntropyLoss(): 0.588 (0.647)\n",
      "-10: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-10: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.882)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.529 (0.549)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.608 (0.627)\n",
      "-10: ReLU()-Identity()-CrossEntropyLoss(): 0.686 (0.725)\n",
      "-10: ReLU()-Identity()-NLLLoss(): 0.549 (0.627)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.882)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.902)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-10: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.882)\n",
      "-10: Identity()-Softmax(dim=1)-NLLLoss(): 0.882 (0.902)\n",
      "-10: Identity()-Identity()-CrossEntropyLoss(): 0.706 (0.706)\n",
      "----- 17.36 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [{'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},]\n",
    "\n",
    "best_accs8 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs8 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=exp['act'], last_act=exp['lact'],\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  K, masks, exp['loss'], device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs8[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs8[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}: {best_val_accs8[j,i]:.3f} ({best_accs8[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}' for exp in EXPS]\n",
    "table_over8 = summary_table(best_accs8, index_name)\n",
    "table8 = summary_table(best_val_accs8, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.043003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.028347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.080964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.066782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.615686</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.052025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Identity()-NLLLoss()</th>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.460784</td>\n",
       "      <td>0.096955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.031677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.839216</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.038021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.023934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.045395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.034018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.036367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.668627</td>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.056456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean accs       med  \\\n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()             0.856863  0.872549   \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                      0.860784  0.862745   \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()          0.578431  0.549020   \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                   0.607843  0.607843   \n",
       "ReLU()-Identity()-CrossEntropyLoss()                 0.615686  0.617647   \n",
       "ReLU()-Identity()-NLLLoss()                          0.441176  0.460784   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()     0.856863  0.852941   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()              0.839216  0.833333   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...   0.841176  0.833333   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...   0.847059  0.843137   \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()         0.868627  0.843137   \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                  0.854902  0.852941   \n",
       "Identity()-Identity()-CrossEntropyLoss()             0.668627  0.696078   \n",
       "\n",
       "                                                         std  \n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()            0.043003  \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                     0.028347  \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()         0.080964  \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                  0.066782  \n",
       "ReLU()-Identity()-CrossEntropyLoss()                0.052025  \n",
       "ReLU()-Identity()-NLLLoss()                         0.096955  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()    0.031677  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()             0.038021  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...  0.023934  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...  0.045395  \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()        0.034018  \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                 0.036367  \n",
       "Identity()-Identity()-CrossEntropyLoss()            0.056456  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPS = [\n",
    "        {'name': 'Kipf', 'norm': 'none'},\n",
    "        {'name': 'Kipf', 'norm': 'both'},\n",
    "\n",
    "        {'name': 'A-GCNN', 'norm': False},\n",
    "        {'name': 'A-GCNN', 'norm': True},\n",
    "\n",
    "        {'name': 'H-GCNN', 'norm': False}, # This should be the same as A-GCNN not norm\n",
    "        {'name': 'H-GCNN', 'norm': True},\n",
    "\n",
    "        {'name': 'W-GCN-A', 'norm': False},\n",
    "        {'name': 'W-GCN-A', 'norm': True},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- RUN: 1\n",
      "\tKipf-none: acc = 0.549  -  acc2 = 0.549  -  acc (over) = 0.608\n",
      "\tKipf-both: acc = 0.588  -  acc2 = 0.588  -  acc (over) = 0.608\n",
      "\tA-GCNN-False: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tA-GCNN-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.863\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.804  -  acc (over) = 0.863\n",
      "\tH-GCNN-True: acc = 0.804  -  acc2 = 0.804  -  acc (over) = 0.843\n",
      "\tW-GCN-A-False: acc = 0.725  -  acc2 = 0.725  -  acc (over) = 0.725\n",
      "\tW-GCN-A-True: acc = 0.725  -  acc2 = 0.784  -  acc (over) = 0.824\n",
      "- RUN: 2\n",
      "\tKipf-none: acc = 0.588  -  acc2 = 0.588  -  acc (over) = 0.725\n",
      "\tKipf-both: acc = 0.647  -  acc2 = 0.686  -  acc (over) = 0.686\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.863  -  acc (over) = 0.882\n",
      "\tA-GCNN-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.922\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.863  -  acc2 = 0.902  -  acc (over) = 0.922\n",
      "\tW-GCN-A-False: acc = 0.804  -  acc2 = 0.784  -  acc (over) = 0.804\n",
      "\tW-GCN-A-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.902\n",
      "- RUN: 3\n",
      "\tKipf-none: acc = 0.490  -  acc2 = 0.490  -  acc (over) = 0.588\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.569  -  acc (over) = 0.647\n",
      "\tA-GCNN-False: acc = 0.863  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.941\n",
      "\tH-GCNN-False: acc = 0.745  -  acc2 = 0.765  -  acc (over) = 0.843\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.902  -  acc (over) = 0.922\n",
      "\tW-GCN-A-False: acc = 0.647  -  acc2 = 0.647  -  acc (over) = 0.686\n",
      "\tW-GCN-A-True: acc = 0.824  -  acc2 = 0.843  -  acc (over) = 0.863\n",
      "- RUN: 4\n",
      "\tKipf-none: acc = 0.529  -  acc2 = 0.529  -  acc (over) = 0.647\n",
      "\tKipf-both: acc = 0.529  -  acc2 = 0.529  -  acc (over) = 0.569\n",
      "\tA-GCNN-False: acc = 0.941  -  acc2 = 0.922  -  acc (over) = 0.961\n",
      "\tA-GCNN-True: acc = 0.804  -  acc2 = 0.804  -  acc (over) = 0.922\n",
      "\tH-GCNN-False: acc = 0.941  -  acc2 = 0.843  -  acc (over) = 0.941\n",
      "\tH-GCNN-True: acc = 0.902  -  acc2 = 0.902  -  acc (over) = 0.961\n",
      "\tW-GCN-A-False: acc = 0.667  -  acc2 = 0.647  -  acc (over) = 0.745\n",
      "\tW-GCN-A-True: acc = 0.765  -  acc2 = 0.824  -  acc (over) = 0.882\n",
      "- RUN: 5\n",
      "\tKipf-none: acc = 0.373  -  acc2 = 0.392  -  acc (over) = 0.392\n",
      "\tKipf-both: acc = 0.392  -  acc2 = 0.490  -  acc (over) = 0.569\n",
      "\tA-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.922\n",
      "\tA-GCNN-True: acc = 0.922  -  acc2 = 0.843  -  acc (over) = 0.941\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tH-GCNN-True: acc = 0.765  -  acc2 = 0.804  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.608  -  acc2 = 0.647  -  acc (over) = 0.706\n",
      "\tW-GCN-A-True: acc = 0.843  -  acc2 = 0.804  -  acc (over) = 0.843\n",
      "- RUN: 6\n",
      "\tKipf-none: acc = 0.471  -  acc2 = 0.451  -  acc (over) = 0.529\n",
      "\tKipf-both: acc = 0.549  -  acc2 = 0.549  -  acc (over) = 0.569\n",
      "\tA-GCNN-False: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.804\n",
      "\tA-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.863\n",
      "\tH-GCNN-True: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.843\n",
      "\tW-GCN-A-False: acc = 0.686  -  acc2 = 0.706  -  acc (over) = 0.725\n",
      "\tW-GCN-A-True: acc = 0.745  -  acc2 = 0.745  -  acc (over) = 0.784\n",
      "- RUN: 7\n",
      "\tKipf-none: acc = 0.471  -  acc2 = 0.471  -  acc (over) = 0.471\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.549  -  acc (over) = 0.627\n",
      "\tA-GCNN-False: acc = 0.843  -  acc2 = 0.784  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.843  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.882\n",
      "\tW-GCN-A-False: acc = 0.725  -  acc2 = 0.745  -  acc (over) = 0.804\n",
      "\tW-GCN-A-True: acc = 0.824  -  acc2 = 0.804  -  acc (over) = 0.843\n",
      "- RUN: 8\n",
      "\tKipf-none: acc = 0.510  -  acc2 = 0.510  -  acc (over) = 0.510\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.529  -  acc (over) = 0.627\n",
      "\tA-GCNN-False: acc = 0.804  -  acc2 = 0.824  -  acc (over) = 0.843\n",
      "\tA-GCNN-True: acc = 0.824  -  acc2 = 0.804  -  acc (over) = 0.882\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.902  -  acc (over) = 0.941\n",
      "\tH-GCNN-True: acc = 0.745  -  acc2 = 0.765  -  acc (over) = 0.784\n",
      "\tW-GCN-A-False: acc = 0.627  -  acc2 = 0.627  -  acc (over) = 0.706\n",
      "\tW-GCN-A-True: acc = 0.765  -  acc2 = 0.765  -  acc (over) = 0.804\n",
      "- RUN: 9\n",
      "\tKipf-none: acc = 0.569  -  acc2 = 0.569  -  acc (over) = 0.588\n",
      "\tKipf-both: acc = 0.490  -  acc2 = 0.490  -  acc (over) = 0.588\n",
      "\tA-GCNN-False: acc = 0.784  -  acc2 = 0.804  -  acc (over) = 0.882\n",
      "\tA-GCNN-True: acc = 0.863  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tH-GCNN-True: acc = 0.804  -  acc2 = 0.765  -  acc (over) = 0.824\n",
      "\tW-GCN-A-False: acc = 0.686  -  acc2 = 0.627  -  acc (over) = 0.706\n",
      "\tW-GCN-A-True: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.784\n",
      "- RUN: 10\n",
      "\tKipf-none: acc = 0.510  -  acc2 = 0.471  -  acc (over) = 0.569\n",
      "\tKipf-both: acc = 0.529  -  acc2 = 0.529  -  acc (over) = 0.549\n",
      "\tA-GCNN-False: acc = 0.784  -  acc2 = 0.804  -  acc (over) = 0.863\n",
      "\tA-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tH-GCNN-False: acc = 0.824  -  acc2 = 0.804  -  acc (over) = 0.863\n",
      "\tH-GCNN-True: acc = 0.784  -  acc2 = 0.765  -  acc (over) = 0.804\n",
      "\tW-GCN-A-False: acc = 0.765  -  acc2 = 0.725  -  acc (over) = 0.784\n",
      "\tW-GCN-A-True: acc = 0.765  -  acc2 = 0.745  -  acc (over) = 0.784\n",
      "- RUN: 11\n",
      "\tKipf-none: acc = 0.549  -  acc2 = 0.549  -  acc (over) = 0.608\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.529  -  acc (over) = 0.647\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.843\n",
      "\tA-GCNN-True: acc = 0.824  -  acc2 = 0.804  -  acc (over) = 0.882\n",
      "\tH-GCNN-False: acc = 0.765  -  acc2 = 0.745  -  acc (over) = 0.804\n",
      "\tH-GCNN-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.843\n",
      "\tW-GCN-A-False: acc = 0.667  -  acc2 = 0.647  -  acc (over) = 0.667\n",
      "\tW-GCN-A-True: acc = 0.784  -  acc2 = 0.843  -  acc (over) = 0.843\n",
      "- RUN: 12\n",
      "\tKipf-none: acc = 0.667  -  acc2 = 0.686  -  acc (over) = 0.765\n",
      "\tKipf-both: acc = 0.588  -  acc2 = 0.647  -  acc (over) = 0.686\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.863  -  acc2 = 0.824  -  acc (over) = 0.902\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.843  -  acc (over) = 0.922\n",
      "\tW-GCN-A-False: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.804\n",
      "\tW-GCN-A-True: acc = 0.843  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "- RUN: 13\n",
      "\tKipf-none: acc = 0.412  -  acc2 = 0.412  -  acc (over) = 0.412\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.569  -  acc (over) = 0.647\n",
      "\tA-GCNN-False: acc = 0.882  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.941\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.627  -  acc2 = 0.627  -  acc (over) = 0.667\n",
      "\tW-GCN-A-True: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.863\n",
      "- RUN: 14\n",
      "\tKipf-none: acc = 0.588  -  acc2 = 0.588  -  acc (over) = 0.608\n",
      "\tKipf-both: acc = 0.510  -  acc2 = 0.510  -  acc (over) = 0.569\n",
      "\tA-GCNN-False: acc = 0.922  -  acc2 = 0.922  -  acc (over) = 0.961\n",
      "\tA-GCNN-True: acc = 0.922  -  acc2 = 0.922  -  acc (over) = 0.941\n",
      "\tH-GCNN-False: acc = 0.902  -  acc2 = 0.902  -  acc (over) = 0.941\n",
      "\tH-GCNN-True: acc = 0.863  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.725  -  acc2 = 0.725  -  acc (over) = 0.765\n",
      "\tW-GCN-A-True: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.863\n",
      "- RUN: 15\n",
      "\tKipf-none: acc = 0.333  -  acc2 = 0.333  -  acc (over) = 0.392\n",
      "\tKipf-both: acc = 0.588  -  acc2 = 0.588  -  acc (over) = 0.667\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.882\n",
      "\tA-GCNN-True: acc = 0.863  -  acc2 = 0.843  -  acc (over) = 0.922\n",
      "\tH-GCNN-False: acc = 0.804  -  acc2 = 0.804  -  acc (over) = 0.882\n",
      "\tH-GCNN-True: acc = 0.843  -  acc2 = 0.804  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.608  -  acc2 = 0.608  -  acc (over) = 0.686\n",
      "\tW-GCN-A-True: acc = 0.804  -  acc2 = 0.725  -  acc (over) = 0.863\n",
      "- RUN: 16\n",
      "\tKipf-none: acc = 0.608  -  acc2 = 0.608  -  acc (over) = 0.667\n",
      "\tKipf-both: acc = 0.549  -  acc2 = 0.588  -  acc (over) = 0.588\n",
      "\tA-GCNN-False: acc = 0.804  -  acc2 = 0.784  -  acc (over) = 0.863\n",
      "\tA-GCNN-True: acc = 0.843  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tH-GCNN-False: acc = 0.804  -  acc2 = 0.804  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.824  -  acc2 = 0.804  -  acc (over) = 0.863\n",
      "\tW-GCN-A-False: acc = 0.667  -  acc2 = 0.667  -  acc (over) = 0.745\n",
      "\tW-GCN-A-True: acc = 0.784  -  acc2 = 0.765  -  acc (over) = 0.804\n",
      "- RUN: 17\n",
      "\tKipf-none: acc = 0.627  -  acc2 = 0.627  -  acc (over) = 0.647\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.549  -  acc (over) = 0.667\n",
      "\tA-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.863  -  acc2 = 0.843  -  acc (over) = 0.922\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.882\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.765  -  acc2 = 0.765  -  acc (over) = 0.765\n",
      "\tW-GCN-A-True: acc = 0.804  -  acc2 = 0.784  -  acc (over) = 0.824\n",
      "- RUN: 18\n",
      "\tKipf-none: acc = 0.686  -  acc2 = 0.686  -  acc (over) = 0.725\n",
      "\tKipf-both: acc = 0.647  -  acc2 = 0.667  -  acc (over) = 0.686\n",
      "\tA-GCNN-False: acc = 0.902  -  acc2 = 0.902  -  acc (over) = 0.922\n",
      "\tA-GCNN-True: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tH-GCNN-False: acc = 0.804  -  acc2 = 0.804  -  acc (over) = 0.863\n",
      "\tH-GCNN-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.863\n",
      "\tW-GCN-A-False: acc = 0.588  -  acc2 = 0.588  -  acc (over) = 0.725\n",
      "\tW-GCN-A-True: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.824\n",
      "- RUN: 19\n",
      "\tKipf-none: acc = 0.471  -  acc2 = 0.471  -  acc (over) = 0.471\n",
      "\tKipf-both: acc = 0.451  -  acc2 = 0.451  -  acc (over) = 0.451\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.882\n",
      "\tA-GCNN-True: acc = 0.824  -  acc2 = 0.843  -  acc (over) = 0.863\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.627  -  acc2 = 0.647  -  acc (over) = 0.686\n",
      "\tW-GCN-A-True: acc = 0.765  -  acc2 = 0.765  -  acc (over) = 0.784\n",
      "- RUN: 20\n",
      "\tKipf-none: acc = 0.314  -  acc2 = 0.314  -  acc (over) = 0.314\n",
      "\tKipf-both: acc = 0.510  -  acc2 = 0.529  -  acc (over) = 0.549\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.882\n",
      "\tA-GCNN-True: acc = 0.863  -  acc2 = 0.902  -  acc (over) = 0.902\n",
      "\tH-GCNN-False: acc = 0.824  -  acc2 = 0.804  -  acc (over) = 0.843\n",
      "\tH-GCNN-True: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.843\n",
      "\tW-GCN-A-False: acc = 0.725  -  acc2 = 0.725  -  acc (over) = 0.765\n",
      "\tW-GCN-A-True: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.784\n"
     ]
    }
   ],
   "source": [
    "N_RUNS = 20\n",
    "\n",
    "best_accs = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs2 = np.zeros((len(EXPS), N_RUNS))\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    print(f'- RUN: {i+1}')\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        # t_i = time.time()\n",
    "        if exp['name'] == 'Kipf':\n",
    "            arch = GCNN_2L(IN_DIM, HID_DIM, OUT_DIM, act=ACT, last_act=LAST_ACT,\n",
    "                           dropout=DROPOUT, norm=exp['norm'])\n",
    "            S = dgl.from_networkx(nx.from_numpy_array(A)).add_self_loop().to(device)\n",
    "            \n",
    "        elif exp['name'] == 'A-GCNN':\n",
    "            arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "            dropout=DROPOUT, diff_layer=GFGCNLayer, init_h0=h0)\n",
    "            if exp['norm']:\n",
    "                S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "            else:\n",
    "                S = torch.Tensor(A).to(device)\n",
    "\n",
    "        elif exp['name'] == 'H-GCNN':\n",
    "            arch = GFGCN_Spows(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                               dropout=DROPOUT, norm=exp['norm'], dev=device)\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        elif exp['name'] == 'W-GCN-A':\n",
    "            arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                         dropout=DROPOUT, diff_layer=GFGCN_noh_Layer)\n",
    "            if exp['norm']:\n",
    "                S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "            else:\n",
    "                S = torch.Tensor(A).to(device)  \n",
    "            \n",
    "        else:\n",
    "            raise Exception(f'ERROR: Unknown architecture: {exp[\"name\"]}')\n",
    "\n",
    "        if exp['name'] in ['Kipf', 'W-GCN-A']:\n",
    "            model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "            loss, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "        else:\n",
    "            model = GF_NodeClassModel(arch, S, K, masks, LOSS_FN, device=device)\n",
    "            loss, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                                    epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs[j,i] = model.test(feat, model.S, labels, masks['test'])\n",
    "        best_val_accs2[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'\\t{exp[\"name\"]}-{exp[\"norm\"]}: acc = {best_val_accs[j,i]:.3f}  -  acc2 = {best_val_accs2[j,i]:.3f}  -  acc (over) = {best_accs[j,i]:.3f}')\n",
    "\n",
    "\n",
    "# Print results\n",
    "index_name = [f'{exp[\"name\"]}-{exp[\"norm\"]}' for exp in EXPS]\n",
    "table_comp_over = summary_table(best_accs, index_name)\n",
    "table_comp = summary_table(best_val_accs, index_name)\n",
    "table_comp2 = summary_table(best_val_accs2, index_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kipf-none</th>\n",
       "      <td>0.515686</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.100192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kipf-both</th>\n",
       "      <td>0.560784</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.064557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-False</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.044756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-True</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.031677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-False</th>\n",
       "      <td>0.840196</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.042633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-True</th>\n",
       "      <td>0.834314</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.045342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-False</th>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.062005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-True</th>\n",
       "      <td>0.795098</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.034230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean accs       med       std\n",
       "Kipf-none       0.515686  0.519608  0.100192\n",
       "Kipf-both       0.560784  0.588235  0.064557\n",
       "A-GCNN-False    0.837255  0.823529  0.044756\n",
       "A-GCNN-True     0.856863  0.862745  0.031677\n",
       "H-GCNN-False    0.840196  0.843137  0.042633\n",
       "H-GCNN-True     0.834314  0.833333  0.045342\n",
       "W-GCN-A-False   0.686275  0.676471  0.062005\n",
       "W-GCN-A-True    0.795098  0.784314  0.034230"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kipf-none</th>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.101484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kipf-both</th>\n",
       "      <td>0.556863</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.057635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-False</th>\n",
       "      <td>0.834314</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.041351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-True</th>\n",
       "      <td>0.851961</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.033087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-False</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.041316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-True</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.048626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-False</th>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.656863</td>\n",
       "      <td>0.058913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-True</th>\n",
       "      <td>0.796078</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.037920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean accs       med       std\n",
       "Kipf-none       0.514706  0.519608  0.101484\n",
       "Kipf-both       0.556863  0.549020  0.057635\n",
       "A-GCNN-False    0.834314  0.823529  0.041351\n",
       "A-GCNN-True     0.851961  0.843137  0.033087\n",
       "H-GCNN-False    0.835294  0.843137  0.041316\n",
       "H-GCNN-True     0.833333  0.823529  0.048626\n",
       "W-GCN-A-False   0.683333  0.656863  0.058913\n",
       "W-GCN-A-True    0.796078  0.784314  0.037920"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_comp2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
