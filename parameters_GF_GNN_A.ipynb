{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f78c141a810>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import utils\n",
    "from gsp_utils.baselines_archs import GCNN_2L\n",
    "from gsp_utils.baselines_models import NodeClassModel, GF_NodeClassModel\n",
    "from gsp_utils.data import normalize_gso\n",
    "from src.arch import GFGCN, GFGCNLayer, GFGCN_noh_Layer, GFGCN_Spows\n",
    "\n",
    "SEED = 15\n",
    "# PATH = 'results/diff_filters/'\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTAS:\n",
    "- Se consiguen resultado prácticamente iguales con y sin bias\n",
    "- Clara diferencia usando optimización alterna vs única pasada (la única iba mejor con LR=0.05 y WD=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "def summary_table(acc, index_name):\n",
    "    mean_accs = acc.mean(axis=1)\n",
    "    med_accs = np.median(acc, axis=1)\n",
    "    std_accs = acc.std(axis=1)\n",
    "    return DataFrame(np.vstack((mean_accs, med_accs, std_accs)).T, columns=['mean accs', 'med', 'std'], index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: WisconsinDataset\n",
      "Number of nodes: 251\n",
      "Number of features: 1703\n",
      "Shape of signals: torch.Size([251, 1703])\n",
      "Number of classes: 5\n",
      "Norm of A: 22.69361114501953\n",
      "Max value of A: 1.0\n",
      "Proportion of validation data: 0.32\n",
      "Proportion of test data: 0.20\n",
      "Node homophily: 0.13\n",
      "Edge homophily: 0.20\n"
     ]
    }
   ],
   "source": [
    "# Dataset must be from DGL\n",
    "dataset_name = 'WisconsinDataset'\n",
    "\n",
    "A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device,\n",
    "                                                     verb=True)\n",
    "N = A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without normalizing the GSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PARAMETERS  -  0.874\n",
    "## Training params\n",
    "N_RUNS = 10\n",
    "N_EPOCHS = 200  # 100\n",
    "EPOCHS_h = 25\n",
    "EPOCHS_W = 25\n",
    "LR = .005\n",
    "WD = .001\n",
    "DROPOUT = .25\n",
    "\n",
    "# BEST PARAMETERS\n",
    "## Architecture params\n",
    "N_LAYERS = 2\n",
    "K = 3  # 2\n",
    "HID_DIM = 32  # 32\n",
    "\n",
    "## Model params\n",
    "h0 = 1  # 1\n",
    "NORM = False\n",
    "\n",
    "IN_DIM = feat.shape[1]\n",
    "OUT_DIM = n_class\n",
    "\n",
    "ACT = nn.LeakyReLU() \n",
    "LAST_ACT = nn.Softmax(dim=1)\n",
    "LOSS_FN = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting eval/test acc/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best val acc: 0.850  -  Best test acc: 0.843\n",
      "Test acc at best val: 0.804\n",
      "Val acc: 0.838  -  Test acc: 0.804\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = N_EPOCHS\n",
    "epochs_h = EPOCHS_h\n",
    "epochs_W = EPOCHS_W\n",
    "lr =  .005 #LR\n",
    "wd = .005  #WD\n",
    "drop = DROPOUT\n",
    "L = N_LAYERS\n",
    "K_aux = K\n",
    "hid_dim = HID_DIM\n",
    "h0_aux = 1\n",
    "norm = False\n",
    "act = ACT\n",
    "lact = LAST_ACT\n",
    "loss_fn = LOSS_FN\n",
    "patience = 200\n",
    "bias = True\n",
    "\n",
    "iters_aux = 1\n",
    "\n",
    "err1 = np.zeros(iters_aux)\n",
    "err2 = np.zeros(iters_aux)\n",
    "for i in range(iters_aux):\n",
    "\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "\n",
    "    # Create model\n",
    "    arch = GFGCN(IN_DIM, hid_dim, OUT_DIM, L, K_aux, act=act, last_act=lact,\n",
    "                dropout=drop, init_h0=h0_aux, bias=bias)\n",
    "    S = torch.Tensor(A).to(device)\n",
    "    model = GF_NodeClassModel(arch, S, K, masks, loss_fn, device=device)\n",
    "    loss, acc = model.train(feat, labels, epochs, lr, wd, epochs_h=epochs_h, epochs_W=epochs_W,\n",
    "                            patience=patience)\n",
    "\n",
    "    idx_max_acc = np.argmax(acc[\"val\"])\n",
    "    print(f'Best val acc: {acc[\"val\"][idx_max_acc]:.3f}  -  Best test acc: {np.max(acc[\"test\"]):.3f}')\n",
    "    print(f'Test acc at best val: {acc[\"test\"][idx_max_acc]:.3f}')\n",
    "\n",
    "    acc_val = model.test(feat, S, labels, masks['val'])\n",
    "    acc_test = model.test(feat, S, labels, masks['test'])\n",
    "    print(f'Val acc: {acc_val:.3f}  -  Test acc: {acc_test:.3f}')\n",
    "\n",
    "    err1[i] = acc[\"test\"][idx_max_acc]\n",
    "    err2[i] = acc_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc at best val acc: 0.804 +- 0.000\n",
      "Acc at test: 0.804 +- 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fbb4d93ae20>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'Acc at best val acc: {err1.mean():.3f} +- {err1.std():.3f}')\n",
    "print(f'Acc at test: {err2.mean():.3f} +- {err1.std():.3f}')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=[6,4])\n",
    "plt.plot(acc['train'], 'b-', label='Acc train')\n",
    "plt.plot(acc['val'], 'g-', label='Acc val')\n",
    "plt.plot(acc['test'], 'r-', label='Acc test')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=[6,4])\n",
    "plt.plot(loss['train'], 'b-', label='Loss train')\n",
    "plt.plot(loss['val'], 'g-', label='Loss val')\n",
    "plt.plot(loss['test'], 'r-', label='Loss test')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training paramerters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 200-0.005-0.0001-0.25: 0.863 (0.882)\n",
      "-1: 200-0.005-0.0005-0.25: 0.863 (0.882)\n",
      "-1: 200-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-1: 200-0.005-0.001-0.25: 0.824 (0.882)\n",
      "-1: 200-0.005-0.001-0.5: 0.804 (0.882)\n",
      "-1: 200-0.001-0.001-0.25: 0.745 (0.765)\n",
      "-1: 200-0.05-0.005-0.25: 0.804 (0.882)\n",
      "-1: 200-0.01-0.005-0.25: 0.863 (0.863)\n",
      "-1: 200-0.005-0.005-0.25: 0.824 (0.863)\n",
      "-1: 200-0.001-0.005-0.25: 0.784 (0.804)\n",
      "-1: 200-0.05-0.01-0.25: 0.843 (0.882)\n",
      "-1: 200-0.005-0.01-0.25: 0.804 (0.824)\n",
      "-1: 200-0.05-0.01-0.5: 0.784 (0.882)\n",
      "-1: 200-0.05-0.005-0.5: 0.804 (0.882)\n",
      "-1: 200-0.01-0.005-0.5: 0.882 (0.882)\n",
      "-1: 200-0.05-0.01-0: 0.804 (0.843)\n",
      "-1: 200-0.05-0.005-0: 0.804 (0.863)\n",
      "-1: 200-0.01-0.005-0: 0.784 (0.784)\n",
      "-1: 500-0.005-0.001-0.5: 0.824 (0.882)\n",
      "-2: 200-0.005-0.0001-0.25: 0.902 (0.922)\n",
      "-2: 200-0.005-0.0005-0.25: 0.824 (0.922)\n",
      "-2: 200-0.01-0.001-0.25: 0.725 (0.941)\n",
      "-2: 200-0.005-0.001-0.25: 0.784 (0.922)\n",
      "-2: 200-0.005-0.001-0.5: 0.843 (0.922)\n",
      "-2: 200-0.001-0.001-0.25: 0.824 (0.882)\n",
      "-2: 200-0.05-0.005-0.25: 0.863 (0.941)\n",
      "-2: 200-0.01-0.005-0.25: 0.882 (0.902)\n",
      "-2: 200-0.005-0.005-0.25: 0.882 (0.922)\n",
      "-2: 200-0.001-0.005-0.25: 0.902 (0.922)\n",
      "-2: 200-0.05-0.01-0.25: 0.882 (0.922)\n",
      "-2: 200-0.005-0.01-0.25: 0.843 (0.922)\n",
      "-2: 200-0.05-0.01-0.5: 0.882 (0.941)\n",
      "-2: 200-0.05-0.005-0.5: 0.843 (0.902)\n",
      "-2: 200-0.01-0.005-0.5: 0.843 (0.941)\n",
      "-2: 200-0.05-0.01-0: 0.843 (0.922)\n",
      "-2: 200-0.05-0.005-0: 0.824 (0.902)\n",
      "-2: 200-0.01-0.005-0: 0.882 (0.882)\n",
      "-2: 500-0.005-0.001-0.5: 0.824 (0.902)\n",
      "-3: 200-0.005-0.0001-0.25: 0.863 (0.902)\n",
      "-3: 200-0.005-0.0005-0.25: 0.863 (0.922)\n",
      "-3: 200-0.01-0.001-0.25: 0.843 (0.922)\n",
      "-3: 200-0.005-0.001-0.25: 0.922 (0.922)\n",
      "-3: 200-0.005-0.001-0.5: 0.902 (0.941)\n",
      "-3: 200-0.001-0.001-0.25: 0.745 (0.765)\n",
      "-3: 200-0.05-0.005-0.25: 0.843 (0.882)\n",
      "-3: 200-0.01-0.005-0.25: 0.882 (0.922)\n",
      "-3: 200-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-3: 200-0.001-0.005-0.25: 0.863 (0.882)\n",
      "-3: 200-0.05-0.01-0.25: 0.902 (0.922)\n",
      "-3: 200-0.005-0.01-0.25: 0.863 (0.922)\n",
      "-3: 200-0.05-0.01-0.5: 0.824 (0.902)\n",
      "-3: 200-0.05-0.005-0.5: 0.824 (0.902)\n",
      "-3: 200-0.01-0.005-0.5: 0.882 (0.922)\n",
      "-3: 200-0.05-0.01-0: 0.863 (0.902)\n",
      "-3: 200-0.05-0.005-0: 0.863 (0.902)\n",
      "-3: 200-0.01-0.005-0: 0.882 (0.902)\n",
      "-3: 500-0.005-0.001-0.5: 0.882 (0.922)\n",
      "-4: 200-0.005-0.0001-0.25: 0.922 (0.922)\n",
      "-4: 200-0.005-0.0005-0.25: 0.902 (0.941)\n",
      "-4: 200-0.01-0.001-0.25: 0.863 (0.961)\n",
      "-4: 200-0.005-0.001-0.25: 0.941 (0.961)\n",
      "-4: 200-0.005-0.001-0.5: 0.902 (0.961)\n",
      "-4: 200-0.001-0.001-0.25: 0.725 (0.745)\n",
      "-4: 200-0.05-0.005-0.25: 0.824 (0.922)\n",
      "-4: 200-0.01-0.005-0.25: 0.922 (0.961)\n",
      "-4: 200-0.005-0.005-0.25: 0.902 (0.961)\n",
      "-4: 200-0.001-0.005-0.25: 0.843 (0.922)\n",
      "-4: 200-0.05-0.01-0.25: 0.843 (0.922)\n",
      "-4: 200-0.005-0.01-0.25: 0.941 (0.961)\n",
      "-4: 200-0.05-0.01-0.5: 0.863 (0.882)\n",
      "-4: 200-0.05-0.005-0.5: 0.824 (0.863)\n",
      "-4: 200-0.01-0.005-0.5: 0.922 (0.961)\n",
      "-4: 200-0.05-0.01-0: 0.824 (0.922)\n",
      "-4: 200-0.05-0.005-0: 0.824 (0.922)\n",
      "-4: 200-0.01-0.005-0: 0.922 (0.922)\n",
      "-4: 500-0.005-0.001-0.5: 0.902 (0.941)\n",
      "-5: 200-0.005-0.0001-0.25: 0.843 (0.961)\n",
      "-5: 200-0.005-0.0005-0.25: 0.882 (0.941)\n",
      "-5: 200-0.01-0.001-0.25: 0.882 (0.941)\n",
      "-5: 200-0.005-0.001-0.25: 0.882 (0.922)\n",
      "-5: 200-0.005-0.001-0.5: 0.902 (0.922)\n",
      "-5: 200-0.001-0.001-0.25: 0.725 (0.745)\n",
      "-5: 200-0.05-0.005-0.25: 0.882 (0.922)\n",
      "-5: 200-0.01-0.005-0.25: 0.922 (0.922)\n",
      "-5: 200-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-5: 200-0.001-0.005-0.25: 0.843 (0.882)\n",
      "-5: 200-0.05-0.01-0.25: 0.882 (0.922)\n",
      "-5: 200-0.005-0.01-0.25: 0.922 (0.922)\n",
      "-5: 200-0.05-0.01-0.5: 0.882 (0.922)\n",
      "-5: 200-0.05-0.005-0.5: 0.765 (0.922)\n",
      "-5: 200-0.01-0.005-0.5: 0.843 (0.941)\n",
      "-5: 200-0.05-0.01-0: 0.843 (0.922)\n",
      "-5: 200-0.05-0.005-0: 0.882 (0.922)\n",
      "-5: 200-0.01-0.005-0: 0.804 (0.863)\n",
      "-5: 500-0.005-0.001-0.5: 0.863 (0.961)\n",
      "-6: 200-0.005-0.0001-0.25: 0.824 (0.863)\n",
      "-6: 200-0.005-0.0005-0.25: 0.843 (0.902)\n",
      "-6: 200-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-6: 200-0.005-0.001-0.25: 0.882 (0.902)\n",
      "-6: 200-0.005-0.001-0.5: 0.765 (0.843)\n",
      "-6: 200-0.001-0.001-0.25: 0.686 (0.745)\n",
      "-6: 200-0.05-0.005-0.25: 0.804 (0.843)\n",
      "-6: 200-0.01-0.005-0.25: 0.824 (0.902)\n",
      "-6: 200-0.005-0.005-0.25: 0.843 (0.902)\n",
      "-6: 200-0.001-0.005-0.25: 0.863 (0.882)\n",
      "-6: 200-0.05-0.01-0.25: 0.804 (0.863)\n",
      "-6: 200-0.005-0.01-0.25: 0.882 (0.882)\n",
      "-6: 200-0.05-0.01-0.5: 0.784 (0.843)\n",
      "-6: 200-0.05-0.005-0.5: 0.784 (0.843)\n",
      "-6: 200-0.01-0.005-0.5: 0.824 (0.882)\n",
      "-6: 200-0.05-0.01-0: 0.863 (0.863)\n",
      "-6: 200-0.05-0.005-0: 0.863 (0.863)\n",
      "-6: 200-0.01-0.005-0: 0.843 (0.863)\n",
      "-6: 500-0.005-0.001-0.5: 0.824 (0.843)\n",
      "-7: 200-0.005-0.0001-0.25: 0.863 (0.902)\n",
      "-7: 200-0.005-0.0005-0.25: 0.882 (0.922)\n",
      "-7: 200-0.01-0.001-0.25: 0.843 (0.922)\n",
      "-7: 200-0.005-0.001-0.25: 0.882 (0.902)\n",
      "-7: 200-0.005-0.001-0.5: 0.902 (0.941)\n",
      "-7: 200-0.001-0.001-0.25: 0.843 (0.863)\n",
      "-7: 200-0.05-0.005-0.25: 0.843 (0.902)\n",
      "-7: 200-0.01-0.005-0.25: 0.843 (0.902)\n",
      "-7: 200-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-7: 200-0.001-0.005-0.25: 0.843 (0.843)\n",
      "-7: 200-0.05-0.01-0.25: 0.843 (0.902)\n",
      "-7: 200-0.005-0.01-0.25: 0.902 (0.902)\n",
      "-7: 200-0.05-0.01-0.5: 0.863 (0.902)\n",
      "-7: 200-0.05-0.005-0.5: 0.882 (0.902)\n",
      "-7: 200-0.01-0.005-0.5: 0.863 (0.922)\n",
      "-7: 200-0.05-0.01-0: 0.824 (0.882)\n",
      "-7: 200-0.05-0.005-0: 0.824 (0.863)\n",
      "-7: 200-0.01-0.005-0: 0.824 (0.882)\n",
      "-7: 500-0.005-0.001-0.5: 0.863 (0.922)\n",
      "-8: 200-0.005-0.0001-0.25: 0.843 (0.882)\n",
      "-8: 200-0.005-0.0005-0.25: 0.882 (0.941)\n",
      "-8: 200-0.01-0.001-0.25: 0.824 (0.922)\n",
      "-8: 200-0.005-0.001-0.25: 0.843 (0.882)\n",
      "-8: 200-0.005-0.001-0.5: 0.843 (0.922)\n",
      "-8: 200-0.001-0.001-0.25: 0.784 (0.804)\n",
      "-8: 200-0.05-0.005-0.25: 0.824 (0.882)\n",
      "-8: 200-0.01-0.005-0.25: 0.843 (0.882)\n",
      "-8: 200-0.005-0.005-0.25: 0.824 (0.922)\n",
      "-8: 200-0.001-0.005-0.25: 0.784 (0.804)\n",
      "-8: 200-0.05-0.01-0.25: 0.804 (0.863)\n",
      "-8: 200-0.005-0.01-0.25: 0.843 (0.863)\n",
      "-8: 200-0.05-0.01-0.5: 0.824 (0.843)\n",
      "-8: 200-0.05-0.005-0.5: 0.784 (0.863)\n",
      "-8: 200-0.01-0.005-0.5: 0.824 (0.902)\n",
      "-8: 200-0.05-0.01-0: 0.784 (0.882)\n",
      "-8: 200-0.05-0.005-0: 0.784 (0.902)\n",
      "-8: 200-0.01-0.005-0: 0.804 (0.863)\n",
      "-8: 500-0.005-0.001-0.5: 0.863 (0.922)\n",
      "-9: 200-0.005-0.0001-0.25: 0.863 (0.882)\n",
      "-9: 200-0.005-0.0005-0.25: 0.824 (0.902)\n",
      "-9: 200-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-9: 200-0.005-0.001-0.25: 0.843 (0.902)\n",
      "-9: 200-0.005-0.001-0.5: 0.804 (0.882)\n",
      "-9: 200-0.001-0.001-0.25: 0.784 (0.824)\n",
      "-9: 200-0.05-0.005-0.25: 0.824 (0.882)\n",
      "-9: 200-0.01-0.005-0.25: 0.843 (0.922)\n",
      "-9: 200-0.005-0.005-0.25: 0.804 (0.882)\n",
      "-9: 200-0.001-0.005-0.25: 0.824 (0.824)\n",
      "-9: 200-0.05-0.01-0.25: 0.843 (0.863)\n",
      "-9: 200-0.005-0.01-0.25: 0.784 (0.863)\n",
      "-9: 200-0.05-0.01-0.5: 0.824 (0.882)\n",
      "-9: 200-0.05-0.005-0.5: 0.784 (0.863)\n",
      "-9: 200-0.01-0.005-0.5: 0.824 (0.902)\n",
      "-9: 200-0.05-0.01-0: 0.804 (0.882)\n",
      "-9: 200-0.05-0.005-0: 0.824 (0.863)\n",
      "-9: 200-0.01-0.005-0: 0.824 (0.843)\n",
      "-9: 500-0.005-0.001-0.5: 0.843 (0.902)\n",
      "-10: 200-0.005-0.0001-0.25: 0.902 (0.922)\n",
      "-10: 200-0.005-0.0005-0.25: 0.902 (0.922)\n",
      "-10: 200-0.01-0.001-0.25: 0.922 (0.922)\n",
      "-10: 200-0.005-0.001-0.25: 0.922 (0.922)\n",
      "-10: 200-0.005-0.001-0.5: 0.882 (0.922)\n",
      "-10: 200-0.001-0.001-0.25: 0.804 (0.804)\n",
      "-10: 200-0.05-0.005-0.25: 0.922 (0.922)\n",
      "-10: 200-0.01-0.005-0.25: 0.824 (0.922)\n",
      "-10: 200-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-10: 200-0.001-0.005-0.25: 0.824 (0.824)\n",
      "-10: 200-0.05-0.01-0.25: 0.902 (0.922)\n",
      "-10: 200-0.005-0.01-0.25: 0.882 (0.882)\n",
      "-10: 200-0.05-0.01-0.5: 0.824 (0.902)\n",
      "-10: 200-0.05-0.005-0.5: 0.843 (0.902)\n",
      "-10: 200-0.01-0.005-0.5: 0.863 (0.922)\n",
      "-10: 200-0.05-0.01-0: 0.902 (0.902)\n",
      "-10: 200-0.05-0.005-0: 0.902 (0.922)\n",
      "-10: 200-0.01-0.005-0: 0.843 (0.863)\n",
      "-10: 500-0.005-0.001-0.5: 0.882 (0.922)\n",
      "----- 62.25 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-4, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-4, 'drop': .25},\n",
    "\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "        \n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-2, 'drop': .25},\n",
    "\n",
    "        # {'epochs': 500, 'lr': .01, 'wd': 1e-3, 'drop': .25},\n",
    "        # {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        # {'epochs': 500, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "\n",
    "        # {'epochs': 500, 'lr': .01, 'wd': 5e-3, 'drop': .25},\n",
    "        # {'epochs': 500, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        # {'epochs': 500, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "\n",
    "\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': 0},\n",
    "\n",
    "        \n",
    "        # {'epochs': 500, 'lr': .005, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        # {'epochs': 500, 'lr': .001, 'wd': 1e-3, 'drop': .5},\n",
    "\n",
    "        # {'epochs': 750, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        # {'epochs': 750, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "        # {'epochs': 750, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "        ]\n",
    "\n",
    "best_accs1 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs1 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=exp['drop'], init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S, K, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, exp['epochs'], exp['lr'], exp['wd'],\n",
    "                             epochs_h=EPOCHS_h, epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs1[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs1[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}: {best_val_accs1[j,i]:.3f} ({best_accs1[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}' for exp in EXPS]\n",
    "table_over1 = summary_table(best_accs1, index_name)\n",
    "table1 = summary_table(best_val_accs1, index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0001-0.25</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.029149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0005-0.25</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.027451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.25</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.047222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.25</th>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.046607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.5</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.048189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.001-0.25</th>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.046772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0.25</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.035076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.25</th>\n",
       "      <td>0.864706</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.034467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.005-0.25</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.028074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.25</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.034018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.25</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.034187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.01-0.25</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.047059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.5</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.034187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0.5</th>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.034244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.5</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.030440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.033044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0</th>\n",
       "      <td>0.839216</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.034856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.040612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0.5</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.026380</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean accs       med       std\n",
       "200-0.005-0.0001-0.25   0.868627  0.862745  0.029149\n",
       "200-0.005-0.0005-0.25   0.866667  0.872549  0.027451\n",
       "200-0.01-0.001-0.25     0.843137  0.843137  0.047222\n",
       "200-0.005-0.001-0.25    0.872549  0.882353  0.046607\n",
       "200-0.005-0.001-0.5     0.854902  0.862745  0.048189\n",
       "200-0.001-0.001-0.25    0.766667  0.764706  0.046772\n",
       "200-0.05-0.005-0.25     0.843137  0.833333  0.035076\n",
       "200-0.01-0.005-0.25     0.864706  0.852941  0.034467\n",
       "200-0.005-0.005-0.25    0.852941  0.862745  0.028074\n",
       "200-0.001-0.005-0.25    0.837255  0.843137  0.034018\n",
       "200-0.05-0.01-0.25      0.854902  0.843137  0.034187\n",
       "200-0.005-0.01-0.25     0.866667  0.872549  0.047059\n",
       "200-0.05-0.01-0.5       0.835294  0.823529  0.034187\n",
       "200-0.05-0.005-0.5      0.813725  0.813725  0.034244\n",
       "200-0.01-0.005-0.5      0.856863  0.852941  0.030440\n",
       "200-0.05-0.01-0         0.835294  0.833333  0.033044\n",
       "200-0.05-0.005-0        0.839216  0.823529  0.034856\n",
       "200-0.01-0.005-0        0.841176  0.833333  0.040612\n",
       "500-0.005-0.001-0.5     0.856863  0.862745  0.026380"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 0.01-1-1-True: 0.529 (0.549)\n",
      "-1: 0.1-1-1-True: 0.471 (0.529)\n",
      "-1: 1-1-1-True: 0.510 (0.588)\n",
      "-1: 1-1-1-False: 0.529 (0.529)\n",
      "-1: 1-1-1-False: 0.451 (0.510)\n",
      "-1: 1-5-1-True: 0.706 (0.745)\n",
      "-1: 1-10-1-True: 0.706 (0.784)\n",
      "-1: 1-10-5-True: 0.843 (0.882)\n",
      "-1: 1-1-5-True: 0.490 (0.588)\n",
      "-1: 1-1-10-True: 0.471 (0.569)\n",
      "-1: 1-5-10-True: 0.725 (0.784)\n",
      "-1: 0.1-10-10-True: 0.843 (0.863)\n",
      "-1: 1-10-10-True: 0.843 (0.882)\n",
      "-1: 1-25-25-True: 0.804 (0.824)\n",
      "-1: 1-25-25-True: 0.824 (0.843)\n",
      "-1: 0.1-25-25-True: 0.882 (0.902)\n",
      "-1: 1-25-25-True: 0.863 (0.882)\n",
      "-1: 1-50-50-True: 0.784 (0.882)\n",
      "-2: 0.01-1-1-True: 0.608 (0.667)\n",
      "-2: 0.1-1-1-True: 0.490 (0.627)\n",
      "-2: 1-1-1-True: 0.451 (0.510)\n",
      "-2: 1-1-1-False: 0.608 (0.647)\n",
      "-2: 1-1-1-False: 0.863 (0.882)\n",
      "-2: 1-5-1-True: 0.804 (0.863)\n",
      "-2: 1-10-1-True: 0.843 (0.863)\n",
      "-2: 1-10-5-True: 0.863 (0.863)\n",
      "-2: 1-1-5-True: 0.627 (0.706)\n",
      "-2: 1-1-10-True: 0.667 (0.706)\n",
      "-2: 1-5-10-True: 0.863 (0.902)\n",
      "-2: 0.1-10-10-True: 0.882 (0.922)\n",
      "-2: 1-10-10-True: 0.882 (0.902)\n",
      "-2: 1-25-25-True: 0.843 (0.882)\n",
      "-2: 1-25-25-True: 0.843 (0.941)\n",
      "-2: 0.1-25-25-True: 0.863 (0.922)\n",
      "-2: 1-25-25-True: 0.902 (0.922)\n",
      "-2: 1-50-50-True: 0.922 (0.922)\n",
      "-3: 0.01-1-1-True: 0.431 (0.471)\n",
      "-3: 0.1-1-1-True: 0.510 (0.569)\n",
      "-3: 1-1-1-True: 0.471 (0.549)\n",
      "-3: 1-1-1-False: 0.529 (0.549)\n",
      "-3: 1-1-1-False: 0.451 (0.471)\n",
      "-3: 1-5-1-True: 0.824 (0.824)\n",
      "-3: 1-10-1-True: 0.824 (0.863)\n",
      "-3: 1-10-5-True: 0.863 (0.902)\n",
      "-3: 1-1-5-True: 0.490 (0.569)\n",
      "-3: 1-1-10-True: 0.431 (0.588)\n",
      "-3: 1-5-10-True: 0.843 (0.863)\n",
      "-3: 0.1-10-10-True: 0.882 (0.902)\n",
      "-3: 1-10-10-True: 0.824 (0.902)\n",
      "-3: 1-25-25-True: 0.863 (0.902)\n",
      "-3: 1-25-25-True: 0.863 (0.922)\n",
      "-3: 0.1-25-25-True: 0.882 (0.922)\n",
      "-3: 1-25-25-True: 0.863 (0.922)\n",
      "-3: 1-50-50-True: 0.824 (0.941)\n",
      "-4: 0.01-1-1-True: 0.490 (0.569)\n",
      "-4: 0.1-1-1-True: 0.471 (0.490)\n",
      "-4: 1-1-1-True: 0.510 (0.549)\n",
      "-4: 1-1-1-False: 0.510 (0.588)\n",
      "-4: 1-1-1-False: 0.471 (0.549)\n",
      "-4: 1-5-1-True: 0.706 (0.745)\n",
      "-4: 1-10-1-True: 0.784 (0.804)\n",
      "-4: 1-10-5-True: 0.941 (0.941)\n",
      "-4: 1-1-5-True: 0.490 (0.608)\n",
      "-4: 1-1-10-True: 0.529 (0.608)\n",
      "-4: 1-5-10-True: 0.824 (0.863)\n",
      "-4: 0.1-10-10-True: 0.941 (0.961)\n",
      "-4: 1-10-10-True: 0.902 (0.961)\n",
      "-4: 1-25-25-True: 0.843 (0.863)\n",
      "-4: 1-25-25-True: 0.902 (0.961)\n",
      "-4: 0.1-25-25-True: 0.902 (0.961)\n",
      "-4: 1-25-25-True: 0.863 (0.961)\n",
      "-4: 1-50-50-True: 0.882 (0.961)\n",
      "-5: 0.01-1-1-True: 0.353 (0.412)\n",
      "-5: 0.1-1-1-True: 0.412 (0.431)\n",
      "-5: 1-1-1-True: 0.353 (0.392)\n",
      "-5: 1-1-1-False: 0.510 (0.510)\n",
      "-5: 1-1-1-False: 0.412 (0.529)\n",
      "-5: 1-5-1-True: 0.706 (0.784)\n",
      "-5: 1-10-1-True: 0.784 (0.804)\n",
      "-5: 1-10-5-True: 0.765 (0.824)\n",
      "-5: 1-1-5-True: 0.451 (0.510)\n",
      "-5: 1-1-10-True: 0.412 (0.529)\n",
      "-5: 1-5-10-True: 0.824 (0.882)\n",
      "-5: 0.1-10-10-True: 0.824 (0.902)\n",
      "-5: 1-10-10-True: 0.882 (0.922)\n",
      "-5: 1-25-25-True: 0.902 (0.922)\n",
      "-5: 1-25-25-True: 0.902 (0.902)\n",
      "-5: 0.1-25-25-True: 0.863 (0.961)\n",
      "-5: 1-25-25-True: 0.804 (0.922)\n",
      "-5: 1-50-50-True: 0.824 (0.922)\n",
      "-6: 0.01-1-1-True: 0.490 (0.490)\n",
      "-6: 0.1-1-1-True: 0.569 (0.608)\n",
      "-6: 1-1-1-True: 0.608 (0.608)\n",
      "-6: 1-1-1-False: 0.569 (0.588)\n",
      "-6: 1-1-1-False: 0.765 (0.784)\n",
      "-6: 1-5-1-True: 0.745 (0.804)\n",
      "-6: 1-10-1-True: 0.784 (0.824)\n",
      "-6: 1-10-5-True: 0.765 (0.863)\n",
      "-6: 1-1-5-True: 0.588 (0.588)\n",
      "-6: 1-1-10-True: 0.569 (0.627)\n",
      "-6: 1-5-10-True: 0.804 (0.843)\n",
      "-6: 0.1-10-10-True: 0.804 (0.824)\n",
      "-6: 1-10-10-True: 0.824 (0.843)\n",
      "-6: 1-25-25-True: 0.843 (0.882)\n",
      "-6: 1-25-25-True: 0.765 (0.882)\n",
      "-6: 0.1-25-25-True: 0.843 (0.922)\n",
      "-6: 1-25-25-True: 0.843 (0.902)\n",
      "-6: 1-50-50-True: 0.863 (0.922)\n",
      "-7: 0.01-1-1-True: 0.373 (0.373)\n",
      "-7: 0.1-1-1-True: 0.490 (0.549)\n",
      "-7: 1-1-1-True: 0.529 (0.569)\n",
      "-7: 1-1-1-False: 0.490 (0.529)\n",
      "-7: 1-1-1-False: 0.471 (0.569)\n",
      "-7: 1-5-1-True: 0.745 (0.804)\n",
      "-7: 1-10-1-True: 0.549 (0.569)\n",
      "-7: 1-10-5-True: 0.863 (0.902)\n",
      "-7: 1-1-5-True: 0.529 (0.588)\n",
      "-7: 1-1-10-True: 0.608 (0.608)\n",
      "-7: 1-5-10-True: 0.784 (0.882)\n",
      "-7: 0.1-10-10-True: 0.843 (0.902)\n",
      "-7: 1-10-10-True: 0.882 (0.882)\n",
      "-7: 1-25-25-True: 0.863 (0.902)\n",
      "-7: 1-25-25-True: 0.843 (0.882)\n",
      "-7: 0.1-25-25-True: 0.882 (0.902)\n",
      "-7: 1-25-25-True: 0.863 (0.902)\n",
      "-7: 1-50-50-True: 0.882 (0.902)\n",
      "-8: 0.01-1-1-True: 0.392 (0.490)\n",
      "-8: 0.1-1-1-True: 0.471 (0.510)\n",
      "-8: 1-1-1-True: 0.471 (0.549)\n",
      "-8: 1-1-1-False: 0.510 (0.569)\n",
      "-8: 1-1-1-False: 0.529 (0.569)\n",
      "-8: 1-5-1-True: 0.686 (0.706)\n",
      "-8: 1-10-1-True: 0.725 (0.745)\n",
      "-8: 1-10-5-True: 0.784 (0.824)\n",
      "-8: 1-1-5-True: 0.490 (0.588)\n",
      "-8: 1-1-10-True: 0.490 (0.549)\n",
      "-8: 1-5-10-True: 0.804 (0.824)\n",
      "-8: 0.1-10-10-True: 0.824 (0.863)\n",
      "-8: 1-10-10-True: 0.784 (0.843)\n",
      "-8: 1-25-25-True: 0.765 (0.804)\n",
      "-8: 1-25-25-True: 0.824 (0.882)\n",
      "-8: 0.1-25-25-True: 0.843 (0.902)\n",
      "-8: 1-25-25-True: 0.843 (0.902)\n",
      "-8: 1-50-50-True: 0.824 (0.882)\n",
      "-9: 0.01-1-1-True: 0.412 (0.490)\n",
      "-9: 0.1-1-1-True: 0.529 (0.549)\n",
      "-9: 1-1-1-True: 0.510 (0.549)\n",
      "-9: 1-1-1-False: 0.549 (0.608)\n",
      "-9: 1-1-1-False: 0.451 (0.451)\n",
      "-9: 1-5-1-True: 0.490 (0.510)\n",
      "-9: 1-10-1-True: 0.706 (0.765)\n",
      "-9: 1-10-5-True: 0.765 (0.824)\n",
      "-9: 1-1-5-True: 0.510 (0.510)\n",
      "-9: 1-1-10-True: 0.490 (0.510)\n",
      "-9: 1-5-10-True: 0.843 (0.882)\n",
      "-9: 0.1-10-10-True: 0.863 (0.922)\n",
      "-9: 1-10-10-True: 0.765 (0.863)\n",
      "-9: 1-25-25-True: 0.843 (0.863)\n",
      "-9: 1-25-25-True: 0.804 (0.863)\n",
      "-9: 0.1-25-25-True: 0.863 (0.902)\n",
      "-9: 1-25-25-True: 0.824 (0.902)\n",
      "-9: 1-50-50-True: 0.843 (0.902)\n",
      "-10: 0.01-1-1-True: 0.431 (0.529)\n",
      "-10: 0.1-1-1-True: 0.451 (0.471)\n",
      "-10: 1-1-1-True: 0.529 (0.627)\n",
      "-10: 1-1-1-False: 0.490 (0.490)\n",
      "-10: 1-1-1-False: 0.510 (0.608)\n",
      "-10: 1-5-1-True: 0.686 (0.706)\n",
      "-10: 1-10-1-True: 0.824 (0.843)\n",
      "-10: 1-10-5-True: 0.824 (0.863)\n",
      "-10: 1-1-5-True: 0.451 (0.510)\n",
      "-10: 1-1-10-True: 0.529 (0.588)\n",
      "-10: 1-5-10-True: 0.843 (0.882)\n",
      "-10: 0.1-10-10-True: 0.863 (0.922)\n",
      "-10: 1-10-10-True: 0.863 (0.882)\n",
      "-10: 1-25-25-True: 0.843 (0.863)\n",
      "-10: 1-25-25-True: 0.843 (0.882)\n",
      "-10: 0.1-25-25-True: 0.922 (0.922)\n",
      "-10: 1-25-25-True: 0.902 (0.922)\n",
      "-10: 1-50-50-True: 0.902 (0.922)\n",
      "----- 25.03 mins -----\n"
     ]
    }
   ],
   "source": [
    "# h0, norm (norm, not norm, norm H), sep vs joint training, optimizers iters (W vs h)\n",
    "EXPS = [\n",
    "        # {'h0': .001, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': .01, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': .1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        \n",
    "        # {'h0': .1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        {'h0': 1, 'epochs': 1000, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 5, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 1, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 5, 'alt': True},\n",
    "\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 5, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 10, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 5, 'epochs_W': 10, 'alt': True},\n",
    "        \n",
    "        {'h0': .1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 50, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 100, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': .1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 50, 'epochs_W': 50, 'alt': True},\n",
    "    ]\n",
    "\n",
    "\n",
    "best_accs2 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs2 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=exp['h0'])\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        if not exp['alt']:\n",
    "            model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "            _, acc = model.train(feat, labels, exp['epochs'], LR, WD)\n",
    "        else:\n",
    "            model = GF_NodeClassModel(arch, S,  K, masks, LOSS_FN, device=device)\n",
    "            _, acc = model.train(feat, labels, exp['epochs'], LR, WD, epochs_h=exp['epochs_h'],\n",
    "                                 epochs_W=exp['epochs_W'])\n",
    "\n",
    "        best_accs2[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs2[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"h0\"]}-{exp[\"epochs_h\"]}-{exp[\"epochs_W\"]}-{exp[\"alt\"]}: {best_val_accs2[j,i]:.3f} ({best_accs2[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"h0\"]}-{exp[\"epochs_h\"]}-{exp[\"epochs_W\"]}-{exp[\"alt\"]}' for exp in EXPS]\n",
    "table_over2 = summary_table(best_accs2, index_name)\n",
    "table2 = summary_table(best_val_accs2, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.01-1-1-True</th>\n",
       "      <td>0.450980</td>\n",
       "      <td>0.431373</td>\n",
       "      <td>0.074407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-1-1-True</th>\n",
       "      <td>0.486275</td>\n",
       "      <td>0.480392</td>\n",
       "      <td>0.040942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-1-True</th>\n",
       "      <td>0.494118</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-1-False</th>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.035076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000-1-1-1-False</th>\n",
       "      <td>0.537255</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.143339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-5-1-True</th>\n",
       "      <td>0.709804</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.085828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-1-True</th>\n",
       "      <td>0.752941</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.082353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-5-True</th>\n",
       "      <td>0.827451</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.055321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-5-True</th>\n",
       "      <td>0.511765</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.053662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-10-True</th>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.509804</td>\n",
       "      <td>0.074536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-5-10-True</th>\n",
       "      <td>0.815686</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.037409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-10-10-True</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.037255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-10-True</th>\n",
       "      <td>0.845098</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.043359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50-1-25-25-True</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.034467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100-1-25-25-True</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.039654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-25-25-True</th>\n",
       "      <td>0.874510</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.023529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-25-25-True</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.029149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-50-50-True</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.040375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mean accs       med       std\n",
       "200-0.01-1-1-True    0.450980  0.431373  0.074407\n",
       "200-0.1-1-1-True     0.486275  0.480392  0.040942\n",
       "200-1-1-1-True       0.494118  0.509804  0.062500\n",
       "200-1-1-1-False      0.529412  0.519608  0.035076\n",
       "1000-1-1-1-False     0.537255  0.470588  0.143339\n",
       "200-1-5-1-True       0.709804  0.705882  0.085828\n",
       "200-1-10-1-True      0.752941  0.784314  0.082353\n",
       "200-1-10-5-True      0.827451  0.833333  0.055321\n",
       "200-1-1-5-True       0.511765  0.490196  0.053662\n",
       "200-1-1-10-True      0.519608  0.509804  0.074536\n",
       "200-1-5-10-True      0.815686  0.823529  0.037409\n",
       "200-0.1-10-10-True   0.856863  0.852941  0.037255\n",
       "200-1-10-10-True     0.845098  0.852941  0.043359\n",
       "50-1-25-25-True      0.841176  0.843137  0.034467\n",
       "100-1-25-25-True     0.841176  0.843137  0.039654\n",
       "200-0.1-25-25-True   0.874510  0.872549  0.023529\n",
       "200-1-25-25-True     0.856863  0.862745  0.029149\n",
       "200-1-50-50-True     0.854902  0.852941  0.040375"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design - Bias True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 2-2-16: 0.784 (0.882)\n",
      "-1: 2-3-8: 0.765 (0.824)\n",
      "-1: 2-3-16: 0.765 (0.882)\n",
      "-1: 2-3-32: 0.843 (0.882)\n",
      "-1: 2-4-16: 0.824 (0.863)\n",
      "-1: 3-2-16: 0.843 (0.882)\n",
      "-1: 4-2-16: 0.804 (0.882)\n",
      "-1: 3-3-16: 0.824 (0.843)\n",
      "-1: 4-3-16: 0.824 (0.863)\n",
      "-1: 2-2-8: 0.784 (0.843)\n",
      "-1: 2-2-32: 0.863 (0.882)\n",
      "-1: 2-2-50: 0.824 (0.882)\n",
      "-1: 2-2-75: 0.843 (0.882)\n",
      "-1: 2-2-100: 0.843 (0.882)\n",
      "-1: 2-3-50: 0.824 (0.882)\n",
      "-1: 2-3-75: 0.843 (0.863)\n",
      "-1: 2-3-100: 0.784 (0.843)\n",
      "-1: 3-2-32: 0.843 (0.882)\n",
      "-1: 3-2-50: 0.843 (0.882)\n",
      "-1: 3-3-50: 0.824 (0.902)\n",
      "-2: 2-2-16: 0.804 (0.902)\n",
      "-2: 2-3-8: 0.863 (0.922)\n",
      "-2: 2-3-16: 0.863 (0.902)\n",
      "-2: 2-3-32: 0.882 (0.922)\n",
      "-2: 2-4-16: 0.824 (0.902)\n",
      "-2: 3-2-16: 0.863 (0.922)\n",
      "-2: 4-2-16: 0.882 (0.902)\n",
      "-2: 3-3-16: 0.863 (0.922)\n",
      "-2: 4-3-16: 0.863 (0.922)\n",
      "-2: 2-2-8: 0.882 (0.902)\n",
      "-2: 2-2-32: 0.804 (0.922)\n",
      "-2: 2-2-50: 0.882 (0.922)\n",
      "-2: 2-2-75: 0.843 (0.922)\n",
      "-2: 2-2-100: 0.824 (0.922)\n",
      "-2: 2-3-50: 0.824 (0.941)\n",
      "-2: 2-3-75: 0.863 (0.922)\n",
      "-2: 2-3-100: 0.824 (0.941)\n",
      "-2: 3-2-32: 0.882 (0.922)\n",
      "-2: 3-2-50: 0.843 (0.922)\n",
      "-2: 3-3-50: 0.843 (0.922)\n",
      "-3: 2-2-16: 0.902 (0.922)\n",
      "-3: 2-3-8: 0.765 (0.765)\n",
      "-3: 2-3-16: 0.882 (0.922)\n",
      "-3: 2-3-32: 0.882 (0.922)\n",
      "-3: 2-4-16: 0.804 (0.902)\n",
      "-3: 3-2-16: 0.863 (0.902)\n",
      "-3: 4-2-16: 0.843 (0.922)\n",
      "-3: 3-3-16: 0.765 (0.882)\n",
      "-3: 4-3-16: 0.706 (0.706)\n",
      "-3: 2-2-8: 0.686 (0.745)\n",
      "-3: 2-2-32: 0.922 (0.922)\n",
      "-3: 2-2-50: 0.843 (0.941)\n",
      "-3: 2-2-75: 0.824 (0.941)\n",
      "-3: 2-2-100: 0.882 (0.922)\n",
      "-3: 2-3-50: 0.843 (0.922)\n",
      "-3: 2-3-75: 0.882 (0.922)\n",
      "-3: 2-3-100: 0.843 (0.922)\n",
      "-3: 3-2-32: 0.902 (0.941)\n",
      "-3: 3-2-50: 0.902 (0.941)\n",
      "-3: 3-3-50: 0.490 (0.510)\n",
      "-4: 2-2-16: 0.863 (0.961)\n",
      "-4: 2-3-8: 0.863 (0.882)\n",
      "-4: 2-3-16: 0.902 (0.941)\n",
      "-4: 2-3-32: 0.863 (0.961)\n",
      "-4: 2-4-16: 0.882 (0.922)\n",
      "-4: 3-2-16: 0.824 (0.922)\n",
      "-4: 4-2-16: 0.863 (0.941)\n",
      "-4: 3-3-16: 0.902 (0.902)\n",
      "-4: 4-3-16: 0.804 (0.824)\n",
      "-4: 2-2-8: 0.863 (0.941)\n",
      "-4: 2-2-32: 0.941 (0.961)\n",
      "-4: 2-2-50: 0.902 (0.961)\n",
      "-4: 2-2-75: 0.941 (0.961)\n",
      "-4: 2-2-100: 0.922 (0.941)\n",
      "-4: 2-3-50: 0.863 (0.961)\n",
      "-4: 2-3-75: 0.882 (0.961)\n",
      "-4: 2-3-100: 0.941 (0.941)\n",
      "-4: 3-2-32: 0.882 (0.941)\n",
      "-4: 3-2-50: 0.980 (0.980)\n",
      "-4: 3-3-50: 0.863 (0.941)\n",
      "-5: 2-2-16: 0.922 (0.922)\n",
      "-5: 2-3-8: 0.882 (0.902)\n",
      "-5: 2-3-16: 0.784 (0.902)\n",
      "-5: 2-3-32: 0.863 (0.941)\n",
      "-5: 2-4-16: 0.863 (0.941)\n",
      "-5: 3-2-16: 0.902 (0.902)\n",
      "-5: 4-2-16: 0.863 (0.902)\n",
      "-5: 3-3-16: 0.863 (0.902)\n",
      "-5: 4-3-16: 0.863 (0.882)\n",
      "-5: 2-2-8: 0.706 (0.824)\n",
      "-5: 2-2-32: 0.882 (0.922)\n",
      "-5: 2-2-50: 0.882 (0.922)\n",
      "-5: 2-2-75: 0.922 (0.941)\n",
      "-5: 2-2-100: 0.922 (0.922)\n",
      "-5: 2-3-50: 0.902 (0.941)\n",
      "-5: 2-3-75: 0.902 (0.902)\n",
      "-5: 2-3-100: 0.863 (0.902)\n",
      "-5: 3-2-32: 0.882 (0.922)\n",
      "-5: 3-2-50: 0.922 (0.941)\n",
      "-5: 3-3-50: 0.922 (0.961)\n",
      "-6: 2-2-16: 0.784 (0.902)\n",
      "-6: 2-3-8: 0.725 (0.824)\n",
      "-6: 2-3-16: 0.804 (0.882)\n",
      "-6: 2-3-32: 0.843 (0.922)\n",
      "-6: 2-4-16: 0.824 (0.882)\n",
      "-6: 3-2-16: 0.863 (0.882)\n",
      "-6: 4-2-16: 0.804 (0.843)\n",
      "-6: 3-3-16: 0.784 (0.824)\n",
      "-6: 4-3-16: 0.451 (0.608)\n",
      "-6: 2-2-8: 0.647 (0.745)\n",
      "-6: 2-2-32: 0.922 (0.922)\n",
      "-6: 2-2-50: 0.863 (0.902)\n",
      "-6: 2-2-75: 0.804 (0.922)\n",
      "-6: 2-2-100: 0.902 (0.902)\n",
      "-6: 2-3-50: 0.843 (0.902)\n",
      "-6: 2-3-75: 0.882 (0.922)\n",
      "-6: 2-3-100: 0.863 (0.922)\n",
      "-6: 3-2-32: 0.902 (0.902)\n",
      "-6: 3-2-50: 0.824 (0.902)\n",
      "-6: 3-3-50: 0.824 (0.922)\n",
      "-7: 2-2-16: 0.843 (0.922)\n",
      "-7: 2-3-8: 0.902 (0.902)\n",
      "-7: 2-3-16: 0.843 (0.922)\n",
      "-7: 2-3-32: 0.902 (0.902)\n",
      "-7: 2-4-16: 0.843 (0.922)\n",
      "-7: 3-2-16: 0.843 (0.941)\n",
      "-7: 4-2-16: 0.843 (0.882)\n",
      "-7: 3-3-16: 0.882 (0.922)\n",
      "-7: 4-3-16: 0.824 (0.843)\n",
      "-7: 2-2-8: 0.745 (0.843)\n",
      "-7: 2-2-32: 0.843 (0.902)\n",
      "-7: 2-2-50: 0.843 (0.902)\n",
      "-7: 2-2-75: 0.863 (0.902)\n",
      "-7: 2-2-100: 0.882 (0.882)\n",
      "-7: 2-3-50: 0.882 (0.902)\n",
      "-7: 2-3-75: 0.843 (0.922)\n",
      "-7: 2-3-100: 0.882 (0.902)\n",
      "-7: 3-2-32: 0.824 (0.922)\n",
      "-7: 3-2-50: 0.863 (0.922)\n",
      "-7: 3-3-50: 0.843 (0.902)\n",
      "-8: 2-2-16: 0.863 (0.882)\n",
      "-8: 2-3-8: 0.804 (0.824)\n",
      "-8: 2-3-16: 0.882 (0.922)\n",
      "-8: 2-3-32: 0.902 (0.922)\n",
      "-8: 2-4-16: 0.824 (0.824)\n",
      "-8: 3-2-16: 0.863 (0.882)\n",
      "-8: 4-2-16: 0.765 (0.863)\n",
      "-8: 3-3-16: 0.843 (0.902)\n",
      "-8: 4-3-16: 0.784 (0.863)\n",
      "-8: 2-2-8: 0.725 (0.804)\n",
      "-8: 2-2-32: 0.863 (0.902)\n",
      "-8: 2-2-50: 0.843 (0.902)\n",
      "-8: 2-2-75: 0.843 (0.902)\n",
      "-8: 2-2-100: 0.843 (0.882)\n",
      "-8: 2-3-50: 0.863 (0.882)\n",
      "-8: 2-3-75: 0.882 (0.902)\n",
      "-8: 2-3-100: 0.882 (0.902)\n",
      "-8: 3-2-32: 0.902 (0.902)\n",
      "-8: 3-2-50: 0.843 (0.902)\n",
      "-8: 3-3-50: 0.863 (0.902)\n",
      "-9: 2-2-16: 0.824 (0.902)\n",
      "-9: 2-3-8: 0.745 (0.824)\n",
      "-9: 2-3-16: 0.863 (0.902)\n",
      "-9: 2-3-32: 0.843 (0.882)\n",
      "-9: 2-4-16: 0.804 (0.922)\n",
      "-9: 3-2-16: 0.843 (0.863)\n",
      "-9: 4-2-16: 0.784 (0.882)\n",
      "-9: 3-3-16: 0.824 (0.882)\n",
      "-9: 4-3-16: 0.824 (0.902)\n",
      "-9: 2-2-8: 0.824 (0.824)\n",
      "-9: 2-2-32: 0.863 (0.902)\n",
      "-9: 2-2-50: 0.882 (0.902)\n",
      "-9: 2-2-75: 0.863 (0.882)\n",
      "-9: 2-2-100: 0.863 (0.882)\n",
      "-9: 2-3-50: 0.882 (0.882)\n",
      "-9: 2-3-75: 0.863 (0.902)\n",
      "-9: 2-3-100: 0.804 (0.863)\n",
      "-9: 3-2-32: 0.824 (0.882)\n",
      "-9: 3-2-50: 0.804 (0.902)\n",
      "-9: 3-3-50: 0.843 (0.902)\n",
      "-10: 2-2-16: 0.824 (0.922)\n",
      "-10: 2-3-8: 0.863 (0.882)\n",
      "-10: 2-3-16: 0.843 (0.922)\n",
      "-10: 2-3-32: 0.922 (0.922)\n",
      "-10: 2-4-16: 0.882 (0.902)\n",
      "-10: 3-2-16: 0.863 (0.902)\n",
      "-10: 4-2-16: 0.882 (0.922)\n",
      "-10: 3-3-16: 0.863 (0.902)\n",
      "-10: 4-3-16: 0.863 (0.863)\n",
      "-10: 2-2-8: 0.882 (0.922)\n",
      "-10: 2-2-32: 0.922 (0.941)\n",
      "-10: 2-2-50: 0.922 (0.922)\n",
      "-10: 2-2-75: 0.902 (0.922)\n",
      "-10: 2-2-100: 0.882 (0.941)\n",
      "-10: 2-3-50: 0.922 (0.922)\n",
      "-10: 2-3-75: 0.902 (0.922)\n",
      "-10: 2-3-100: 0.902 (0.922)\n",
      "-10: 3-2-32: 0.922 (0.922)\n",
      "-10: 3-2-50: 0.902 (0.941)\n",
      "-10: 3-3-50: 0.863 (0.922)\n",
      "----- 74.07 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'L': 2, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 8},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 8},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 50},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 75},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 100},\n",
    "\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 50},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 75},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 100},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 50},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 50},\n",
    "        ]\n",
    "\n",
    "best_accs3 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs3 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, exp['hid_dim'], OUT_DIM, exp['L'], exp['K'], act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  exp['K'], masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs3[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs3[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}: {best_val_accs3[j,i]:.3f} ({best_accs3[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}' for exp in EXPS]\n",
    "table_over3 = summary_table(best_accs3, index_name)\n",
    "table3 = summary_table(best_val_accs3, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.044237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-8</th>\n",
       "      <td>0.817647</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.060784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.042959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.874510</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.026597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-16</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.027799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.019706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-16</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.039460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-16</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.040612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-16</th>\n",
       "      <td>0.780392</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.118559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-8</th>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.080964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.041130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-50</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.029149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-75</th>\n",
       "      <td>0.864706</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.041548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-100</th>\n",
       "      <td>0.876471</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.031677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-50</th>\n",
       "      <td>0.864706</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.030941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-75</th>\n",
       "      <td>0.874510</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.019996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-100</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.044540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-32</th>\n",
       "      <td>0.876471</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.032869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-50</th>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.050564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-50</th>\n",
       "      <td>0.817647</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.112314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean accs       med       std\n",
       "2-2-16    0.841176  0.833333  0.044237\n",
       "2-3-8     0.817647  0.833333  0.060784\n",
       "2-3-16    0.843137  0.852941  0.042959\n",
       "2-3-32    0.874510  0.872549  0.026597\n",
       "2-4-16    0.837255  0.823529  0.027799\n",
       "3-2-16    0.856863  0.862745  0.019706\n",
       "4-2-16    0.833333  0.843137  0.039460\n",
       "3-3-16    0.841176  0.852941  0.040612\n",
       "4-3-16    0.780392  0.823529  0.118559\n",
       "2-2-8     0.774510  0.764706  0.080964\n",
       "2-2-32    0.882353  0.872549  0.041130\n",
       "2-2-50    0.868627  0.872549  0.029149\n",
       "2-2-75    0.864706  0.852941  0.041548\n",
       "2-2-100   0.876471  0.882353  0.031677\n",
       "2-3-50    0.864706  0.862745  0.030941\n",
       "2-3-75    0.874510  0.882353  0.019996\n",
       "2-3-100   0.858824  0.862745  0.044540\n",
       "3-2-32    0.876471  0.882353  0.032869\n",
       "3-2-50    0.872549  0.852941  0.050564\n",
       "3-3-50    0.817647  0.843137  0.112314"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSO, bias, and batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1: orig-True-False: 0.824 (0.863)\n",
      "- 1: trans-True-False: 0.843 (0.882)\n",
      "- 1: sym-True-False: 0.529 (0.529)\n",
      "- 1: orig-True-True: 0.765 (0.863)\n",
      "- 1: trans-True-True: 0.882 (0.882)\n",
      "- 1: sym-True-True: 0.804 (0.863)\n",
      "- 1: orig-False-False: 0.843 (0.882)\n",
      "- 1: sym-False-False: 0.824 (0.843)\n",
      "- 2: orig-True-False: 0.922 (0.922)\n",
      "- 2: trans-True-False: 0.804 (0.882)\n",
      "- 2: sym-True-False: 0.627 (0.627)\n",
      "- 2: orig-True-True: 0.765 (0.882)\n",
      "- 2: trans-True-True: 0.765 (0.784)\n",
      "- 2: sym-True-True: 0.765 (0.882)\n",
      "- 2: orig-False-False: 0.863 (0.922)\n",
      "- 2: sym-False-False: 0.627 (0.627)\n",
      "- 3: orig-True-False: 0.882 (0.922)\n",
      "- 3: trans-True-False: 0.863 (0.922)\n",
      "- 3: sym-True-False: 0.941 (0.941)\n",
      "- 3: orig-True-True: 0.706 (0.863)\n",
      "- 3: trans-True-True: 0.745 (0.843)\n",
      "- 3: sym-True-True: 0.843 (0.902)\n",
      "- 3: orig-False-False: 0.902 (0.922)\n",
      "- 3: sym-False-False: 0.412 (0.412)\n",
      "- 4: orig-True-False: 0.882 (0.961)\n",
      "- 4: trans-True-False: 0.882 (0.941)\n",
      "- 4: sym-True-False: 0.431 (0.431)\n",
      "- 4: orig-True-True: 0.922 (0.961)\n",
      "- 4: trans-True-True: 0.804 (0.922)\n",
      "- 4: sym-True-True: 0.824 (0.941)\n",
      "- 4: orig-False-False: 0.902 (0.961)\n",
      "- 4: sym-False-False: 0.431 (0.431)\n",
      "- 5: orig-True-False: 0.902 (0.941)\n",
      "- 5: trans-True-False: 0.824 (0.863)\n",
      "- 5: sym-True-False: 0.392 (0.392)\n",
      "- 5: orig-True-True: 0.882 (0.922)\n",
      "- 5: trans-True-True: 0.804 (0.863)\n",
      "- 5: sym-True-True: 0.882 (0.941)\n",
      "- 5: orig-False-False: 0.882 (0.922)\n",
      "- 5: sym-False-False: 0.471 (0.490)\n",
      "- 6: orig-True-False: 0.882 (0.902)\n",
      "- 6: trans-True-False: 0.824 (0.882)\n",
      "- 6: sym-True-False: 0.510 (0.529)\n",
      "- 6: orig-True-True: 0.804 (0.843)\n",
      "- 6: trans-True-True: 0.745 (0.863)\n",
      "- 6: sym-True-True: 0.765 (0.824)\n",
      "- 6: orig-False-False: 0.784 (0.902)\n",
      "- 6: sym-False-False: 0.529 (0.529)\n",
      "- 7: orig-True-False: 0.843 (0.922)\n",
      "- 7: trans-True-False: 0.824 (0.863)\n",
      "- 7: sym-True-False: 0.471 (0.471)\n",
      "- 7: orig-True-True: 0.824 (0.843)\n",
      "- 7: trans-True-True: 0.824 (0.863)\n",
      "- 7: sym-True-True: 0.843 (0.882)\n",
      "- 7: orig-False-False: 0.863 (0.902)\n",
      "- 7: sym-False-False: 0.882 (0.882)\n",
      "- 8: orig-True-False: 0.882 (0.922)\n",
      "- 8: trans-True-False: 0.843 (0.922)\n",
      "- 8: sym-True-False: 0.863 (0.902)\n",
      "- 8: orig-True-True: 0.745 (0.843)\n",
      "- 8: trans-True-True: 0.765 (0.843)\n",
      "- 8: sym-True-True: 0.745 (0.882)\n",
      "- 8: orig-False-False: 0.882 (0.922)\n",
      "- 8: sym-False-False: 0.510 (0.510)\n",
      "- 9: orig-True-False: 0.902 (0.922)\n",
      "- 9: trans-True-False: 0.804 (0.863)\n",
      "- 9: sym-True-False: 0.451 (0.451)\n",
      "- 9: orig-True-True: 0.804 (0.882)\n",
      "- 9: trans-True-True: 0.843 (0.902)\n",
      "- 9: sym-True-True: 0.843 (0.882)\n",
      "- 9: orig-False-False: 0.863 (0.902)\n",
      "- 9: sym-False-False: 0.451 (0.451)\n",
      "- 10: orig-True-False: 0.902 (0.922)\n",
      "- 10: trans-True-False: 0.863 (0.882)\n",
      "- 10: sym-True-False: 0.451 (0.451)\n",
      "- 10: orig-True-True: 0.863 (0.922)\n",
      "- 10: trans-True-True: 0.882 (0.902)\n",
      "- 10: sym-True-True: 0.863 (0.941)\n",
      "- 10: orig-False-False: 0.882 (0.922)\n",
      "- 10: sym-False-False: 0.451 (0.451)\n",
      "----- 24.37 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [{'GSO': 'orig', 'bias': True, 'bn': False},\n",
    "        {'GSO': 'trans', 'bias': True, 'bn': False},\n",
    "        {'GSO': 'sym', 'bias': True, 'bn': False},\n",
    "\n",
    "        {'GSO': 'orig', 'bias': True, 'bn': True},\n",
    "        {'GSO': 'trans', 'bias': True, 'bn': True},\n",
    "        {'GSO': 'sym', 'bias': True, 'bn': True},\n",
    "\n",
    "        {'GSO': 'orig', 'bias': False, 'bn': False},\n",
    "        {'GSO': 'sym', 'bias': False, 'bn': False},\n",
    "        ]\n",
    "\n",
    "best_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0, bias=False, batch_norm=exp['bn'])\n",
    "        \n",
    "        if exp['GSO'] == 'sym':\n",
    "            A_aux = (A + A.T)/2\n",
    "            A_aux = np.where(A_aux > 0, 1, 0)\n",
    "        elif exp['GSO'] == 'trans':\n",
    "            A_aux = A.T\n",
    "        else:\n",
    "            A_aux = A\n",
    "\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A_aux, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A_aux).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  K, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs3b[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs3b[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'- {i+1}: {exp[\"GSO\"]}-{exp[\"bias\"]}-{exp[\"bn\"]}: {best_val_accs3b[j,i]:.3f} ({best_accs3b[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"GSO\"]}-{exp[\"bias\"]}-{exp[\"bn\"]}' for exp in EXPS]\n",
    "table_over3b = summary_table(best_accs3b, index_name)\n",
    "table3b = summary_table(best_val_accs3b, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>orig-True-False</th>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.027730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trans-True-False</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.024880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sym-True-False</th>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.179162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig-True-True</th>\n",
       "      <td>0.807843</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.063112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trans-True-True</th>\n",
       "      <td>0.805882</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.049176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sym-True-True</th>\n",
       "      <td>0.817647</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.043888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig-False-False</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.032575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sym-False-False</th>\n",
       "      <td>0.558824</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.158630</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean accs       med       std\n",
       "orig-True-False    0.882353  0.882353  0.027730\n",
       "trans-True-False   0.837255  0.833333  0.024880\n",
       "sym-True-False     0.566667  0.490196  0.179162\n",
       "orig-True-True     0.807843  0.803922  0.063112\n",
       "trans-True-True    0.805882  0.803922  0.049176\n",
       "sym-True-True      0.817647  0.833333  0.043888\n",
       "orig-False-False   0.866667  0.872549  0.032575\n",
       "sym-False-False    0.558824  0.490196  0.158630"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearities and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.863)\n",
      "-1: ReLU()-Softmax(dim=1)-NLLLoss(): 0.882 (0.882)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.412 (0.549)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.431 (0.471)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.843)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.804 (0.863)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.804 (0.882)\n",
      "-1: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.725 (0.824)\n",
      "-1: Identity()-Softmax(dim=1)-NLLLoss(): 0.824 (0.843)\n",
      "-1: Identity()-Identity()-CrossEntropyLoss(): 0.588 (0.588)\n",
      "-2: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.922)\n",
      "-2: ReLU()-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.608 (0.667)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.529 (0.588)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.941)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.941)\n",
      "-2: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.922)\n",
      "-2: Identity()-Softmax(dim=1)-NLLLoss(): 0.824 (0.941)\n",
      "-2: Identity()-Identity()-CrossEntropyLoss(): 0.706 (0.725)\n",
      "-3: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-3: ReLU()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.431 (0.608)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.471 (0.549)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.804 (0.922)\n",
      "-3: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-3: Identity()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-3: Identity()-Identity()-CrossEntropyLoss(): 0.706 (0.706)\n",
      "-4: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.922 (0.961)\n",
      "-4: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.392 (0.549)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.451 (0.627)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.922 (0.961)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.922 (0.961)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.922 (0.961)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-4: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.961)\n",
      "-4: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-4: Identity()-Identity()-CrossEntropyLoss(): 0.784 (0.784)\n",
      "-5: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.882)\n",
      "-5: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.667 (0.706)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.431 (0.510)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.902)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.961)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-5: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-5: Identity()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-5: Identity()-Identity()-CrossEntropyLoss(): 0.608 (0.627)\n",
      "-6: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.902)\n",
      "-6: ReLU()-Softmax(dim=1)-NLLLoss(): 0.882 (0.902)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.294 (0.569)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.569 (0.667)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.902)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-6: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.882)\n",
      "-6: Identity()-Softmax(dim=1)-NLLLoss(): 0.765 (0.843)\n",
      "-6: Identity()-Identity()-CrossEntropyLoss(): 0.588 (0.608)\n",
      "-7: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-7: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.471 (0.529)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.431 (0.490)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.804 (0.922)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.941)\n",
      "-7: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-7: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.863)\n",
      "-7: Identity()-Identity()-CrossEntropyLoss(): 0.824 (0.824)\n",
      "-8: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.902)\n",
      "-8: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.412 (0.471)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.510 (0.569)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.882)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.902)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.902 (0.902)\n",
      "-8: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.863)\n",
      "-8: Identity()-Softmax(dim=1)-NLLLoss(): 0.804 (0.863)\n",
      "-8: Identity()-Identity()-CrossEntropyLoss(): 0.647 (0.667)\n",
      "-9: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-9: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.412 (0.569)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.431 (0.647)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.902)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-9: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.902)\n",
      "-9: Identity()-Softmax(dim=1)-NLLLoss(): 0.804 (0.863)\n",
      "-9: Identity()-Identity()-CrossEntropyLoss(): 0.686 (0.706)\n",
      "-10: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-10: ReLU()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.451 (0.569)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.471 (0.490)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.863 (0.882)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-10: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.902)\n",
      "-10: Identity()-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-10: Identity()-Identity()-CrossEntropyLoss(): 0.706 (0.706)\n",
      "----- 35.75 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        # {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},]\n",
    "\n",
    "best_accs4 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs4 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=exp['act'], last_act=exp['lact'],\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  K, masks, exp['loss'], device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs4[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs4[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}: {best_val_accs4[j,i]:.3f} ({best_accs4[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}' for exp in EXPS]\n",
    "table_over4 = summary_table(best_accs4, index_name)\n",
    "table4 = summary_table(best_val_accs4, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.044063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.019706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.454902</td>\n",
       "      <td>0.421569</td>\n",
       "      <td>0.102187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.472549</td>\n",
       "      <td>0.460784</td>\n",
       "      <td>0.045943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.037255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.038273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.019608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.029346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.051281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.040232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.684314</td>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.075152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean accs       med  \\\n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()             0.852941  0.843137   \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                      0.868627  0.872549   \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()          0.454902  0.421569   \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                   0.472549  0.460784   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()     0.856863  0.852941   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()              0.856863  0.872549   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...   0.882353  0.882353   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...   0.850980  0.852941   \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()         0.835294  0.843137   \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                  0.837255  0.833333   \n",
       "Identity()-Identity()-CrossEntropyLoss()             0.684314  0.696078   \n",
       "\n",
       "                                                         std  \n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()            0.044063  \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                     0.019706  \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()         0.102187  \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                  0.045943  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()    0.037255  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()             0.038273  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...  0.019608  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...  0.029346  \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()        0.051281  \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                 0.040232  \n",
       "Identity()-Identity()-CrossEntropyLoss()            0.075152  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the GSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PARAMETERS  -  0.874\n",
    "## Training params\n",
    "N_RUNS = 10\n",
    "NORM = True\n",
    "N_EPOCHS = 200  # 750? --> repeat training params section\n",
    "EPOCHS_h = 10 # 10\n",
    "EPOCHS_W = 10 # 5\n",
    "LR = .005\n",
    "WD = .001\n",
    "DROPOUT = .25\n",
    "\n",
    "# BEST PARAMETERS\n",
    "## Architecture params\n",
    "N_LAYERS = 3\n",
    "K = 2\n",
    "HID_DIM = 50  # 32\n",
    "\n",
    "## Model params\n",
    "h0 = 1  # 1\n",
    "\n",
    "IN_DIM = feat.shape[1]\n",
    "OUT_DIM = n_class\n",
    "\n",
    "ACT = nn.ReLU() \n",
    "LAST_ACT = nn.Softmax(dim=1)\n",
    "LOSS_FN = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training paramerters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 200-0.005-0.0001-0.25: 0.647 (0.725)\n",
      "-1: 200-0.005-0.0005-0.25: 0.804 (0.863)\n",
      "-1: 200-0.01-0.001-0.25: 0.804 (0.863)\n",
      "-1: 200-0.005-0.001-0.25: 0.824 (0.863)\n",
      "-1: 200-0.005-0.001-0.5: 0.843 (0.882)\n",
      "-1: 200-0.001-0.001-0.25: 0.647 (0.686)\n",
      "-1: 200-0.05-0.005-0.25: 0.804 (0.902)\n",
      "-1: 200-0.01-0.005-0.25: 0.843 (0.882)\n",
      "-1: 200-0.005-0.005-0.25: 0.824 (0.863)\n",
      "-1: 200-0.001-0.005-0.25: 0.765 (0.804)\n",
      "-1: 200-0.05-0.01-0.25: 0.804 (0.882)\n",
      "-1: 200-0.005-0.01-0.25: 0.824 (0.863)\n",
      "-1: 500-0.01-0.001-0.25: 0.784 (0.863)\n",
      "-1: 500-0.005-0.001-0.25: 0.843 (0.863)\n",
      "-1: 500-0.001-0.001-0.25: 0.745 (0.804)\n",
      "-1: 500-0.005-0.005-0.25: 0.843 (0.863)\n",
      "-1: 500-0.001-0.005-0.25: 0.804 (0.843)\n",
      "-1: 200-0.05-0.01-0.5: 0.824 (0.843)\n",
      "-1: 200-0.05-0.005-0.5: 0.824 (0.863)\n",
      "-1: 200-0.01-0.005-0.5: 0.804 (0.882)\n",
      "-1: 200-0.05-0.01-0: 0.784 (0.863)\n",
      "-1: 200-0.05-0.005-0: 0.804 (0.863)\n",
      "-1: 200-0.01-0.005-0: 0.784 (0.824)\n",
      "-1: 500-0.005-0.005-0.5: 0.784 (0.843)\n",
      "-1: 500-0.005-0.001-0.5: 0.745 (0.784)\n",
      "-1: 500-0.001-0.001-0.5: 0.667 (0.667)\n",
      "-1: 750-0.005-0.005-0.25: 0.804 (0.863)\n",
      "-1: 750-0.005-0.001-0.25: 0.843 (0.882)\n",
      "-1: 750-0.001-0.001-0.25: 0.784 (0.843)\n",
      "-2: 200-0.005-0.0001-0.25: 0.863 (0.902)\n",
      "-2: 200-0.005-0.0005-0.25: 0.882 (0.922)\n",
      "-2: 200-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-2: 200-0.005-0.001-0.25: 0.863 (0.922)\n",
      "-2: 200-0.005-0.001-0.5: 0.824 (0.882)\n",
      "-2: 200-0.001-0.001-0.25: 0.804 (0.824)\n",
      "-2: 200-0.05-0.005-0.25: 0.824 (0.902)\n",
      "-2: 200-0.01-0.005-0.25: 0.824 (0.922)\n",
      "-2: 200-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-2: 200-0.001-0.005-0.25: 0.863 (0.882)\n",
      "-2: 200-0.05-0.01-0.25: 0.882 (0.922)\n",
      "-2: 200-0.005-0.01-0.25: 0.863 (0.922)\n",
      "-2: 500-0.01-0.001-0.25: 0.824 (0.902)\n",
      "-2: 500-0.005-0.001-0.25: 0.863 (0.922)\n",
      "-2: 500-0.001-0.001-0.25: 0.843 (0.882)\n",
      "-2: 500-0.005-0.005-0.25: 0.882 (0.922)\n",
      "-2: 500-0.001-0.005-0.25: 0.843 (0.902)\n",
      "-2: 200-0.05-0.01-0.5: 0.863 (0.922)\n",
      "-2: 200-0.05-0.005-0.5: 0.863 (0.922)\n",
      "-2: 200-0.01-0.005-0.5: 0.863 (0.902)\n",
      "-2: 200-0.05-0.01-0: 0.824 (0.902)\n",
      "-2: 200-0.05-0.005-0: 0.804 (0.922)\n",
      "-2: 200-0.01-0.005-0: 0.863 (0.882)\n",
      "-2: 500-0.005-0.005-0.5: 0.843 (0.902)\n",
      "-2: 500-0.005-0.001-0.5: 0.843 (0.902)\n",
      "-2: 500-0.001-0.001-0.5: 0.706 (0.725)\n",
      "-2: 750-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-2: 750-0.005-0.001-0.25: 0.824 (0.902)\n",
      "-2: 750-0.001-0.001-0.25: 0.863 (0.922)\n",
      "-3: 200-0.005-0.0001-0.25: 0.725 (0.784)\n",
      "-3: 200-0.005-0.0005-0.25: 0.804 (0.882)\n",
      "-3: 200-0.01-0.001-0.25: 0.843 (0.941)\n",
      "-3: 200-0.005-0.001-0.25: 0.843 (0.902)\n",
      "-3: 200-0.005-0.001-0.5: 0.824 (0.843)\n",
      "-3: 200-0.001-0.001-0.25: 0.686 (0.686)\n",
      "-3: 200-0.05-0.005-0.25: 0.824 (0.922)\n",
      "-3: 200-0.01-0.005-0.25: 0.922 (0.922)\n",
      "-3: 200-0.005-0.005-0.25: 0.902 (0.941)\n",
      "-3: 200-0.001-0.005-0.25: 0.843 (0.863)\n",
      "-3: 200-0.05-0.01-0.25: 0.863 (0.941)\n",
      "-3: 200-0.005-0.01-0.25: 0.863 (0.922)\n",
      "-3: 500-0.01-0.001-0.25: 0.922 (0.922)\n",
      "-3: 500-0.005-0.001-0.25: 0.882 (0.941)\n",
      "-3: 500-0.001-0.001-0.25: 0.784 (0.843)\n",
      "-3: 500-0.005-0.005-0.25: 0.882 (0.941)\n",
      "-3: 500-0.001-0.005-0.25: 0.863 (0.922)\n",
      "-3: 200-0.05-0.01-0.5: 0.804 (0.902)\n",
      "-3: 200-0.05-0.005-0.5: 0.863 (0.882)\n",
      "-3: 200-0.01-0.005-0.5: 0.882 (0.922)\n",
      "-3: 200-0.05-0.01-0: 0.882 (0.941)\n",
      "-3: 200-0.05-0.005-0: 0.804 (0.922)\n",
      "-3: 200-0.01-0.005-0: 0.863 (0.882)\n",
      "-3: 500-0.005-0.005-0.5: 0.882 (0.922)\n",
      "-3: 500-0.005-0.001-0.5: 0.706 (0.804)\n",
      "-3: 500-0.001-0.001-0.5: 0.667 (0.667)\n",
      "-3: 750-0.005-0.005-0.25: 0.882 (0.922)\n",
      "-3: 750-0.005-0.001-0.25: 0.863 (0.922)\n",
      "-3: 750-0.001-0.001-0.25: 0.863 (0.882)\n",
      "-4: 200-0.005-0.0001-0.25: 0.824 (0.863)\n",
      "-4: 200-0.005-0.0005-0.25: 0.882 (0.922)\n",
      "-4: 200-0.01-0.001-0.25: 0.863 (0.922)\n",
      "-4: 200-0.005-0.001-0.25: 0.882 (0.961)\n",
      "-4: 200-0.005-0.001-0.5: 0.784 (0.882)\n",
      "-4: 200-0.001-0.001-0.25: 0.745 (0.765)\n",
      "-4: 200-0.05-0.005-0.25: 0.745 (0.902)\n",
      "-4: 200-0.01-0.005-0.25: 0.902 (0.961)\n",
      "-4: 200-0.005-0.005-0.25: 0.902 (0.961)\n",
      "-4: 200-0.001-0.005-0.25: 0.824 (0.882)\n",
      "-4: 200-0.05-0.01-0.25: 0.882 (0.922)\n",
      "-4: 200-0.005-0.01-0.25: 0.922 (0.922)\n",
      "-4: 500-0.01-0.001-0.25: 0.804 (0.922)\n",
      "-4: 500-0.005-0.001-0.25: 0.882 (0.941)\n",
      "-4: 500-0.001-0.001-0.25: 0.765 (0.961)\n",
      "-4: 500-0.005-0.005-0.25: 0.902 (0.941)\n",
      "-4: 500-0.001-0.005-0.25: 0.863 (0.922)\n",
      "-4: 200-0.05-0.01-0.5: 0.843 (0.863)\n",
      "-4: 200-0.05-0.005-0.5: 0.824 (0.843)\n",
      "-4: 200-0.01-0.005-0.5: 0.882 (0.902)\n",
      "-4: 200-0.05-0.01-0: 0.902 (0.922)\n",
      "-4: 200-0.05-0.005-0: 0.804 (0.941)\n",
      "-4: 200-0.01-0.005-0: 0.922 (0.941)\n",
      "-4: 500-0.005-0.005-0.5: 0.882 (0.941)\n",
      "-4: 500-0.005-0.001-0.5: 0.784 (0.882)\n",
      "-4: 500-0.001-0.001-0.5: 0.725 (0.745)\n",
      "-4: 750-0.005-0.005-0.25: 0.922 (0.961)\n",
      "-4: 750-0.005-0.001-0.25: 0.922 (0.922)\n",
      "-4: 750-0.001-0.001-0.25: 0.941 (0.961)\n",
      "-5: 200-0.005-0.0001-0.25: 0.824 (0.843)\n",
      "-5: 200-0.005-0.0005-0.25: 0.804 (0.902)\n",
      "-5: 200-0.01-0.001-0.25: 0.882 (0.941)\n",
      "-5: 200-0.005-0.001-0.25: 0.922 (0.922)\n",
      "-5: 200-0.005-0.001-0.5: 0.804 (0.882)\n",
      "-5: 200-0.001-0.001-0.25: 0.608 (0.725)\n",
      "-5: 200-0.05-0.005-0.25: 0.824 (0.922)\n",
      "-5: 200-0.01-0.005-0.25: 0.863 (0.922)\n",
      "-5: 200-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-5: 200-0.001-0.005-0.25: 0.784 (0.784)\n",
      "-5: 200-0.05-0.01-0.25: 0.804 (0.961)\n",
      "-5: 200-0.005-0.01-0.25: 0.863 (0.902)\n",
      "-5: 500-0.01-0.001-0.25: 0.843 (0.941)\n",
      "-5: 500-0.005-0.001-0.25: 0.843 (0.922)\n",
      "-5: 500-0.001-0.001-0.25: 0.765 (0.843)\n",
      "-5: 500-0.005-0.005-0.25: 0.804 (0.902)\n",
      "-5: 500-0.001-0.005-0.25: 0.784 (0.882)\n",
      "-5: 200-0.05-0.01-0.5: 0.804 (0.922)\n",
      "-5: 200-0.05-0.005-0.5: 0.725 (0.882)\n",
      "-5: 200-0.01-0.005-0.5: 0.784 (0.922)\n",
      "-5: 200-0.05-0.01-0: 0.843 (0.922)\n",
      "-5: 200-0.05-0.005-0: 0.843 (0.882)\n",
      "-5: 200-0.01-0.005-0: 0.784 (0.843)\n",
      "-5: 500-0.005-0.005-0.5: 0.745 (0.882)\n",
      "-5: 500-0.005-0.001-0.5: 0.824 (0.882)\n",
      "-5: 500-0.001-0.001-0.5: 0.706 (0.725)\n",
      "-5: 750-0.005-0.005-0.25: 0.863 (0.922)\n",
      "-5: 750-0.005-0.001-0.25: 0.922 (0.941)\n",
      "-5: 750-0.001-0.001-0.25: 0.784 (0.804)\n",
      "-6: 200-0.005-0.0001-0.25: 0.784 (0.824)\n",
      "-6: 200-0.005-0.0005-0.25: 0.824 (0.863)\n",
      "-6: 200-0.01-0.001-0.25: 0.824 (0.882)\n",
      "-6: 200-0.005-0.001-0.25: 0.824 (0.882)\n",
      "-6: 200-0.005-0.001-0.5: 0.784 (0.804)\n",
      "-6: 200-0.001-0.001-0.25: 0.667 (0.725)\n",
      "-6: 200-0.05-0.005-0.25: 0.804 (0.882)\n",
      "-6: 200-0.01-0.005-0.25: 0.843 (0.922)\n",
      "-6: 200-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-6: 200-0.001-0.005-0.25: 0.863 (0.863)\n",
      "-6: 200-0.05-0.01-0.25: 0.824 (0.882)\n",
      "-6: 200-0.005-0.01-0.25: 0.863 (0.882)\n",
      "-6: 500-0.01-0.001-0.25: 0.863 (0.882)\n",
      "-6: 500-0.005-0.001-0.25: 0.863 (0.922)\n",
      "-6: 500-0.001-0.001-0.25: 0.765 (0.843)\n",
      "-6: 500-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-6: 500-0.001-0.005-0.25: 0.882 (0.882)\n",
      "-6: 200-0.05-0.01-0.5: 0.745 (0.843)\n",
      "-6: 200-0.05-0.005-0.5: 0.804 (0.824)\n",
      "-6: 200-0.01-0.005-0.5: 0.863 (0.882)\n",
      "-6: 200-0.05-0.01-0: 0.843 (0.902)\n",
      "-6: 200-0.05-0.005-0: 0.843 (0.882)\n",
      "-6: 200-0.01-0.005-0: 0.824 (0.843)\n",
      "-6: 500-0.005-0.005-0.5: 0.863 (0.882)\n",
      "-6: 500-0.005-0.001-0.5: 0.725 (0.843)\n",
      "-6: 500-0.001-0.001-0.5: 0.667 (0.706)\n",
      "-6: 750-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-6: 750-0.005-0.001-0.25: 0.824 (0.902)\n",
      "-6: 750-0.001-0.001-0.25: 0.824 (0.843)\n",
      "-7: 200-0.005-0.0001-0.25: 0.824 (0.902)\n",
      "-7: 200-0.005-0.0005-0.25: 0.843 (0.902)\n",
      "-7: 200-0.01-0.001-0.25: 0.843 (0.922)\n",
      "-7: 200-0.005-0.001-0.25: 0.843 (0.902)\n",
      "-7: 200-0.005-0.001-0.5: 0.843 (0.882)\n",
      "-7: 200-0.001-0.001-0.25: 0.804 (0.843)\n",
      "-7: 200-0.05-0.005-0.25: 0.863 (0.902)\n",
      "-7: 200-0.01-0.005-0.25: 0.882 (0.902)\n",
      "-7: 200-0.005-0.005-0.25: 0.843 (0.902)\n",
      "-7: 200-0.001-0.005-0.25: 0.843 (0.863)\n",
      "-7: 200-0.05-0.01-0.25: 0.863 (0.902)\n",
      "-7: 200-0.005-0.01-0.25: 0.863 (0.902)\n",
      "-7: 500-0.01-0.001-0.25: 0.843 (0.922)\n",
      "-7: 500-0.005-0.001-0.25: 0.922 (0.922)\n",
      "-7: 500-0.001-0.001-0.25: 0.863 (0.863)\n",
      "-7: 500-0.005-0.005-0.25: 0.843 (0.902)\n",
      "-7: 500-0.001-0.005-0.25: 0.863 (0.882)\n",
      "-7: 200-0.05-0.01-0.5: 0.843 (0.902)\n",
      "-7: 200-0.05-0.005-0.5: 0.882 (0.922)\n",
      "-7: 200-0.01-0.005-0.5: 0.843 (0.902)\n",
      "-7: 200-0.05-0.01-0: 0.843 (0.882)\n",
      "-7: 200-0.05-0.005-0: 0.863 (0.882)\n",
      "-7: 200-0.01-0.005-0: 0.824 (0.824)\n",
      "-7: 500-0.005-0.005-0.5: 0.863 (0.902)\n",
      "-7: 500-0.005-0.001-0.5: 0.863 (0.902)\n",
      "-7: 500-0.001-0.001-0.5: 0.824 (0.824)\n",
      "-7: 750-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-7: 750-0.005-0.001-0.25: 0.863 (0.902)\n",
      "-7: 750-0.001-0.001-0.25: 0.882 (0.882)\n",
      "-8: 200-0.005-0.0001-0.25: 0.804 (0.843)\n",
      "-8: 200-0.005-0.0005-0.25: 0.765 (0.843)\n",
      "-8: 200-0.01-0.001-0.25: 0.765 (0.882)\n",
      "-8: 200-0.005-0.001-0.25: 0.843 (0.902)\n",
      "-8: 200-0.005-0.001-0.5: 0.725 (0.784)\n",
      "-8: 200-0.001-0.001-0.25: 0.627 (0.647)\n",
      "-8: 200-0.05-0.005-0.25: 0.804 (0.882)\n",
      "-8: 200-0.01-0.005-0.25: 0.863 (0.922)\n",
      "-8: 200-0.005-0.005-0.25: 0.824 (0.882)\n",
      "-8: 200-0.001-0.005-0.25: 0.784 (0.804)\n",
      "-8: 200-0.05-0.01-0.25: 0.882 (0.882)\n",
      "-8: 200-0.005-0.01-0.25: 0.843 (0.882)\n",
      "-8: 500-0.01-0.001-0.25: 0.824 (0.902)\n",
      "-8: 500-0.005-0.001-0.25: 0.863 (0.902)\n",
      "-8: 500-0.001-0.001-0.25: 0.804 (0.804)\n",
      "-8: 500-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-8: 500-0.001-0.005-0.25: 0.804 (0.843)\n",
      "-8: 200-0.05-0.01-0.5: 0.765 (0.882)\n",
      "-8: 200-0.05-0.005-0.5: 0.843 (0.882)\n",
      "-8: 200-0.01-0.005-0.5: 0.824 (0.843)\n",
      "-8: 200-0.05-0.01-0: 0.843 (0.902)\n",
      "-8: 200-0.05-0.005-0: 0.784 (0.863)\n",
      "-8: 200-0.01-0.005-0: 0.765 (0.784)\n",
      "-8: 500-0.005-0.005-0.5: 0.843 (0.863)\n",
      "-8: 500-0.005-0.001-0.5: 0.784 (0.863)\n",
      "-8: 500-0.001-0.001-0.5: 0.627 (0.706)\n",
      "-8: 750-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-8: 750-0.005-0.001-0.25: 0.804 (0.882)\n",
      "-8: 750-0.001-0.001-0.25: 0.765 (0.784)\n",
      "-9: 200-0.005-0.0001-0.25: 0.824 (0.843)\n",
      "-9: 200-0.005-0.0005-0.25: 0.843 (0.902)\n",
      "-9: 200-0.01-0.001-0.25: 0.843 (0.902)\n",
      "-9: 200-0.005-0.001-0.25: 0.824 (0.902)\n",
      "-9: 200-0.005-0.001-0.5: 0.824 (0.863)\n",
      "-9: 200-0.001-0.001-0.25: 0.667 (0.725)\n",
      "-9: 200-0.05-0.005-0.25: 0.824 (0.863)\n",
      "-9: 200-0.01-0.005-0.25: 0.824 (0.882)\n",
      "-9: 200-0.005-0.005-0.25: 0.843 (0.882)\n",
      "-9: 200-0.001-0.005-0.25: 0.765 (0.824)\n",
      "-9: 200-0.05-0.01-0.25: 0.804 (0.882)\n",
      "-9: 200-0.005-0.01-0.25: 0.824 (0.863)\n",
      "-9: 500-0.01-0.001-0.25: 0.784 (0.882)\n",
      "-9: 500-0.005-0.001-0.25: 0.863 (0.902)\n",
      "-9: 500-0.001-0.001-0.25: 0.706 (0.784)\n",
      "-9: 500-0.005-0.005-0.25: 0.843 (0.882)\n",
      "-9: 500-0.001-0.005-0.25: 0.824 (0.824)\n",
      "-9: 200-0.05-0.01-0.5: 0.804 (0.882)\n",
      "-9: 200-0.05-0.005-0.5: 0.824 (0.863)\n",
      "-9: 200-0.01-0.005-0.5: 0.863 (0.882)\n",
      "-9: 200-0.05-0.01-0: 0.824 (0.843)\n",
      "-9: 200-0.05-0.005-0: 0.784 (0.843)\n",
      "-9: 200-0.01-0.005-0: 0.824 (0.843)\n",
      "-9: 500-0.005-0.005-0.5: 0.784 (0.843)\n",
      "-9: 500-0.005-0.001-0.5: 0.863 (0.882)\n",
      "-9: 500-0.001-0.001-0.5: 0.686 (0.725)\n",
      "-9: 750-0.005-0.005-0.25: 0.824 (0.882)\n",
      "-9: 750-0.005-0.001-0.25: 0.843 (0.882)\n",
      "-9: 750-0.001-0.001-0.25: 0.824 (0.863)\n",
      "-10: 200-0.005-0.0001-0.25: 0.824 (0.882)\n",
      "-10: 200-0.005-0.0005-0.25: 0.863 (0.902)\n",
      "-10: 200-0.01-0.001-0.25: 0.804 (0.922)\n",
      "-10: 200-0.005-0.001-0.25: 0.902 (0.902)\n",
      "-10: 200-0.005-0.001-0.5: 0.824 (0.882)\n",
      "-10: 200-0.001-0.001-0.25: 0.725 (0.765)\n",
      "-10: 200-0.05-0.005-0.25: 0.863 (0.902)\n",
      "-10: 200-0.01-0.005-0.25: 0.902 (0.902)\n",
      "-10: 200-0.005-0.005-0.25: 0.882 (0.902)\n",
      "-10: 200-0.001-0.005-0.25: 0.765 (0.804)\n",
      "-10: 200-0.05-0.01-0.25: 0.882 (0.941)\n",
      "-10: 200-0.005-0.01-0.25: 0.882 (0.882)\n",
      "-10: 500-0.01-0.001-0.25: 0.882 (0.922)\n",
      "-10: 500-0.005-0.001-0.25: 0.824 (0.902)\n",
      "-10: 500-0.001-0.001-0.25: 0.863 (0.882)\n",
      "-10: 500-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-10: 500-0.001-0.005-0.25: 0.843 (0.882)\n",
      "-10: 200-0.05-0.01-0.5: 0.843 (0.902)\n",
      "-10: 200-0.05-0.005-0.5: 0.804 (0.882)\n",
      "-10: 200-0.01-0.005-0.5: 0.843 (0.902)\n",
      "-10: 200-0.05-0.01-0: 0.843 (0.902)\n",
      "-10: 200-0.05-0.005-0: 0.843 (0.922)\n",
      "-10: 200-0.01-0.005-0: 0.843 (0.843)\n",
      "-10: 500-0.005-0.005-0.5: 0.824 (0.863)\n",
      "-10: 500-0.005-0.001-0.5: 0.804 (0.863)\n",
      "-10: 500-0.001-0.001-0.5: 0.745 (0.804)\n",
      "-10: 750-0.005-0.005-0.25: 0.863 (0.902)\n",
      "-10: 750-0.005-0.001-0.25: 0.863 (0.902)\n",
      "-10: 750-0.001-0.001-0.25: 0.843 (0.882)\n",
      "----- 55.59 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-4, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-4, 'drop': .25},\n",
    "\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "        \n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-2, 'drop': .25},\n",
    "\n",
    "        {'epochs': 500, 'lr': .01, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "\n",
    "\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .05, 'wd': 5e-3, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .01, 'wd': 5e-3, 'drop': 0},\n",
    "\n",
    "        \n",
    "        {'epochs': 500, 'lr': .005, 'wd': 5e-3, 'drop': .5},\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 500, 'lr': .001, 'wd': 1e-3, 'drop': .5},\n",
    "\n",
    "        {'epochs': 750, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 750, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 750, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "        ]\n",
    "\n",
    "best_accs5 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs5 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=exp['drop'], init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S, K, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, exp['epochs'], exp['lr'], exp['wd'], epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs5[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs5[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}: {best_val_accs5[j,i]:.3f} ({best_accs5[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}' for exp in EXPS]\n",
    "table_over5 = summary_table(best_accs5, index_name)\n",
    "table5 = summary_table(best_val_accs5, index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0001-0.25</th>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.059635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.0005-0.25</th>\n",
       "      <td>0.831373</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.036367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.001-0.25</th>\n",
       "      <td>0.831373</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.031859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.25</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.032869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.5</th>\n",
       "      <td>0.807843</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.033735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.001-0.25</th>\n",
       "      <td>0.698039</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.065737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0.25</th>\n",
       "      <td>0.817647</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.031677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.25</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.032575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.005-0.25</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.026956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.001-0.005-0.25</th>\n",
       "      <td>0.809804</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.039265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.25</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.034018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.01-0.25</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.026956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.01-0.001-0.25</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.041176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0.25</th>\n",
       "      <td>0.864706</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.025490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.001-0.25</th>\n",
       "      <td>0.790196</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.049643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.005-0.25</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.026013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.005-0.25</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.030440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0.5</th>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.035349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0.5</th>\n",
       "      <td>0.825490</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.041548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0.5</th>\n",
       "      <td>0.845098</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.030941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.01-0</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.030376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.05-0.005-0</th>\n",
       "      <td>0.817647</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.026380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.01-0.005-0</th>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.043888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.005-0.5</th>\n",
       "      <td>0.831373</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.044019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0.5</th>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.052796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.001-0.5</th>\n",
       "      <td>0.701961</td>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.051729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750-0.005-0.005-0.25</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.029672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750-0.005-0.001-0.25</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.037255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750-0.001-0.001-0.25</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.050412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean accs       med       std\n",
       "200-0.005-0.0001-0.25   0.794118  0.823529  0.059635\n",
       "200-0.005-0.0005-0.25   0.831373  0.833333  0.036367\n",
       "200-0.01-0.001-0.25     0.831373  0.843137  0.031859\n",
       "200-0.005-0.001-0.25    0.856863  0.843137  0.032869\n",
       "200-0.005-0.001-0.5     0.807843  0.823529  0.033735\n",
       "200-0.001-0.001-0.25    0.698039  0.676471  0.065737\n",
       "200-0.05-0.005-0.25     0.817647  0.823529  0.031677\n",
       "200-0.01-0.005-0.25     0.866667  0.862745  0.032575\n",
       "200-0.005-0.005-0.25    0.860784  0.862745  0.026956\n",
       "200-0.001-0.005-0.25    0.809804  0.803922  0.039265\n",
       "200-0.05-0.01-0.25      0.849020  0.862745  0.034018\n",
       "200-0.005-0.01-0.25     0.860784  0.862745  0.026956\n",
       "500-0.01-0.001-0.25     0.837255  0.833333  0.041176\n",
       "500-0.005-0.001-0.25    0.864706  0.862745  0.025490\n",
       "500-0.001-0.001-0.25    0.790196  0.774510  0.049643\n",
       "500-0.005-0.005-0.25    0.858824  0.862745  0.026013\n",
       "500-0.001-0.005-0.25    0.837255  0.843137  0.030440\n",
       "200-0.05-0.01-0.5       0.813725  0.813725  0.035349\n",
       "200-0.05-0.005-0.5      0.825490  0.823529  0.041548\n",
       "200-0.01-0.005-0.5      0.845098  0.852941  0.030941\n",
       "200-0.05-0.01-0         0.843137  0.843137  0.030376\n",
       "200-0.05-0.005-0        0.817647  0.803922  0.026380\n",
       "200-0.01-0.005-0        0.829412  0.823529  0.043888\n",
       "500-0.005-0.005-0.5     0.831373  0.843137  0.044019\n",
       "500-0.005-0.001-0.5     0.794118  0.794118  0.052796\n",
       "500-0.001-0.001-0.5     0.701961  0.696078  0.051729\n",
       "750-0.005-0.005-0.25    0.860784  0.862745  0.029672\n",
       "750-0.005-0.001-0.25    0.856863  0.852941  0.037255\n",
       "750-0.001-0.001-0.25    0.837255  0.833333  0.050412"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 0.01-1-1-True: 0.471 (0.569)\n",
      "-1: 0.1-1-1-True: 0.490 (0.549)\n",
      "-1: 1-1-1-True: 0.608 (0.647)\n",
      "-1: 1-1-1-True: 0.843 (0.843)\n",
      "-1: 1-1-1-True: 0.725 (0.745)\n",
      "-1: 1-1-1-False: 0.588 (0.667)\n",
      "-1: 1-5-1-True: 0.765 (0.784)\n",
      "-1: 1-10-1-True: 0.765 (0.804)\n",
      "-1: 1-10-5-True: 0.824 (0.863)\n",
      "-1: 1-1-5-True: 0.627 (0.686)\n",
      "-1: 1-1-10-True: 0.588 (0.745)\n",
      "-1: 1-5-10-True: 0.745 (0.824)\n",
      "-1: 0.1-10-10-True: 0.784 (0.863)\n",
      "-1: 1-10-10-True: 0.863 (0.863)\n",
      "-1: 1-10-10-True: 0.804 (0.863)\n",
      "-1: 1-25-25-True: 0.804 (0.863)\n",
      "-1: 1-25-25-True: 0.843 (0.863)\n",
      "-1: 0.1-25-25-True: 0.824 (0.882)\n",
      "-1: 1-25-25-True: 0.843 (0.882)\n",
      "-1: 1-50-50-True: 0.843 (0.863)\n",
      "-2: 0.01-1-1-True: 0.569 (0.686)\n",
      "-2: 0.1-1-1-True: 0.706 (0.784)\n",
      "-2: 1-1-1-True: 0.627 (0.745)\n",
      "-2: 1-1-1-True: 0.843 (0.902)\n",
      "-2: 1-1-1-True: 0.843 (0.882)\n",
      "-2: 1-1-1-False: 0.804 (0.882)\n",
      "-2: 1-5-1-True: 0.863 (0.882)\n",
      "-2: 1-10-1-True: 0.765 (0.922)\n",
      "-2: 1-10-5-True: 0.843 (0.902)\n",
      "-2: 1-1-5-True: 0.784 (0.843)\n",
      "-2: 1-1-10-True: 0.725 (0.804)\n",
      "-2: 1-5-10-True: 0.784 (0.922)\n",
      "-2: 0.1-10-10-True: 0.863 (0.902)\n",
      "-2: 1-10-10-True: 0.863 (0.902)\n",
      "-2: 1-10-10-True: 0.804 (0.902)\n",
      "-2: 1-25-25-True: 0.843 (0.922)\n",
      "-2: 1-25-25-True: 0.882 (0.922)\n",
      "-2: 0.1-25-25-True: 0.824 (0.902)\n",
      "-2: 1-25-25-True: 0.863 (0.902)\n",
      "-2: 1-50-50-True: 0.863 (0.922)\n",
      "-3: 0.01-1-1-True: 0.529 (0.627)\n",
      "-3: 0.1-1-1-True: 0.725 (0.725)\n",
      "-3: 1-1-1-True: 0.725 (0.725)\n",
      "-3: 1-1-1-True: 0.824 (0.843)\n",
      "-3: 1-1-1-True: 0.843 (0.863)\n",
      "-3: 1-1-1-False: 0.843 (0.843)\n",
      "-3: 1-5-1-True: 0.765 (0.863)\n",
      "-3: 1-10-1-True: 0.824 (0.824)\n",
      "-3: 1-10-5-True: 0.882 (0.882)\n",
      "-3: 1-1-5-True: 0.686 (0.706)\n",
      "-3: 1-1-10-True: 0.627 (0.725)\n",
      "-3: 1-5-10-True: 0.843 (0.922)\n",
      "-3: 0.1-10-10-True: 0.843 (0.922)\n",
      "-3: 1-10-10-True: 0.843 (0.922)\n",
      "-3: 1-10-10-True: 0.863 (0.922)\n",
      "-3: 1-25-25-True: 0.824 (0.902)\n",
      "-3: 1-25-25-True: 0.902 (0.922)\n",
      "-3: 0.1-25-25-True: 0.882 (0.941)\n",
      "-3: 1-25-25-True: 0.863 (0.941)\n",
      "-3: 1-50-50-True: 0.863 (0.941)\n",
      "-4: 0.01-1-1-True: 0.667 (0.706)\n",
      "-4: 0.1-1-1-True: 0.608 (0.627)\n",
      "-4: 1-1-1-True: 0.667 (0.725)\n",
      "-4: 1-1-1-True: 0.549 (0.647)\n",
      "-4: 1-1-1-True: 0.765 (0.804)\n",
      "-4: 1-1-1-False: 0.804 (0.902)\n",
      "-4: 1-5-1-True: 0.843 (0.882)\n",
      "-4: 1-10-1-True: 0.745 (0.765)\n",
      "-4: 1-10-5-True: 0.902 (0.941)\n",
      "-4: 1-1-5-True: 0.706 (0.765)\n",
      "-4: 1-1-10-True: 0.784 (0.824)\n",
      "-4: 1-5-10-True: 0.922 (0.961)\n",
      "-4: 0.1-10-10-True: 0.922 (0.941)\n",
      "-4: 1-10-10-True: 0.902 (0.941)\n",
      "-4: 1-10-10-True: 0.902 (0.961)\n",
      "-4: 1-25-25-True: 0.922 (0.922)\n",
      "-4: 1-25-25-True: 0.843 (0.941)\n",
      "-4: 0.1-25-25-True: 0.922 (0.941)\n",
      "-4: 1-25-25-True: 0.882 (0.922)\n",
      "-4: 1-50-50-True: 0.863 (0.941)\n",
      "-5: 0.01-1-1-True: 0.471 (0.490)\n",
      "-5: 0.1-1-1-True: 0.412 (0.529)\n",
      "-5: 1-1-1-True: 0.627 (0.647)\n",
      "-5: 1-1-1-True: 0.725 (0.784)\n",
      "-5: 1-1-1-True: 0.725 (0.843)\n",
      "-5: 1-1-1-False: 0.686 (0.824)\n",
      "-5: 1-5-1-True: 0.804 (0.824)\n",
      "-5: 1-10-1-True: 0.804 (0.804)\n",
      "-5: 1-10-5-True: 0.765 (0.902)\n",
      "-5: 1-1-5-True: 0.706 (0.765)\n",
      "-5: 1-1-10-True: 0.686 (0.784)\n",
      "-5: 1-5-10-True: 0.824 (0.882)\n",
      "-5: 0.1-10-10-True: 0.843 (0.922)\n",
      "-5: 1-10-10-True: 0.882 (0.922)\n",
      "-5: 1-10-10-True: 0.882 (0.922)\n",
      "-5: 1-25-25-True: 0.784 (0.882)\n",
      "-5: 1-25-25-True: 0.843 (0.922)\n",
      "-5: 0.1-25-25-True: 0.902 (0.922)\n",
      "-5: 1-25-25-True: 0.941 (0.941)\n",
      "-5: 1-50-50-True: 0.902 (0.941)\n",
      "-6: 0.01-1-1-True: 0.588 (0.647)\n",
      "-6: 0.1-1-1-True: 0.627 (0.667)\n",
      "-6: 1-1-1-True: 0.706 (0.725)\n",
      "-6: 1-1-1-True: 0.804 (0.863)\n",
      "-6: 1-1-1-True: 0.725 (0.784)\n",
      "-6: 1-1-1-False: 0.608 (0.686)\n",
      "-6: 1-5-1-True: 0.784 (0.804)\n",
      "-6: 1-10-1-True: 0.784 (0.784)\n",
      "-6: 1-10-5-True: 0.824 (0.902)\n",
      "-6: 1-1-5-True: 0.667 (0.706)\n",
      "-6: 1-1-10-True: 0.627 (0.725)\n",
      "-6: 1-5-10-True: 0.882 (0.902)\n",
      "-6: 0.1-10-10-True: 0.863 (0.902)\n",
      "-6: 1-10-10-True: 0.863 (0.902)\n",
      "-6: 1-10-10-True: 0.843 (0.902)\n",
      "-6: 1-25-25-True: 0.843 (0.882)\n",
      "-6: 1-25-25-True: 0.863 (0.882)\n",
      "-6: 0.1-25-25-True: 0.882 (0.902)\n",
      "-6: 1-25-25-True: 0.882 (0.922)\n",
      "-6: 1-50-50-True: 0.882 (0.922)\n",
      "-7: 0.01-1-1-True: 0.627 (0.667)\n",
      "-7: 0.1-1-1-True: 0.569 (0.588)\n",
      "-7: 1-1-1-True: 0.706 (0.706)\n",
      "-7: 1-1-1-True: 0.843 (0.882)\n",
      "-7: 1-1-1-True: 0.804 (0.804)\n",
      "-7: 1-1-1-False: 0.765 (0.843)\n",
      "-7: 1-5-1-True: 0.804 (0.843)\n",
      "-7: 1-10-1-True: 0.745 (0.824)\n",
      "-7: 1-10-5-True: 0.863 (0.902)\n",
      "-7: 1-1-5-True: 0.745 (0.784)\n",
      "-7: 1-1-10-True: 0.824 (0.824)\n",
      "-7: 1-5-10-True: 0.863 (0.922)\n",
      "-7: 0.1-10-10-True: 0.863 (0.902)\n",
      "-7: 1-10-10-True: 0.882 (0.902)\n",
      "-7: 1-10-10-True: 0.863 (0.902)\n",
      "-7: 1-25-25-True: 0.824 (0.863)\n",
      "-7: 1-25-25-True: 0.902 (0.902)\n",
      "-7: 0.1-25-25-True: 0.843 (0.902)\n",
      "-7: 1-25-25-True: 0.824 (0.902)\n",
      "-7: 1-50-50-True: 0.882 (0.902)\n",
      "-8: 0.01-1-1-True: 0.529 (0.588)\n",
      "-8: 0.1-1-1-True: 0.529 (0.569)\n",
      "-8: 1-1-1-True: 0.471 (0.510)\n",
      "-8: 1-1-1-True: 0.608 (0.706)\n",
      "-8: 1-1-1-True: 0.804 (0.863)\n",
      "-8: 1-1-1-False: 0.706 (0.784)\n",
      "-8: 1-5-1-True: 0.647 (0.725)\n",
      "-8: 1-10-1-True: 0.706 (0.725)\n",
      "-8: 1-10-5-True: 0.745 (0.804)\n",
      "-8: 1-1-5-True: 0.608 (0.667)\n",
      "-8: 1-1-10-True: 0.667 (0.725)\n",
      "-8: 1-5-10-True: 0.824 (0.882)\n",
      "-8: 0.1-10-10-True: 0.824 (0.882)\n",
      "-8: 1-10-10-True: 0.843 (0.902)\n",
      "-8: 1-10-10-True: 0.882 (0.902)\n",
      "-8: 1-25-25-True: 0.745 (0.824)\n",
      "-8: 1-25-25-True: 0.824 (0.843)\n",
      "-8: 0.1-25-25-True: 0.863 (0.882)\n",
      "-8: 1-25-25-True: 0.804 (0.882)\n",
      "-8: 1-50-50-True: 0.843 (0.882)\n",
      "-9: 0.01-1-1-True: 0.647 (0.686)\n",
      "-9: 0.1-1-1-True: 0.549 (0.569)\n",
      "-9: 1-1-1-True: 0.608 (0.608)\n",
      "-9: 1-1-1-True: 0.725 (0.784)\n",
      "-9: 1-1-1-True: 0.745 (0.804)\n",
      "-9: 1-1-1-False: 0.784 (0.824)\n",
      "-9: 1-5-1-True: 0.784 (0.863)\n",
      "-9: 1-10-1-True: 0.725 (0.765)\n",
      "-9: 1-10-5-True: 0.843 (0.882)\n",
      "-9: 1-1-5-True: 0.647 (0.667)\n",
      "-9: 1-1-10-True: 0.647 (0.686)\n",
      "-9: 1-5-10-True: 0.843 (0.882)\n",
      "-9: 0.1-10-10-True: 0.863 (0.882)\n",
      "-9: 1-10-10-True: 0.843 (0.882)\n",
      "-9: 1-10-10-True: 0.824 (0.882)\n",
      "-9: 1-25-25-True: 0.765 (0.863)\n",
      "-9: 1-25-25-True: 0.824 (0.863)\n",
      "-9: 0.1-25-25-True: 0.843 (0.882)\n",
      "-9: 1-25-25-True: 0.843 (0.902)\n",
      "-9: 1-50-50-True: 0.882 (0.902)\n",
      "-10: 0.01-1-1-True: 0.686 (0.784)\n",
      "-10: 0.1-1-1-True: 0.569 (0.627)\n",
      "-10: 1-1-1-True: 0.588 (0.627)\n",
      "-10: 1-1-1-True: 0.745 (0.804)\n",
      "-10: 1-1-1-True: 0.824 (0.882)\n",
      "-10: 1-1-1-False: 0.725 (0.745)\n",
      "-10: 1-5-1-True: 0.745 (0.804)\n",
      "-10: 1-10-1-True: 0.804 (0.824)\n",
      "-10: 1-10-5-True: 0.843 (0.863)\n",
      "-10: 1-1-5-True: 0.588 (0.706)\n",
      "-10: 1-1-10-True: 0.667 (0.745)\n",
      "-10: 1-5-10-True: 0.882 (0.902)\n",
      "-10: 0.1-10-10-True: 0.863 (0.882)\n",
      "-10: 1-10-10-True: 0.882 (0.902)\n",
      "-10: 1-10-10-True: 0.843 (0.902)\n",
      "-10: 1-25-25-True: 0.902 (0.902)\n",
      "-10: 1-25-25-True: 0.882 (0.902)\n",
      "-10: 0.1-25-25-True: 0.882 (0.902)\n",
      "-10: 1-25-25-True: 0.863 (0.902)\n",
      "-10: 1-50-50-True: 0.843 (0.902)\n",
      "----- 30.66 mins -----\n"
     ]
    }
   ],
   "source": [
    "# h0, norm (norm, not norm, norm H), sep vs joint training, optimizers iters (W vs h)\n",
    "EXPS = [\n",
    "        # {'h0': .001, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': .01, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': .1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 1000, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 1500, 'epochs_h': 1, 'epochs_W': 1, 'alt': True},\n",
    "\n",
    "        # {'h0': .1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "        {'h0': 1, 'epochs': 1000, 'epochs_h': 1, 'epochs_W': 1, 'alt': False},\n",
    "\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 5, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 1, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 1, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 5, 'alt': True},\n",
    "\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 5, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 10, 'alt': True},\n",
    "        # {'h0': 1, 'epochs': 200, 'epochs_h': 1, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 5, 'epochs_W': 10, 'alt': True},\n",
    "        \n",
    "        {'h0': .1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 500, 'epochs_h': 10, 'epochs_W': 10, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 50, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 100, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': .1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 25, 'epochs_W': 25, 'alt': True},\n",
    "        {'h0': 1, 'epochs': 200, 'epochs_h': 50, 'epochs_W': 50, 'alt': True},\n",
    "    ]\n",
    "\n",
    "\n",
    "best_accs6 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs6 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=exp['h0'])\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        if not exp['alt']:\n",
    "            model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "            _, acc = model.train(feat, labels, exp['epochs'], LR, WD)\n",
    "        else:\n",
    "            model = GF_NodeClassModel(arch, S,  K, masks, LOSS_FN, device=device)\n",
    "            _, acc = model.train(feat, labels, exp['epochs'], LR, WD, epochs_h=exp['epochs_h'],\n",
    "                                 epochs_W=exp['epochs_W'])\n",
    "\n",
    "        best_accs6[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs6[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"h0\"]}-{exp[\"epochs_h\"]}-{exp[\"epochs_W\"]}-{exp[\"alt\"]}: {best_val_accs6[j,i]:.3f} ({best_accs6[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"h0\"]}-{exp[\"epochs_h\"]}-{exp[\"epochs_W\"]}-{exp[\"alt\"]}' for exp in EXPS]\n",
    "table_over6 = summary_table(best_accs6, index_name)\n",
    "table6 = summary_table(best_val_accs6, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.01-1-1-True</th>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.074018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-1-1-True</th>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.089533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-1-True</th>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.070724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000-1-1-1-True</th>\n",
       "      <td>0.750980</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.098059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500-1-1-1-True</th>\n",
       "      <td>0.780392</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.046235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000-1-1-1-False</th>\n",
       "      <td>0.731373</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.080869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-5-1-True</th>\n",
       "      <td>0.780392</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.056011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-1-True</th>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.035565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-5-True</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.045775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-5-True</th>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.058331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-1-10-True</th>\n",
       "      <td>0.684314</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.069849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-5-10-True</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.048388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-10-10-True</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.033102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-10-10-True</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.019212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-1-10-10-True</th>\n",
       "      <td>0.850980</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.031859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50-1-25-25-True</th>\n",
       "      <td>0.825490</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.052941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100-1-25-25-True</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.028347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-25-25-True</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.031373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-25-25-True</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.035565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-1-50-50-True</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.019212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    mean accs       med       std\n",
       "200-0.01-1-1-True    0.578431  0.578431  0.074018\n",
       "200-0.1-1-1-True     0.578431  0.568627  0.089533\n",
       "200-1-1-1-True       0.633333  0.627451  0.070724\n",
       "1000-1-1-1-True      0.750980  0.774510  0.098059\n",
       "1500-1-1-1-True      0.780392  0.784314  0.046235\n",
       "1000-1-1-1-False     0.731373  0.745098  0.080869\n",
       "200-1-5-1-True       0.780392  0.784314  0.056011\n",
       "200-1-10-1-True      0.766667  0.764706  0.035565\n",
       "200-1-10-5-True      0.833333  0.843137  0.045775\n",
       "200-1-1-5-True       0.676471  0.676471  0.058331\n",
       "200-1-1-10-True      0.684314  0.666667  0.069849\n",
       "200-1-5-10-True      0.841176  0.843137  0.048388\n",
       "200-0.1-10-10-True   0.852941  0.862745  0.033102\n",
       "200-1-10-10-True     0.866667  0.862745  0.019212\n",
       "500-1-10-10-True     0.850980  0.852941  0.031859\n",
       "50-1-25-25-True      0.825490  0.823529  0.052941\n",
       "100-1-25-25-True     0.860784  0.852941  0.028347\n",
       "200-0.1-25-25-True   0.866667  0.872549  0.031373\n",
       "200-1-25-25-True     0.860784  0.862745  0.035565\n",
       "200-1-50-50-True     0.866667  0.862745  0.019212"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 2-2-16: 0.765 (0.843)\n",
      "-1: 2-3-16: 0.784 (0.882)\n",
      "-1: 2-3-32: 0.745 (0.863)\n",
      "-1: 2-4-16: 0.824 (0.863)\n",
      "-1: 3-2-16: 0.804 (0.843)\n",
      "-1: 4-2-16: 0.725 (0.843)\n",
      "-1: 3-3-16: 0.804 (0.843)\n",
      "-1: 4-3-16: 0.725 (0.804)\n",
      "-1: 2-2-32: 0.824 (0.863)\n",
      "-1: 2-2-50: 0.745 (0.843)\n",
      "-1: 2-2-75: 0.784 (0.863)\n",
      "-1: 2-2-100: 0.745 (0.843)\n",
      "-1: 2-3-50: 0.725 (0.843)\n",
      "-1: 2-3-75: 0.804 (0.863)\n",
      "-1: 2-3-100: 0.824 (0.843)\n",
      "-1: 3-2-50: 0.843 (0.863)\n",
      "-1: 3-2-64: 0.804 (0.863)\n",
      "-1: 3-2-128: 0.804 (0.863)\n",
      "-2: 2-2-16: 0.863 (0.902)\n",
      "-2: 2-3-16: 0.843 (0.922)\n",
      "-2: 2-3-32: 0.863 (0.941)\n",
      "-2: 2-4-16: 0.843 (0.922)\n",
      "-2: 3-2-16: 0.863 (0.902)\n",
      "-2: 4-2-16: 0.824 (0.922)\n",
      "-2: 3-3-16: 0.824 (0.902)\n",
      "-2: 4-3-16: 0.843 (0.882)\n",
      "-2: 2-2-32: 0.804 (0.941)\n",
      "-2: 2-2-50: 0.784 (0.922)\n",
      "-2: 2-2-75: 0.824 (0.902)\n",
      "-2: 2-2-100: 0.824 (0.922)\n",
      "-2: 2-3-50: 0.824 (0.922)\n",
      "-2: 2-3-75: 0.863 (0.922)\n",
      "-2: 2-3-100: 0.765 (0.882)\n",
      "-2: 3-2-50: 0.804 (0.941)\n",
      "-2: 3-2-64: 0.804 (0.941)\n",
      "-2: 3-2-128: 0.902 (0.941)\n",
      "-3: 2-2-16: 0.882 (0.922)\n",
      "-3: 2-3-16: 0.902 (0.902)\n",
      "-3: 2-3-32: 0.843 (0.922)\n",
      "-3: 2-4-16: 0.804 (0.902)\n",
      "-3: 3-2-16: 0.863 (0.941)\n",
      "-3: 4-2-16: 0.745 (0.843)\n",
      "-3: 3-3-16: 0.863 (0.922)\n",
      "-3: 4-3-16: 0.843 (0.863)\n",
      "-3: 2-2-32: 0.843 (0.922)\n",
      "-3: 2-2-50: 0.863 (0.922)\n",
      "-3: 2-2-75: 0.863 (0.922)\n",
      "-3: 2-2-100: 0.863 (0.922)\n",
      "-3: 2-3-50: 0.882 (0.922)\n",
      "-3: 2-3-75: 0.882 (0.922)\n",
      "-3: 2-3-100: 0.824 (0.902)\n",
      "-3: 3-2-50: 0.882 (0.941)\n",
      "-3: 3-2-64: 0.882 (0.941)\n",
      "-3: 3-2-128: 0.824 (0.941)\n",
      "-4: 2-2-16: 0.863 (0.941)\n",
      "-4: 2-3-16: 0.882 (0.941)\n",
      "-4: 2-3-32: 0.922 (0.961)\n",
      "-4: 2-4-16: 0.824 (0.922)\n",
      "-4: 3-2-16: 0.863 (0.941)\n",
      "-4: 4-2-16: 0.804 (0.863)\n",
      "-4: 3-3-16: 0.882 (0.902)\n",
      "-4: 4-3-16: 0.843 (0.882)\n",
      "-4: 2-2-32: 0.922 (0.941)\n",
      "-4: 2-2-50: 0.882 (0.980)\n",
      "-4: 2-2-75: 0.922 (0.961)\n",
      "-4: 2-2-100: 0.882 (0.941)\n",
      "-4: 2-3-50: 0.882 (0.961)\n",
      "-4: 2-3-75: 0.843 (0.961)\n",
      "-4: 2-3-100: 0.922 (0.941)\n",
      "-4: 3-2-50: 0.922 (0.961)\n",
      "-4: 3-2-64: 0.961 (0.961)\n",
      "-4: 3-2-128: 0.902 (0.941)\n",
      "-5: 2-2-16: 0.863 (0.922)\n",
      "-5: 2-3-16: 0.863 (0.902)\n",
      "-5: 2-3-32: 0.843 (0.922)\n",
      "-5: 2-4-16: 0.824 (0.902)\n",
      "-5: 3-2-16: 0.745 (0.882)\n",
      "-5: 4-2-16: 0.804 (0.902)\n",
      "-5: 3-3-16: 0.765 (0.882)\n",
      "-5: 4-3-16: 0.725 (0.902)\n",
      "-5: 2-2-32: 0.863 (0.922)\n",
      "-5: 2-2-50: 0.863 (0.941)\n",
      "-5: 2-2-75: 0.882 (0.922)\n",
      "-5: 2-2-100: 0.804 (0.922)\n",
      "-5: 2-3-50: 0.784 (0.922)\n",
      "-5: 2-3-75: 0.824 (0.922)\n",
      "-5: 2-3-100: 0.824 (0.922)\n",
      "-5: 3-2-50: 0.882 (0.922)\n",
      "-5: 3-2-64: 0.882 (0.941)\n",
      "-5: 3-2-128: 0.863 (0.941)\n",
      "-6: 2-2-16: 0.843 (0.922)\n",
      "-6: 2-3-16: 0.843 (0.863)\n",
      "-6: 2-3-32: 0.863 (0.902)\n",
      "-6: 2-4-16: 0.882 (0.902)\n",
      "-6: 3-2-16: 0.863 (0.882)\n",
      "-6: 4-2-16: 0.882 (0.882)\n",
      "-6: 3-3-16: 0.824 (0.863)\n",
      "-6: 4-3-16: 0.824 (0.843)\n",
      "-6: 2-2-32: 0.843 (0.902)\n",
      "-6: 2-2-50: 0.843 (0.922)\n",
      "-6: 2-2-75: 0.843 (0.902)\n",
      "-6: 2-2-100: 0.843 (0.882)\n",
      "-6: 2-3-50: 0.882 (0.902)\n",
      "-6: 2-3-75: 0.843 (0.882)\n",
      "-6: 2-3-100: 0.843 (0.882)\n",
      "-6: 3-2-50: 0.843 (0.922)\n",
      "-6: 3-2-64: 0.863 (0.922)\n",
      "-6: 3-2-128: 0.863 (0.922)\n",
      "-7: 2-2-16: 0.863 (0.902)\n",
      "-7: 2-3-16: 0.882 (0.902)\n",
      "-7: 2-3-32: 0.902 (0.902)\n",
      "-7: 2-4-16: 0.882 (0.902)\n",
      "-7: 3-2-16: 0.882 (0.922)\n",
      "-7: 4-2-16: 0.882 (0.882)\n",
      "-7: 3-3-16: 0.843 (0.941)\n",
      "-7: 4-3-16: 0.843 (0.902)\n",
      "-7: 2-2-32: 0.824 (0.922)\n",
      "-7: 2-2-50: 0.863 (0.902)\n",
      "-7: 2-2-75: 0.843 (0.902)\n",
      "-7: 2-2-100: 0.843 (0.882)\n",
      "-7: 2-3-50: 0.843 (0.902)\n",
      "-7: 2-3-75: 0.882 (0.882)\n",
      "-7: 2-3-100: 0.843 (0.882)\n",
      "-7: 3-2-50: 0.902 (0.922)\n",
      "-7: 3-2-64: 0.882 (0.902)\n",
      "-7: 3-2-128: 0.882 (0.902)\n",
      "-8: 2-2-16: 0.843 (0.902)\n",
      "-8: 2-3-16: 0.824 (0.902)\n",
      "-8: 2-3-32: 0.902 (0.922)\n",
      "-8: 2-4-16: 0.882 (0.902)\n",
      "-8: 3-2-16: 0.824 (0.882)\n",
      "-8: 4-2-16: 0.824 (0.882)\n",
      "-8: 3-3-16: 0.824 (0.863)\n",
      "-8: 4-3-16: 0.765 (0.824)\n",
      "-8: 2-2-32: 0.843 (0.902)\n",
      "-8: 2-2-50: 0.863 (0.882)\n",
      "-8: 2-2-75: 0.784 (0.863)\n",
      "-8: 2-2-100: 0.784 (0.902)\n",
      "-8: 2-3-50: 0.843 (0.902)\n",
      "-8: 2-3-75: 0.824 (0.863)\n",
      "-8: 2-3-100: 0.765 (0.902)\n",
      "-8: 3-2-50: 0.843 (0.922)\n",
      "-8: 3-2-64: 0.843 (0.922)\n",
      "-8: 3-2-128: 0.804 (0.922)\n",
      "-9: 2-2-16: 0.863 (0.902)\n",
      "-9: 2-3-16: 0.824 (0.902)\n",
      "-9: 2-3-32: 0.843 (0.882)\n",
      "-9: 2-4-16: 0.784 (0.882)\n",
      "-9: 3-2-16: 0.843 (0.882)\n",
      "-9: 4-2-16: 0.804 (0.902)\n",
      "-9: 3-3-16: 0.824 (0.843)\n",
      "-9: 4-3-16: 0.784 (0.863)\n",
      "-9: 2-2-32: 0.843 (0.882)\n",
      "-9: 2-2-50: 0.824 (0.882)\n",
      "-9: 2-2-75: 0.843 (0.863)\n",
      "-9: 2-2-100: 0.843 (0.863)\n",
      "-9: 2-3-50: 0.863 (0.882)\n",
      "-9: 2-3-75: 0.843 (0.882)\n",
      "-9: 2-3-100: 0.784 (0.863)\n",
      "-9: 3-2-50: 0.902 (0.902)\n",
      "-9: 3-2-64: 0.863 (0.882)\n",
      "-9: 3-2-128: 0.824 (0.882)\n",
      "-10: 2-2-16: 0.961 (0.961)\n",
      "-10: 2-3-16: 0.882 (0.882)\n",
      "-10: 2-3-32: 0.843 (0.902)\n",
      "-10: 2-4-16: 0.863 (0.882)\n",
      "-10: 3-2-16: 0.882 (0.922)\n",
      "-10: 4-2-16: 0.882 (0.941)\n",
      "-10: 3-3-16: 0.843 (0.882)\n",
      "-10: 4-3-16: 0.843 (0.902)\n",
      "-10: 2-2-32: 0.882 (0.941)\n",
      "-10: 2-2-50: 0.902 (0.941)\n",
      "-10: 2-2-75: 0.882 (0.941)\n",
      "-10: 2-2-100: 0.902 (0.941)\n",
      "-10: 2-3-50: 0.863 (0.902)\n",
      "-10: 2-3-75: 0.882 (0.882)\n",
      "-10: 2-3-100: 0.863 (0.902)\n",
      "-10: 3-2-50: 0.902 (0.941)\n",
      "-10: 3-2-64: 0.902 (0.941)\n",
      "-10: 3-2-128: 0.922 (0.961)\n",
      "----- 28.23 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'L': 2, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 50},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 75},\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 100},\n",
    "\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 50},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 75},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 100},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 50},\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 128},\n",
    "        ]\n",
    "\n",
    "best_accs7 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs7 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, exp['hid_dim'], OUT_DIM, exp['L'], exp['K'], act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  exp['K'], masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs7[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs7[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}: {best_val_accs7[j,i]:.3f} ({best_accs7[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}' for exp in EXPS]\n",
    "table_over7 = summary_table(best_accs7, index_name)\n",
    "table7 = summary_table(best_val_accs7, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.045098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.034244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.046442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-16</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.040184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-16</th>\n",
       "      <td>0.817647</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.051915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-16</th>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.030440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-16</th>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.047222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.031677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-50</th>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.044713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-75</th>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.040942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-100</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.044063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-50</th>\n",
       "      <td>0.839216</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.047869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-75</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.026380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-100</th>\n",
       "      <td>0.825490</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.045098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-50</th>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.035349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-64</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.043888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-128</th>\n",
       "      <td>0.858824</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.040942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean accs       med       std\n",
       "2-2-16    0.860784  0.862745  0.045098\n",
       "2-3-16    0.852941  0.852941  0.034244\n",
       "2-3-32    0.856863  0.852941  0.046442\n",
       "2-4-16    0.841176  0.833333  0.033333\n",
       "3-2-16    0.843137  0.862745  0.040184\n",
       "4-2-16    0.817647  0.813725  0.051915\n",
       "3-3-16    0.829412  0.823529  0.030440\n",
       "4-3-16    0.803922  0.833333  0.047222\n",
       "2-2-32    0.849020  0.843137  0.031677\n",
       "2-2-50    0.843137  0.862745  0.044713\n",
       "2-2-75    0.847059  0.843137  0.040942\n",
       "2-2-100   0.833333  0.843137  0.044063\n",
       "2-3-50    0.839216  0.852941  0.047869\n",
       "2-3-75    0.849020  0.843137  0.026380\n",
       "2-3-100   0.825490  0.823529  0.045098\n",
       "3-2-50    0.872549  0.882353  0.035349\n",
       "3-2-64    0.868627  0.872549  0.043888\n",
       "3-2-128   0.858824  0.862745  0.040942"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GSO, bias, and batch norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1: orig-True-False: 0.804 (0.882)\n",
      "- 1: trans-True-False: 0.784 (0.863)\n",
      "- 1: sym-True-False: 0.529 (0.529)\n",
      "- 1: orig-True-True: 0.784 (0.843)\n",
      "- 1: trans-True-True: 0.784 (0.882)\n",
      "- 1: sym-True-True: 0.863 (0.882)\n",
      "- 1: orig-False-False: 0.824 (0.863)\n",
      "- 1: sym-False-False: 0.863 (0.863)\n",
      "- 2: orig-True-False: 0.843 (0.941)\n",
      "- 2: trans-True-False: 0.843 (0.882)\n",
      "- 2: sym-True-False: 0.627 (0.627)\n",
      "- 2: orig-True-True: 0.804 (0.843)\n",
      "- 2: trans-True-True: 0.765 (0.804)\n",
      "- 2: sym-True-True: 0.745 (0.863)\n",
      "- 2: orig-False-False: 0.863 (0.922)\n",
      "- 2: sym-False-False: 0.627 (0.627)\n",
      "- 3: orig-True-False: 0.882 (0.922)\n",
      "- 3: trans-True-False: 0.843 (0.882)\n",
      "- 3: sym-True-False: 0.804 (0.882)\n",
      "- 3: orig-True-True: 0.804 (0.863)\n",
      "- 3: trans-True-True: 0.765 (0.863)\n",
      "- 3: sym-True-True: 0.843 (0.922)\n",
      "- 3: orig-False-False: 0.902 (0.922)\n",
      "- 3: sym-False-False: 0.412 (0.412)\n",
      "- 4: orig-True-False: 0.941 (0.961)\n",
      "- 4: trans-True-False: 0.902 (0.961)\n",
      "- 4: sym-True-False: 0.882 (0.902)\n",
      "- 4: orig-True-True: 0.843 (0.902)\n",
      "- 4: trans-True-True: 0.941 (0.941)\n",
      "- 4: sym-True-True: 0.941 (0.961)\n",
      "- 4: orig-False-False: 0.941 (0.961)\n",
      "- 4: sym-False-False: 0.431 (0.431)\n",
      "- 5: orig-True-False: 0.902 (0.941)\n",
      "- 5: trans-True-False: 0.882 (0.882)\n",
      "- 5: sym-True-False: 0.843 (0.902)\n",
      "- 5: orig-True-True: 0.882 (0.922)\n",
      "- 5: trans-True-True: 0.824 (0.902)\n",
      "- 5: sym-True-True: 0.882 (0.941)\n",
      "- 5: orig-False-False: 0.882 (0.922)\n",
      "- 5: sym-False-False: 0.392 (0.392)\n",
      "- 6: orig-True-False: 0.902 (0.902)\n",
      "- 6: trans-True-False: 0.882 (0.902)\n",
      "- 6: sym-True-False: 0.510 (0.529)\n",
      "- 6: orig-True-True: 0.824 (0.843)\n",
      "- 6: trans-True-True: 0.863 (0.863)\n",
      "- 6: sym-True-True: 0.824 (0.882)\n",
      "- 6: orig-False-False: 0.843 (0.902)\n",
      "- 6: sym-False-False: 0.804 (0.843)\n",
      "- 7: orig-True-False: 0.882 (0.941)\n",
      "- 7: trans-True-False: 0.824 (0.882)\n",
      "- 7: sym-True-False: 0.471 (0.471)\n",
      "- 7: orig-True-True: 0.843 (0.882)\n",
      "- 7: trans-True-True: 0.843 (0.863)\n",
      "- 7: sym-True-True: 0.843 (0.882)\n",
      "- 7: orig-False-False: 0.882 (0.902)\n",
      "- 7: sym-False-False: 0.471 (0.471)\n",
      "- 8: orig-True-False: 0.882 (0.902)\n",
      "- 8: trans-True-False: 0.765 (0.882)\n",
      "- 8: sym-True-False: 0.510 (0.510)\n",
      "- 8: orig-True-True: 0.784 (0.882)\n",
      "- 8: trans-True-True: 0.843 (0.882)\n",
      "- 8: sym-True-True: 0.882 (0.882)\n",
      "- 8: orig-False-False: 0.843 (0.902)\n",
      "- 8: sym-False-False: 0.765 (0.804)\n",
      "- 9: orig-True-False: 0.863 (0.902)\n",
      "- 9: trans-True-False: 0.804 (0.843)\n",
      "- 9: sym-True-False: 0.451 (0.451)\n",
      "- 9: orig-True-True: 0.843 (0.902)\n",
      "- 9: trans-True-True: 0.824 (0.882)\n",
      "- 9: sym-True-True: 0.824 (0.863)\n",
      "- 9: orig-False-False: 0.843 (0.882)\n",
      "- 9: sym-False-False: 0.451 (0.451)\n",
      "- 10: orig-True-False: 0.902 (0.922)\n",
      "- 10: trans-True-False: 0.824 (0.882)\n",
      "- 10: sym-True-False: 0.451 (0.451)\n",
      "- 10: orig-True-True: 0.863 (0.922)\n",
      "- 10: trans-True-True: 0.843 (0.902)\n",
      "- 10: sym-True-True: 0.843 (0.882)\n",
      "- 10: orig-False-False: 0.882 (0.902)\n",
      "- 10: sym-False-False: 0.451 (0.451)\n",
      "----- 26.17 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [{'GSO': 'orig', 'bias': True, 'bn': False},\n",
    "        {'GSO': 'trans', 'bias': True, 'bn': False},\n",
    "        {'GSO': 'sym', 'bias': True, 'bn': False},\n",
    "\n",
    "        {'GSO': 'orig', 'bias': True, 'bn': True},\n",
    "        {'GSO': 'trans', 'bias': True, 'bn': True},\n",
    "        {'GSO': 'sym', 'bias': True, 'bn': True},\n",
    "\n",
    "        {'GSO': 'orig', 'bias': False, 'bn': False},\n",
    "        {'GSO': 'sym', 'bias': False, 'bn': False},\n",
    "        ]\n",
    "\n",
    "best_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0, bias=False, batch_norm=exp['bn'])\n",
    "        \n",
    "        if exp['GSO'] == 'sym':\n",
    "            A_aux = (A + A.T)/2\n",
    "            A_aux = np.where(A_aux > 0, 1, 0)\n",
    "        elif exp['GSO'] == 'trans':\n",
    "            A_aux = A.T\n",
    "        else:\n",
    "            A_aux = A\n",
    "\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A_aux, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A_aux).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  K, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs3b[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs3b[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'- {i+1}: {exp[\"GSO\"]}-{exp[\"bias\"]}-{exp[\"bn\"]}: {best_val_accs3b[j,i]:.3f} ({best_accs3b[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"GSO\"]}-{exp[\"bias\"]}-{exp[\"bn\"]}' for exp in EXPS]\n",
    "table_over7b = summary_table(best_accs3b, index_name)\n",
    "table7b = summary_table(best_val_accs3b, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>orig-True-False</th>\n",
       "      <td>0.880392</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.035565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trans-True-False</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.042237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sym-True-False</th>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.162165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig-True-True</th>\n",
       "      <td>0.827451</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.031373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trans-True-True</th>\n",
       "      <td>0.829412</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.049643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sym-True-True</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.048069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>orig-False-False</th>\n",
       "      <td>0.870588</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.033044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sym-False-False</th>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.460784</td>\n",
       "      <td>0.171935</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean accs       med       std\n",
       "orig-True-False    0.880392  0.882353  0.035565\n",
       "trans-True-False   0.835294  0.833333  0.042237\n",
       "sym-True-False     0.607843  0.519608  0.162165\n",
       "orig-True-True     0.827451  0.833333  0.031373\n",
       "trans-True-True    0.829412  0.833333  0.049643\n",
       "sym-True-True      0.849020  0.843137  0.048069\n",
       "orig-False-False   0.870588  0.872549  0.033044\n",
       "sym-False-False    0.566667  0.460784  0.171935"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table7b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearities and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.863)\n",
      "-1: ReLU()-Softmax(dim=1)-NLLLoss(): 0.804 (0.882)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.647 (0.706)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.667 (0.725)\n",
      "-1: ReLU()-Identity()-CrossEntropyLoss(): 0.627 (0.647)\n",
      "-1: ReLU()-Identity()-NLLLoss(): 0.275 (0.294)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.843)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.784 (0.843)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.863)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.784 (0.863)\n",
      "-1: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.843)\n",
      "-1: Identity()-Softmax(dim=1)-NLLLoss(): 0.804 (0.863)\n",
      "-1: Identity()-Identity()-CrossEntropyLoss(): 0.686 (0.725)\n",
      "-2: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-2: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.784 (0.784)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.745 (0.765)\n",
      "-2: ReLU()-Identity()-CrossEntropyLoss(): 0.608 (0.824)\n",
      "-2: ReLU()-Identity()-NLLLoss(): 0.333 (0.392)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.922)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.843 (0.941)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.902)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.804 (0.902)\n",
      "-2: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-2: Identity()-Softmax(dim=1)-NLLLoss(): 0.824 (0.922)\n",
      "-2: Identity()-Identity()-CrossEntropyLoss(): 0.745 (0.765)\n",
      "-3: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.941)\n",
      "-3: ReLU()-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.549 (0.569)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.588 (0.588)\n",
      "-3: ReLU()-Identity()-CrossEntropyLoss(): 0.627 (0.667)\n",
      "-3: ReLU()-Identity()-NLLLoss(): 0.510 (0.510)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.922)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.922 (0.922)\n",
      "-3: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.922)\n",
      "-3: Identity()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-3: Identity()-Identity()-CrossEntropyLoss(): 0.725 (0.725)\n",
      "-4: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.941)\n",
      "-4: ReLU()-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.608 (0.627)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.608 (0.647)\n",
      "-4: ReLU()-Identity()-CrossEntropyLoss(): 0.608 (0.627)\n",
      "-4: ReLU()-Identity()-NLLLoss(): 0.431 (0.431)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.922 (0.961)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.902)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.882 (0.941)\n",
      "-4: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.941 (0.980)\n",
      "-4: Identity()-Softmax(dim=1)-NLLLoss(): 0.922 (0.922)\n",
      "-4: Identity()-Identity()-CrossEntropyLoss(): 0.706 (0.784)\n",
      "-5: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-5: ReLU()-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.569 (0.627)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.608 (0.627)\n",
      "-5: ReLU()-Identity()-CrossEntropyLoss(): 0.569 (0.608)\n",
      "-5: ReLU()-Identity()-NLLLoss(): 0.451 (0.471)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.784 (0.902)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-5: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.922)\n",
      "-5: Identity()-Softmax(dim=1)-NLLLoss(): 0.804 (0.922)\n",
      "-5: Identity()-Identity()-CrossEntropyLoss(): 0.608 (0.627)\n",
      "-6: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.843)\n",
      "-6: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.882)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.549 (0.588)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.588 (0.608)\n",
      "-6: ReLU()-Identity()-CrossEntropyLoss(): 0.686 (0.706)\n",
      "-6: ReLU()-Identity()-NLLLoss(): 0.569 (0.569)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.882)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-6: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-6: Identity()-Softmax(dim=1)-NLLLoss(): 0.882 (0.902)\n",
      "-6: Identity()-Identity()-CrossEntropyLoss(): 0.706 (0.765)\n",
      "-7: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-7: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.922)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.549 (0.569)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.647 (0.765)\n",
      "-7: ReLU()-Identity()-CrossEntropyLoss(): 0.529 (0.569)\n",
      "-7: ReLU()-Identity()-NLLLoss(): 0.314 (0.314)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.902)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.882 (0.922)\n",
      "-7: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.902 (0.902)\n",
      "-7: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.882)\n",
      "-7: Identity()-Identity()-CrossEntropyLoss(): 0.627 (0.647)\n",
      "-8: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.882)\n",
      "-8: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.882)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.510 (0.588)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.529 (0.569)\n",
      "-8: ReLU()-Identity()-CrossEntropyLoss(): 0.549 (0.549)\n",
      "-8: ReLU()-Identity()-NLLLoss(): 0.510 (0.510)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.882)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.882)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.804 (0.882)\n",
      "-8: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-8: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.882)\n",
      "-8: Identity()-Identity()-CrossEntropyLoss(): 0.588 (0.627)\n",
      "-9: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-9: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.882)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.490 (0.549)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.490 (0.529)\n",
      "-9: ReLU()-Identity()-CrossEntropyLoss(): 0.667 (0.667)\n",
      "-9: ReLU()-Identity()-NLLLoss(): 0.471 (0.471)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.902)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.882)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.804 (0.882)\n",
      "-9: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-9: Identity()-Softmax(dim=1)-NLLLoss(): 0.863 (0.882)\n",
      "-9: Identity()-Identity()-CrossEntropyLoss(): 0.588 (0.647)\n",
      "-10: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.902)\n",
      "-10: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.882)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.529 (0.549)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.608 (0.627)\n",
      "-10: ReLU()-Identity()-CrossEntropyLoss(): 0.686 (0.725)\n",
      "-10: ReLU()-Identity()-NLLLoss(): 0.549 (0.627)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.882 (0.882)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.902)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.902 (0.922)\n",
      "-10: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.882)\n",
      "-10: Identity()-Softmax(dim=1)-NLLLoss(): 0.882 (0.902)\n",
      "-10: Identity()-Identity()-CrossEntropyLoss(): 0.706 (0.706)\n",
      "----- 17.36 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [{'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},]\n",
    "\n",
    "best_accs8 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs8 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=exp['act'], last_act=exp['lact'],\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = GF_NodeClassModel(arch, S,  K, masks, exp['loss'], device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                              epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs8[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs8[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}: {best_val_accs8[j,i]:.3f} ({best_accs8[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}' for exp in EXPS]\n",
    "table_over8 = summary_table(best_accs8, index_name)\n",
    "table8 = summary_table(best_val_accs8, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.872549</td>\n",
       "      <td>0.043003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.028347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.080964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.066782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.615686</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>0.052025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Identity()-NLLLoss()</th>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.460784</td>\n",
       "      <td>0.096955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.031677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.839216</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.038021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.023934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.847059</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.045395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.868627</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.034018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.854902</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.036367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.668627</td>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.056456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean accs       med  \\\n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()             0.856863  0.872549   \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                      0.860784  0.862745   \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()          0.578431  0.549020   \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                   0.607843  0.607843   \n",
       "ReLU()-Identity()-CrossEntropyLoss()                 0.615686  0.617647   \n",
       "ReLU()-Identity()-NLLLoss()                          0.441176  0.460784   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()     0.856863  0.852941   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()              0.839216  0.833333   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...   0.841176  0.833333   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...   0.847059  0.843137   \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()         0.868627  0.843137   \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                  0.854902  0.852941   \n",
       "Identity()-Identity()-CrossEntropyLoss()             0.668627  0.696078   \n",
       "\n",
       "                                                         std  \n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()            0.043003  \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                     0.028347  \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()         0.080964  \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                  0.066782  \n",
       "ReLU()-Identity()-CrossEntropyLoss()                0.052025  \n",
       "ReLU()-Identity()-NLLLoss()                         0.096955  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()    0.031677  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()             0.038021  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...  0.023934  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...  0.045395  \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()        0.034018  \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                 0.036367  \n",
       "Identity()-Identity()-CrossEntropyLoss()            0.056456  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPS = [\n",
    "        {'name': 'Kipf', 'norm': 'none'},\n",
    "        {'name': 'Kipf', 'norm': 'both'},\n",
    "\n",
    "        {'name': 'A-GCNN', 'norm': False},\n",
    "        {'name': 'A-GCNN', 'norm': True},\n",
    "\n",
    "        {'name': 'H-GCNN', 'norm': False}, # This should be the same as A-GCNN not norm\n",
    "        {'name': 'H-GCNN', 'norm': True},\n",
    "\n",
    "        {'name': 'W-GCN-A', 'norm': False},\n",
    "        {'name': 'W-GCN-A', 'norm': True},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- RUN: 1\n",
      "\tKipf-none: acc = 0.549  -  acc2 = 0.549  -  acc (over) = 0.608\n",
      "\tKipf-both: acc = 0.588  -  acc2 = 0.588  -  acc (over) = 0.608\n",
      "\tA-GCNN-False: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:44: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(np.where(np.isclose(deg_vec, 0), 0, 1/deg_vec))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tA-GCNN-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.863\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.804  -  acc (over) = 0.863\n",
      "\tH-GCNN-True: acc = 0.804  -  acc2 = 0.804  -  acc (over) = 0.843\n",
      "\tW-GCN-A-False: acc = 0.725  -  acc2 = 0.725  -  acc (over) = 0.725\n",
      "\tW-GCN-A-True: acc = 0.725  -  acc2 = 0.784  -  acc (over) = 0.824\n",
      "- RUN: 2\n",
      "\tKipf-none: acc = 0.588  -  acc2 = 0.588  -  acc (over) = 0.725\n",
      "\tKipf-both: acc = 0.647  -  acc2 = 0.686  -  acc (over) = 0.686\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.863  -  acc (over) = 0.882\n",
      "\tA-GCNN-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.922\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.863  -  acc2 = 0.902  -  acc (over) = 0.922\n",
      "\tW-GCN-A-False: acc = 0.804  -  acc2 = 0.784  -  acc (over) = 0.804\n",
      "\tW-GCN-A-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.902\n",
      "- RUN: 3\n",
      "\tKipf-none: acc = 0.490  -  acc2 = 0.490  -  acc (over) = 0.588\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.569  -  acc (over) = 0.647\n",
      "\tA-GCNN-False: acc = 0.863  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.941\n",
      "\tH-GCNN-False: acc = 0.745  -  acc2 = 0.765  -  acc (over) = 0.843\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.902  -  acc (over) = 0.922\n",
      "\tW-GCN-A-False: acc = 0.647  -  acc2 = 0.647  -  acc (over) = 0.686\n",
      "\tW-GCN-A-True: acc = 0.824  -  acc2 = 0.843  -  acc (over) = 0.863\n",
      "- RUN: 4\n",
      "\tKipf-none: acc = 0.529  -  acc2 = 0.529  -  acc (over) = 0.647\n",
      "\tKipf-both: acc = 0.529  -  acc2 = 0.529  -  acc (over) = 0.569\n",
      "\tA-GCNN-False: acc = 0.941  -  acc2 = 0.922  -  acc (over) = 0.961\n",
      "\tA-GCNN-True: acc = 0.804  -  acc2 = 0.804  -  acc (over) = 0.922\n",
      "\tH-GCNN-False: acc = 0.941  -  acc2 = 0.843  -  acc (over) = 0.941\n",
      "\tH-GCNN-True: acc = 0.902  -  acc2 = 0.902  -  acc (over) = 0.961\n",
      "\tW-GCN-A-False: acc = 0.667  -  acc2 = 0.647  -  acc (over) = 0.745\n",
      "\tW-GCN-A-True: acc = 0.765  -  acc2 = 0.824  -  acc (over) = 0.882\n",
      "- RUN: 5\n",
      "\tKipf-none: acc = 0.373  -  acc2 = 0.392  -  acc (over) = 0.392\n",
      "\tKipf-both: acc = 0.392  -  acc2 = 0.490  -  acc (over) = 0.569\n",
      "\tA-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.922\n",
      "\tA-GCNN-True: acc = 0.922  -  acc2 = 0.843  -  acc (over) = 0.941\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tH-GCNN-True: acc = 0.765  -  acc2 = 0.804  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.608  -  acc2 = 0.647  -  acc (over) = 0.706\n",
      "\tW-GCN-A-True: acc = 0.843  -  acc2 = 0.804  -  acc (over) = 0.843\n",
      "- RUN: 6\n",
      "\tKipf-none: acc = 0.471  -  acc2 = 0.451  -  acc (over) = 0.529\n",
      "\tKipf-both: acc = 0.549  -  acc2 = 0.549  -  acc (over) = 0.569\n",
      "\tA-GCNN-False: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.804\n",
      "\tA-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.863\n",
      "\tH-GCNN-True: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.843\n",
      "\tW-GCN-A-False: acc = 0.686  -  acc2 = 0.706  -  acc (over) = 0.725\n",
      "\tW-GCN-A-True: acc = 0.745  -  acc2 = 0.745  -  acc (over) = 0.784\n",
      "- RUN: 7\n",
      "\tKipf-none: acc = 0.471  -  acc2 = 0.471  -  acc (over) = 0.471\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.549  -  acc (over) = 0.627\n",
      "\tA-GCNN-False: acc = 0.843  -  acc2 = 0.784  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.843  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.882\n",
      "\tW-GCN-A-False: acc = 0.725  -  acc2 = 0.745  -  acc (over) = 0.804\n",
      "\tW-GCN-A-True: acc = 0.824  -  acc2 = 0.804  -  acc (over) = 0.843\n",
      "- RUN: 8\n",
      "\tKipf-none: acc = 0.510  -  acc2 = 0.510  -  acc (over) = 0.510\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.529  -  acc (over) = 0.627\n",
      "\tA-GCNN-False: acc = 0.804  -  acc2 = 0.824  -  acc (over) = 0.843\n",
      "\tA-GCNN-True: acc = 0.824  -  acc2 = 0.804  -  acc (over) = 0.882\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.902  -  acc (over) = 0.941\n",
      "\tH-GCNN-True: acc = 0.745  -  acc2 = 0.765  -  acc (over) = 0.784\n",
      "\tW-GCN-A-False: acc = 0.627  -  acc2 = 0.627  -  acc (over) = 0.706\n",
      "\tW-GCN-A-True: acc = 0.765  -  acc2 = 0.765  -  acc (over) = 0.804\n",
      "- RUN: 9\n",
      "\tKipf-none: acc = 0.569  -  acc2 = 0.569  -  acc (over) = 0.588\n",
      "\tKipf-both: acc = 0.490  -  acc2 = 0.490  -  acc (over) = 0.588\n",
      "\tA-GCNN-False: acc = 0.784  -  acc2 = 0.804  -  acc (over) = 0.882\n",
      "\tA-GCNN-True: acc = 0.863  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tH-GCNN-True: acc = 0.804  -  acc2 = 0.765  -  acc (over) = 0.824\n",
      "\tW-GCN-A-False: acc = 0.686  -  acc2 = 0.627  -  acc (over) = 0.706\n",
      "\tW-GCN-A-True: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.784\n",
      "- RUN: 10\n",
      "\tKipf-none: acc = 0.510  -  acc2 = 0.471  -  acc (over) = 0.569\n",
      "\tKipf-both: acc = 0.529  -  acc2 = 0.529  -  acc (over) = 0.549\n",
      "\tA-GCNN-False: acc = 0.784  -  acc2 = 0.804  -  acc (over) = 0.863\n",
      "\tA-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tH-GCNN-False: acc = 0.824  -  acc2 = 0.804  -  acc (over) = 0.863\n",
      "\tH-GCNN-True: acc = 0.784  -  acc2 = 0.765  -  acc (over) = 0.804\n",
      "\tW-GCN-A-False: acc = 0.765  -  acc2 = 0.725  -  acc (over) = 0.784\n",
      "\tW-GCN-A-True: acc = 0.765  -  acc2 = 0.745  -  acc (over) = 0.784\n",
      "- RUN: 11\n",
      "\tKipf-none: acc = 0.549  -  acc2 = 0.549  -  acc (over) = 0.608\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.529  -  acc (over) = 0.647\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.843\n",
      "\tA-GCNN-True: acc = 0.824  -  acc2 = 0.804  -  acc (over) = 0.882\n",
      "\tH-GCNN-False: acc = 0.765  -  acc2 = 0.745  -  acc (over) = 0.804\n",
      "\tH-GCNN-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.843\n",
      "\tW-GCN-A-False: acc = 0.667  -  acc2 = 0.647  -  acc (over) = 0.667\n",
      "\tW-GCN-A-True: acc = 0.784  -  acc2 = 0.843  -  acc (over) = 0.843\n",
      "- RUN: 12\n",
      "\tKipf-none: acc = 0.667  -  acc2 = 0.686  -  acc (over) = 0.765\n",
      "\tKipf-both: acc = 0.588  -  acc2 = 0.647  -  acc (over) = 0.686\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.863  -  acc2 = 0.824  -  acc (over) = 0.902\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.843  -  acc (over) = 0.922\n",
      "\tW-GCN-A-False: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.804\n",
      "\tW-GCN-A-True: acc = 0.843  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "- RUN: 13\n",
      "\tKipf-none: acc = 0.412  -  acc2 = 0.412  -  acc (over) = 0.412\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.569  -  acc (over) = 0.647\n",
      "\tA-GCNN-False: acc = 0.882  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.941\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.627  -  acc2 = 0.627  -  acc (over) = 0.667\n",
      "\tW-GCN-A-True: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.863\n",
      "- RUN: 14\n",
      "\tKipf-none: acc = 0.588  -  acc2 = 0.588  -  acc (over) = 0.608\n",
      "\tKipf-both: acc = 0.510  -  acc2 = 0.510  -  acc (over) = 0.569\n",
      "\tA-GCNN-False: acc = 0.922  -  acc2 = 0.922  -  acc (over) = 0.961\n",
      "\tA-GCNN-True: acc = 0.922  -  acc2 = 0.922  -  acc (over) = 0.941\n",
      "\tH-GCNN-False: acc = 0.902  -  acc2 = 0.902  -  acc (over) = 0.941\n",
      "\tH-GCNN-True: acc = 0.863  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.725  -  acc2 = 0.725  -  acc (over) = 0.765\n",
      "\tW-GCN-A-True: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.863\n",
      "- RUN: 15\n",
      "\tKipf-none: acc = 0.333  -  acc2 = 0.333  -  acc (over) = 0.392\n",
      "\tKipf-both: acc = 0.588  -  acc2 = 0.588  -  acc (over) = 0.667\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.882\n",
      "\tA-GCNN-True: acc = 0.863  -  acc2 = 0.843  -  acc (over) = 0.922\n",
      "\tH-GCNN-False: acc = 0.804  -  acc2 = 0.804  -  acc (over) = 0.882\n",
      "\tH-GCNN-True: acc = 0.843  -  acc2 = 0.804  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.608  -  acc2 = 0.608  -  acc (over) = 0.686\n",
      "\tW-GCN-A-True: acc = 0.804  -  acc2 = 0.725  -  acc (over) = 0.863\n",
      "- RUN: 16\n",
      "\tKipf-none: acc = 0.608  -  acc2 = 0.608  -  acc (over) = 0.667\n",
      "\tKipf-both: acc = 0.549  -  acc2 = 0.588  -  acc (over) = 0.588\n",
      "\tA-GCNN-False: acc = 0.804  -  acc2 = 0.784  -  acc (over) = 0.863\n",
      "\tA-GCNN-True: acc = 0.843  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tH-GCNN-False: acc = 0.804  -  acc2 = 0.804  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.824  -  acc2 = 0.804  -  acc (over) = 0.863\n",
      "\tW-GCN-A-False: acc = 0.667  -  acc2 = 0.667  -  acc (over) = 0.745\n",
      "\tW-GCN-A-True: acc = 0.784  -  acc2 = 0.765  -  acc (over) = 0.804\n",
      "- RUN: 17\n",
      "\tKipf-none: acc = 0.627  -  acc2 = 0.627  -  acc (over) = 0.647\n",
      "\tKipf-both: acc = 0.608  -  acc2 = 0.549  -  acc (over) = 0.667\n",
      "\tA-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.863  -  acc2 = 0.843  -  acc (over) = 0.922\n",
      "\tH-GCNN-False: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.882\n",
      "\tH-GCNN-True: acc = 0.882  -  acc2 = 0.882  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.765  -  acc2 = 0.765  -  acc (over) = 0.765\n",
      "\tW-GCN-A-True: acc = 0.804  -  acc2 = 0.784  -  acc (over) = 0.824\n",
      "- RUN: 18\n",
      "\tKipf-none: acc = 0.686  -  acc2 = 0.686  -  acc (over) = 0.725\n",
      "\tKipf-both: acc = 0.647  -  acc2 = 0.667  -  acc (over) = 0.686\n",
      "\tA-GCNN-False: acc = 0.902  -  acc2 = 0.902  -  acc (over) = 0.922\n",
      "\tA-GCNN-True: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.882\n",
      "\tH-GCNN-False: acc = 0.804  -  acc2 = 0.804  -  acc (over) = 0.863\n",
      "\tH-GCNN-True: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.863\n",
      "\tW-GCN-A-False: acc = 0.588  -  acc2 = 0.588  -  acc (over) = 0.725\n",
      "\tW-GCN-A-True: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.824\n",
      "- RUN: 19\n",
      "\tKipf-none: acc = 0.471  -  acc2 = 0.471  -  acc (over) = 0.471\n",
      "\tKipf-both: acc = 0.451  -  acc2 = 0.451  -  acc (over) = 0.451\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.882\n",
      "\tA-GCNN-True: acc = 0.824  -  acc2 = 0.843  -  acc (over) = 0.863\n",
      "\tH-GCNN-False: acc = 0.843  -  acc2 = 0.843  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.863  -  acc2 = 0.863  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.627  -  acc2 = 0.647  -  acc (over) = 0.686\n",
      "\tW-GCN-A-True: acc = 0.765  -  acc2 = 0.765  -  acc (over) = 0.784\n",
      "- RUN: 20\n",
      "\tKipf-none: acc = 0.314  -  acc2 = 0.314  -  acc (over) = 0.314\n",
      "\tKipf-both: acc = 0.510  -  acc2 = 0.529  -  acc (over) = 0.549\n",
      "\tA-GCNN-False: acc = 0.824  -  acc2 = 0.824  -  acc (over) = 0.882\n",
      "\tA-GCNN-True: acc = 0.863  -  acc2 = 0.902  -  acc (over) = 0.902\n",
      "\tH-GCNN-False: acc = 0.824  -  acc2 = 0.804  -  acc (over) = 0.843\n",
      "\tH-GCNN-True: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.843\n",
      "\tW-GCN-A-False: acc = 0.725  -  acc2 = 0.725  -  acc (over) = 0.765\n",
      "\tW-GCN-A-True: acc = 0.784  -  acc2 = 0.784  -  acc (over) = 0.784\n"
     ]
    }
   ],
   "source": [
    "N_RUNS = 20\n",
    "\n",
    "best_accs = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs2 = np.zeros((len(EXPS), N_RUNS))\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    print(f'- RUN: {i+1}')\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        # t_i = time.time()\n",
    "        if exp['name'] == 'Kipf':\n",
    "            arch = GCNN_2L(IN_DIM, HID_DIM, OUT_DIM, act=ACT, last_act=LAST_ACT,\n",
    "                           dropout=DROPOUT, norm=exp['norm'])\n",
    "            S = dgl.from_networkx(nx.from_numpy_array(A)).add_self_loop().to(device)\n",
    "            \n",
    "        elif exp['name'] == 'A-GCNN':\n",
    "            arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "            dropout=DROPOUT, diff_layer=GFGCNLayer, init_h0=h0)\n",
    "            if exp['norm']:\n",
    "                S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "            else:\n",
    "                S = torch.Tensor(A).to(device)\n",
    "\n",
    "        elif exp['name'] == 'H-GCNN':\n",
    "            arch = GFGCN_Spows(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                               dropout=DROPOUT, norm=exp['norm'], dev=device)\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        elif exp['name'] == 'W-GCN-A':\n",
    "            arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                         dropout=DROPOUT, diff_layer=GFGCN_noh_Layer)\n",
    "            if exp['norm']:\n",
    "                S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "            else:\n",
    "                S = torch.Tensor(A).to(device)  \n",
    "            \n",
    "        else:\n",
    "            raise Exception(f'ERROR: Unknown architecture: {exp[\"name\"]}')\n",
    "\n",
    "        if exp['name'] in ['Kipf', 'W-GCN-A']:\n",
    "            model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "            loss, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "        else:\n",
    "            model = GF_NodeClassModel(arch, S, K, masks, LOSS_FN, device=device)\n",
    "            loss, acc = model.train(feat, labels, N_EPOCHS, LR, WD, epochs_h=EPOCHS_h,\n",
    "                                    epochs_W=EPOCHS_W)\n",
    "\n",
    "        best_accs[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs[j,i] = model.test(feat, model.S, labels, masks['test'])\n",
    "        best_val_accs2[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'\\t{exp[\"name\"]}-{exp[\"norm\"]}: acc = {best_val_accs[j,i]:.3f}  -  acc2 = {best_val_accs2[j,i]:.3f}  -  acc (over) = {best_accs[j,i]:.3f}')\n",
    "\n",
    "\n",
    "# Print results\n",
    "index_name = [f'{exp[\"name\"]}-{exp[\"norm\"]}' for exp in EXPS]\n",
    "table_comp_over = summary_table(best_accs, index_name)\n",
    "table_comp = summary_table(best_val_accs, index_name)\n",
    "table_comp2 = summary_table(best_val_accs2, index_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kipf-none</th>\n",
       "      <td>0.515686</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.100192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kipf-both</th>\n",
       "      <td>0.560784</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.064557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-False</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.044756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-True</th>\n",
       "      <td>0.856863</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.031677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-False</th>\n",
       "      <td>0.840196</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.042633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-True</th>\n",
       "      <td>0.834314</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.045342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-False</th>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.062005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-True</th>\n",
       "      <td>0.795098</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.034230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean accs       med       std\n",
       "Kipf-none       0.515686  0.519608  0.100192\n",
       "Kipf-both       0.560784  0.588235  0.064557\n",
       "A-GCNN-False    0.837255  0.823529  0.044756\n",
       "A-GCNN-True     0.856863  0.862745  0.031677\n",
       "H-GCNN-False    0.840196  0.843137  0.042633\n",
       "H-GCNN-True     0.834314  0.833333  0.045342\n",
       "W-GCN-A-False   0.686275  0.676471  0.062005\n",
       "W-GCN-A-True    0.795098  0.784314  0.034230"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kipf-none</th>\n",
       "      <td>0.514706</td>\n",
       "      <td>0.519608</td>\n",
       "      <td>0.101484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kipf-both</th>\n",
       "      <td>0.556863</td>\n",
       "      <td>0.549020</td>\n",
       "      <td>0.057635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-False</th>\n",
       "      <td>0.834314</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.041351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-True</th>\n",
       "      <td>0.851961</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.033087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-False</th>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.041316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-True</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.048626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-False</th>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.656863</td>\n",
       "      <td>0.058913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-True</th>\n",
       "      <td>0.796078</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.037920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean accs       med       std\n",
       "Kipf-none       0.514706  0.519608  0.101484\n",
       "Kipf-both       0.556863  0.549020  0.057635\n",
       "A-GCNN-False    0.834314  0.823529  0.041351\n",
       "A-GCNN-True     0.851961  0.843137  0.033087\n",
       "H-GCNN-False    0.835294  0.843137  0.041316\n",
       "H-GCNN-True     0.833333  0.823529  0.048626\n",
       "W-GCN-A-False   0.683333  0.656863  0.058913\n",
       "W-GCN-A-True    0.796078  0.784314  0.037920"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_comp2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
