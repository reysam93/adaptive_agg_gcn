{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f486f766890>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "import dgl\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import utils\n",
    "from gsp_utils.baselines_archs import GCNN_2L\n",
    "from gsp_utils.baselines_models import NodeClassModel, GF_NodeClassModel\n",
    "from gsp_utils.data import normalize_gso\n",
    "from src.arch import GFGCN, GFGCNLayer, GFGCN_noh_Layer, GFGCN_Spows\n",
    "\n",
    "SEED = 15\n",
    "# PATH = 'results/diff_filters/'\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary functions\n",
    "def summary_table(acc, index_name):\n",
    "    mean_accs = acc.mean(axis=1)\n",
    "    med_accs = np.median(acc, axis=1)\n",
    "    std_accs = acc.std(axis=1)\n",
    "    return DataFrame(np.vstack((mean_accs, med_accs, std_accs)).T, columns=['mean accs', 'med', 'std'], index=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: WisconsinDataset\n",
      "Number of nodes: 251\n",
      "Number of features: 1703\n",
      "Shape of signals: torch.Size([251, 1703])\n",
      "Number of classes: 5\n",
      "Norm of A: 22.69361114501953\n",
      "Max value of A: 1.0\n",
      "Proportion of validation data: 0.32\n",
      "Proportion of test data: 0.20\n",
      "Node homophily: 0.13\n",
      "Edge homophily: 0.20\n"
     ]
    }
   ],
   "source": [
    "# Dataset must be from DGL\n",
    "dataset_name = 'WisconsinDataset'\n",
    "\n",
    "A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device,\n",
    "                                                     verb=True)\n",
    "N = A.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without normalizing the GSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BEST PARAMETERS  - 0.61\n",
    "## Reaining params\n",
    "N_RUNS = 20\n",
    "N_EPOCHS = 500  # 500\n",
    "LR = .005  # .01\n",
    "WD = .01  # .005\n",
    "DROPOUT = .25\n",
    "\n",
    "# BEST PARAMETERS\n",
    "## Architecture params\n",
    "N_LAYERS = 2\n",
    "K = 2  # 2\n",
    "HID_DIM = 32 # 8\n",
    "\n",
    "## Model params\n",
    "h0 = 1  # 1\n",
    "NORM = False\n",
    "\n",
    "IN_DIM = feat.shape[1]\n",
    "OUT_DIM = n_class\n",
    "\n",
    "ACT = nn.LeakyReLU()\n",
    "LAST_ACT = nn.Softmax(dim=1)\n",
    "LOSS_FN = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting eval/test acc/loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test acc at best val: 0.745  -  Best test acc: 0.804\n",
      "Test acc (based on loss): 0.765\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "epochs = N_EPOCHS\n",
    "lr = LR\n",
    "wd = WD\n",
    "drop = DROPOUT\n",
    "L = N_LAYERS\n",
    "K_aux = K\n",
    "hid_dim = HID_DIM\n",
    "norm = False\n",
    "act = ACT\n",
    "lact = LAST_ACT\n",
    "loss = LOSS_FN\n",
    "patience = 300\n",
    "\n",
    "iters_aux = 1\n",
    "\n",
    "err1 = np.zeros(iters_aux)\n",
    "err2 = np.zeros(iters_aux)\n",
    "for i in range(iters_aux):\n",
    "    # Create model\n",
    "    arch = GFGCN(IN_DIM, hid_dim, OUT_DIM, L, K_aux, act=act, last_act=lact,\n",
    "                dropout=drop, diff_layer=GFGCN_noh_Layer)\n",
    "\n",
    "    S = torch.Tensor(A).to(device)\n",
    "    model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "    loss, acc = model.train(feat, labels, epochs, lr, wd, patience=patience)\n",
    "\n",
    "    idx_max_acc = np.argmax(acc[\"val\"])\n",
    "    print(f'Test acc at best val: {acc[\"test\"][idx_max_acc]:.3f}  -  Best test acc: {np.max(acc[\"test\"]):.3f}')\n",
    "    acc_val = model.test(feat, model.S, labels, masks['val'])\n",
    "    acc_test = model.test(feat, model.S, labels, masks['test'])\n",
    "    print(f'Test acc (based on loss): {acc_test:.3f}')\n",
    "\n",
    "    err1[i] = acc[\"test\"][idx_max_acc]\n",
    "    err2[i] = acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc at best val acc: 0.745 +- 0.000\n",
      "Acc at test: 0.765 +- 0.000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb478027ca0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeMAAAFlCAYAAADYnoD9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAAsTAAALEwEAmpwYAABYv0lEQVR4nO3dd3xUVd7H8c+ZSW+QUEIJPQhSQ28WUEGs6Fqww+rquvZdy7ruruvq6upad22PrHUt2EVULCgEUOkQ6SWBABNakgnpPef548xMJiSBhAyZYe7v/XpBptzcOXNmMt855Z6rtNYIIYQQwn9s/i6AEEIIYXUSxkIIIYSfSRgLIYQQfiZhLIQQQviZhLEQQgjhZxLGQgghhJ+F+OuB27dvr3v27Omz/RUXFxMdHe2z/VmV1GPLSR22nNShb0g9tpwv63D16tU5WusODd3ntzDu2bMnq1at8tn+UlNTmThxos/2Z1VSjy0nddhyUoe+IfXYcr6sQ6XUrsbuk25qIYQQws8kjIUQQgg/kzAWQggh/MxvY8ZCCCECV2VlJQ6Hg7KyMn8Xxa/atGnD5s2bm/U7ERERJCUlERoa2uTfkTAWQghRj8PhIDY2lp49e6KU8ndx/KawsJDY2Ngmb6+1Jjc3F4fDQa9evZr8e9JNLYQQop6ysjLatWtn6SA+Fkop2rVr1+weBQljIYQQDZIgPjbHUm8SxkIIIQLWnDlzUEqxZcuWFu8rMzOT995775h+d/z48S1+/CORMBZCCBGwZs+ezSmnnMLs2bNbvK8jhXFVVdURf/fnn39u8eMfyVHDWCn1ulLqoFJqQyP3K6XUf5RS6UqpdUqp4b4vphBCCKspKirixx9/5LXXXuP999/33F5dXc0999zDoEGDGDJkCM8//zwAK1euZPz48QwdOpTRo0dTWFhYZ3/3338/S5YsISUlhWeffZY333yTCy+8kDPOOIMzzzyToqIizjzzTIYPH87gwYP5/PPPPb8bExMD1K7Idemll9K/f3+uvvpqtNYtfq5NmU39JvAC8L9G7j8H6Ov6NwZ42fVTCCFEELjrLkhL8+0+U1LgueeOvM3nn3/O1KlTOemkk2jXrh2rV69mxIgRzJo1i8zMTNLS0ggJCcHpdFJRUcH06dP54IMPGDVqFAUFBURGRtbZ3+OPP85TTz3Fl19+CcCbb77JmjVrWLduHQkJCVRVVfHZZ58RFxdHTk4OY8eOZc2aNfXKtXbtWjZu3EiXLl2YMGECP/30E6ecckqL6uOoYay1XqyU6nmETaYB/9Pmq8EypVRbpVRnrfW+FpVMHDd798IvvzR837p1CZSWtm55gs3xqsOUFOjYERYtgvJy3+8/kFjxfdipEwwb5u9SBJbZs2dz5513AnDFFVcwe/ZsRowYwffff8/NN99MSIiJsISEBNavX0/nzp0ZNWoUAHFxcU16jMmTJ5OQkACYw5IeeOABFi9ejM1mIysri4MHD9bb1+jRo0lKSgIgJSWFzMzM4x/GTdAV2ON13eG6rV4YK6VuAm4CSExMJDU11QcPbxQVFfl0f8Hst78dwbZtjR03N6RVyxKcjk8dDhyYz3nn7eNf/+p/XPYfWKz3PrTZNJ9//iMxMdU+22dLPhfbtGnj6eZ95BGfFamOw3qR63A6nSxYsIB169ahlKK6uhqlFA8++CBVVVWUlJTU6YYuLi6murq6Xte0t5KSEqqqqjzblJWVERoa6rn+7rvvsm/fPlJTUwkNDWXQoEF1HqewsJCSkhLsdrvnturqaoqKiuo9bllZWbPqvlUX/dBazwJmAYwcOVL78mwicnaSpsnIgG3b4N574ZJL6t/v7gYSx+541OH778Nzz7UB2tCrF/hgLktAs9r78JNP4MknFSNHnkqnTr7bb0s+Fzdv3tysxS58bfbs2Vx77bW88sornttOP/100tLSOOecc3j77bc577zzPN3Uw4cP5+DBg2zZsoVRo0ZRWFhIZGSkp/UMphFYWlrqeV4RERGEhYV5rpeXl9OlSxcSEhJYuHAhu3fvxmazee6PjY0lKiqKkJAQz21hYWFERETUq6uIiAiGNaOrwxdhnAV087qe5LpN+MiuXXDFFfik2+7QIfPz1luhR4/695eWFjJGRvxb5HjUYXy8GV/buBHuu4+gf42s9j5sYFjS8mbPns0f//jHOrddcsklzJ49m+eff55t27YxZMgQQkNDufHGG7ntttv44IMPuP322yktLSUyMpLvv//eM/EKYMiQIdjtdoYOHcrMmTOJj4+vs/+rr76aCy64gMGDBzNy5Ej692+9XihfhPFc4Dal1PuYiVv5Ml7sW2++CcuXwwUXgC+OwZ8xo+EgFoHrpJPggQdg+3a4+WZ/l0b4mvvv2geTcoPGwoUL6912xx13eC4/88wzPPPMM3XuHzVqFMuWLWt0n6GhoSxYsKDObTNnzvRcbt++PUuXLq1zv7v7uaioCICJEyfW6W144YUXjvxEmuioYayUmg1MBNorpRzA34BQAK31/wHzgHOBdKAE+LVPSmYRWsO8eUceO3nvPTj1VPCaZS8s6NFH/V0CcbxJGFtXU2ZTX3mU+zVwq89KZDHffgvnn3/07f7wh+NfFiGEf8iqk0LO2uRnH34IcXHw889gtze8TUgI9OnTuuUSQrQe6aYWEsZ+VF0Nc+bAhRfCwIH+Lo0Qwl8kjIWsTe1HO3ZAXh5MmuTvkgghAoGEsXVJGPvRpk3mp7SKhbA2GTMWEsZ+5A7jk0/2bzmEEP4l3dSN8+UpFJsrMzOTMa10wLuEsR9t2gTdupkJXEIIIWFcny9PoRjIJIz9aP16aRULIaSbujG+PoXiFVdcwVdffeW5PnPmTD7++GMyMzM59dRTGT58OMOHDz/u5y5uiMym9pM9e8yZk2QhByFEoHdT3/XNXaTtT/PpPlM6pfDc1OeOuI2vT6E4ffp0PvzwQ8477zwqKir44YcfePnll9FaM3/+fCIiIti+fTtXXnklq1at8unzPRoJ41awezd88EHdPzT3WrSXXuqfMgkhAk+ghrG/+PoUiueccw533nkn5eXlfPPNN5x22mlERkaSn5/PbbfdRlpaGna7nW3btrXek3SRMG4Ff/qTWdLycKecYtYcFkJYW6B3Ux+tBXs8uE+huH79+jqnUHzyySePeZ8RERFMnDiRb7/9lg8++IArrrgCgGeffZbExER++eUXampqiIiI8NXTaDIZMz7Oysrgiy/g17+G4uK6/xYt8nfphBCBINC7qf3h448/5tprr2XXrl1kZmayZ88eevXqxZIlS5g8eTKvvPIKVVVVgAnufv36sW/fPlauXAmYEzy47/c2ffp03njjDZYsWcLUqVMByM/Pp3PnzthsNt5++22qq313TummkjD2kSlTzHKWh/+LijIngZg+3Vz2/meT2hdCIGHckNmzZ3PxxRfXuc19CsXf/OY3dO/enSFDhjB06FDee+89wsLCPKdQHDp0KJMnT6asrKzefqdMmcKiRYs466yzCAsLA+CWW27hrbfeYujQoWzZsoXo6OhWeY7epJvaB9LTYf58mDYNBg+uf398PJx1VuuXSwhxYpEwrnU8TqEI5jSKTqezzm19+/Zl3bp1nutPPPEEAD179mT58uXNLvuxkDA+Rh9+CO7XLi3N/PzPf6B7d78VSQhxggr0MWNx/EkYHwOnE66+2pzowd3VfP75EsRCiGMj3dRCwvgwJSVQU3PkbT76CKqqYNUqGDGidcolhAh+EsbWJWHs5Y034Prrm7Ztz54wfPhxLY4QwiKkm1pIGHtZuRJiY+HBB4++7amnyh+QEMI3pJtaSBh7cTigd2+45x5/l0QIYSUSxkKOdPXicEBSkr9LIYQQws2Xp1DMzMzkvYaWQ2yixx57rMVlaIyEsRcJYyGEP0jLuHG+PIWihPEJoKwMsrMljIUQrU/CuGG+PoXi/fffz5IlS0hJSeHZZ5+lurqae++9l1GjRjFkyBBeeeUVAPbt28dpp51GSkoKY8aMYcmSJdx///2UlpaSkpLC1Vdf7fPnKmPGLnv3mp8SxkIIfwnYML7rrtrVjXwlJQWee+6Im/j6FIqPP/44Tz31FF9++SUAs2bNok2bNqxcuZLy8nImTJjAlClT+PTTTzn77LP585//zKFDh7Db7Zx66qm88MILpPm6HlwkjIGnn4YffjCXJYyFEK1NjsxomK9PoXi47777jnXr1vHxxx8D5oQR27dvZ9SoUVx//fVUVlYyefJkJkyYcJyeYS3Lh7HDYWZPd+hgvqilpPi7REIIqwn4buqjtGCPh+NxCsXDaa15/vnnOfvss+vdt3jxYr766it+97vfcc8993Ddddf57HEbYtkxY61h+3b473/N9R9/hLVroX17/5ZLCGFdARvGfnA8TqEYGxtbZxz57LPP5uWXX6ayshKAbdu2UVxczK5du0hMTOTGG2/kuuuuY82aNYA5yYR7W1+zbMv4nXfA/UVnyBA46ST/lkcIYV3STV3f7Nmz+eMf/1jnNvcpFJ9//nm2bdvGkCFDCA0N5cYbb+S2227znEKxtLSUyMhIvv/+e2JiYjy/P2TIEOx2O0OHDmXmzJnceeedZGZmMnz4cLTWdOjQgTlz5pCamsqTTz5JaGgokZGRvPvuuwDcdNNNDBkyhOHDh3tu8xmttV/+jRgxQvvSwoULm7X91Klad++u9QcfaJ2R4dOinNCaW4+iPqnDlrNaHX78sdag9S+/+Ha/LanHTZs2+a4gJ7CCgoJj+r2G6g9YpRvJRMu1jLWG22+Hb76Be++Fyy/3d4mEEFYX8GPG4riz3JjxL7/Aiy9CTAz85jf+Lo0QQghhwTHjjz8Gux127DAzqIUQwt+kZSws1TLW2pyLeOJECWIhROAI1DDWgVagE8Sx1JulwnjjRti2DS691N8lEUKI+gIp+yIiIsjNzZVAbiatNbm5uURERDTr9yzRTb1/P1x/PWRkgM0GF1/s7xIJIUStQDy0KSkpCYfDQXZ2tr+L4ldlZWXNDtaIiAiSmrmcoyXC+O234euv4bTTTKs4MdHfJRJCiFqB2E0dGhpKr169/F0Mv0tNTWXYsGHH/XEsEcYffwwjR8KiRf4uiRBCNC6Qwli0rqAfM87JgRUr4Fe/8ndJhBCiYYHYTS1aV9CH8caN5ueIEf4thxBCNCYQu6lF6wr6MN60yfwcMMC/5RBCiMZIGAtLhHFcHHTt6u+SCCHEkUkYW5clwnjAABmTEUIELvl8EkEfxhs3She1ECKwSTe1COowzs2FAwckjIUQJwYJY+sK6jDevNn8lDAWQgQy6aYWQR3GMpNaCHEikG5qEfRhHB0N3br5uyRCCNE4CWMR9GF88snm5BBCCCFEoArqmHIf1iSEEIFMWsYiaMM4Px+ysmDgQH+XRAghjkzCWARtGMtMaiHEiUbC2LqaFMZKqalKqa1KqXSl1P0N3N9dKbVQKbVWKbVOKXWu74vaPO4TREgYCyECnRzaJI4axkopO/AicA4wALhSKXV4xP0F+FBrPQy4AnjJ1wVtrp9/NmtS9+jh75K0kqIif5dACHGMpJtaNKVlPBpI11rv0FpXAO8D0w7bRgNxrsttgL2+K2LzVVbCnDlw4YVgt/uzJK1k926IjYVXXvF3SYQQLSBhbF1NCeOuwB6v6w7Xbd4eAq5RSjmAecDtPindMVqyBJxOuOQSf5aiFe1xvTyvvurfcjSgqKKIbs92I+ThEPq90I+qmip/F0mIgCPd1CLER/u5EnhTa/20Umoc8LZSapDWusZ7I6XUTcBNAImJiaSmpvro4aGoqMizv48+6gb0QakfSU0N/g//+FWrGAqU7t3L8hbWqXc9+kJ6UTqOAgfdo7qzLXcbn83/jA7hHXy2/0Dk6zq0IqvV4S+/tAVSWLs2DTjks/1arR6Ph9aqw6aEcRbgvYZVkus2bzcAUwG01kuVUhFAe+Cg90Za61nALICRI0fqiRMnHlupG5Camop7f//7HyQmwrRpp/hs/wHtwAEAIouLaWmdetejLxRtK4LVMHPkTB5e/DBJA5IY122cz/YfiHxdh1ZktTp0t4yHDk3Bl0/bavV4PLRWHTalm3ol0Fcp1UspFYaZoDX3sG12A2cCKKVOBiKAbF8WtDlOxMU+CssL2ZO/5+gbNsTpND/z831XoCbIPJTJDzt+oKyqrNFtHAUOAMYmja1z3ZecpU4OFB3w+X4Bdufvpqii6ZPjsouzcVY4j0tZTgTlVeVkODN8tr8tOVtIzUyV4Q0R9I4axlrrKuA24FtgM2bW9Eal1MNKqQtdm90N3KiU+gWYDczU2j9TEbQ+McP4j9//kdPePO3Yftnp9eG/YIFvCtQE57x7Dme9fRb/Wf6fRrdxFDiwKzsju4z0XPe1dv9qR6enOx19w61bm/2FpcdzPZj45sQmb3/Vp1fxt41/O/JGGRnw0Ue1/1avblaZKCysPQvKsSothXXrWraPigqYOxfWrvXc9NTPTzHk/4Yc8Qtak3dfXcHJL57MpLcmMWfLnBbvz2PXLk9vks/98oup22aS2dSiSccZa63naa1P0lr30Vo/6rrtQa31XNflTVrrCVrroVrrFK31d8ez0EeSmWk+q060MF60axF78vdwTN9hvMP4zDPNh+RxVqNr2J67HYAfd//Y6HaOAgddYrvQPqo9kSGRPg/jmrrTEo7s1FPh8cebvHlBeQEAq/c1LSwrqyv5afdPbC7cTGnlET6Qr7oKLr+89t9pp0F1dZPLxb/+BaNHm8MGjtXtt8PQoS0LpU8+gWnTYPhwqDGvw6JdiyipLGFvYcsPqEjbn+a57MvWNpdeCr//ve/255aXByNHwssvN/tXJYxF0K3ANdfVgX7WWf4tR3McKjvEpuxNVOtq8suPoavZ6YSkJHjwQXM9M9On5WtIdnE21doEyDLHska/RDgKHCTFJaGUIikuCUehb8N4S86Wpm1YXg7Z2WaN1CbKKqjdtild1esPrqe0qpRqXc3a/Wsb3khrszzctdfChg3wyCNQUlI7I74pNm6E4mJzSNuxWr7c/Ny589j3sW9fncs1uoblWWa/vvjStcyxzHPZp1/iHI5mvQ+abPt2qKpqUa+FhLF1BV0Yf/wxDB4MJ53k75LU2pqzlXfWvdPo/SuyVngu55bk1ru/tLKUp35+ispq0xL67+r/sjPP60PU6YR27WDKFHM9Pb3ePqpqqnj656cbDZVPNn3Cqr2ryCnP4eWVR/9m7/5wnJo8leySbHbk7fDcl12czT3f3cMdX99B2v40kuKSAEwY+7hl7P2BXVFdQVZBFq+sauB467w889PZ9PFc77Ku2ruqWWXxvjxny5za1zg723TdjBxpFk4/xTXJMD2dksqSOq9zo9yvbwOvM0BOSQ7/Wf6fI/eyJCSYnxkZVFRX8NcFf+Ufi//RpJ6Gedvnmd4Q77pMT2dz9mZPb8LzK55nZdbKo+6rIVkFWbyU8RL/XfNfkuKSGNhhoO++xGkNTic1zlz+suAv3PH1Hby25jXeXfcuGw9ubNm+Xa/H7tULm91NL4c2iaAL47VrYdIkf5eirn/++E9mzJnR6Ies9we3s7R+WPxlwV+4d/69fLr5UxwFDm768iaeXfZs7QZOp/lwTU421xv4kH4z7U3umX8PT//8dL37qmqqmDFnBn9Z8Bce2PAAt8y75aih6b7/0pMvrfcc3l73Nk8vfZp31r2DRnNWb9NN0Se+D1tythxbV3wj1h9Y77nsLHUy6a1J3PzVzeSU5NTd0B0cxxjG3o/TmA0HNxAfEU98aDybs83i6DW6hplzZvLADw+Yjdyvjfu18nrN/rbwb9w7/14+2fxJ4w+i9VHD+JVVr3DnN3ey89ARWr1t23r2sWDnAv6x5B/8deFfj9rToLXmxi9u5O7v7q79guPaj/d74ONNH/PAggeOuK/GvLrmVT5yfERWQRZXD77at1/iSkuhooKKg/t5dMmjvLL6FX775W+5bs51PLrk0Zbt2/V62DJ2MD9jfrN+VbqpRdCFcXk5REf7uxR1LXMso0bXsL9of6P325VZKiy3tH7L+Kc9PwEQZg/zfOB5f/B5wrhjR4iJMROEDrMp23SdubuWvW08uJHiymKWZy1ne5EZB/buom2Id8s4OjS6XquwR5seOP/oJPe+XG4acRNgZlQ7S51sd24/4r6bw7vFlFuS69l3vR6GFraMswqP3q3pKHDQvU132oS28byOW3K2kF+ez4qsFVTXVNcP4y5dICICMjJIO5AGcORW1b59tROEGnidAZZlmdei3hcSbyUlnn00pzvYUeBgb+Fe1u5bS3VuNvTqBaGhkJ7OUsdS4iPiPdsudyw3z7mZlmUto1d0L5x/dPL4WY/7Noxdr7/9UD42ZePNaW9Sraup0TV1/6aOhev1SCqEfQd3HGXjhkkYW5evFv0ICDU1ZsgmLMzfJanlLHWyNXcrYD7IurXpVud+94fA+G7jWbJ7SYPd1O4gLa4sJm1/GvZqOOOj1VRm3EfoiFG1YayU+ZBvoMXk3kd+mWtM+quv4OSToXdvz4fQobJDnu0dBQ7GfOKonSi0YYNZX7SoCJ57jgGb5nFuiI3OsZ0Z3XW0JwAANm/7mSfXx8Of/2wG711dFe7Dm5Y5lnFSO9c4wk8/QXi46bY9Bo4CB6MdcOFWiDrwGP/YALvaePUw5ObCCy/U1snhYfzeezB5MnRwLUTy1luwbRt07Iij1x46RnckKjSqbhhUVcFrr8H118OyZebb3/DhnvFxXaY9j7/MsYwzdsCB6EI2ZW9icEYG2GzQs6fZl80GvXtDejp7ephx48q0NeDsDmecYbbZvt2U/5xz6r62DbzOWmvP69lQL4uHux7efptDvcfw5+XhRBaXk7j7PzAjwkwqa8DGRR8zOR3mJ1dSuG8XbTt2hJAQ0zLutplzo1MoX72QvAjYH1PI9m3L6L9qJ1xzTd0d/fQTfP21mczmNduyRtew3LGc6aWD4S9/AaUY0j+K7x37qfzkI0Ivuazx59SQNWvM+Pqpp9Z53qEVVYxsO9jTawOw89BODhQdIDEmse4+1q+H9983r9UNN9S+dlrD88/XToJbtMjzK3HfpsI3B83vXH+9+dLSmPffJ0qncDWrgGsa304EtaAKY/ck4vBw/5Xhx90/csPcGxjfbTxvTHujznhwQ9/u053p5JXlcW7fc1mye0m9D9AZc2ZQWFEImNm9y7OWM3a/ncfnV8P8J9EREaiqqtowSU6ud8jKXd/cxbcZ35oyuFqSZZdcxCdjYjj5/R9YnrWcEFtInWM59x1Ih8v+BH/4gwng1183rakvv4S//pVJQHKCHZuyMTZpLE/+/CS3z7udr7Z/xamrsrhsThawwWz/yy8AnNzhZGLDYpkxZwYZzgw+2PgB85/cT0GUnQt/15b4iHjmXzuf+Mh4juTPP/yZDzZ+wGNnPoajwMH7P8cwYVMRlT+9x/0a7Brm358O3caZGb8PPeT5Xe10MujFAfSM78WXp72CuvpqePRReOAB8zxnzvRsW/3UaXSN7Up0WDTvrn+X4spiPrn8E2zffw8332xatffdB507w4IF5otB19HkO/PJLc3l6+1fc8PcG3B8Biu7wJDEIRRumkZMjx4QFsYv+3/hyk+u5EUy6bAqnW0pZhhjzMtz0Xs/46Inh3P76NuZOPhcQsoruei9Cznpi6X8C6geMgi7K4xTM1N5eunT9Invg0J5WsQNfbHz8PpS8vcnltOmDKoV2Jd8xd6f0uiypeGWaKen/4/3VkGH+6Ds4D7oNYjtOClfNpeNgyp4ZVsCEz6C/HBY0AsKQp+F/3zCU7ZlqJ69uHv83WZH991nzuaye7dZpcfF/ffwmx9y4EfTbXzm5ZMgE2z/vhxdXIKKjKx9jWqq+dWHv+KO0XdwZu8z65V3142XUZm1h7Mf6EZMWAw/dP8r7V33TWozlA7RHegT34dd+buoqqliedZyLux3IZXVlUx5Zwq783fzf7OLmLzMtX5RaSk89RSPLn6U+Mz93HLnC1QrsNnNR+mXJ9u4YHMNFz77FZTMcRWymrkzxvLZls94Y9obdQt48CBceSXDgHeAhTlnAF0af91E0Aqqbmp3GPuzZfzRxo/YlruN//3yvzqtFGg8jAEmdJsA1O2m1loze/1sz/WC8gLSnelM62Am/bw2DFRZmWmpeY9B7txpbsOMB89aPYsQWwidYjrhKHCgS0qIKK/C5jzEcsdydufvZlSXUfzplD9xfufzASjfusl889++3fyrrDQfnNtNN/AbZ7ajS141lJczNmksVTVVvLDyBaJCozg3YbQpy69/bbruXH1vNmXj6SlmzPrhxQ+zLXsr7ffmk+BwEh0azep9q1m8a/ER61drzSurXyEjL4M3095kX+E+OpeH8VVfCHsQLpthPqirtrnGPnPrBpKqrmZ31mbmbZ9H0UbXjGfXc/J0+/761wDs27WBAR0GeCagzdkyh605W2u337LF/M727ZRVlZFdkk1SXBJxoXHkluQye8NsoiqgayGMKI5z1esG6NMHgM+2fMbW3K2EdOhEYkUoM1NmEhsWS7wjF+Vw8N2Gubyz7h1Cyk1IL189l165VVTaYP+oAeaxq6t5Z907fLntS/69/N88t/w5z3NtaMjDw+mEO+8k/1fn0cbVKz7+3gReHAVRu/c2Oq4fmZlF+xLoWdMG+6F8SEhgSeheeuZU8euhM0kpMmNEbcqhf56Nss0bAFiy4A1eXuU1MdBdh4e17t2Hy3U9WGIO0xsxgmSnZnBBOHYNe9f9VGf7jdkbmbt1Ltd81nCLMmzHbrrnVDKo3cmsO7COHem1E/F+3eMiAJ6a8hSvXmDWdXcfQrU7fzepmalUVldSlXOQFV1gQwdTXq01L6x8gWUL3wZg3A3w/pr/sXrXUmZcYCbARZVUwtlnmx6rwkLmbZ/HW2lv1e+2P+z51wTXR7JohqB65QMhjJc6lgKmuy23NJeljqUMSRxCVGgUewrqH77iDuiebXvSNqJtndZMbmkulTWVPHf2c4TYQsgpyeFA0QGStZkJu8L7dB3eYVxZ6TlUZv0Bc7jN2xe/zbnJ57Infw/lOWbsOqHUBHxuaS7totrx2JmPcfdJd5OckFwbTOnptR8YGRmQkUFNly4sjMvFroHMTMZ0HeMpxr3j72V658nmdFnDh5suQq9jWW8ccSPnn2QCv3MRRFaZn69MeoZQW+hRx+0y8jI8IfN1+tdU62oSSsHpaiw99bs5ANh2uCYvNTBGnOAacs3ftKb2eXn/HG2+TOhcJ+OSxpEUm+T53WWOZbXbLVxo6trhYO8B13ihO4xdr/1v2pou+qScClQNRO3a53mtljmWMajjIE5NuZAOZXZen/Y6E7udSuLBYgB659XOFwDo44Rz6EtmW9jbNc684bOy6tVZdKgJxEa7qSsroaAAEhLY2c7MVdB2O7vaQnoCtC3VODLrLwhSUVVOpwOmbGPK2xNZWEphTBhpMcXElNXw+vgniN5dOy+iVx5EZpr3YacDJWTkZZBdnG0eOzu7bp27uP8e4g/kmnpKTiY808G4CtN1nLGq7hIGyx3mUKrOMZ3rlTcndw+dD1URVg2vDnsIgPz9mZ77+9lMb9JF/S/iuqHX1RmOcP+8Zsg1nvdXegJUbd/KnoI97C/aT7t9ZsgnPcG8lsscy8iLgoIo16nikpPNHI6iInJLc9HoOkNBDT1/GTS2rqAK4/Jy89OXYVxVU8WWnC1sy91GdU01O/J2sCVnC1tytpDhzKC6ppqC8gIKygvYcHADafvTPOOhu/N3s9yx3Hyguyah5JflexbM2JKzhR15O7ApG51iOtEush3OMif7Cvext3Cv5wOhW5tuxIXHsTV3KxpN5wrTD58zsHYcqrq367Kr1UVGBrvzd/Plti8BM16bFJfE/qL95O81QeUJ45Jc2kW28+wrKS6J0i2u2cObNnmOZ92f9iMlm9dzsHMc211HxpCRQWJMIr3a9vI8Dk4nxMdD375mm8O+/Y/tasaO7+lQeybOkaXxpHRKYdGuRZ66dddTYXmhZzt38Nw66lbPbVFF5Z4w7plyOlU2CMt0HYPbQBinhJpx+4KNJoyrt2+lsrqSg7/8DMDOvh089TM2aSzhIbXjHsscy2qfzzffeG7P3bTKU3dxIXFUVFeQ7kzn9ErzjUmVlTEmL4rIghIOdo5jc/ZmM+TQdaxpPRUUQFUVgyraElZlPpCTnabnpDjUPMbJ+SEkHSwjPQF2tDN/uvkbVnvmA7iN7jq63he7Og4dMs+7bRvm21wTjXr25ECFk3TX6/rTgrc87/OiiiIOFh9k/vLZtHH9jQ0viCamuJI99iLP73jGtl0iKmoYnl7ieS6AOQ7Z+0vPwYPs3r3B07XuKHAQX6YIzy/0hDHp6UTsMK+nY00qGc4MT8vd/X6ICImo9zQ3LvvCc7nd3jxCbaHkOLbWbuD13jj8OHjvZVzdYZyRAGRksHTXT57n5IyAvChYvHsxP+z8gS6xXTiUZDrCdZ8+tWHsei3cXyR35+82z/nwcX8JY8uSMeOjuG/+fZ7DiEZ3HV1nDBhgfLfxbM7eTMfojp6JWtMHTueRxY/ww44fyC/PZ2zSWLY7t5O2P422T7StNzu0S2wXQu2htI9qz4qsFXR5xowZPXf2c4DrAz48zvOh277cDkrRYfQkKmw7qbTDE1tf4eGuj3haXTtWfkefnyZ79t+jTQ+6xnVFo8natZ5EasPYWeqsE8a92/amw77UenXx7mcPc/VG+PIk2NXODtTODj6l+ykUVxabLyLuCWXuLwbp6bXH07q2BZhUXTuZLXTnLiZ0m8Bzy5/j5BdPrlPfI7uMZOWN5pjVlVkriQ6N5sbhN/Liyhex1UBYQTF5EdA2oi22sHB2tbURs9vVGnc6OdCjHYm7aoPpwg6nMLdkNhuXzmUAYN9/kEkvj+WaL9dwURSM/O5ScoHOFWEMSRziWYHLruxmolq6K5G8PjiLN5tx8W5x3YgLjfPcPrQwxnP5kj0xQAk3bv4Xc1/6FwATuk+AXebYXA4dYkhR7aEAI4rjmEsBRWEQXQmnVXQhZEcmewaEsjfOvNn//d6d6L6aTjGdqNE15JXmcUr3U9idv7vxbmrXzPK3ds9lTuUG7gVUnz6MS+pIxkHTszN33rNc6TDv+4EdBrIjbwdDd5ZynmsXw/aa7tjt2sme9qFApRkDLqp7HLvdVUUD8kMJsWmW7lnK+c4Uc+PZZ8OKFUx7bDCbksIouL8AR6GDMWXtgWzz/nF9cXBzrl9J8vPJvHPxO1w95GpW7TNfghqa7b5v7RLPZVvGDrrGdSV/X6bXzup+UfP+u3T/HNN1DCFeLeOQ8kr+++XfiQiJ4KS8CjISaugS24W0/Wmk7U/jsgGXUdIjHbYdoKhbIrGuMHb3UuSW5PJzyc9MeH0CCkXFtsvqfAjrGgljqwrKMPZJyzgvD373O6ZvXcAFlVHUKMWr/VbwwBY7I9sNAaW4te82Pse0pq5cnMftA07nPEckMd0TGfUexH/yKF+Uwamb5vDjJZ1ZsNOsG+0+BKZn254s3rWYbnEmlIYmDqX7k7MY6urpcy59AsbVhnH3xev49yroUTgf2rblH1OfoKz71+yvcLIgcyEPg+dQmbYvvc4XkdA35Qwq/v0MSik6RJkWn9NhAjShFLJLsjlzfTGT8nfA2RC1cycvfrKXwq02qmw1hLjWgKiywczN4bQrLmfUaVfwya23w8tTPWH8zNnPUPnqLNSnn9a2jHv0MN3Vjz9uVmNxOQ3ILR1PvOM7tN2Oqq6GBx/kiW5dubdkOBrNxoMbqKg2X3zstjXoL6ehHnyQXfm76B3fm6GdhvLdNd9hdx6Chy/nd1Me4LbbzRKHWR0jGLpmt1mqcf58HN1C8J4fe8U7a0kshfF7oNqmsNdoHnp6DSm5odT07clLVz0E/7qa3/ebQegjj3LjqpVcVjqe7JJstjvXoXeEoOz2OktY1mzbCh2ga1xXzpuzmtPXmsntvSq+MXVQXc2Z28w4fkT/gcy+5C+E28M576TzYKOrbpxOLrIPMvuz2/hDh2mMuHI67R+/BCjnqtXlUFCAs2si35Rv4E92CM/cw5BY+GnXQErHjiTrpitITkgm58uPuO6N76H4fyb0HnkEnn7afFN1hdDS4q3kdU0AnJCczFdX/YP9UzLRL4/g5bQuPJnTkQNFB3gjcSO3ZMNFBV2AvWC3M26x6V3ZaTtETY/uYNsJzz1nnofd1U3rqp8au42zHRH88LGNxXvnwCv/NPe7yvXFezDg1gpPb9CYkjZAtvli6RXG2m7ntpXVLO+uGLLwUYiZzRPpm6iqgbKwPVRfnoW9S1czi3/LFjqUbaotz9NP82p8ETnFhygKhZhKTH18+imMGAE1NTz99TZyS3Pg0/M57+AG1gyMJjGqAzVlkBcJU6b8Dr56meptW/lLSF8mp29nz7kTWPvbT1mUuYhqXc3pPU4nb8HtMH8texOj6OcK4z4b9/LId9A79Q4qqyt5Nw++SdaEzPmQGrsNW7XrD01axpYVVGHs027qRYvggw9I6BxOu8hIeu4u4NRNEFZTDcOAzZu4qzyez5NMS+CRBdB2/k/YKqvQn/zAGcDm9vn0q7TR5sPPGXnKdbzmtfvTe5xOSqcUFu9aTIjNvAxju47h2h9nkR2jqLJBt+37uHOkncToROLC47h1BUzNADgIffrQPqo9/OFPfLftc1bv+4mK6grC7GFwyy3kfvYaQ52hdPtoATxhWmrtokzrt+DALgDalMEu5w6Wvg8wB56A9kuXEvHVNxT278afuu3h7xXjqbbBY7afuWdfFzi5E0Ov/xN0H1LnMKr2Ue3huVfNuSsrK80xz6GhZtbxsmWwv3YsUQEJALGxcMstZkWq9esJy86lCwpQlJZHcch9GBY15lCs5GQc/RyeCVWT+0yGGjPhJ7H7yRBlugd/mNCFDj8coN+8eVBVxR674rOL21JUcohzdtqYEhpFp2zYEQ/tb7ydg+/+l7iSUsq7dKDzLfcwfehV0OYWupTY4blHUJ07E9+pE7ZyKCiEgkF9afPrm2H2bBg7FmbNQu/dS9tubYmxRXDqJ4vYHwX58ZHYOrSDK6+Ep55i8FbTIh08dhpXDLqi9s0Q75o97nQSscsB4eHYBg8mZvc+zut+JpSVQ5s22LskQa8+pI+C5ft/Zke86Sq9cU8HYr7+gZjV6+nwoFl7+7Kl+YxNy4Znn4V58+CDD8whXNOmmeUggVU6i+smPwYHMmH6dOIj44nvFg833USbVatoUwztM4t5Og3CaqCmZ6j5/aFDKfvoLdbGFvNh3B4S2w2E306BFSugXz+46CLT7Z6WBtnZ2C6+GNu77zIubQ1jN24CDVxyCYwahaNTFEn7S5iwx3xJdRQ4uL7A1bXVu7f5OWUKaI066yz44x95bY4mrHoz1YNstC+oIlqFcvL2Sg59+Sltr/0NPPYYAKeGKAqi7MT99g747jvOXJFDuR02t4eTJ15KePpOU8avvwat6dUuhuqwMvT+/fTZ4ODmsigoKMCmTct44qnXAi/TuRB+vfYQAN1uuheiO3LZwNpDrg5Ou4C3f/6E9u3t9IuOBqeTaT87mZwBRSE5hJSUclUmXOr6rvDWtUP49Ztp5oqEsWUFVRj7tJvaFTJn3RjOtLHX8uC9X9J+407KosKJWL0azj6bvnvMX1N8CSSUAZiWj6qsZOmACCZfXsYNUeN59b4f6ZtXd3h+bNJYUjqlALVdYuM7jiBEw1dT+3Cgcwx//k8aY8raY7fZiQuPo4/Xgkee5Qxvvx29sRNlH//A77/5Pc+f+zwLbz2Xs+Ke4Z/2s7n/r9+aMbpevUiINL9TetAs4m8DcvfVXZwgpKAAwsPJWvA5T80aTm5KP8qry3lv/c/ccPt3tEtIrt04Odl8mIGp/F27TDdlbCz0729uf+GFY6r+j5b8kwcWPMBpPU5j8a7F7H+rHcWrvie9wy6Gdx5eu6G7q9FdH8CqM/vz6sAyVr8US8f0veREaBLu/SuPfnc3aVecytkzUxn1d7PkUemfn+DBlDzeXvc2P1z3Np17nVG7v7VrzcHrjz4KM2ZQU5rHqH8lMKCD4rPpU/lmjDk5xE3vhVFy0PUlYc8e7NU1/HUSRNw0k5fOe8ns79NPCdm6FUcsDOs1vu6TdZfd6TTvu969zXquS5fWLlby+OPmiw2gP78B0n4mPcGEcWxyR8C1zKZLtxzzx1Cdvh2nvZwOwGfpX7Lj53Su+SWLREy369hu4+CVP9Utz//9n+di9YMPEPWIacna3nwLTj8dgLXXncKUd6YAB7k27mx46aUjv6B33cWhIcl0WJ9BdXQU9o8+ogbNpOvtbH/MTE57b/17bMnZQu/cAZS3b0+4e/Web7+t3c/ixYR99RUA6fPeYdTrw7im30W8edVHFG9ZR1uvtbbDqjS7e8QR98wzph4TEgivhh3tbQyd/aHpunjrLc/hbKtuv4Sz9Jt8e81jFJ1/NiOc2vP+KowOoW0X8+WgXSl03F8Id91lvpwcpt34M7nuVzBz26f0KnPQuyCPnlXVrOgKq1+9g8q9e7jv0mcJq4FFp3bjb0Ny+fFCeG0uEsYWFlQTuHzaTZ2RgU6IZ7etgKS4JCL7DwagqncPz+IaifsLaRvRlhvanF7v18t6mIk7w8ZeDErRLbvcc19ceByTe09mWKdhANw34T4A+oabGaF9u6fQe4QZ7z2zpicA8fYYeh7yeoCoKM/F03qYBRpeWvUSS3Yt4eIPLgZg4DjXB4Xri4V7XLgqp3Z2c+R+r3FFrQkpKoKEBAYlDqZ7m+68kfYG761/j5PanUSf+D51n2SfPrWHUe3aZYIrO9vc5hWOx+KcvufQJrwNv04xhxktDTtIyeZ15Jfne1rGQINhPDRxKI4CB0tCzZeO4tgwrh1yLYM7DubCfuasn9MHTqdfu35EhERwYb8L6RLbhdFdR9fuNyEBVq6sfZ5AfGQ8Y5PGsil7E6e9cRp3fnMn98y/hx3qENU5OaZcrrrO6xLPBSddULeucAWga/GTOo/lfi7p6WbbPn1Mnbp7FLyen/t0lOU9upKcp+ivXMeYl5ZCmTlOqesBM2XcXlSM3myW5vxw6avcM/8evvn2BfZHQ2mEnVFdRh3xdYjqP6jecwAz+9+tzutxBBH9zOIeh7qaBWq25mwlPbSQisgwkp3wf6vNl4CeudWUdmnkWFtXGfa3DWF3hZmRPa7PRPbEQcXWTfUmRBUkueomPp7yOBPupd27oNzrT3o9p+iThwBw6YeXkpEAXQ6WQo6ZWNap+wCU6zUYdiiCkNKy2iMYDtMpppNZ3SvtTRbnrqXIuZ8+eWYCWG5pLrvCyyhyfUb9HHqAPQV7cEewjBlbV1CFsU+7qdPTKetpxnKT4pI8f6gxJw819ycnE3KogLwbM3iy12/r/fq559xJ4Z8KufXUP0D37rTfa7pce7btSf79+fRt15fI0Ej03zS3jLoFAFuxmXl6+qDzuPKivwBwt2vGcY98G6E1sGmI60PK67y8iTGJOH5vWtdvpL1BYUUhL5/3MhdM+q3pJnCHsaubGmdtE3uk95nuCgsJdR3yEmILIf32dAruL6Dg/gI23rKx9gPMLTnZBPGePXU/BLVucRindErh0P2HOMPVUk1PgD55oGo4ahg/MukRCu4vMM8fuPWsP9MhugPrfreOP4z7AwDvX/o+W24zxyJfOuBSsv6QRUxY7WQrEhI8pwX0/tD98dc/EhkSyYHiA8SExVBwfwE18W1JKMUcAuWqh8/+sp5z+p5Tt66AiWfdUPs6eD8WmGOi09NrZxHX1JgVpA57fr8b9TuK/lTEry74I9EVmu57vSZN5eWB00lkQQnbhpr3b0fXypejI5O5YtAVdM+uJD0BDt1/iOiwo6wd637uERFmPoJLn4TaEGtqGMecnALAznamQ26pYykoqOnd2zPb+qnJT5G4r6DxMHaVZ1t8jedQwcm9J7Ozvc30ALnqv2qi+YJa3rO751fDXV8Grrzor/WfH3DS6HMBKKwopNPQCYRWVpvVt4B/XPqSGXaJjeX6StcXlD6HfTl1CbGFeJa3LQqD6MJyuhaa93BuSS65ZU4c7c2H1MY2pgUR7TXpT1hTUIWxr7up3YcoJMUl1V/Y3/ukDA0sS2jr27f2wz05mdg9pjXqnqzVIPdM1JgYiIuDjh2xuQ4DCcs047yFk1xdnIfNBO0a15U+8X1465e3AEyI2WzmA8NVvjB7GDFhMUQUlHh+b7T3JFSnk5DCQs8Hf6g9lNjwWGLDYz3j2nUcqQ5aGMZu7uNH0xPMMcldCo8exkopYsNjCevQCYCQstpeiSZz7y862oyDu9htdvq2M4dsdYvrZuqmXQcSSjHDAOnpVIeHm1W5vB3+vvHmPmnDxo2mdesOYzDjsIc9P4DosGhs7kPHXCucAaY+XO+Zqiln1fmdQSGdOaXbKSQ7oSCpfd0vH41xl6NPH/N+crEpm6enpalhrFzlTYsx3enLHMuIj4gnvP8ATxhP7jQe9u2jtGvXhnfiDuO2NWw4aBYU6d6mO4XdOhHvcH2ZiY8nb7B5LE8def2uzfuUbomJ5jWOiyOh+0n0TTDbJw2faO4/vP4TEmrru5GWMUBljVmoJaFDN8Jd8/zSXS1jZ6mTfZ1iPLeF2cPoG2e+5EvL2LqCMoxb2jKuKS+jZvcu5lSahQ/qhLH727D7+pgx5pAO9xJ97p/e35qTkwnZuYtwe/iRP7i8w9j9GK4P1shdpgkbd+Fldbfx4u7+jI+I93yoeCZZnXEGJCWx7YlSpm2BXFcxZ6R57cDp9LSMm8Q7jDMyTOvJzUdhHGo3B9m6j2V9ZCFMPO06+NvfPGUGagPNW5Krrm3H8DZ3l79Pn3rnt/M+JSSAvX1HEkqh//dp8MwzlHXuXP8xD3//eLPbTfnfeqt2G/f27tsaqk/3NpWVdbu6Xe+ZzhfVXZWqt27LuHYpdCsAkvvSJAkJZoJZA+VOds0f8D4s7ohc5V0ekctTPz/Ff9f8lzFJY1DJfTkpF/Y8DYPHmyGWo7WMMxJMy7pjdEfCQ8IJOak/bYurqHrtVQ52iuXJg58BEOndzd7Qa+Bez931Orv/hpJHTzX3H17/CQmmvu12c7TAUQxLPtVzOatjBLmlueZfl7au28IZ3nk4CREdASivKj7qPkVwCqoJXL7qpt63ZzNdazTZMTauGnwVPdr0gDHdzeEhl1xiNurfH847z8zy/fZbs7D+xRebEx78/HPdb83JyaicHP6ecj9D+9cfX/ZoKIwXmMOhrut0NjVqFieddrGZFDV1ar1f/83w35BVmMU5yefUdiknJ8MXX5hW16mnsqymjNySXFIHx3Bz9TC2bFpCUlkoUzdX1msZH1XnzubLh7tl3L8/XHst7NhhDlvxkaenPE3fglB4+w5m/AI2fcCsOf33v5sFSTp3NicrONyMGWbM9c47m/+gN95oPnTPP7/eXe4VudxhnJw8GuYtZfoO8w1n5/XXM+jwXzrjDFPec89t+PGeeMK0wmJjzSSpiAgzcWzHDvP8Gvrgdx86Vl1tXucVK2rHnYH4YeOYe8uZ9M2uoe9ni+heHYMqMe+tgeMubHpdvPhigyc6ePdX7/LI4kc8Y9hHNWoUGb+fycdhb5I/38yTuGXkLTDyJDJ3rKaoNJ+kxCEQFUXe6NEN7yM5mb1/vYv/FT/HPsdyc5w2EHLlVby0ZgHh1ZXM6b+bJd2hy7kJ3HjO1bW/e/315otF0mFfiP/5T8+Xp9+O+C3xEfEkDRpv1jTfswe6dTNHB0Dt30bPnqbbuhFfXPkFa/atod8vMcB7APQZNYX52SvN2dfOHcalU+7i1mHFZi5GmlmgpKxawtiytNZ++TdixAjtSwsXLtTvvqs1aL1lS8v2tXrJR1qDTnv890feMCfHPCBofcMNjW/36admm1Wrjry/zz4z261da67//e/mekmJ1rfdpnV8fHOehvHSS7Vl/PZbfdb/ztI8hO75XE/93NLnNA+hr31kuLn/gw90VUSE1nff3fT9Dxyo9YUXat2vn9aXXNL88jVVVZXWoaG1zyUiQuvqaq1PO03rU045fo/bgIdTH9Y8hP7rgr+aGx5/3JRp0CCtzzpLL1y4sPUK07u3eeyrrjI/X39d6xkztO7ate52w4Zpff75Ws+ZY7ZbsaL1yuglvyxfq4eU5iH040seb3S7I9VhZXWljn40WvMQ+p5v79Faa72/cL/mITz/1ENKHyo95Ovia33ZZab+pkxp2vZvvWW2b9dOP7/8eU/57vr6rjqbfXj7dVqD/vDfS31a3FZ9LwYpX9YhsEo3konSTe1SWV1JZbUZ58nONuOzbdsfZSwsIaG2e/QI40d1unOPpKGWMZjWkXtVq+Y6rIXuPrwpLjzOc5amTt1dp7Dbuxd7WVnzHic5GbZuNWU8Uh20lN1e2zqLiDCzhvfurZ3w1IoO76b21NeGDa1elnpdr+6W8eHlSEio02pu9XK6xIXHMbDjQKCBWeVNFGILYVTXUXX24b0kK5gzhLWJaNPC0jbA/Vo3tf68/pa9n6/779BNuVrmWte0uIjixCRhjDkJe5vH2xD7z1h25+8mL9usg5vQofuRf9E93gRH/uN0L17Q1DB2H1/pHeItDeOQEOjenY5RprutbURbbMq8/Cknm5MZeNYMPpYwrqw8/h/w7v27u+jXrTOB3MrB0qOt6TLu0cbVdexdX/4KY3eX9dHCOCOjdhzYT8YnjSfEFtL07u1G9gF1A318t/FEhkQSagtlXNK4FpezQS0I46GJQz03d4zueNiGZlhJ10gYW1VQjhk3dzb1j7t/pLTKHJe58eBG8nPNFOOYhE5H/+XkZFi16sh/nNHR5rCQTZvMIUltGvnG3ljLOCPj2MO4WzczttWzJ4SEcPf4u+nepjuTek1iSOIQusZ15bIBl0HU7eA6HrVZH9SHtbyPK+8wnjPHnCu5NR73MBN7TmT2JbNrT0zvXV/+CuN27cz7IyPDnCWroTDOyTGnffRTq9jtwdMf5NIBlx79sKoj+MO4PzA2aSxd42pnXT96xqPcMOwGqnU1/dv390VR63O/1scQxqH2UOZdNY+tuVuZPmh63e1s7kmCMpvaqoIqjI+1ZXz4OYcLnfsAULGxR//lfv1qDyE6kpNOgvfeM/8+/RRmzTIh7j6VHNSGsXtBD3c3uDuMj/YYDQkJqZ0tijnO+d4J93ruvnzg5eZChw7www+1l5vK+zCRvk2coXus+vUzP886y3zjevnl1nncw9iUre5ylh29Wjl9+5oTxrcWd5107Gj+ffBBbTm8dehgwnjhQrj6avypa1zXOiF6LNpFteOCfhfUua1H2x6eXovjxn2Ym/f7/kjcX6Bdr9M5fc+pe/y5m5KWsdVJGGPCeFinYaTtT2N51nLKM8zJHxo6fKieO++EiRPNLNgjeeEFMzP67rvNrFev0+95FBWZIHYvtA+mxXPo0LG3jMGsoXy05/Luu7BmDVscDvqfdlrT93366fDGG+a46MaODfWVmTNNl3+fPmaG+JYtpqUybNjxfdyjGTgQ3n7bvG4nn9y6YTx1Knz+OYwbB6+/DsuXmxnuh88Cv+02Myu7urrBGeKiiS6/3HyxcX8JOpr+/c179ShHF7iPfpAxY+sKqjA+lkObqmqqWLl3Jb8Z9hv2F+3ntbWvcbMr1JsUxvHxJoyPZuBA8++FF+qvVuU+DKm4uP5jRkebRfdda+sek6FDj77NhAkwYQL7U1Pp39BhQo2x2z1r+x53UVG148WTJ5t/gUApuOaao293PNhscKHrMKXRo82/hiQmmpNyiJaJiIBzGmjZHklTvvy45m8gYWxZQTeBKySkeWs8rD+wnpLKkjrjTykxru7gpoRxcyUnw+rVtddLalfDoqio/mPGxJhJSj5YYlIIEaBs7paxjBlbVdCF8bGOF4/rNo6SShOMo+MGmNaOezUtX3KfXMHNe1nLxsJ4t5ndLWEsRHByd1PXyHKYlhVUYVxe3vyZ1MuylpEYnUiPNj3YW2iWnOxuTzDdw8eyjOLRHD4Lsylh7DpzjISxEEHKPVQlE7gsK6jC+FhbxmOTxqKU4v1L3ue8vueRUB12fLqoAQYdtlCidxgXFjYcxm4SxkIEpdozokkYW5Wlwzi3JJdtuds8CwecnXw2X171JaqhiVS+MmmSOaTpk0/Mde8wbmiSlnc5/LhQgxDiOHJN4JKzNllX0M2mbk439Yosc3q0esvyNdRd7CtKwYgRtccreodxQ4cvSctYiOAnhzZZnqVbxksdS7EpW/1l+Y5nGLt5n/IOzFjR0cJYWsZCBCVPN7WEsWVZOoyXOZYxJHFI/ZOst0YYR0aaZrw7jAsLTSAfHrjucsTEtPzckEKIwCSHNlleUIVxnW7q7Gyz9F8janQNy7OWM7arq4t67lyz6AaYMI4+9nVzm0Qp0wrOzob//tcsBgKNt4yli1qIoKXci37IbGrLCqowrtMyfuklswRddXWD227J2UJBeYEZL165EqZNg/vMCc/Jza09NeLx1KGDWcrwppvgL38xt0kYC2E9MmZsecEbxnl55rR+3itceXEv9jE2aWztIhx795pW8YEDx3ZShubq06fuBC6QMBbCgmrXppZuaqsKqjCu003tPgOS++dhlu5ZSnxEPH3b9YUsc8pE4uNrz+nbGqeZayjwJYyFsBzlXmBIWsaWFVRhXKdlfJQwXpa1jDFJY7ApW20A22y1J3FojTB2P4b36e4kjIWwHC2zqS3PkmFcUF7AxoMbGZc0ztzgDuC8vNpgbo1uancYe58btbHZ1BLGQgQtd8tYOqmtK6jCuMFu6sJC+Oc/YdEiePxx0JqtOVvRaIYmDoVPP4VvvzXbOp0mmDt0MOfnPd7cYezdCj981RJ3GMsxxkIELfdimFpmU1tWUK3A1WDLeMMGeOCB2o1mzCC3NBeAjtEd4RGvc7w6neBwQPfurVPgpCS46CJzPtqUFFiypP42nTqZ86FOmtQ6ZRJCtD45n7HlBX8Yp6XV3Sg/n9wKE8YJEfGmJXzHHaYFPX++WYyjXbvWKbDdDp99Vnt95sz624SGwhdftE55hBB+oWTRD8sLqm7qogEvsCn2edcVVxj/8kvdjZxOT8u4fVGN2S452YzJ5uU1vCSlEEIcR7IcpgiqlnHZGbezCIDba8N4/fq6Gzmd5EabMG6bZX6SnAwFBWYFrn37ZHxWCNGqPIc2yVmbLCtoWsZ1ene0rg3j0tK6GzqdOEudtI1oi32Ha7GPPn1qW8NFRdIyFkK0MlmBy+qCpmVcU6M8l6tLS7B7L4MZFmYGlAF+/3vGndeda38ug+I/mmOLe/asG8ASxkKI1mR3T+CSlrFVBU0YV1TUhnHiQzHkeN85Y4Y5ZOiFF8DpZPo7TuwamNgfbrnFhHXXrrXbSxgLIVpR7ZixhLFVBU0YV1XV9rjHVBx25/DhcPPNMGsWVFSYIAZ4913o0sVc9j7WV8JYCNGa5EQRlhc0Y8aVlbUt43ph7A7aito7ysPs0Llz7TaJibWXJYyFEK3I3TJW0jK2rCAK48Zbxp+Up1FQXlDnttzObT3fRoG6l2U2tRCiNbmXw5SWsWUFTTe1d8s4utL8rFJQo2D68nt5OK4Cr3W4KOvZlUZJy1gI0Yps7m5qWZ3asprUMlZKTVVKbVVKpSul7m9km8uVUpuUUhuVUu/5tphHV1pZ5bkcV+660LULqk8fkjv2Y6ljqef+GgW9Rk2pvxP3CRukZSyEaFUygcvqjtoyVkrZgReByYADWKmUmqu13uS1TV/gT8AErXWeUqrj8SpwY0oqasO4e775GfLW2xAZyfiD/+WLbV/ww7wXeeSDW3nxlMcYeO6M+jtZtAhWrICIiFYqtRBCeC+HKd3UVtWUlvFoIF1rvUNrXQG8D0w7bJsbgRe11nkAWuuDvi3m0ZVW1g4UJzsxZ12aNAnGjWNs0lhySnK4NeM//NQnhF4z7qydRe2tUydz0gYhhGhFnhW4pGVsWU0ZM+4K7PG67gDGHLbNSQBKqZ8AO/CQ1vqbw3eklLoJuAkgMTGR1NTUYyhyww65VtwKI5JzK7pRmKhZvcgsjhlfHs+A2AGUlZVxQacLWPHTCp89brApKiry6etiRVKHLWe1OszM3AXAwQP7ffq8rVaPx0Nr1aGvJnCFAH2BiUASsFgpNVhrfch7I631LGAWwMiRI/XEiRN99PDw9crPAbh/wGv0eedBGDYM7/1fdvZlPnusYJaamoovXxcrkjpsOavVYU3GWgA6dujg0+dttXo8HlqrDpvSTZ0FdPO6nuS6zZsDmKu1rtRa7wS2YcK51ZS5uqk7FpWY0yJ6L+IhhBABTMmiH5bXlDBeCfRVSvVSSoUBVwBzD9tmDqZVjFKqPabbeofvinl0ZdWVoOE3t91hbjj55NZ8eCGEOHZy1ibLO2oYa62rgNuAb4HNwIda641KqYeVUu7ZTt8CuUqpTcBC4F6tde7xKnRDSivL6VAM4SUlcP75MH16az68EEIcM5tn0SFpGVtVk8aMtdbzgHmH3fag12UN/MH1zy/KqyvNLGqA3/3OnPxBCCFOBNIytrygWQ6zvLqiNoxlvFgIcQKRMWMRNGFcUVNJnzzQ7vMTCyHECcJznLGwrKB5B0QV5HLHcqjo0kW6qIUQJxT3ClwyZmxdQRPGVy/9jvgyqBw61N9FEUKIZlGutam1jBlbVtCEcZIzm51tofR/r/u7KEII0TzKvRymtIytKnjCON/Jj90hOibW30URQohmsdnd3dTSMraq4Ajj8nISCwtIT4CI0HB/l0YIIZpFKTm0yeqCI4x37sQGpLexY1PB8ZSEENYhhzaJ4Eiu9HQAMtpKq1gIcQJyHdqkpJvasoIjjJ1OSkPs7Ixq7++SCCFEs7mPM5aWsXX56hSK/nXddXRZ+xKlZaH+LokQQjSbzX2csZaWsVUFR8sYKAvfT1hZt6NvKIQQgcZzaJOEsVUFRRhrrSkP30d4eZK/iyKEEM1W2zKWbmqrCoowzinJQdsqiKyUMBZCnIDcY8Z+Lobwn6AIY0eBA4CoKgljIcSJR8kKXJYXVGEcXS1jxkKIE4+7m1oObbKuoAjjrMIsAGJrpGUshDjxuBf9kBW4rCsowvi3I35Lrw93EmtL9HdRhBCi2Ww2mU1tdUERxkopaoo7EB4WFE9HCGExWsmJIqwuaNKrqkoRFubvUgghRPMpuxzaZHVBFMY2wmVpaiHECcgmi35YXtCEcUWFTVrGQogTk4Sx5QVNGEs3tRDiRGWTbmrLC5owrqyUbmohxInJfdYmmcBlXUEUxtIyFkKcmBTSTW11QRHGNTVQXS1jxkKIE5PNLi1jqwuKMK6sND+lm1oIcSJScj5jywuKMC4vNz+lZSyEOBEpWYHL8oIijCsqzE8JYyHEiUjhXoFLZlNbVVCFsXRTCyFORNIyFkERxtJNLYQ4sclZm6wuKMJYuqmFECcyOc5YBFUYSze1EOJEpFzLYSpZgcuygiKMpZtaCHEikzFjERRhLN3UQogTmut8xlq6qS0rqMJYuqmFECcid8tYScvYsoIijKWbWghxIlNyCkXLC4owlm5qIcSJTGZTi6AKY+mmFkKciKRlLIIqjKVlLIQ4ISn3cpgSxlYVFGEsY8ZCiBOZZ21qOc7YsoIijNu3h4ED84mO9ndJhBCi+eQ4YxHi7wL4wvnnQ0zMWjp2nOjvogghRLN5VuCSbmrLCoqWsRBCnMikZSwkjIUQws9kNrWQMBZCCD/zrMAl3dSWJWEshBD+5j60SVrGliVhLIQQfiZjxkLCWAgh/Kz2fMYSxlYlYSyEEP4mp1C0vCaFsVJqqlJqq1IqXSl1/xG2u0QppZVSI31XRCGECG5yoghx1DBWStmBF4FzgAHAlUqpAQ1sFwvcCSz3dSGFECKYSTe1aErLeDSQrrXeobWuAN4HpjWw3SPAE0CZD8snhBBBT8mJIiyvKcthdgX2eF13AGO8N1BKDQe6aa2/Ukrd29iOlFI3ATcBJCYmkpqa2uwCN6aoqMin+7MqqceWkzpsOavVYVlJPlOB8tJS+VwMMK1Vhy1em1qZ/pVngJlH21ZrPQuYBTBy5Eg9ceLElj68R2pqKr7cn1VJPbac1GHLWa0Oi4qcAEREhPv0eVutHo+H1qrDpnRTZwHdvK4nuW5ziwUGAalKqUxgLDBXJnEJIUTTyJixaEoYrwT6KqV6KaXCgCuAue47tdb5Wuv2WuueWuuewDLgQq31quNSYiGECDKy6Ic4ahhrrauA24Bvgc3Ah1rrjUqph5VSFx7vAgohRLDznChCJnBZVpPGjLXW84B5h932YCPbTmx5sYQQwjqkZSxkBS4hhPAzGTMWEsZCCOFnsgKXkDAWQgg/U8ja1FYnYSyEEH6mlKIG6aa2MgljIYTwM4VCK1DSMrYsCWMhhPAzz9rU0jK2LAljIYTwM+VqE0vL2LokjIUQws+UMt3U0jK2LgljIYQIABokjC1MwlgIIQKAmcAlrErCWAghAoBGDm2yMgljIYQIAFqBrMBlXRLGQggRAGTM2NokjIUQIgDIoh/WJmEshBABQHv9L6xHwlgIIQKAHGdsbRLGQggRAMxsan+XQviLhLEQQgQMSWOrkjAWQogAIBO4rE3CWAghAoAc2mRtEsZCCBEApGVsbRLGQggRAGQCl7VJGAshRACQQ5usTcJYCCECgEZJN7WFSRgLIUQAkBW4rE3CWAghAoBWcgpFK5MwFkKIAKCRdrGVSRgLIUQAkEObrE3CWAghAoA5tEnC2KokjIUQIlBIGFuWhLEQQgQAreTQJiuTMBZCiACgAeXvQgi/kTAWQogAICtwWZuEsRBCBADTMpYwtioJYyGECAAym9raJIyFECIAaCUjxlYmYSyEEAFAWsbWJmEshBABQFbgsjYJYyGECACyNrW1SRgLIUQAkEU/rE3CWAghAoSMGVuXhLEQQgQIaRlbl4SxEEIEAO35T1iRhLEQQgQAGTO2NgljIYQIALIcprVJGAshRAAwJ4rwdymEv0gYCyFEAJAVuKxNwlgIIQKARsn5jC1MwlgIIQKALIdpbRLGQggRAMyhTRLGVtWkMFZKTVVKbVVKpSul7m/g/j8opTYppdYppX5QSvXwfVGFECJ4ySkUre2oYayUsgMvAucAA4ArlVIDDttsLTBSaz0E+Bj4l68LKoQQwUwObbK2prSMRwPpWusdWusK4H1gmvcGWuuFWusS19VlQJJviymEEMFNK1CSxZbVlDDuCuzxuu5w3daYG4CvW1IoIYSwGoW0jK0sxJc7U0pdA4wETm/k/puAmwASExNJTU312WMXFRX5dH9WJfXYclKHLWfFOowHdE2NfC4GmNaqw6aEcRbQzet6kuu2OpRSZwF/Bk7XWpc3tCOt9SxgFsDIkSP1xIkTm1veRqWmpuLL/VmV1GPLSR22nBXr8BcUNqV8+rytWI++1lp12JRu6pVAX6VUL6VUGHAFMNd7A6XUMOAV4EKt9UHfF1MIIYKbHGdsbUcNY611FXAb8C2wGfhQa71RKfWwUupC12ZPAjHAR0qpNKXU3EZ2J4QQogEaJRO4LKxJY8Za63nAvMNue9Dr8lk+LpcQQliKViBnirAuWYFLCCECgGkZSxhblYSxEEIEADNmLKxKwlgIIYTwMwljIYQIAFopmU1tYRLGQggRAOSsTdYmYSyEEAFAo2TM2MIkjIUQIkBIN7V1SRgLIUQAkLM2WZuEsRBCBADTTS1pbFUSxkIIEQikZWxpEsZCCBEANApZDtO6JIyFECIAaKRlbGUSxkIIEQBk0Q9rkzAWQogAoJG1qa1MwlgIIQKBkjFjK5MwFkKIACBjxtYmYSyEEAHAjBkLq5IwFkKIAKC9/hfWI2EshBABQUk3tYVJGAshRADQSk4UYWUSxkIIERCkZWxlEsZCCBEAzHHGksZWJWEshBABQGZTW5uEsRBCBAA5ztjaJIyFECIAyNrU1iZhLIQQAUJaxtYlYSyEEAFAK2kXW5mEsRBCBACZTW1tEsZCCBEAZDa1tUkYCyFEgFBaWsZWJWEshBABQCMtYyuTMBZCiAAhs6mtS8JYCCECgRxnbGkSxkIIEQCkm9raJIyFECIAaCXd1FYmYSyEEAHAtIwlja1KwlgIIQKBkvMZW5mEsRBCBACzApewKgljIYQIAFpJFFuZhLEQQgQEGTO2MgljIYQIAFqBZLF1SRgLIUQAkOOMrU3CWAghAoFScqIIC5MwFkKIACEtY+uSMBZCiAAg3dTWJmEshBCBQBb9sDQJYyGECABm0Q9JY6uSMBZCiEAgLWNLkzAWQogAIGPG1iZhLIQQAUArCWMrkzAWQggh/KxJYayUmqqU2qqUSldK3d/A/eFKqQ9c9y9XSvX0eUmFECKIaVn0w9KOGsZKKTvwInAOMAC4Uik14LDNbgDytNbJwLPAE74uqBBCBDMZM7a2kCZsMxpI11rvAFBKvQ9MAzZ5bTMNeMh1+WPgBaWU0lq+5gkhRJMoRceSGlZ0jvXZLiO1ZoWcmvGYFYeFod76pFUeqylh3BXY43XdAYxpbButdZVSKh9oB+R4b6SUugm4CSAxMZHU1NRjK3UDioqKfLo/q5J6bDmpw5azYh06RpzForyDcqxxACkJCUO10nuxKWHsM1rrWcAsgJEjR+qJEyf6bN+pqan4cn9WJfXYclKHLWfFOjTP93mf7tOK9ehrrVWHTZnAlQV087qe5LqtwW2UUiFAGyDXFwUUQgghgl1Twngl0Fcp1UspFQZcAcw9bJu5wAzX5UuBBTJeLIQQQjTNUbupXWPAtwHfAnbgda31RqXUw8AqrfVc4DXgbaVUOuDEBLYQQgghmqBJY8Za63nAvMNue9DrchlwmW+LJoQQQliDrMAlhBBC+JmEsRBCCOFnEsZCCCGEn0kYCyGEEH4mYSyEEEL4mYSxEEII4WcSxkIIIYSfSRgLIYQQfiZhLIQQQviZ8tcS0kqpbGCXD3fZnsNO2SiOidRjy0kdtpzUoW9IPbacL+uwh9a6Q0N3+C2MfU0ptUprPdLf5TjRST22nNRhy0kd+obUY8u1Vh1KN7UQQgjhZxLGQgghhJ8FUxjP8ncBgoTUY8tJHbac1KFvSD22XKvUYdCMGQshhBAnqmBqGQshhBAnpKAIY6XUVKXUVqVUulLqfn+XJ1AppV5XSh1USm3wui1BKTVfKbXd9TPedbtSSv3HVafrlFLD/VfywKGU6qaUWqiU2qSU2qiUutN1u9RjMyilIpRSK5RSv7jq8e+u23sppZa76usDpVSY6/Zw1/V01/09/foEAohSyq6UWquU+tJ1XeqwGZRSmUqp9UqpNKXUKtdtrf73fMKHsVLKDrwInAMMAK5USg3wb6kC1pvA1MNuux/4QWvdF/jBdR1MffZ1/bsJeLmVyhjoqoC7tdYDgLHAra73m9Rj85QDZ2ithwIpwFSl1FjgCeBZrXUykAfc4Nr+BiDPdfuzru2EcSew2eu61GHzTdJap3gdwtTqf88nfBgDo4F0rfUOrXUF8D4wzc9lCkha68WA87CbpwFvuS6/BVzkdfv/tLEMaKuU6twqBQ1gWut9Wus1rsuFmA/Brkg9NourPopcV0Nd/zRwBvCx6/bD69Fdvx8DZyqlVOuUNnAppZKA84BXXdcVUoe+0Op/z8EQxl2BPV7XHa7bRNMkaq33uS7vBxJdl6Vej8LVzTcMWI7UY7O5ulfTgIPAfCADOKS1rnJt4l1Xnnp03Z8PtGvVAgem54D7gBrX9XZIHTaXBr5TSq1WSt3kuq3V/55DfLETERy01lopJdPrm0ApFQN8AtyltS7wbmBIPTaN1roaSFFKtQU+A/r7t0QnFqXU+cBBrfVqpdREPxfnRHaK1jpLKdURmK+U2uJ9Z2v9PQdDyzgL6OZ1Pcl1m2iaA+5uFtfPg67bpV4boZQKxQTxu1rrT103Sz0eI631IWAhMA7T7eduJHjXlaceXfe3AXJbt6QBZwJwoVIqEzM8dwbwb6QOm0VrneX6eRDzpXA0fvh7DoYwXgn0dc0gDAOuAOb6uUwnkrnADNflGcDnXrdf55o9OBbI9+q2sSzXGNtrwGat9TNed0k9NoNSqoOrRYxSKhKYjBl/Xwhc6trs8Hp01++lwAJt8UUStNZ/0lonaa17Yj73Fmitr0bqsMmUUtFKqVj3ZWAKsAF//D1rrU/4f8C5wDbMmNOf/V2eQP0HzAb2AZWYsY4bMGNGPwDbge+BBNe2CjNLPQNYD4z0d/kD4R9wCmaMaR2Q5vp3rtRjs+txCLDWVY8bgAddt/cGVgDpwEdAuOv2CNf1dNf9vf39HALpHzAR+FLqsNn11hv4xfVvozs//PH3LCtwCSGEEH4WDN3UQgghxAlNwlgIIYTwMwljIYQQws8kjIUQQgg/kzAWQggh/EzCWAghhPAzCWMhhBDCzySMhRBCCD/7fyyh0hitQ/8jAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'Acc at best val acc: {err1.mean():.3f} +- {err1.std():.3f}')\n",
    "print(f'Acc at test: {err2.mean():.3f} +- {err1.std():.3f}')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(acc['train'], 'b-', label='Acc train')\n",
    "plt.plot(acc['val'], 'g-', label='Acc val')\n",
    "plt.plot(acc['test'], 'r-', label='Acc test')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(loss['train'], 'b-', label='Loss train')\n",
    "plt.plot(loss['val'], 'g-', label='Loss val')\n",
    "plt.plot(loss['test'], 'r-', label='Loss test')\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training paramerters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 500-0.05-0.0005-0.25: 0.706 (0.765)\n",
      "-1: 500-0.01-0.0005-0.25: 0.686 (0.784)\n",
      "-1: 500-0.005-0.0005-0.25: 0.745 (0.784)\n",
      "-1: 500-0.001-0.0005-0.25: 0.784 (0.784)\n",
      "-1: 500-0.05-0.001-0.25: 0.686 (0.765)\n",
      "-1: 500-0.01-0.001-0.25: 0.667 (0.745)\n",
      "-1: 500-0.005-0.001-0.25: 0.667 (0.784)\n",
      "-1: 500-0.001-0.001-0.25: 0.725 (0.765)\n",
      "-1: 500-0.05-0.005-0.25: 0.725 (0.824)\n",
      "-1: 500-0.01-0.005-0.25: 0.686 (0.784)\n",
      "-1: 500-0.005-0.005-0.25: 0.765 (0.784)\n",
      "-1: 500-0.001-0.005-0.25: 0.745 (0.784)\n",
      "-1: 500-0.05-0.01-0.25: 0.725 (0.804)\n",
      "-1: 500-0.01-0.01-0.25: 0.706 (0.784)\n",
      "-1: 500-0.005-0.01-0.25: 0.725 (0.804)\n",
      "-1: 500-0.001-0.01-0.25: 0.745 (0.784)\n",
      "-1: 500-0.005-0.001-0: 0.765 (0.784)\n",
      "-1: 500-0.005-0.001-0.5: 0.667 (0.765)\n",
      "-1: 500-0.001-0.001-0: 0.765 (0.765)\n",
      "-1: 500-0.001-0.001-0.5: 0.725 (0.725)\n",
      "-1: 100-0.005-0.001-0.25: 0.647 (0.765)\n",
      "-1: 200-0.005-0.001-0.25: 0.725 (0.745)\n",
      "-1: 1000-0.005-0.001-0.25: 0.706 (0.784)\n",
      "-2: 500-0.05-0.0005-0.25: 0.784 (0.824)\n",
      "-2: 500-0.01-0.0005-0.25: 0.843 (0.863)\n",
      "-2: 500-0.005-0.0005-0.25: 0.863 (0.882)\n",
      "-2: 500-0.001-0.0005-0.25: 0.824 (0.863)\n",
      "-2: 500-0.05-0.001-0.25: 0.765 (0.824)\n",
      "-2: 500-0.01-0.001-0.25: 0.882 (0.902)\n",
      "-2: 500-0.005-0.001-0.25: 0.863 (0.882)\n",
      "-2: 500-0.001-0.001-0.25: 0.784 (0.843)\n",
      "-2: 500-0.05-0.005-0.25: 0.824 (0.843)\n",
      "-2: 500-0.01-0.005-0.25: 0.863 (0.902)\n",
      "-2: 500-0.005-0.005-0.25: 0.882 (0.902)\n",
      "-2: 500-0.001-0.005-0.25: 0.863 (0.863)\n",
      "-2: 500-0.05-0.01-0.25: 0.824 (0.863)\n",
      "-2: 500-0.01-0.01-0.25: 0.843 (0.902)\n",
      "-2: 500-0.005-0.01-0.25: 0.843 (0.882)\n",
      "-2: 500-0.001-0.01-0.25: 0.882 (0.882)\n",
      "-2: 500-0.005-0.001-0: 0.882 (0.902)\n",
      "-2: 500-0.005-0.001-0.5: 0.882 (0.882)\n",
      "-2: 500-0.001-0.001-0: 0.843 (0.863)\n",
      "-2: 500-0.001-0.001-0.5: 0.784 (0.843)\n",
      "-2: 100-0.005-0.001-0.25: 0.843 (0.863)\n",
      "-2: 200-0.005-0.001-0.25: 0.882 (0.902)\n",
      "-2: 1000-0.005-0.001-0.25: 0.863 (0.902)\n",
      "-3: 500-0.05-0.0005-0.25: 0.686 (0.784)\n",
      "-3: 500-0.01-0.0005-0.25: 0.784 (0.804)\n",
      "-3: 500-0.005-0.0005-0.25: 0.765 (0.784)\n",
      "-3: 500-0.001-0.0005-0.25: 0.706 (0.765)\n",
      "-3: 500-0.05-0.001-0.25: 0.765 (0.784)\n",
      "-3: 500-0.01-0.001-0.25: 0.804 (0.824)\n",
      "-3: 500-0.005-0.001-0.25: 0.804 (0.804)\n",
      "-3: 500-0.001-0.001-0.25: 0.765 (0.765)\n",
      "-3: 500-0.05-0.005-0.25: 0.725 (0.784)\n",
      "-3: 500-0.01-0.005-0.25: 0.804 (0.804)\n",
      "-3: 500-0.005-0.005-0.25: 0.765 (0.804)\n",
      "-3: 500-0.001-0.005-0.25: 0.765 (0.784)\n",
      "-3: 500-0.05-0.01-0.25: 0.686 (0.784)\n",
      "-3: 500-0.01-0.01-0.25: 0.765 (0.804)\n",
      "-3: 500-0.005-0.01-0.25: 0.784 (0.804)\n",
      "-3: 500-0.001-0.01-0.25: 0.725 (0.765)\n",
      "-3: 500-0.005-0.001-0: 0.784 (0.804)\n",
      "-3: 500-0.005-0.001-0.5: 0.745 (0.765)\n",
      "-3: 500-0.001-0.001-0: 0.706 (0.745)\n",
      "-3: 500-0.001-0.001-0.5: 0.725 (0.725)\n",
      "-3: 100-0.005-0.001-0.25: 0.686 (0.765)\n",
      "-3: 200-0.005-0.001-0.25: 0.725 (0.784)\n",
      "-3: 1000-0.005-0.001-0.25: 0.784 (0.804)\n",
      "-4: 500-0.05-0.0005-0.25: 0.706 (0.765)\n",
      "-4: 500-0.01-0.0005-0.25: 0.804 (0.824)\n",
      "-4: 500-0.005-0.0005-0.25: 0.765 (0.863)\n",
      "-4: 500-0.001-0.0005-0.25: 0.765 (0.784)\n",
      "-4: 500-0.05-0.001-0.25: 0.725 (0.804)\n",
      "-4: 500-0.01-0.001-0.25: 0.725 (0.843)\n",
      "-4: 500-0.005-0.001-0.25: 0.784 (0.824)\n",
      "-4: 500-0.001-0.001-0.25: 0.804 (0.843)\n",
      "-4: 500-0.05-0.005-0.25: 0.667 (0.784)\n",
      "-4: 500-0.01-0.005-0.25: 0.765 (0.882)\n",
      "-4: 500-0.005-0.005-0.25: 0.765 (0.843)\n",
      "-4: 500-0.001-0.005-0.25: 0.784 (0.824)\n",
      "-4: 500-0.05-0.01-0.25: 0.686 (0.824)\n",
      "-4: 500-0.01-0.01-0.25: 0.765 (0.843)\n",
      "-4: 500-0.005-0.01-0.25: 0.804 (0.843)\n",
      "-4: 500-0.001-0.01-0.25: 0.784 (0.843)\n",
      "-4: 500-0.005-0.001-0: 0.804 (0.824)\n",
      "-4: 500-0.005-0.001-0.5: 0.765 (0.824)\n",
      "-4: 500-0.001-0.001-0: 0.804 (0.824)\n",
      "-4: 500-0.001-0.001-0.5: 0.765 (0.824)\n",
      "-4: 100-0.005-0.001-0.25: 0.745 (0.804)\n",
      "-4: 200-0.005-0.001-0.25: 0.784 (0.824)\n",
      "-4: 1000-0.005-0.001-0.25: 0.725 (0.843)\n",
      "-5: 500-0.05-0.0005-0.25: 0.725 (0.784)\n",
      "-5: 500-0.01-0.0005-0.25: 0.667 (0.784)\n",
      "-5: 500-0.005-0.0005-0.25: 0.686 (0.765)\n",
      "-5: 500-0.001-0.0005-0.25: 0.725 (0.725)\n",
      "-5: 500-0.05-0.001-0.25: 0.745 (0.784)\n",
      "-5: 500-0.01-0.001-0.25: 0.686 (0.784)\n",
      "-5: 500-0.005-0.001-0.25: 0.725 (0.745)\n",
      "-5: 500-0.001-0.001-0.25: 0.725 (0.745)\n",
      "-5: 500-0.05-0.005-0.25: 0.725 (0.804)\n",
      "-5: 500-0.01-0.005-0.25: 0.725 (0.804)\n",
      "-5: 500-0.005-0.005-0.25: 0.725 (0.824)\n",
      "-5: 500-0.001-0.005-0.25: 0.706 (0.745)\n",
      "-5: 500-0.05-0.01-0.25: 0.745 (0.784)\n",
      "-5: 500-0.01-0.01-0.25: 0.745 (0.824)\n",
      "-5: 500-0.005-0.01-0.25: 0.765 (0.824)\n",
      "-5: 500-0.001-0.01-0.25: 0.686 (0.725)\n",
      "-5: 500-0.005-0.001-0: 0.686 (0.745)\n",
      "-5: 500-0.005-0.001-0.5: 0.706 (0.745)\n",
      "-5: 500-0.001-0.001-0: 0.686 (0.725)\n",
      "-5: 500-0.001-0.001-0.5: 0.706 (0.725)\n",
      "-5: 100-0.005-0.001-0.25: 0.647 (0.686)\n",
      "-5: 200-0.005-0.001-0.25: 0.706 (0.745)\n",
      "-5: 1000-0.005-0.001-0.25: 0.686 (0.765)\n",
      "-6: 500-0.05-0.0005-0.25: 0.608 (0.725)\n",
      "-6: 500-0.01-0.0005-0.25: 0.667 (0.725)\n",
      "-6: 500-0.005-0.0005-0.25: 0.667 (0.725)\n",
      "-6: 500-0.001-0.0005-0.25: 0.706 (0.706)\n",
      "-6: 500-0.05-0.001-0.25: 0.706 (0.745)\n",
      "-6: 500-0.01-0.001-0.25: 0.667 (0.745)\n",
      "-6: 500-0.005-0.001-0.25: 0.725 (0.784)\n",
      "-6: 500-0.001-0.001-0.25: 0.667 (0.706)\n",
      "-6: 500-0.05-0.005-0.25: 0.667 (0.765)\n",
      "-6: 500-0.01-0.005-0.25: 0.686 (0.725)\n",
      "-6: 500-0.005-0.005-0.25: 0.745 (0.765)\n",
      "-6: 500-0.001-0.005-0.25: 0.686 (0.745)\n",
      "-6: 500-0.05-0.01-0.25: 0.706 (0.725)\n",
      "-6: 500-0.01-0.01-0.25: 0.686 (0.765)\n",
      "-6: 500-0.005-0.01-0.25: 0.725 (0.784)\n",
      "-6: 500-0.001-0.01-0.25: 0.686 (0.725)\n",
      "-6: 500-0.005-0.001-0: 0.686 (0.765)\n",
      "-6: 500-0.005-0.001-0.5: 0.686 (0.745)\n",
      "-6: 500-0.001-0.001-0: 0.686 (0.706)\n",
      "-6: 500-0.001-0.001-0.5: 0.725 (0.745)\n",
      "-6: 100-0.005-0.001-0.25: 0.667 (0.706)\n",
      "-6: 200-0.005-0.001-0.25: 0.686 (0.706)\n",
      "-6: 1000-0.005-0.001-0.25: 0.667 (0.745)\n",
      "-7: 500-0.05-0.0005-0.25: 0.471 (0.529)\n",
      "-7: 500-0.01-0.0005-0.25: 0.804 (0.843)\n",
      "-7: 500-0.005-0.0005-0.25: 0.824 (0.824)\n",
      "-7: 500-0.001-0.0005-0.25: 0.784 (0.804)\n",
      "-7: 500-0.05-0.001-0.25: 0.784 (0.824)\n",
      "-7: 500-0.01-0.001-0.25: 0.745 (0.843)\n",
      "-7: 500-0.005-0.001-0.25: 0.824 (0.863)\n",
      "-7: 500-0.001-0.001-0.25: 0.804 (0.824)\n",
      "-7: 500-0.05-0.005-0.25: 0.804 (0.843)\n",
      "-7: 500-0.01-0.005-0.25: 0.765 (0.843)\n",
      "-7: 500-0.005-0.005-0.25: 0.824 (0.843)\n",
      "-7: 500-0.001-0.005-0.25: 0.824 (0.824)\n",
      "-7: 500-0.05-0.01-0.25: 0.843 (0.843)\n",
      "-7: 500-0.01-0.01-0.25: 0.804 (0.863)\n",
      "-7: 500-0.005-0.01-0.25: 0.824 (0.863)\n",
      "-7: 500-0.001-0.01-0.25: 0.804 (0.843)\n",
      "-7: 500-0.005-0.001-0: 0.843 (0.843)\n",
      "-7: 500-0.005-0.001-0.5: 0.804 (0.843)\n",
      "-7: 500-0.001-0.001-0: 0.784 (0.824)\n",
      "-7: 500-0.001-0.001-0.5: 0.824 (0.843)\n",
      "-7: 100-0.005-0.001-0.25: 0.784 (0.843)\n",
      "-7: 200-0.005-0.001-0.25: 0.804 (0.824)\n",
      "-7: 1000-0.005-0.001-0.25: 0.784 (0.863)\n",
      "-8: 500-0.05-0.0005-0.25: 0.686 (0.804)\n",
      "-8: 500-0.01-0.0005-0.25: 0.627 (0.784)\n",
      "-8: 500-0.005-0.0005-0.25: 0.784 (0.843)\n",
      "-8: 500-0.001-0.0005-0.25: 0.725 (0.784)\n",
      "-8: 500-0.05-0.001-0.25: 0.725 (0.804)\n",
      "-8: 500-0.01-0.001-0.25: 0.745 (0.863)\n",
      "-8: 500-0.005-0.001-0.25: 0.784 (0.824)\n",
      "-8: 500-0.001-0.001-0.25: 0.824 (0.843)\n",
      "-8: 500-0.05-0.005-0.25: 0.745 (0.784)\n",
      "-8: 500-0.01-0.005-0.25: 0.725 (0.863)\n",
      "-8: 500-0.005-0.005-0.25: 0.765 (0.863)\n",
      "-8: 500-0.001-0.005-0.25: 0.804 (0.824)\n",
      "-8: 500-0.05-0.01-0.25: 0.784 (0.824)\n",
      "-8: 500-0.01-0.01-0.25: 0.784 (0.863)\n",
      "-8: 500-0.005-0.01-0.25: 0.804 (0.863)\n",
      "-8: 500-0.001-0.01-0.25: 0.824 (0.843)\n",
      "-8: 500-0.005-0.001-0: 0.725 (0.824)\n",
      "-8: 500-0.005-0.001-0.5: 0.725 (0.804)\n",
      "-8: 500-0.001-0.001-0: 0.804 (0.804)\n",
      "-8: 500-0.001-0.001-0.5: 0.784 (0.804)\n",
      "-8: 100-0.005-0.001-0.25: 0.784 (0.784)\n",
      "-8: 200-0.005-0.001-0.25: 0.745 (0.824)\n",
      "-8: 1000-0.005-0.001-0.25: 0.784 (0.824)\n",
      "-9: 500-0.05-0.0005-0.25: 0.588 (0.588)\n",
      "-9: 500-0.01-0.0005-0.25: 0.706 (0.804)\n",
      "-9: 500-0.005-0.0005-0.25: 0.765 (0.784)\n",
      "-9: 500-0.001-0.0005-0.25: 0.745 (0.765)\n",
      "-9: 500-0.05-0.001-0.25: 0.706 (0.725)\n",
      "-9: 500-0.01-0.001-0.25: 0.745 (0.804)\n",
      "-9: 500-0.005-0.001-0.25: 0.725 (0.784)\n",
      "-9: 500-0.001-0.001-0.25: 0.745 (0.745)\n",
      "-9: 500-0.05-0.005-0.25: 0.745 (0.765)\n",
      "-9: 500-0.01-0.005-0.25: 0.765 (0.804)\n",
      "-9: 500-0.005-0.005-0.25: 0.745 (0.804)\n",
      "-9: 500-0.001-0.005-0.25: 0.745 (0.765)\n",
      "-9: 500-0.05-0.01-0.25: 0.706 (0.745)\n",
      "-9: 500-0.01-0.01-0.25: 0.804 (0.804)\n",
      "-9: 500-0.005-0.01-0.25: 0.745 (0.804)\n",
      "-9: 500-0.001-0.01-0.25: 0.765 (0.784)\n",
      "-9: 500-0.005-0.001-0: 0.765 (0.765)\n",
      "-9: 500-0.005-0.001-0.5: 0.765 (0.784)\n",
      "-9: 500-0.001-0.001-0: 0.745 (0.745)\n",
      "-9: 500-0.001-0.001-0.5: 0.725 (0.765)\n",
      "-9: 100-0.005-0.001-0.25: 0.745 (0.784)\n",
      "-9: 200-0.005-0.001-0.25: 0.745 (0.784)\n",
      "-9: 1000-0.005-0.001-0.25: 0.765 (0.804)\n",
      "-10: 500-0.05-0.0005-0.25: 0.804 (0.824)\n",
      "-10: 500-0.01-0.0005-0.25: 0.804 (0.824)\n",
      "-10: 500-0.005-0.0005-0.25: 0.784 (0.843)\n",
      "-10: 500-0.001-0.0005-0.25: 0.765 (0.804)\n",
      "-10: 500-0.05-0.001-0.25: 0.706 (0.804)\n",
      "-10: 500-0.01-0.001-0.25: 0.765 (0.824)\n",
      "-10: 500-0.005-0.001-0.25: 0.824 (0.843)\n",
      "-10: 500-0.001-0.001-0.25: 0.784 (0.784)\n",
      "-10: 500-0.05-0.005-0.25: 0.745 (0.824)\n",
      "-10: 500-0.01-0.005-0.25: 0.765 (0.824)\n",
      "-10: 500-0.005-0.005-0.25: 0.863 (0.863)\n",
      "-10: 500-0.001-0.005-0.25: 0.824 (0.863)\n",
      "-10: 500-0.05-0.01-0.25: 0.686 (0.824)\n",
      "-10: 500-0.01-0.01-0.25: 0.843 (0.882)\n",
      "-10: 500-0.005-0.01-0.25: 0.804 (0.882)\n",
      "-10: 500-0.001-0.01-0.25: 0.824 (0.843)\n",
      "-10: 500-0.005-0.001-0: 0.765 (0.804)\n",
      "-10: 500-0.005-0.001-0.5: 0.804 (0.863)\n",
      "-10: 500-0.001-0.001-0: 0.725 (0.784)\n",
      "-10: 500-0.001-0.001-0.5: 0.784 (0.824)\n",
      "-10: 100-0.005-0.001-0.25: 0.745 (0.804)\n",
      "-10: 200-0.005-0.001-0.25: 0.765 (0.784)\n",
      "-10: 1000-0.005-0.001-0.25: 0.843 (0.843)\n",
      "-11: 500-0.05-0.0005-0.25: 0.725 (0.765)\n",
      "-11: 500-0.01-0.0005-0.25: 0.647 (0.745)\n",
      "-11: 500-0.005-0.0005-0.25: 0.725 (0.745)\n",
      "-11: 500-0.001-0.0005-0.25: 0.765 (0.765)\n",
      "-11: 500-0.05-0.001-0.25: 0.608 (0.765)\n",
      "-11: 500-0.01-0.001-0.25: 0.706 (0.765)\n",
      "-11: 500-0.005-0.001-0.25: 0.667 (0.765)\n",
      "-11: 500-0.001-0.001-0.25: 0.745 (0.745)\n",
      "-11: 500-0.05-0.005-0.25: 0.725 (0.784)\n",
      "-11: 500-0.01-0.005-0.25: 0.784 (0.804)\n",
      "-11: 500-0.005-0.005-0.25: 0.745 (0.784)\n",
      "-11: 500-0.001-0.005-0.25: 0.765 (0.784)\n",
      "-11: 500-0.05-0.01-0.25: 0.765 (0.804)\n",
      "-11: 500-0.01-0.01-0.25: 0.725 (0.784)\n",
      "-11: 500-0.005-0.01-0.25: 0.745 (0.824)\n",
      "-11: 500-0.001-0.01-0.25: 0.745 (0.784)\n",
      "-11: 500-0.005-0.001-0: 0.725 (0.765)\n",
      "-11: 500-0.005-0.001-0.5: 0.706 (0.804)\n",
      "-11: 500-0.001-0.001-0: 0.725 (0.745)\n",
      "-11: 500-0.001-0.001-0.5: 0.725 (0.745)\n",
      "-11: 100-0.005-0.001-0.25: 0.765 (0.784)\n",
      "-11: 200-0.005-0.001-0.25: 0.706 (0.745)\n",
      "-11: 1000-0.005-0.001-0.25: 0.725 (0.784)\n",
      "-12: 500-0.05-0.0005-0.25: 0.784 (0.843)\n",
      "-12: 500-0.01-0.0005-0.25: 0.824 (0.863)\n",
      "-12: 500-0.005-0.0005-0.25: 0.863 (0.882)\n",
      "-12: 500-0.001-0.0005-0.25: 0.843 (0.863)\n",
      "-12: 500-0.05-0.001-0.25: 0.745 (0.843)\n",
      "-12: 500-0.01-0.001-0.25: 0.804 (0.863)\n",
      "-12: 500-0.005-0.001-0.25: 0.843 (0.882)\n",
      "-12: 500-0.001-0.001-0.25: 0.882 (0.882)\n",
      "-12: 500-0.05-0.005-0.25: 0.804 (0.863)\n",
      "-12: 500-0.01-0.005-0.25: 0.882 (0.902)\n",
      "-12: 500-0.005-0.005-0.25: 0.804 (0.902)\n",
      "-12: 500-0.001-0.005-0.25: 0.843 (0.882)\n",
      "-12: 500-0.05-0.01-0.25: 0.784 (0.882)\n",
      "-12: 500-0.01-0.01-0.25: 0.824 (0.922)\n",
      "-12: 500-0.005-0.01-0.25: 0.863 (0.902)\n",
      "-12: 500-0.001-0.01-0.25: 0.863 (0.863)\n",
      "-12: 500-0.005-0.001-0: 0.863 (0.882)\n",
      "-12: 500-0.005-0.001-0.5: 0.863 (0.863)\n",
      "-12: 500-0.001-0.001-0: 0.882 (0.882)\n",
      "-12: 500-0.001-0.001-0.5: 0.843 (0.843)\n",
      "-12: 100-0.005-0.001-0.25: 0.824 (0.863)\n",
      "-12: 200-0.005-0.001-0.25: 0.843 (0.863)\n",
      "-12: 1000-0.005-0.001-0.25: 0.824 (0.882)\n",
      "-13: 500-0.05-0.0005-0.25: 0.686 (0.765)\n",
      "-13: 500-0.01-0.0005-0.25: 0.784 (0.804)\n",
      "-13: 500-0.005-0.0005-0.25: 0.804 (0.804)\n",
      "-13: 500-0.001-0.0005-0.25: 0.706 (0.765)\n",
      "-13: 500-0.05-0.001-0.25: 0.745 (0.784)\n",
      "-13: 500-0.01-0.001-0.25: 0.765 (0.804)\n",
      "-13: 500-0.005-0.001-0.25: 0.745 (0.784)\n",
      "-13: 500-0.001-0.001-0.25: 0.745 (0.804)\n",
      "-13: 500-0.05-0.005-0.25: 0.765 (0.784)\n",
      "-13: 500-0.01-0.005-0.25: 0.745 (0.824)\n",
      "-13: 500-0.005-0.005-0.25: 0.765 (0.804)\n",
      "-13: 500-0.001-0.005-0.25: 0.745 (0.765)\n",
      "-13: 500-0.05-0.01-0.25: 0.765 (0.784)\n",
      "-13: 500-0.01-0.01-0.25: 0.765 (0.804)\n",
      "-13: 500-0.005-0.01-0.25: 0.765 (0.804)\n",
      "-13: 500-0.001-0.01-0.25: 0.765 (0.784)\n",
      "-13: 500-0.005-0.001-0: 0.745 (0.784)\n",
      "-13: 500-0.005-0.001-0.5: 0.784 (0.824)\n",
      "-13: 500-0.001-0.001-0: 0.725 (0.765)\n",
      "-13: 500-0.001-0.001-0.5: 0.725 (0.784)\n",
      "-13: 100-0.005-0.001-0.25: 0.745 (0.765)\n",
      "-13: 200-0.005-0.001-0.25: 0.784 (0.784)\n",
      "-13: 1000-0.005-0.001-0.25: 0.725 (0.804)\n",
      "-14: 500-0.05-0.0005-0.25: 0.725 (0.765)\n",
      "-14: 500-0.01-0.0005-0.25: 0.765 (0.863)\n",
      "-14: 500-0.005-0.0005-0.25: 0.765 (0.824)\n",
      "-14: 500-0.001-0.0005-0.25: 0.765 (0.824)\n",
      "-14: 500-0.05-0.001-0.25: 0.765 (0.843)\n",
      "-14: 500-0.01-0.001-0.25: 0.784 (0.843)\n",
      "-14: 500-0.005-0.001-0.25: 0.765 (0.843)\n",
      "-14: 500-0.001-0.001-0.25: 0.784 (0.804)\n",
      "-14: 500-0.05-0.005-0.25: 0.745 (0.784)\n",
      "-14: 500-0.01-0.005-0.25: 0.765 (0.863)\n",
      "-14: 500-0.005-0.005-0.25: 0.765 (0.863)\n",
      "-14: 500-0.001-0.005-0.25: 0.804 (0.824)\n",
      "-14: 500-0.05-0.01-0.25: 0.725 (0.804)\n",
      "-14: 500-0.01-0.01-0.25: 0.784 (0.863)\n",
      "-14: 500-0.005-0.01-0.25: 0.765 (0.843)\n",
      "-14: 500-0.001-0.01-0.25: 0.765 (0.824)\n",
      "-14: 500-0.005-0.001-0: 0.784 (0.843)\n",
      "-14: 500-0.005-0.001-0.5: 0.804 (0.863)\n",
      "-14: 500-0.001-0.001-0: 0.804 (0.804)\n",
      "-14: 500-0.001-0.001-0.5: 0.765 (0.824)\n",
      "-14: 100-0.005-0.001-0.25: 0.765 (0.784)\n",
      "-14: 200-0.005-0.001-0.25: 0.804 (0.843)\n",
      "-14: 1000-0.005-0.001-0.25: 0.765 (0.843)\n",
      "-15: 500-0.05-0.0005-0.25: 0.686 (0.804)\n",
      "-15: 500-0.01-0.0005-0.25: 0.725 (0.824)\n",
      "-15: 500-0.005-0.0005-0.25: 0.706 (0.804)\n",
      "-15: 500-0.001-0.0005-0.25: 0.667 (0.725)\n",
      "-15: 500-0.05-0.001-0.25: 0.686 (0.784)\n",
      "-15: 500-0.01-0.001-0.25: 0.725 (0.804)\n",
      "-15: 500-0.005-0.001-0.25: 0.725 (0.804)\n",
      "-15: 500-0.001-0.001-0.25: 0.725 (0.725)\n",
      "-15: 500-0.05-0.005-0.25: 0.725 (0.784)\n",
      "-15: 500-0.01-0.005-0.25: 0.647 (0.804)\n",
      "-15: 500-0.005-0.005-0.25: 0.725 (0.804)\n",
      "-15: 500-0.001-0.005-0.25: 0.667 (0.784)\n",
      "-15: 500-0.05-0.01-0.25: 0.706 (0.804)\n",
      "-15: 500-0.01-0.01-0.25: 0.686 (0.804)\n",
      "-15: 500-0.005-0.01-0.25: 0.725 (0.765)\n",
      "-15: 500-0.001-0.01-0.25: 0.725 (0.765)\n",
      "-15: 500-0.005-0.001-0: 0.706 (0.706)\n",
      "-15: 500-0.005-0.001-0.5: 0.725 (0.745)\n",
      "-15: 500-0.001-0.001-0: 0.667 (0.686)\n",
      "-15: 500-0.001-0.001-0.5: 0.706 (0.745)\n",
      "-15: 100-0.005-0.001-0.25: 0.725 (0.725)\n",
      "-15: 200-0.005-0.001-0.25: 0.686 (0.765)\n",
      "-15: 1000-0.005-0.001-0.25: 0.706 (0.784)\n",
      "-16: 500-0.05-0.0005-0.25: 0.706 (0.725)\n",
      "-16: 500-0.01-0.0005-0.25: 0.647 (0.745)\n",
      "-16: 500-0.005-0.0005-0.25: 0.706 (0.765)\n",
      "-16: 500-0.001-0.0005-0.25: 0.647 (0.686)\n",
      "-16: 500-0.05-0.001-0.25: 0.725 (0.745)\n",
      "-16: 500-0.01-0.001-0.25: 0.647 (0.784)\n",
      "-16: 500-0.005-0.001-0.25: 0.706 (0.765)\n",
      "-16: 500-0.001-0.001-0.25: 0.706 (0.725)\n",
      "-16: 500-0.05-0.005-0.25: 0.686 (0.745)\n",
      "-16: 500-0.01-0.005-0.25: 0.667 (0.765)\n",
      "-16: 500-0.005-0.005-0.25: 0.725 (0.784)\n",
      "-16: 500-0.001-0.005-0.25: 0.686 (0.706)\n",
      "-16: 500-0.05-0.01-0.25: 0.686 (0.745)\n",
      "-16: 500-0.01-0.01-0.25: 0.745 (0.784)\n",
      "-16: 500-0.005-0.01-0.25: 0.725 (0.784)\n",
      "-16: 500-0.001-0.01-0.25: 0.745 (0.745)\n",
      "-16: 500-0.005-0.001-0: 0.745 (0.745)\n",
      "-16: 500-0.005-0.001-0.5: 0.686 (0.725)\n",
      "-16: 500-0.001-0.001-0: 0.686 (0.706)\n",
      "-16: 500-0.001-0.001-0.5: 0.686 (0.706)\n",
      "-16: 100-0.005-0.001-0.25: 0.647 (0.686)\n",
      "-16: 200-0.005-0.001-0.25: 0.667 (0.725)\n",
      "-16: 1000-0.005-0.001-0.25: 0.725 (0.745)\n",
      "-17: 500-0.05-0.0005-0.25: 0.784 (0.863)\n",
      "-17: 500-0.01-0.0005-0.25: 0.804 (0.843)\n",
      "-17: 500-0.005-0.0005-0.25: 0.824 (0.843)\n",
      "-17: 500-0.001-0.0005-0.25: 0.765 (0.804)\n",
      "-17: 500-0.05-0.001-0.25: 0.804 (0.824)\n",
      "-17: 500-0.01-0.001-0.25: 0.804 (0.843)\n",
      "-17: 500-0.005-0.001-0.25: 0.824 (0.843)\n",
      "-17: 500-0.001-0.001-0.25: 0.765 (0.824)\n",
      "-17: 500-0.05-0.005-0.25: 0.745 (0.745)\n",
      "-17: 500-0.01-0.005-0.25: 0.824 (0.863)\n",
      "-17: 500-0.005-0.005-0.25: 0.804 (0.843)\n",
      "-17: 500-0.001-0.005-0.25: 0.784 (0.824)\n",
      "-17: 500-0.05-0.01-0.25: 0.824 (0.843)\n",
      "-17: 500-0.01-0.01-0.25: 0.804 (0.843)\n",
      "-17: 500-0.005-0.01-0.25: 0.804 (0.843)\n",
      "-17: 500-0.001-0.01-0.25: 0.804 (0.824)\n",
      "-17: 500-0.005-0.001-0: 0.804 (0.843)\n",
      "-17: 500-0.005-0.001-0.5: 0.824 (0.824)\n",
      "-17: 500-0.001-0.001-0: 0.804 (0.824)\n",
      "-17: 500-0.001-0.001-0.5: 0.804 (0.824)\n",
      "-17: 100-0.005-0.001-0.25: 0.765 (0.824)\n",
      "-17: 200-0.005-0.001-0.25: 0.804 (0.843)\n",
      "-17: 1000-0.005-0.001-0.25: 0.804 (0.843)\n",
      "-18: 500-0.05-0.0005-0.25: 0.765 (0.784)\n",
      "-18: 500-0.01-0.0005-0.25: 0.765 (0.804)\n",
      "-18: 500-0.005-0.0005-0.25: 0.784 (0.824)\n",
      "-18: 500-0.001-0.0005-0.25: 0.725 (0.784)\n",
      "-18: 500-0.05-0.001-0.25: 0.745 (0.824)\n",
      "-18: 500-0.01-0.001-0.25: 0.765 (0.824)\n",
      "-18: 500-0.005-0.001-0.25: 0.745 (0.843)\n",
      "-18: 500-0.001-0.001-0.25: 0.824 (0.843)\n",
      "-18: 500-0.05-0.005-0.25: 0.745 (0.824)\n",
      "-18: 500-0.01-0.005-0.25: 0.804 (0.843)\n",
      "-18: 500-0.005-0.005-0.25: 0.824 (0.863)\n",
      "-18: 500-0.001-0.005-0.25: 0.824 (0.824)\n",
      "-18: 500-0.05-0.01-0.25: 0.686 (0.804)\n",
      "-18: 500-0.01-0.01-0.25: 0.804 (0.882)\n",
      "-18: 500-0.005-0.01-0.25: 0.804 (0.843)\n",
      "-18: 500-0.001-0.01-0.25: 0.804 (0.843)\n",
      "-18: 500-0.005-0.001-0: 0.784 (0.843)\n",
      "-18: 500-0.005-0.001-0.5: 0.784 (0.843)\n",
      "-18: 500-0.001-0.001-0: 0.824 (0.824)\n",
      "-18: 500-0.001-0.001-0.5: 0.725 (0.784)\n",
      "-18: 100-0.005-0.001-0.25: 0.765 (0.804)\n",
      "-18: 200-0.005-0.001-0.25: 0.804 (0.804)\n",
      "-18: 1000-0.005-0.001-0.25: 0.784 (0.824)\n",
      "-19: 500-0.05-0.0005-0.25: 0.627 (0.706)\n",
      "-19: 500-0.01-0.0005-0.25: 0.745 (0.804)\n",
      "-19: 500-0.005-0.0005-0.25: 0.745 (0.784)\n",
      "-19: 500-0.001-0.0005-0.25: 0.706 (0.725)\n",
      "-19: 500-0.05-0.001-0.25: 0.706 (0.706)\n",
      "-19: 500-0.01-0.001-0.25: 0.745 (0.804)\n",
      "-19: 500-0.005-0.001-0.25: 0.725 (0.804)\n",
      "-19: 500-0.001-0.001-0.25: 0.765 (0.784)\n",
      "-19: 500-0.05-0.005-0.25: 0.667 (0.725)\n",
      "-19: 500-0.01-0.005-0.25: 0.784 (0.784)\n",
      "-19: 500-0.005-0.005-0.25: 0.765 (0.804)\n",
      "-19: 500-0.001-0.005-0.25: 0.765 (0.784)\n",
      "-19: 500-0.05-0.01-0.25: 0.745 (0.745)\n",
      "-19: 500-0.01-0.01-0.25: 0.725 (0.784)\n",
      "-19: 500-0.005-0.01-0.25: 0.765 (0.804)\n",
      "-19: 500-0.001-0.01-0.25: 0.765 (0.765)\n",
      "-19: 500-0.005-0.001-0: 0.725 (0.784)\n",
      "-19: 500-0.005-0.001-0.5: 0.804 (0.804)\n",
      "-19: 500-0.001-0.001-0: 0.706 (0.745)\n",
      "-19: 500-0.001-0.001-0.5: 0.725 (0.745)\n",
      "-19: 100-0.005-0.001-0.25: 0.667 (0.706)\n",
      "-19: 200-0.005-0.001-0.25: 0.745 (0.784)\n",
      "-19: 1000-0.005-0.001-0.25: 0.725 (0.784)\n",
      "-20: 500-0.05-0.0005-0.25: 0.765 (0.784)\n",
      "-20: 500-0.01-0.0005-0.25: 0.765 (0.863)\n",
      "-20: 500-0.005-0.0005-0.25: 0.824 (0.824)\n",
      "-20: 500-0.001-0.0005-0.25: 0.765 (0.804)\n",
      "-20: 500-0.05-0.001-0.25: 0.784 (0.824)\n",
      "-20: 500-0.01-0.001-0.25: 0.784 (0.863)\n",
      "-20: 500-0.005-0.001-0.25: 0.804 (0.863)\n",
      "-20: 500-0.001-0.001-0.25: 0.725 (0.804)\n",
      "-20: 500-0.05-0.005-0.25: 0.725 (0.824)\n",
      "-20: 500-0.01-0.005-0.25: 0.784 (0.824)\n",
      "-20: 500-0.005-0.005-0.25: 0.843 (0.882)\n",
      "-20: 500-0.001-0.005-0.25: 0.824 (0.843)\n",
      "-20: 500-0.05-0.01-0.25: 0.686 (0.824)\n",
      "-20: 500-0.01-0.01-0.25: 0.784 (0.863)\n",
      "-20: 500-0.005-0.01-0.25: 0.863 (0.882)\n",
      "-20: 500-0.001-0.01-0.25: 0.745 (0.824)\n",
      "-20: 500-0.005-0.001-0: 0.725 (0.765)\n",
      "-20: 500-0.005-0.001-0.5: 0.745 (0.843)\n",
      "-20: 500-0.001-0.001-0: 0.804 (0.804)\n",
      "-20: 500-0.001-0.001-0.5: 0.765 (0.784)\n",
      "-20: 100-0.005-0.001-0.25: 0.745 (0.784)\n",
      "-20: 200-0.005-0.001-0.25: 0.745 (0.843)\n",
      "-20: 1000-0.005-0.001-0.25: 0.804 (0.824)\n",
      "----- 8.88 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [\n",
    "        {'epochs': 500, 'lr': .05, 'wd': 5e-4, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .01, 'wd': 5e-4, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 5e-4, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .001, 'wd': 5e-4, 'drop': .25},\n",
    "        \n",
    "        {'epochs': 500, 'lr': .05, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .01, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .001, 'wd': 1e-3, 'drop': .25},\n",
    "\n",
    "        {'epochs': 500, 'lr': .05, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .01, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 5e-3, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .001, 'wd': 5e-3, 'drop': .25},\n",
    "\n",
    "        {'epochs': 500, 'lr': .05, 'wd': 1e-2, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .01, 'wd': 1e-2, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 1e-2, 'drop': .25},\n",
    "        {'epochs': 500, 'lr': .001, 'wd': 1e-2, 'drop': .25},\n",
    "\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': 0},\n",
    "        {'epochs': 500, 'lr': .005, 'wd': 1e-3, 'drop': .5},\n",
    "        {'epochs': 500, 'lr': .001, 'wd': 1e-3, 'drop': 0},\n",
    "        {'epochs': 500, 'lr': .001, 'wd': 1e-3, 'drop': .5},\n",
    "\n",
    "        {'epochs': 100, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 200, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        {'epochs': 1000, 'lr': .005, 'wd': 1e-3, 'drop': .25},\n",
    "        ]\n",
    "\n",
    "best_accs1 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs1 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=exp['drop'], init_h0=h0, diff_layer=GFGCN_noh_Layer)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, exp['epochs'], exp['lr'], exp['wd'])\n",
    "\n",
    "        best_accs1[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs1[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}: {best_val_accs1[j,i]:.3f} ({best_accs1[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}' for exp in EXPS]\n",
    "table_over1 = summary_table(best_accs1, index_name)\n",
    "table1 = summary_table(best_val_accs1, index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500-0.05-0.0005-0.25</th>\n",
       "      <td>0.700980</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.078155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.01-0.0005-0.25</th>\n",
       "      <td>0.743137</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.065002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.0005-0.25</th>\n",
       "      <td>0.769608</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.053294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.0005-0.25</th>\n",
       "      <td>0.744118</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.047008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.05-0.001-0.25</th>\n",
       "      <td>0.731373</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.043003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.01-0.001-0.25</th>\n",
       "      <td>0.748039</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.055207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0.25</th>\n",
       "      <td>0.763725</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.055277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.001-0.25</th>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.047627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.05-0.005-0.25</th>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.042282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.01-0.005-0.25</th>\n",
       "      <td>0.761765</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.059239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.005-0.25</th>\n",
       "      <td>0.780392</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.044970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.005-0.25</th>\n",
       "      <td>0.772549</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.054197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.05-0.01-0.25</th>\n",
       "      <td>0.738235</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.050478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.01-0.01-0.25</th>\n",
       "      <td>0.769608</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.045932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.01-0.25</th>\n",
       "      <td>0.782353</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.043359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.01-0.25</th>\n",
       "      <td>0.772549</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.050526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0</th>\n",
       "      <td>0.765686</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.053150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0.5</th>\n",
       "      <td>0.763725</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.057660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.001-0</th>\n",
       "      <td>0.758824</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.059506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.001-0.5</th>\n",
       "      <td>0.750980</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.041176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100-0.005-0.001-0.25</th>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.056319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.25</th>\n",
       "      <td>0.757843</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.054507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000-0.005-0.001-0.25</th>\n",
       "      <td>0.759804</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.051831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean accs       med       std\n",
       "500-0.05-0.0005-0.25    0.700980  0.705882  0.078155\n",
       "500-0.01-0.0005-0.25    0.743137  0.764706  0.065002\n",
       "500-0.005-0.0005-0.25   0.769608  0.764706  0.053294\n",
       "500-0.001-0.0005-0.25   0.744118  0.754902  0.047008\n",
       "500-0.05-0.001-0.25     0.731373  0.735294  0.043003\n",
       "500-0.01-0.001-0.25     0.748039  0.745098  0.055207\n",
       "500-0.005-0.001-0.25    0.763725  0.754902  0.055277\n",
       "500-0.001-0.001-0.25    0.764706  0.764706  0.047627\n",
       "500-0.05-0.005-0.25     0.735294  0.735294  0.042282\n",
       "500-0.01-0.005-0.25     0.761765  0.764706  0.059239\n",
       "500-0.005-0.005-0.25    0.780392  0.764706  0.044970\n",
       "500-0.001-0.005-0.25    0.772549  0.774510  0.054197\n",
       "500-0.05-0.01-0.25      0.738235  0.725490  0.050478\n",
       "500-0.01-0.01-0.25      0.769608  0.774510  0.045932\n",
       "500-0.005-0.01-0.25     0.782353  0.774510  0.043359\n",
       "500-0.001-0.01-0.25     0.772549  0.764706  0.050526\n",
       "500-0.005-0.001-0       0.765686  0.764706  0.053150\n",
       "500-0.005-0.001-0.5     0.763725  0.764706  0.057660\n",
       "500-0.001-0.001-0       0.758824  0.754902  0.059506\n",
       "500-0.001-0.001-0.5     0.750980  0.725490  0.041176\n",
       "100-0.005-0.001-0.25    0.735294  0.745098  0.056319\n",
       "200-0.005-0.001-0.25    0.757843  0.745098  0.054507\n",
       "1000-0.005-0.001-0.25   0.759804  0.764706  0.051831"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500-0.05-0.0005-0.25</th>\n",
       "      <td>0.759804</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.077414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.01-0.0005-0.25</th>\n",
       "      <td>0.809804</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.039751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.0005-0.25</th>\n",
       "      <td>0.809804</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.042100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.0005-0.25</th>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.045733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.05-0.001-0.25</th>\n",
       "      <td>0.790196</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.037767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.01-0.001-0.25</th>\n",
       "      <td>0.818627</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.040124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0.25</th>\n",
       "      <td>0.816667</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.039350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.001-0.25</th>\n",
       "      <td>0.790196</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.047668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.05-0.005-0.25</th>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.035349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.01-0.005-0.25</th>\n",
       "      <td>0.825490</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.044237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.005-0.25</th>\n",
       "      <td>0.831373</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.039896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.005-0.25</th>\n",
       "      <td>0.801961</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.043801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.05-0.01-0.25</th>\n",
       "      <td>0.802941</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.039932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.01-0.01-0.25</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.044063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.01-0.25</th>\n",
       "      <td>0.832353</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.037448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.01-0.25</th>\n",
       "      <td>0.802941</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.044917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0</th>\n",
       "      <td>0.800980</td>\n",
       "      <td>0.794118</td>\n",
       "      <td>0.048139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.005-0.001-0.5</th>\n",
       "      <td>0.807843</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.045817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.001-0</th>\n",
       "      <td>0.778431</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.052283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500-0.001-0.001-0.5</th>\n",
       "      <td>0.780392</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.044540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100-0.005-0.001-0.25</th>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.051655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.005-0.001-0.25</th>\n",
       "      <td>0.796078</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.048586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000-0.005-0.001-0.25</th>\n",
       "      <td>0.814706</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.041351</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       mean accs       med       std\n",
       "500-0.05-0.0005-0.25    0.759804  0.774510  0.077414\n",
       "500-0.01-0.0005-0.25    0.809804  0.803922  0.039751\n",
       "500-0.005-0.0005-0.25   0.809804  0.813725  0.042100\n",
       "500-0.001-0.0005-0.25   0.776471  0.784314  0.045733\n",
       "500-0.05-0.001-0.25     0.790196  0.794118  0.037767\n",
       "500-0.01-0.001-0.25     0.818627  0.823529  0.040124\n",
       "500-0.005-0.001-0.25    0.816667  0.813725  0.039350\n",
       "500-0.001-0.001-0.25    0.790196  0.794118  0.047668\n",
       "500-0.05-0.005-0.25     0.794118  0.784314  0.035349\n",
       "500-0.01-0.005-0.25     0.825490  0.823529  0.044237\n",
       "500-0.005-0.005-0.25    0.831373  0.833333  0.039896\n",
       "500-0.001-0.005-0.25    0.801961  0.803922  0.043801\n",
       "500-0.05-0.01-0.25      0.802941  0.803922  0.039932\n",
       "500-0.01-0.01-0.25      0.833333  0.833333  0.044063\n",
       "500-0.005-0.01-0.25     0.832353  0.833333  0.037448\n",
       "500-0.001-0.01-0.25     0.802941  0.803922  0.044917\n",
       "500-0.005-0.001-0       0.800980  0.794118  0.048139\n",
       "500-0.005-0.001-0.5     0.807843  0.813725  0.045817\n",
       "500-0.001-0.001-0       0.778431  0.774510  0.052283\n",
       "500-0.001-0.001-0.5     0.780392  0.784314  0.044540\n",
       "100-0.005-0.001-0.25    0.776471  0.784314  0.051655\n",
       "200-0.005-0.001-0.25    0.796078  0.784314  0.048586\n",
       "1000-0.005-0.001-0.25   0.814706  0.813725  0.041351"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_over1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design - Bias True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 2-2-8: 0.745 (0.804)\n",
      "-1: 2-3-8: 0.706 (0.725)\n",
      "-1: 2-4-8: 0.588 (0.686)\n",
      "-1: 3-2-8: 0.706 (0.745)\n",
      "-1: 4-2-8: 0.725 (0.745)\n",
      "-1: 3-3-8: 0.647 (0.725)\n",
      "-1: 4-3-8: 0.686 (0.725)\n",
      "-1: 2-2-16: 0.725 (0.784)\n",
      "-1: 2-3-16: 0.647 (0.745)\n",
      "-1: 2-4-16: 0.588 (0.706)\n",
      "-1: 3-2-16: 0.725 (0.804)\n",
      "-1: 4-2-16: 0.706 (0.784)\n",
      "-1: 3-3-16: 0.647 (0.706)\n",
      "-1: 4-3-16: 0.490 (0.608)\n",
      "-1: 2-2-32: 0.745 (0.765)\n",
      "-1: 2-3-32: 0.667 (0.725)\n",
      "-1: 2-4-32: 0.608 (0.647)\n",
      "-1: 3-2-32: 0.725 (0.804)\n",
      "-1: 4-2-32: 0.725 (0.784)\n",
      "-1: 3-3-32: 0.647 (0.706)\n",
      "-1: 4-3-32: 0.627 (0.706)\n",
      "-1: 2-2-64: 0.804 (0.804)\n",
      "-1: 2-3-64: 0.627 (0.706)\n",
      "-1: 2-4-64: 0.608 (0.706)\n",
      "-1: 3-2-64: 0.784 (0.784)\n",
      "-1: 4-2-64: 0.725 (0.784)\n",
      "-1: 3-3-64: 0.686 (0.745)\n",
      "-1: 4-3-64: 0.471 (0.608)\n",
      "-1: 2-2-100: 0.745 (0.784)\n",
      "-1: 2-3-100: 0.706 (0.706)\n",
      "-1: 2-4-100: 0.627 (0.706)\n",
      "-1: 3-2-100: 0.745 (0.784)\n",
      "-1: 4-2-100: 0.745 (0.765)\n",
      "-1: 3-3-100: 0.686 (0.725)\n",
      "-1: 4-3-100: 0.569 (0.706)\n",
      "-2: 2-2-8: 0.824 (0.882)\n",
      "-2: 2-3-8: 0.824 (0.843)\n",
      "-2: 2-4-8: 0.784 (0.824)\n",
      "-2: 3-2-8: 0.843 (0.902)\n",
      "-2: 4-2-8: 0.843 (0.882)\n",
      "-2: 3-3-8: 0.765 (0.824)\n",
      "-2: 4-3-8: 0.784 (0.824)\n",
      "-2: 2-2-16: 0.843 (0.902)\n",
      "-2: 2-3-16: 0.784 (0.843)\n",
      "-2: 2-4-16: 0.765 (0.804)\n",
      "-2: 3-2-16: 0.863 (0.863)\n",
      "-2: 4-2-16: 0.824 (0.863)\n",
      "-2: 3-3-16: 0.784 (0.824)\n",
      "-2: 4-3-16: 0.765 (0.824)\n",
      "-2: 2-2-32: 0.863 (0.882)\n",
      "-2: 2-3-32: 0.784 (0.882)\n",
      "-2: 2-4-32: 0.627 (0.745)\n",
      "-2: 3-2-32: 0.843 (0.863)\n",
      "-2: 4-2-32: 0.863 (0.882)\n",
      "-2: 3-3-32: 0.765 (0.843)\n",
      "-2: 4-3-32: 0.706 (0.784)\n",
      "-2: 2-2-64: 0.843 (0.902)\n",
      "-2: 2-3-64: 0.804 (0.863)\n",
      "-2: 2-4-64: 0.725 (0.784)\n",
      "-2: 3-2-64: 0.863 (0.902)\n",
      "-2: 4-2-64: 0.784 (0.863)\n",
      "-2: 3-3-64: 0.804 (0.843)\n",
      "-2: 4-3-64: 0.824 (0.824)\n",
      "-2: 2-2-100: 0.843 (0.902)\n",
      "-2: 2-3-100: 0.765 (0.843)\n",
      "-2: 2-4-100: 0.667 (0.745)\n",
      "-2: 3-2-100: 0.863 (0.902)\n",
      "-2: 4-2-100: 0.843 (0.863)\n",
      "-2: 3-3-100: 0.784 (0.824)\n",
      "-2: 4-3-100: 0.725 (0.804)\n",
      "-3: 2-2-8: 0.745 (0.804)\n",
      "-3: 2-3-8: 0.686 (0.706)\n",
      "-3: 2-4-8: 0.529 (0.686)\n",
      "-3: 3-2-8: 0.765 (0.804)\n",
      "-3: 4-2-8: 0.686 (0.804)\n",
      "-3: 3-3-8: 0.608 (0.667)\n",
      "-3: 4-3-8: 0.569 (0.647)\n",
      "-3: 2-2-16: 0.765 (0.804)\n",
      "-3: 2-3-16: 0.627 (0.725)\n",
      "-3: 2-4-16: 0.549 (0.627)\n",
      "-3: 3-2-16: 0.765 (0.824)\n",
      "-3: 4-2-16: 0.706 (0.784)\n",
      "-3: 3-3-16: 0.686 (0.765)\n",
      "-3: 4-3-16: 0.471 (0.549)\n",
      "-3: 2-2-32: 0.745 (0.804)\n",
      "-3: 2-3-32: 0.627 (0.706)\n",
      "-3: 2-4-32: 0.569 (0.627)\n",
      "-3: 3-2-32: 0.745 (0.784)\n",
      "-3: 4-2-32: 0.745 (0.784)\n",
      "-3: 3-3-32: 0.686 (0.765)\n",
      "-3: 4-3-32: 0.451 (0.569)\n",
      "-3: 2-2-64: 0.784 (0.804)\n",
      "-3: 2-3-64: 0.706 (0.725)\n",
      "-3: 2-4-64: 0.549 (0.647)\n",
      "-3: 3-2-64: 0.784 (0.804)\n",
      "-3: 4-2-64: 0.686 (0.784)\n",
      "-3: 3-3-64: 0.647 (0.647)\n",
      "-3: 4-3-64: 0.510 (0.588)\n",
      "-3: 2-2-100: 0.745 (0.804)\n",
      "-3: 2-3-100: 0.627 (0.725)\n",
      "-3: 2-4-100: 0.549 (0.627)\n",
      "-3: 3-2-100: 0.745 (0.824)\n",
      "-3: 4-2-100: 0.667 (0.706)\n",
      "-3: 3-3-100: 0.667 (0.686)\n",
      "-3: 4-3-100: 0.490 (0.647)\n",
      "-4: 2-2-8: 0.765 (0.824)\n",
      "-4: 2-3-8: 0.706 (0.784)\n",
      "-4: 2-4-8: 0.647 (0.745)\n",
      "-4: 3-2-8: 0.745 (0.843)\n",
      "-4: 4-2-8: 0.804 (0.843)\n",
      "-4: 3-3-8: 0.627 (0.765)\n",
      "-4: 4-3-8: 0.451 (0.510)\n",
      "-4: 2-2-16: 0.765 (0.843)\n",
      "-4: 2-3-16: 0.804 (0.824)\n",
      "-4: 2-4-16: 0.647 (0.725)\n",
      "-4: 3-2-16: 0.804 (0.843)\n",
      "-4: 4-2-16: 0.647 (0.784)\n",
      "-4: 3-3-16: 0.686 (0.765)\n",
      "-4: 4-3-16: 0.569 (0.667)\n",
      "-4: 2-2-32: 0.784 (0.843)\n",
      "-4: 2-3-32: 0.745 (0.824)\n",
      "-4: 2-4-32: 0.627 (0.706)\n",
      "-4: 3-2-32: 0.804 (0.804)\n",
      "-4: 4-2-32: 0.765 (0.843)\n",
      "-4: 3-3-32: 0.667 (0.765)\n",
      "-4: 4-3-32: 0.569 (0.608)\n",
      "-4: 2-2-64: 0.784 (0.824)\n",
      "-4: 2-3-64: 0.745 (0.824)\n",
      "-4: 2-4-64: 0.588 (0.686)\n",
      "-4: 3-2-64: 0.745 (0.843)\n",
      "-4: 4-2-64: 0.784 (0.843)\n",
      "-4: 3-3-64: 0.725 (0.765)\n",
      "-4: 4-3-64: 0.569 (0.686)\n",
      "-4: 2-2-100: 0.784 (0.843)\n",
      "-4: 2-3-100: 0.588 (0.784)\n",
      "-4: 2-4-100: 0.569 (0.706)\n",
      "-4: 3-2-100: 0.765 (0.863)\n",
      "-4: 4-2-100: 0.686 (0.745)\n",
      "-4: 3-3-100: 0.706 (0.765)\n",
      "-4: 4-3-100: 0.549 (0.627)\n",
      "-5: 2-2-8: 0.804 (0.843)\n",
      "-5: 2-3-8: 0.588 (0.667)\n",
      "-5: 2-4-8: 0.608 (0.647)\n",
      "-5: 3-2-8: 0.745 (0.784)\n",
      "-5: 4-2-8: 0.784 (0.804)\n",
      "-5: 3-3-8: 0.588 (0.667)\n",
      "-5: 4-3-8: 0.392 (0.490)\n",
      "-5: 2-2-16: 0.706 (0.765)\n",
      "-5: 2-3-16: 0.647 (0.725)\n",
      "-5: 2-4-16: 0.667 (0.667)\n",
      "-5: 3-2-16: 0.765 (0.804)\n",
      "-5: 4-2-16: 0.784 (0.863)\n",
      "-5: 3-3-16: 0.647 (0.686)\n",
      "-5: 4-3-16: 0.569 (0.608)\n",
      "-5: 2-2-32: 0.725 (0.824)\n",
      "-5: 2-3-32: 0.667 (0.706)\n",
      "-5: 2-4-32: 0.647 (0.686)\n",
      "-5: 3-2-32: 0.725 (0.804)\n",
      "-5: 4-2-32: 0.745 (0.824)\n",
      "-5: 3-3-32: 0.627 (0.725)\n",
      "-5: 4-3-32: 0.471 (0.608)\n",
      "-5: 2-2-64: 0.706 (0.824)\n",
      "-5: 2-3-64: 0.667 (0.725)\n",
      "-5: 2-4-64: 0.510 (0.608)\n",
      "-5: 3-2-64: 0.706 (0.804)\n",
      "-5: 4-2-64: 0.686 (0.765)\n",
      "-5: 3-3-64: 0.706 (0.725)\n",
      "-5: 4-3-64: 0.588 (0.647)\n",
      "-5: 2-2-100: 0.745 (0.824)\n",
      "-5: 2-3-100: 0.667 (0.706)\n",
      "-5: 2-4-100: 0.588 (0.667)\n",
      "-5: 3-2-100: 0.725 (0.804)\n",
      "-5: 4-2-100: 0.804 (0.804)\n",
      "-5: 3-3-100: 0.686 (0.725)\n",
      "-5: 4-3-100: 0.392 (0.549)\n",
      "-6: 2-2-8: 0.706 (0.745)\n",
      "-6: 2-3-8: 0.725 (0.765)\n",
      "-6: 2-4-8: 0.667 (0.706)\n",
      "-6: 3-2-8: 0.765 (0.765)\n",
      "-6: 4-2-8: 0.647 (0.706)\n",
      "-6: 3-3-8: 0.647 (0.725)\n",
      "-6: 4-3-8: 0.569 (0.647)\n",
      "-6: 2-2-16: 0.686 (0.765)\n",
      "-6: 2-3-16: 0.647 (0.725)\n",
      "-6: 2-4-16: 0.667 (0.725)\n",
      "-6: 3-2-16: 0.686 (0.745)\n",
      "-6: 4-2-16: 0.569 (0.667)\n",
      "-6: 3-3-16: 0.706 (0.725)\n",
      "-6: 4-3-16: 0.588 (0.725)\n",
      "-6: 2-2-32: 0.745 (0.765)\n",
      "-6: 2-3-32: 0.667 (0.725)\n",
      "-6: 2-4-32: 0.569 (0.647)\n",
      "-6: 3-2-32: 0.667 (0.765)\n",
      "-6: 4-2-32: 0.627 (0.686)\n",
      "-6: 3-3-32: 0.706 (0.725)\n",
      "-6: 4-3-32: 0.569 (0.647)\n",
      "-6: 2-2-64: 0.725 (0.765)\n",
      "-6: 2-3-64: 0.686 (0.745)\n",
      "-6: 2-4-64: 0.686 (0.725)\n",
      "-6: 3-2-64: 0.627 (0.745)\n",
      "-6: 4-2-64: 0.667 (0.784)\n",
      "-6: 3-3-64: 0.686 (0.725)\n",
      "-6: 4-3-64: 0.549 (0.608)\n",
      "-6: 2-2-100: 0.686 (0.725)\n",
      "-6: 2-3-100: 0.647 (0.745)\n",
      "-6: 2-4-100: 0.569 (0.667)\n",
      "-6: 3-2-100: 0.706 (0.765)\n",
      "-6: 4-2-100: 0.706 (0.745)\n",
      "-6: 3-3-100: 0.647 (0.725)\n",
      "-6: 4-3-100: 0.608 (0.667)\n",
      "-7: 2-2-8: 0.824 (0.843)\n",
      "-7: 2-3-8: 0.804 (0.804)\n",
      "-7: 2-4-8: 0.686 (0.745)\n",
      "-7: 3-2-8: 0.765 (0.824)\n",
      "-7: 4-2-8: 0.765 (0.804)\n",
      "-7: 3-3-8: 0.725 (0.804)\n",
      "-7: 4-3-8: 0.569 (0.627)\n",
      "-7: 2-2-16: 0.804 (0.843)\n",
      "-7: 2-3-16: 0.784 (0.824)\n",
      "-7: 2-4-16: 0.706 (0.784)\n",
      "-7: 3-2-16: 0.804 (0.824)\n",
      "-7: 4-2-16: 0.804 (0.824)\n",
      "-7: 3-3-16: 0.725 (0.745)\n",
      "-7: 4-3-16: 0.588 (0.608)\n",
      "-7: 2-2-32: 0.824 (0.863)\n",
      "-7: 2-3-32: 0.804 (0.824)\n",
      "-7: 2-4-32: 0.647 (0.686)\n",
      "-7: 3-2-32: 0.784 (0.824)\n",
      "-7: 4-2-32: 0.804 (0.843)\n",
      "-7: 3-3-32: 0.569 (0.627)\n",
      "-7: 4-3-32: 0.549 (0.647)\n",
      "-7: 2-2-64: 0.824 (0.863)\n",
      "-7: 2-3-64: 0.804 (0.824)\n",
      "-7: 2-4-64: 0.667 (0.706)\n",
      "-7: 3-2-64: 0.804 (0.824)\n",
      "-7: 4-2-64: 0.784 (0.843)\n",
      "-7: 3-3-64: 0.765 (0.824)\n",
      "-7: 4-3-64: 0.588 (0.686)\n",
      "-7: 2-2-100: 0.843 (0.863)\n",
      "-7: 2-3-100: 0.784 (0.824)\n",
      "-7: 2-4-100: 0.647 (0.706)\n",
      "-7: 3-2-100: 0.765 (0.863)\n",
      "-7: 4-2-100: 0.804 (0.863)\n",
      "-7: 3-3-100: 0.627 (0.725)\n",
      "-7: 4-3-100: 0.647 (0.686)\n",
      "-8: 2-2-8: 0.843 (0.863)\n",
      "-8: 2-3-8: 0.745 (0.804)\n",
      "-8: 2-4-8: 0.627 (0.725)\n",
      "-8: 3-2-8: 0.706 (0.745)\n",
      "-8: 4-2-8: 0.627 (0.745)\n",
      "-8: 3-3-8: 0.549 (0.569)\n",
      "-8: 4-3-8: 0.627 (0.686)\n",
      "-8: 2-2-16: 0.784 (0.863)\n",
      "-8: 2-3-16: 0.667 (0.745)\n",
      "-8: 2-4-16: 0.608 (0.667)\n",
      "-8: 3-2-16: 0.824 (0.824)\n",
      "-8: 4-2-16: 0.784 (0.843)\n",
      "-8: 3-3-16: 0.725 (0.745)\n",
      "-8: 4-3-16: 0.647 (0.647)\n",
      "-8: 2-2-32: 0.804 (0.843)\n",
      "-8: 2-3-32: 0.725 (0.804)\n",
      "-8: 2-4-32: 0.627 (0.706)\n",
      "-8: 3-2-32: 0.745 (0.824)\n",
      "-8: 4-2-32: 0.647 (0.745)\n",
      "-8: 3-3-32: 0.706 (0.784)\n",
      "-8: 4-3-32: 0.627 (0.706)\n",
      "-8: 2-2-64: 0.804 (0.843)\n",
      "-8: 2-3-64: 0.706 (0.765)\n",
      "-8: 2-4-64: 0.549 (0.686)\n",
      "-8: 3-2-64: 0.765 (0.804)\n",
      "-8: 4-2-64: 0.647 (0.765)\n",
      "-8: 3-3-64: 0.686 (0.804)\n",
      "-8: 4-3-64: 0.627 (0.686)\n",
      "-8: 2-2-100: 0.765 (0.863)\n",
      "-8: 2-3-100: 0.706 (0.784)\n",
      "-8: 2-4-100: 0.471 (0.706)\n",
      "-8: 3-2-100: 0.725 (0.843)\n",
      "-8: 4-2-100: 0.804 (0.824)\n",
      "-8: 3-3-100: 0.706 (0.745)\n",
      "-8: 4-3-100: 0.588 (0.667)\n",
      "-9: 2-2-8: 0.745 (0.804)\n",
      "-9: 2-3-8: 0.706 (0.765)\n",
      "-9: 2-4-8: 0.627 (0.647)\n",
      "-9: 3-2-8: 0.765 (0.824)\n",
      "-9: 4-2-8: 0.569 (0.647)\n",
      "-9: 3-3-8: 0.647 (0.686)\n",
      "-9: 4-3-8: 0.569 (0.608)\n",
      "-9: 2-2-16: 0.706 (0.804)\n",
      "-9: 2-3-16: 0.647 (0.725)\n",
      "-9: 2-4-16: 0.647 (0.667)\n",
      "-9: 3-2-16: 0.765 (0.784)\n",
      "-9: 4-2-16: 0.569 (0.627)\n",
      "-9: 3-3-16: 0.608 (0.667)\n",
      "-9: 4-3-16: 0.569 (0.588)\n",
      "-9: 2-2-32: 0.765 (0.804)\n",
      "-9: 2-3-32: 0.745 (0.745)\n",
      "-9: 2-4-32: 0.667 (0.667)\n",
      "-9: 3-2-32: 0.745 (0.784)\n",
      "-9: 4-2-32: 0.627 (0.686)\n",
      "-9: 3-3-32: 0.647 (0.667)\n",
      "-9: 4-3-32: 0.608 (0.608)\n",
      "-9: 2-2-64: 0.725 (0.804)\n",
      "-9: 2-3-64: 0.667 (0.725)\n",
      "-9: 2-4-64: 0.627 (0.647)\n",
      "-9: 3-2-64: 0.784 (0.824)\n",
      "-9: 4-2-64: 0.627 (0.667)\n",
      "-9: 3-3-64: 0.588 (0.588)\n",
      "-9: 4-3-64: 0.588 (0.588)\n",
      "-9: 2-2-100: 0.784 (0.824)\n",
      "-9: 2-3-100: 0.647 (0.706)\n",
      "-9: 2-4-100: 0.608 (0.627)\n",
      "-9: 3-2-100: 0.706 (0.824)\n",
      "-9: 4-2-100: 0.588 (0.667)\n",
      "-9: 3-3-100: 0.608 (0.686)\n",
      "-9: 4-3-100: 0.510 (0.588)\n",
      "-10: 2-2-8: 0.804 (0.843)\n",
      "-10: 2-3-8: 0.725 (0.784)\n",
      "-10: 2-4-8: 0.627 (0.725)\n",
      "-10: 3-2-8: 0.765 (0.843)\n",
      "-10: 4-2-8: 0.686 (0.843)\n",
      "-10: 3-3-8: 0.667 (0.706)\n",
      "-10: 4-3-8: 0.686 (0.706)\n",
      "-10: 2-2-16: 0.784 (0.863)\n",
      "-10: 2-3-16: 0.725 (0.745)\n",
      "-10: 2-4-16: 0.627 (0.725)\n",
      "-10: 3-2-16: 0.745 (0.804)\n",
      "-10: 4-2-16: 0.824 (0.843)\n",
      "-10: 3-3-16: 0.647 (0.765)\n",
      "-10: 4-3-16: 0.647 (0.667)\n",
      "-10: 2-2-32: 0.843 (0.863)\n",
      "-10: 2-3-32: 0.725 (0.843)\n",
      "-10: 2-4-32: 0.647 (0.667)\n",
      "-10: 3-2-32: 0.765 (0.824)\n",
      "-10: 4-2-32: 0.745 (0.843)\n",
      "-10: 3-3-32: 0.647 (0.725)\n",
      "-10: 4-3-32: 0.608 (0.667)\n",
      "-10: 2-2-64: 0.784 (0.882)\n",
      "-10: 2-3-64: 0.804 (0.824)\n",
      "-10: 2-4-64: 0.608 (0.667)\n",
      "-10: 3-2-64: 0.765 (0.843)\n",
      "-10: 4-2-64: 0.647 (0.706)\n",
      "-10: 3-3-64: 0.647 (0.765)\n",
      "-10: 4-3-64: 0.569 (0.627)\n",
      "-10: 2-2-100: 0.804 (0.863)\n",
      "-10: 2-3-100: 0.784 (0.784)\n",
      "-10: 2-4-100: 0.549 (0.667)\n",
      "-10: 3-2-100: 0.765 (0.824)\n",
      "-10: 4-2-100: 0.647 (0.784)\n",
      "-10: 3-3-100: 0.706 (0.784)\n",
      "-10: 4-3-100: 0.608 (0.667)\n",
      "-11: 2-2-8: 0.745 (0.804)\n",
      "-11: 2-3-8: 0.667 (0.706)\n",
      "-11: 2-4-8: 0.569 (0.706)\n",
      "-11: 3-2-8: 0.706 (0.784)\n",
      "-11: 4-2-8: 0.686 (0.765)\n",
      "-11: 3-3-8: 0.667 (0.725)\n",
      "-11: 4-3-8: 0.608 (0.725)\n",
      "-11: 2-2-16: 0.745 (0.784)\n",
      "-11: 2-3-16: 0.745 (0.745)\n",
      "-11: 2-4-16: 0.627 (0.706)\n",
      "-11: 3-2-16: 0.706 (0.784)\n",
      "-11: 4-2-16: 0.706 (0.765)\n",
      "-11: 3-3-16: 0.608 (0.725)\n",
      "-11: 4-3-16: 0.627 (0.686)\n",
      "-11: 2-2-32: 0.725 (0.784)\n",
      "-11: 2-3-32: 0.627 (0.765)\n",
      "-11: 2-4-32: 0.667 (0.725)\n",
      "-11: 3-2-32: 0.667 (0.745)\n",
      "-11: 4-2-32: 0.725 (0.824)\n",
      "-11: 3-3-32: 0.686 (0.745)\n",
      "-11: 4-3-32: 0.549 (0.569)\n",
      "-11: 2-2-64: 0.765 (0.804)\n",
      "-11: 2-3-64: 0.686 (0.745)\n",
      "-11: 2-4-64: 0.647 (0.667)\n",
      "-11: 3-2-64: 0.745 (0.784)\n",
      "-11: 4-2-64: 0.765 (0.784)\n",
      "-11: 3-3-64: 0.627 (0.706)\n",
      "-11: 4-3-64: 0.686 (0.686)\n",
      "-11: 2-2-100: 0.745 (0.784)\n",
      "-11: 2-3-100: 0.608 (0.706)\n",
      "-11: 2-4-100: 0.627 (0.725)\n",
      "-11: 3-2-100: 0.686 (0.765)\n",
      "-11: 4-2-100: 0.706 (0.784)\n",
      "-11: 3-3-100: 0.667 (0.706)\n",
      "-11: 4-3-100: 0.490 (0.608)\n",
      "-12: 2-2-8: 0.843 (0.902)\n",
      "-12: 2-3-8: 0.784 (0.804)\n",
      "-12: 2-4-8: 0.725 (0.804)\n",
      "-12: 3-2-8: 0.843 (0.902)\n",
      "-12: 4-2-8: 0.804 (0.863)\n",
      "-12: 3-3-8: 0.765 (0.804)\n",
      "-12: 4-3-8: 0.588 (0.706)\n",
      "-12: 2-2-16: 0.843 (0.902)\n",
      "-12: 2-3-16: 0.824 (0.843)\n",
      "-12: 2-4-16: 0.745 (0.784)\n",
      "-12: 3-2-16: 0.863 (0.882)\n",
      "-12: 4-2-16: 0.843 (0.882)\n",
      "-12: 3-3-16: 0.765 (0.843)\n",
      "-12: 4-3-16: 0.804 (0.843)\n",
      "-12: 2-2-32: 0.863 (0.902)\n",
      "-12: 2-3-32: 0.804 (0.843)\n",
      "-12: 2-4-32: 0.686 (0.765)\n",
      "-12: 3-2-32: 0.843 (0.882)\n",
      "-12: 4-2-32: 0.804 (0.843)\n",
      "-12: 3-3-32: 0.804 (0.843)\n",
      "-12: 4-3-32: 0.725 (0.784)\n",
      "-12: 2-2-64: 0.804 (0.902)\n",
      "-12: 2-3-64: 0.765 (0.843)\n",
      "-12: 2-4-64: 0.608 (0.784)\n",
      "-12: 3-2-64: 0.843 (0.863)\n",
      "-12: 4-2-64: 0.804 (0.863)\n",
      "-12: 3-3-64: 0.765 (0.824)\n",
      "-12: 4-3-64: 0.784 (0.804)\n",
      "-12: 2-2-100: 0.843 (0.922)\n",
      "-12: 2-3-100: 0.804 (0.843)\n",
      "-12: 2-4-100: 0.647 (0.667)\n",
      "-12: 3-2-100: 0.863 (0.902)\n",
      "-12: 4-2-100: 0.804 (0.882)\n",
      "-12: 3-3-100: 0.706 (0.804)\n",
      "-12: 4-3-100: 0.843 (0.843)\n",
      "-13: 2-2-8: 0.765 (0.784)\n",
      "-13: 2-3-8: 0.647 (0.725)\n",
      "-13: 2-4-8: 0.569 (0.627)\n",
      "-13: 3-2-8: 0.745 (0.765)\n",
      "-13: 4-2-8: 0.745 (0.745)\n",
      "-13: 3-3-8: 0.647 (0.706)\n",
      "-13: 4-3-8: 0.627 (0.686)\n",
      "-13: 2-2-16: 0.765 (0.804)\n",
      "-13: 2-3-16: 0.608 (0.706)\n",
      "-13: 2-4-16: 0.529 (0.608)\n",
      "-13: 3-2-16: 0.745 (0.784)\n",
      "-13: 4-2-16: 0.647 (0.745)\n",
      "-13: 3-3-16: 0.667 (0.765)\n",
      "-13: 4-3-16: 0.608 (0.706)\n",
      "-13: 2-2-32: 0.765 (0.804)\n",
      "-13: 2-3-32: 0.706 (0.725)\n",
      "-13: 2-4-32: 0.549 (0.647)\n",
      "-13: 3-2-32: 0.784 (0.804)\n",
      "-13: 4-2-32: 0.686 (0.706)\n",
      "-13: 3-3-32: 0.706 (0.804)\n",
      "-13: 4-3-32: 0.647 (0.686)\n",
      "-13: 2-2-64: 0.745 (0.804)\n",
      "-13: 2-3-64: 0.706 (0.725)\n",
      "-13: 2-4-64: 0.588 (0.647)\n",
      "-13: 3-2-64: 0.745 (0.804)\n",
      "-13: 4-2-64: 0.725 (0.804)\n",
      "-13: 3-3-64: 0.686 (0.745)\n",
      "-13: 4-3-64: 0.490 (0.569)\n",
      "-13: 2-2-100: 0.745 (0.824)\n",
      "-13: 2-3-100: 0.706 (0.765)\n",
      "-13: 2-4-100: 0.588 (0.608)\n",
      "-13: 3-2-100: 0.725 (0.804)\n",
      "-13: 4-2-100: 0.745 (0.804)\n",
      "-13: 3-3-100: 0.627 (0.647)\n",
      "-13: 4-3-100: 0.490 (0.588)\n",
      "-14: 2-2-8: 0.765 (0.824)\n",
      "-14: 2-3-8: 0.725 (0.784)\n",
      "-14: 2-4-8: 0.627 (0.706)\n",
      "-14: 3-2-8: 0.784 (0.824)\n",
      "-14: 4-2-8: 0.667 (0.784)\n",
      "-14: 3-3-8: 0.686 (0.745)\n",
      "-14: 4-3-8: 0.549 (0.667)\n",
      "-14: 2-2-16: 0.765 (0.804)\n",
      "-14: 2-3-16: 0.765 (0.824)\n",
      "-14: 2-4-16: 0.569 (0.686)\n",
      "-14: 3-2-16: 0.804 (0.824)\n",
      "-14: 4-2-16: 0.765 (0.824)\n",
      "-14: 3-3-16: 0.725 (0.745)\n",
      "-14: 4-3-16: 0.471 (0.510)\n",
      "-14: 2-2-32: 0.804 (0.843)\n",
      "-14: 2-3-32: 0.725 (0.804)\n",
      "-14: 2-4-32: 0.569 (0.686)\n",
      "-14: 3-2-32: 0.765 (0.824)\n",
      "-14: 4-2-32: 0.725 (0.765)\n",
      "-14: 3-3-32: 0.647 (0.765)\n",
      "-14: 4-3-32: 0.588 (0.627)\n",
      "-14: 2-2-64: 0.824 (0.863)\n",
      "-14: 2-3-64: 0.765 (0.824)\n",
      "-14: 2-4-64: 0.569 (0.686)\n",
      "-14: 3-2-64: 0.804 (0.843)\n",
      "-14: 4-2-64: 0.824 (0.843)\n",
      "-14: 3-3-64: 0.706 (0.765)\n",
      "-14: 4-3-64: 0.549 (0.647)\n",
      "-14: 2-2-100: 0.765 (0.843)\n",
      "-14: 2-3-100: 0.765 (0.824)\n",
      "-14: 2-4-100: 0.608 (0.667)\n",
      "-14: 3-2-100: 0.765 (0.824)\n",
      "-14: 4-2-100: 0.745 (0.784)\n",
      "-14: 3-3-100: 0.588 (0.706)\n",
      "-14: 4-3-100: 0.588 (0.627)\n",
      "-15: 2-2-8: 0.686 (0.765)\n",
      "-15: 2-3-8: 0.667 (0.745)\n",
      "-15: 2-4-8: 0.549 (0.627)\n",
      "-15: 3-2-8: 0.824 (0.843)\n",
      "-15: 4-2-8: 0.667 (0.725)\n",
      "-15: 3-3-8: 0.627 (0.686)\n",
      "-15: 4-3-8: 0.608 (0.706)\n",
      "-15: 2-2-16: 0.725 (0.784)\n",
      "-15: 2-3-16: 0.647 (0.745)\n",
      "-15: 2-4-16: 0.569 (0.647)\n",
      "-15: 3-2-16: 0.804 (0.824)\n",
      "-15: 4-2-16: 0.725 (0.824)\n",
      "-15: 3-3-16: 0.667 (0.706)\n",
      "-15: 4-3-16: 0.490 (0.569)\n",
      "-15: 2-2-32: 0.706 (0.765)\n",
      "-15: 2-3-32: 0.686 (0.725)\n",
      "-15: 2-4-32: 0.686 (0.725)\n",
      "-15: 3-2-32: 0.745 (0.804)\n",
      "-15: 4-2-32: 0.725 (0.765)\n",
      "-15: 3-3-32: 0.647 (0.706)\n",
      "-15: 4-3-32: 0.588 (0.627)\n",
      "-15: 2-2-64: 0.745 (0.824)\n",
      "-15: 2-3-64: 0.667 (0.745)\n",
      "-15: 2-4-64: 0.588 (0.725)\n",
      "-15: 3-2-64: 0.765 (0.784)\n",
      "-15: 4-2-64: 0.725 (0.745)\n",
      "-15: 3-3-64: 0.627 (0.745)\n",
      "-15: 4-3-64: 0.549 (0.627)\n",
      "-15: 2-2-100: 0.725 (0.784)\n",
      "-15: 2-3-100: 0.608 (0.745)\n",
      "-15: 2-4-100: 0.608 (0.647)\n",
      "-15: 3-2-100: 0.686 (0.804)\n",
      "-15: 4-2-100: 0.745 (0.804)\n",
      "-15: 3-3-100: 0.667 (0.725)\n",
      "-15: 4-3-100: 0.471 (0.549)\n",
      "-16: 2-2-8: 0.667 (0.745)\n",
      "-16: 2-3-8: 0.647 (0.686)\n",
      "-16: 2-4-8: 0.725 (0.745)\n",
      "-16: 3-2-8: 0.667 (0.706)\n",
      "-16: 4-2-8: 0.529 (0.608)\n",
      "-16: 3-3-8: 0.667 (0.745)\n",
      "-16: 4-3-8: 0.686 (0.765)\n",
      "-16: 2-2-16: 0.706 (0.745)\n",
      "-16: 2-3-16: 0.667 (0.725)\n",
      "-16: 2-4-16: 0.647 (0.725)\n",
      "-16: 3-2-16: 0.667 (0.765)\n",
      "-16: 4-2-16: 0.647 (0.784)\n",
      "-16: 3-3-16: 0.667 (0.745)\n",
      "-16: 4-3-16: 0.647 (0.686)\n",
      "-16: 2-2-32: 0.706 (0.765)\n",
      "-16: 2-3-32: 0.647 (0.745)\n",
      "-16: 2-4-32: 0.647 (0.725)\n",
      "-16: 3-2-32: 0.706 (0.745)\n",
      "-16: 4-2-32: 0.627 (0.706)\n",
      "-16: 3-3-32: 0.706 (0.745)\n",
      "-16: 4-3-32: 0.608 (0.627)\n",
      "-16: 2-2-64: 0.725 (0.784)\n",
      "-16: 2-3-64: 0.647 (0.745)\n",
      "-16: 2-4-64: 0.588 (0.706)\n",
      "-16: 3-2-64: 0.706 (0.745)\n",
      "-16: 4-2-64: 0.706 (0.765)\n",
      "-16: 3-3-64: 0.706 (0.745)\n",
      "-16: 4-3-64: 0.608 (0.647)\n",
      "-16: 2-2-100: 0.725 (0.765)\n",
      "-16: 2-3-100: 0.667 (0.745)\n",
      "-16: 2-4-100: 0.588 (0.745)\n",
      "-16: 3-2-100: 0.627 (0.745)\n",
      "-16: 4-2-100: 0.706 (0.745)\n",
      "-16: 3-3-100: 0.745 (0.784)\n",
      "-16: 4-3-100: 0.608 (0.686)\n",
      "-17: 2-2-8: 0.824 (0.863)\n",
      "-17: 2-3-8: 0.745 (0.824)\n",
      "-17: 2-4-8: 0.549 (0.686)\n",
      "-17: 3-2-8: 0.725 (0.804)\n",
      "-17: 4-2-8: 0.667 (0.725)\n",
      "-17: 3-3-8: 0.667 (0.765)\n",
      "-17: 4-3-8: 0.647 (0.686)\n",
      "-17: 2-2-16: 0.824 (0.824)\n",
      "-17: 2-3-16: 0.784 (0.804)\n",
      "-17: 2-4-16: 0.627 (0.667)\n",
      "-17: 3-2-16: 0.745 (0.843)\n",
      "-17: 4-2-16: 0.765 (0.804)\n",
      "-17: 3-3-16: 0.745 (0.765)\n",
      "-17: 4-3-16: 0.510 (0.647)\n",
      "-17: 2-2-32: 0.824 (0.843)\n",
      "-17: 2-3-32: 0.804 (0.824)\n",
      "-17: 2-4-32: 0.588 (0.686)\n",
      "-17: 3-2-32: 0.765 (0.824)\n",
      "-17: 4-2-32: 0.804 (0.843)\n",
      "-17: 3-3-32: 0.765 (0.824)\n",
      "-17: 4-3-32: 0.627 (0.627)\n",
      "-17: 2-2-64: 0.784 (0.863)\n",
      "-17: 2-3-64: 0.745 (0.804)\n",
      "-17: 2-4-64: 0.686 (0.725)\n",
      "-17: 3-2-64: 0.824 (0.843)\n",
      "-17: 4-2-64: 0.686 (0.824)\n",
      "-17: 3-3-64: 0.765 (0.824)\n",
      "-17: 4-3-64: 0.667 (0.686)\n",
      "-17: 2-2-100: 0.824 (0.863)\n",
      "-17: 2-3-100: 0.784 (0.824)\n",
      "-17: 2-4-100: 0.627 (0.686)\n",
      "-17: 3-2-100: 0.804 (0.843)\n",
      "-17: 4-2-100: 0.784 (0.843)\n",
      "-17: 3-3-100: 0.765 (0.765)\n",
      "-17: 4-3-100: 0.549 (0.647)\n",
      "-18: 2-2-8: 0.784 (0.863)\n",
      "-18: 2-3-8: 0.706 (0.804)\n",
      "-18: 2-4-8: 0.667 (0.725)\n",
      "-18: 3-2-8: 0.745 (0.824)\n",
      "-18: 4-2-8: 0.745 (0.784)\n",
      "-18: 3-3-8: 0.686 (0.706)\n",
      "-18: 4-3-8: 0.706 (0.706)\n",
      "-18: 2-2-16: 0.745 (0.863)\n",
      "-18: 2-3-16: 0.745 (0.804)\n",
      "-18: 2-4-16: 0.608 (0.608)\n",
      "-18: 3-2-16: 0.745 (0.843)\n",
      "-18: 4-2-16: 0.627 (0.725)\n",
      "-18: 3-3-16: 0.706 (0.765)\n",
      "-18: 4-3-16: 0.627 (0.667)\n",
      "-18: 2-2-32: 0.765 (0.843)\n",
      "-18: 2-3-32: 0.725 (0.765)\n",
      "-18: 2-4-32: 0.510 (0.686)\n",
      "-18: 3-2-32: 0.784 (0.843)\n",
      "-18: 4-2-32: 0.686 (0.824)\n",
      "-18: 3-3-32: 0.627 (0.725)\n",
      "-18: 4-3-32: 0.667 (0.667)\n",
      "-18: 2-2-64: 0.804 (0.863)\n",
      "-18: 2-3-64: 0.569 (0.745)\n",
      "-18: 2-4-64: 0.686 (0.706)\n",
      "-18: 3-2-64: 0.706 (0.824)\n",
      "-18: 4-2-64: 0.667 (0.765)\n",
      "-18: 3-3-64: 0.647 (0.784)\n",
      "-18: 4-3-64: 0.412 (0.569)\n",
      "-18: 2-2-100: 0.784 (0.863)\n",
      "-18: 2-3-100: 0.686 (0.784)\n",
      "-18: 2-4-100: 0.431 (0.529)\n",
      "-18: 3-2-100: 0.804 (0.843)\n",
      "-18: 4-2-100: 0.627 (0.706)\n",
      "-18: 3-3-100: 0.667 (0.686)\n",
      "-18: 4-3-100: 0.647 (0.725)\n",
      "-19: 2-2-8: 0.804 (0.843)\n",
      "-19: 2-3-8: 0.667 (0.686)\n",
      "-19: 2-4-8: 0.569 (0.627)\n",
      "-19: 3-2-8: 0.608 (0.627)\n",
      "-19: 4-2-8: 0.569 (0.647)\n",
      "-19: 3-3-8: 0.608 (0.667)\n",
      "-19: 4-3-8: 0.588 (0.647)\n",
      "-19: 2-2-16: 0.784 (0.784)\n",
      "-19: 2-3-16: 0.706 (0.725)\n",
      "-19: 2-4-16: 0.588 (0.608)\n",
      "-19: 3-2-16: 0.784 (0.804)\n",
      "-19: 4-2-16: 0.588 (0.667)\n",
      "-19: 3-3-16: 0.588 (0.686)\n",
      "-19: 4-3-16: 0.588 (0.608)\n",
      "-19: 2-2-32: 0.784 (0.784)\n",
      "-19: 2-3-32: 0.647 (0.725)\n",
      "-19: 2-4-32: 0.608 (0.647)\n",
      "-19: 3-2-32: 0.706 (0.784)\n",
      "-19: 4-2-32: 0.627 (0.647)\n",
      "-19: 3-3-32: 0.627 (0.686)\n",
      "-19: 4-3-32: 0.529 (0.588)\n",
      "-19: 2-2-64: 0.765 (0.804)\n",
      "-19: 2-3-64: 0.686 (0.706)\n",
      "-19: 2-4-64: 0.608 (0.667)\n",
      "-19: 3-2-64: 0.765 (0.804)\n",
      "-19: 4-2-64: 0.588 (0.627)\n",
      "-19: 3-3-64: 0.667 (0.725)\n",
      "-19: 4-3-64: 0.569 (0.608)\n",
      "-19: 2-2-100: 0.784 (0.804)\n",
      "-19: 2-3-100: 0.667 (0.706)\n",
      "-19: 2-4-100: 0.569 (0.627)\n",
      "-19: 3-2-100: 0.706 (0.824)\n",
      "-19: 4-2-100: 0.549 (0.627)\n",
      "-19: 3-3-100: 0.608 (0.686)\n",
      "-19: 4-3-100: 0.608 (0.608)\n",
      "-20: 2-2-8: 0.804 (0.843)\n",
      "-20: 2-3-8: 0.745 (0.765)\n",
      "-20: 2-4-8: 0.647 (0.725)\n",
      "-20: 3-2-8: 0.784 (0.843)\n",
      "-20: 4-2-8: 0.647 (0.706)\n",
      "-20: 3-3-8: 0.667 (0.706)\n",
      "-20: 4-3-8: 0.627 (0.686)\n",
      "-20: 2-2-16: 0.843 (0.863)\n",
      "-20: 2-3-16: 0.784 (0.784)\n",
      "-20: 2-4-16: 0.667 (0.745)\n",
      "-20: 3-2-16: 0.745 (0.882)\n",
      "-20: 4-2-16: 0.647 (0.765)\n",
      "-20: 3-3-16: 0.667 (0.765)\n",
      "-20: 4-3-16: 0.667 (0.667)\n",
      "-20: 2-2-32: 0.824 (0.863)\n",
      "-20: 2-3-32: 0.725 (0.804)\n",
      "-20: 2-4-32: 0.706 (0.725)\n",
      "-20: 3-2-32: 0.824 (0.843)\n",
      "-20: 4-2-32: 0.745 (0.843)\n",
      "-20: 3-3-32: 0.686 (0.784)\n",
      "-20: 4-3-32: 0.588 (0.627)\n",
      "-20: 2-2-64: 0.784 (0.843)\n",
      "-20: 2-3-64: 0.686 (0.765)\n",
      "-20: 2-4-64: 0.608 (0.686)\n",
      "-20: 3-2-64: 0.784 (0.824)\n",
      "-20: 4-2-64: 0.784 (0.863)\n",
      "-20: 3-3-64: 0.667 (0.745)\n",
      "-20: 4-3-64: 0.627 (0.647)\n",
      "-20: 2-2-100: 0.765 (0.882)\n",
      "-20: 2-3-100: 0.706 (0.784)\n",
      "-20: 2-4-100: 0.569 (0.706)\n",
      "-20: 3-2-100: 0.824 (0.863)\n",
      "-20: 4-2-100: 0.667 (0.784)\n",
      "-20: 3-3-100: 0.686 (0.745)\n",
      "-20: 4-3-100: 0.608 (0.627)\n",
      "----- 18.76 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'L': 2, 'K': 2, 'hid_dim': 8},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 8},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 8},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 8},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 8},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 8},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 8},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 16},\n",
    "    \n",
    "        {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 32},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 32},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 64},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 64},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 64},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 64},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 64},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 100},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 100},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 100},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 100},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 100},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 100},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 100},\n",
    "        ]\n",
    "\n",
    "best_accs3 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs3 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, exp['hid_dim'], OUT_DIM, exp['L'], exp['K'], act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0, diff_layer=GFGCN_noh_Layer)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "\n",
    "        best_accs3[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs3[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}: {best_val_accs3[j,i]:.3f} ({best_accs3[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}' for exp in EXPS]\n",
    "table_over3 = summary_table(best_accs3, index_name)\n",
    "table3 = summary_table(best_val_accs3, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-8</th>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.049020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-8</th>\n",
       "      <td>0.710784</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.055068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-8</th>\n",
       "      <td>0.629412</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.065297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-8</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.054718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-8</th>\n",
       "      <td>0.693137</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.082324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-8</th>\n",
       "      <td>0.657843</td>\n",
       "      <td>0.656863</td>\n",
       "      <td>0.051683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-8</th>\n",
       "      <td>0.606863</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.084672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.765686</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.047415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.712745</td>\n",
       "      <td>0.715686</td>\n",
       "      <td>0.067148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-16</th>\n",
       "      <td>0.632353</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.059433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.767647</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.050858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-16</th>\n",
       "      <td>0.708824</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.085755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-16</th>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.051234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-16</th>\n",
       "      <td>0.597059</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.086691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.780392</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.047869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.712745</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.056242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-32</th>\n",
       "      <td>0.622549</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.049942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-32</th>\n",
       "      <td>0.756863</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.048980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-32</th>\n",
       "      <td>0.722549</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.066284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-32</th>\n",
       "      <td>0.678431</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.054197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-32</th>\n",
       "      <td>0.595098</td>\n",
       "      <td>0.598039</td>\n",
       "      <td>0.065993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-64</th>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.036892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-64</th>\n",
       "      <td>0.706863</td>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.060586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-64</th>\n",
       "      <td>0.614706</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.053077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-64</th>\n",
       "      <td>0.765686</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.052053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-64</th>\n",
       "      <td>0.715686</td>\n",
       "      <td>0.715686</td>\n",
       "      <td>0.063989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-64</th>\n",
       "      <td>0.690196</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.053195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-64</th>\n",
       "      <td>0.591176</td>\n",
       "      <td>0.578431</td>\n",
       "      <td>0.094703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-100</th>\n",
       "      <td>0.772549</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.041779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-100</th>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.064884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-100</th>\n",
       "      <td>0.585294</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.055555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-100</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.058455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-100</th>\n",
       "      <td>0.718627</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.076765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-100</th>\n",
       "      <td>0.677451</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.050554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-100</th>\n",
       "      <td>0.579412</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.095350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean accs       med       std\n",
       "2-2-8     0.774510  0.774510  0.049020\n",
       "2-3-8     0.710784  0.705882  0.055068\n",
       "2-4-8     0.629412  0.627451  0.065297\n",
       "3-2-8     0.750000  0.754902  0.054718\n",
       "4-2-8     0.693137  0.686275  0.082324\n",
       "3-3-8     0.657843  0.656863  0.051683\n",
       "4-3-8     0.606863  0.607843  0.084672\n",
       "2-2-16    0.765686  0.764706  0.047415\n",
       "2-3-16    0.712745  0.715686  0.067148\n",
       "2-4-16    0.632353  0.627451  0.059433\n",
       "3-2-16    0.767647  0.764706  0.050858\n",
       "4-2-16    0.708824  0.705882  0.085755\n",
       "3-3-16    0.683333  0.676471  0.051234\n",
       "4-3-16    0.597059  0.588235  0.086691\n",
       "2-2-32    0.780392  0.774510  0.047869\n",
       "2-3-32    0.712745  0.725490  0.056242\n",
       "2-4-32    0.622549  0.627451  0.049942\n",
       "3-2-32    0.756863  0.754902  0.048980\n",
       "4-2-32    0.722549  0.725490  0.066284\n",
       "3-3-32    0.678431  0.676471  0.054197\n",
       "4-3-32    0.595098  0.598039  0.065993\n",
       "2-2-64    0.776471  0.784314  0.036892\n",
       "2-3-64    0.706863  0.696078  0.060586\n",
       "2-4-64    0.614706  0.607843  0.053077\n",
       "3-2-64    0.765686  0.764706  0.052053\n",
       "4-2-64    0.715686  0.715686  0.063989\n",
       "3-3-64    0.690196  0.686275  0.053195\n",
       "4-3-64    0.591176  0.578431  0.094703\n",
       "2-2-100   0.772549  0.764706  0.041779\n",
       "2-3-100   0.696078  0.696078  0.064884\n",
       "2-4-100   0.585294  0.588235  0.055555\n",
       "3-2-100   0.750000  0.745098  0.058455\n",
       "4-2-100   0.718627  0.725490  0.076765\n",
       "3-3-100   0.677451  0.676471  0.050554\n",
       "4-3-100   0.579412  0.588235  0.095350"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-8</th>\n",
       "      <td>0.824510</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.041813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-8</th>\n",
       "      <td>0.758824</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.049254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-8</th>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.052977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-8</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-8</th>\n",
       "      <td>0.758824</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.072070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-8</th>\n",
       "      <td>0.719608</td>\n",
       "      <td>0.715686</td>\n",
       "      <td>0.056863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-8</th>\n",
       "      <td>0.672549</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.074174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.819608</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.044540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.045098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-16</th>\n",
       "      <td>0.694118</td>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.057968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.817647</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.035130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-16</th>\n",
       "      <td>0.783333</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.067775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-16</th>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.042054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-16</th>\n",
       "      <td>0.653922</td>\n",
       "      <td>0.656863</td>\n",
       "      <td>0.079714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.822549</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.041351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.775490</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.052053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-32</th>\n",
       "      <td>0.690196</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.036473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-32</th>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.034453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-32</th>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.803922</td>\n",
       "      <td>0.066204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-32</th>\n",
       "      <td>0.748039</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.055207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-32</th>\n",
       "      <td>0.649020</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.058791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-64</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.037461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-64</th>\n",
       "      <td>0.768627</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.047869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-64</th>\n",
       "      <td>0.693137</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.042633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-64</th>\n",
       "      <td>0.814706</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.036407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-64</th>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.062622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-64</th>\n",
       "      <td>0.751961</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.059239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-64</th>\n",
       "      <td>0.651961</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.066167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-100</th>\n",
       "      <td>0.831373</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.047384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-100</th>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.047181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-100</th>\n",
       "      <td>0.671569</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.050326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-100</th>\n",
       "      <td>0.825490</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.041083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-100</th>\n",
       "      <td>0.776471</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.064259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-100</th>\n",
       "      <td>0.732353</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.043965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-100</th>\n",
       "      <td>0.655882</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.072701</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         mean accs       med       std\n",
       "2-2-8     0.824510  0.833333  0.041813\n",
       "2-3-8     0.758824  0.764706  0.049254\n",
       "2-4-8     0.705882  0.705882  0.052977\n",
       "3-2-8     0.800000  0.813725  0.062500\n",
       "4-2-8     0.758824  0.754902  0.072070\n",
       "3-3-8     0.719608  0.715686  0.056863\n",
       "4-3-8     0.672549  0.686275  0.074174\n",
       "2-2-16    0.819608  0.803922  0.044540\n",
       "2-3-16    0.766667  0.745098  0.045098\n",
       "2-4-16    0.694118  0.696078  0.057968\n",
       "3-2-16    0.817647  0.823529  0.035130\n",
       "4-2-16    0.783333  0.784314  0.067775\n",
       "3-3-16    0.745098  0.745098  0.042054\n",
       "4-3-16    0.653922  0.656863  0.079714\n",
       "2-2-32    0.822549  0.833333  0.041351\n",
       "2-3-32    0.775490  0.764706  0.052053\n",
       "2-4-32    0.690196  0.686275  0.036473\n",
       "3-2-32    0.808824  0.803922  0.034453\n",
       "4-2-32    0.784314  0.803922  0.066204\n",
       "3-3-32    0.748039  0.745098  0.055207\n",
       "4-3-32    0.649020  0.627451  0.058791\n",
       "2-2-64    0.833333  0.823529  0.037461\n",
       "2-3-64    0.768627  0.745098  0.047869\n",
       "2-4-64    0.693137  0.686275  0.042633\n",
       "3-2-64    0.814706  0.813725  0.036407\n",
       "4-2-64    0.784314  0.784314  0.062622\n",
       "3-3-64    0.751961  0.745098  0.059239\n",
       "4-3-64    0.651961  0.647059  0.066167\n",
       "2-2-100   0.831373  0.833333  0.047384\n",
       "2-3-100   0.766667  0.774510  0.047181\n",
       "2-4-100   0.671569  0.666667  0.050326\n",
       "3-2-100   0.825490  0.823529  0.041083\n",
       "4-2-100   0.776471  0.784314  0.064259\n",
       "3-3-100   0.732353  0.725490  0.043965\n",
       "4-3-100   0.655882  0.647059  0.072701"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_over3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design - Bias False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 2-2-4: 0.745 (0.804)\n",
      "-1: 2-3-4: 0.706 (0.725)\n",
      "-1: 2-4-4: 0.588 (0.686)\n",
      "-1: 3-2-4: 0.706 (0.745)\n",
      "-1: 4-2-4: 0.725 (0.745)\n",
      "-1: 3-3-4: 0.647 (0.725)\n",
      "-1: 4-3-4: 0.686 (0.725)\n",
      "-1: 2-2-8: 0.725 (0.784)\n",
      "-1: 2-3-8: 0.647 (0.745)\n",
      "-1: 2-4-8: 0.588 (0.706)\n",
      "-1: 3-2-8: 0.725 (0.804)\n",
      "-1: 4-2-8: 0.706 (0.784)\n",
      "-1: 3-3-8: 0.647 (0.706)\n",
      "-1: 4-3-8: 0.490 (0.608)\n",
      "-1: 2-2-16: 0.745 (0.765)\n",
      "-1: 2-3-16: 0.667 (0.725)\n",
      "-1: 2-4-16: 0.608 (0.647)\n",
      "-1: 3-2-16: 0.725 (0.804)\n",
      "-1: 4-2-16: 0.725 (0.784)\n",
      "-1: 3-3-16: 0.647 (0.706)\n",
      "-1: 4-3-16: 0.627 (0.706)\n",
      "-1: 2-2-32: 0.804 (0.804)\n",
      "-1: 2-3-32: 0.627 (0.706)\n",
      "-1: 2-4-32: 0.608 (0.706)\n",
      "-1: 3-2-32: 0.784 (0.784)\n",
      "-1: 4-2-32: 0.725 (0.784)\n",
      "-1: 3-3-32: 0.686 (0.745)\n",
      "-1: 4-3-32: 0.471 (0.608)\n",
      "-1: 2-2-50: 0.745 (0.784)\n",
      "-1: 2-3-50: 0.706 (0.706)\n",
      "-1: 2-4-50: 0.627 (0.706)\n",
      "-1: 3-2-50: 0.745 (0.784)\n",
      "-1: 4-2-50: 0.745 (0.765)\n",
      "-1: 3-3-50: 0.686 (0.725)\n",
      "-1: 4-3-50: 0.569 (0.706)\n",
      "-2: 2-2-4: 0.824 (0.882)\n",
      "-2: 2-3-4: 0.824 (0.843)\n",
      "-2: 2-4-4: 0.784 (0.824)\n",
      "-2: 3-2-4: 0.843 (0.902)\n",
      "-2: 4-2-4: 0.843 (0.882)\n",
      "-2: 3-3-4: 0.765 (0.824)\n",
      "-2: 4-3-4: 0.784 (0.824)\n",
      "-2: 2-2-8: 0.843 (0.902)\n",
      "-2: 2-3-8: 0.784 (0.843)\n",
      "-2: 2-4-8: 0.765 (0.804)\n",
      "-2: 3-2-8: 0.863 (0.863)\n",
      "-2: 4-2-8: 0.824 (0.863)\n",
      "-2: 3-3-8: 0.784 (0.824)\n",
      "-2: 4-3-8: 0.765 (0.824)\n",
      "-2: 2-2-16: 0.863 (0.882)\n",
      "-2: 2-3-16: 0.784 (0.882)\n",
      "-2: 2-4-16: 0.627 (0.745)\n",
      "-2: 3-2-16: 0.843 (0.863)\n",
      "-2: 4-2-16: 0.863 (0.882)\n",
      "-2: 3-3-16: 0.765 (0.843)\n",
      "-2: 4-3-16: 0.706 (0.784)\n",
      "-2: 2-2-32: 0.843 (0.902)\n",
      "-2: 2-3-32: 0.804 (0.863)\n",
      "-2: 2-4-32: 0.725 (0.784)\n",
      "-2: 3-2-32: 0.863 (0.902)\n",
      "-2: 4-2-32: 0.784 (0.863)\n",
      "-2: 3-3-32: 0.804 (0.843)\n",
      "-2: 4-3-32: 0.824 (0.824)\n",
      "-2: 2-2-50: 0.843 (0.902)\n",
      "-2: 2-3-50: 0.765 (0.843)\n",
      "-2: 2-4-50: 0.667 (0.745)\n",
      "-2: 3-2-50: 0.863 (0.902)\n",
      "-2: 4-2-50: 0.843 (0.863)\n",
      "-2: 3-3-50: 0.784 (0.824)\n",
      "-2: 4-3-50: 0.725 (0.804)\n",
      "-3: 2-2-4: 0.745 (0.804)\n",
      "-3: 2-3-4: 0.686 (0.706)\n",
      "-3: 2-4-4: 0.529 (0.686)\n",
      "-3: 3-2-4: 0.765 (0.804)\n",
      "-3: 4-2-4: 0.686 (0.804)\n",
      "-3: 3-3-4: 0.608 (0.667)\n",
      "-3: 4-3-4: 0.569 (0.647)\n",
      "-3: 2-2-8: 0.765 (0.804)\n",
      "-3: 2-3-8: 0.627 (0.725)\n",
      "-3: 2-4-8: 0.549 (0.627)\n",
      "-3: 3-2-8: 0.765 (0.824)\n",
      "-3: 4-2-8: 0.706 (0.784)\n",
      "-3: 3-3-8: 0.686 (0.765)\n",
      "-3: 4-3-8: 0.471 (0.549)\n",
      "-3: 2-2-16: 0.745 (0.804)\n",
      "-3: 2-3-16: 0.627 (0.706)\n",
      "-3: 2-4-16: 0.569 (0.627)\n",
      "-3: 3-2-16: 0.745 (0.784)\n",
      "-3: 4-2-16: 0.745 (0.784)\n",
      "-3: 3-3-16: 0.686 (0.765)\n",
      "-3: 4-3-16: 0.451 (0.569)\n",
      "-3: 2-2-32: 0.784 (0.804)\n",
      "-3: 2-3-32: 0.706 (0.725)\n",
      "-3: 2-4-32: 0.549 (0.647)\n",
      "-3: 3-2-32: 0.784 (0.804)\n",
      "-3: 4-2-32: 0.686 (0.784)\n",
      "-3: 3-3-32: 0.647 (0.647)\n",
      "-3: 4-3-32: 0.510 (0.588)\n",
      "-3: 2-2-50: 0.745 (0.804)\n",
      "-3: 2-3-50: 0.627 (0.725)\n",
      "-3: 2-4-50: 0.549 (0.627)\n",
      "-3: 3-2-50: 0.745 (0.824)\n",
      "-3: 4-2-50: 0.667 (0.706)\n",
      "-3: 3-3-50: 0.667 (0.686)\n",
      "-3: 4-3-50: 0.490 (0.647)\n",
      "-4: 2-2-4: 0.765 (0.824)\n",
      "-4: 2-3-4: 0.706 (0.784)\n",
      "-4: 2-4-4: 0.647 (0.745)\n",
      "-4: 3-2-4: 0.745 (0.843)\n",
      "-4: 4-2-4: 0.804 (0.843)\n",
      "-4: 3-3-4: 0.627 (0.765)\n",
      "-4: 4-3-4: 0.451 (0.510)\n",
      "-4: 2-2-8: 0.765 (0.843)\n",
      "-4: 2-3-8: 0.804 (0.824)\n",
      "-4: 2-4-8: 0.647 (0.725)\n",
      "-4: 3-2-8: 0.804 (0.843)\n",
      "-4: 4-2-8: 0.647 (0.784)\n",
      "-4: 3-3-8: 0.686 (0.765)\n",
      "-4: 4-3-8: 0.569 (0.667)\n",
      "-4: 2-2-16: 0.784 (0.843)\n",
      "-4: 2-3-16: 0.745 (0.824)\n",
      "-4: 2-4-16: 0.627 (0.706)\n",
      "-4: 3-2-16: 0.804 (0.804)\n",
      "-4: 4-2-16: 0.765 (0.843)\n",
      "-4: 3-3-16: 0.667 (0.765)\n",
      "-4: 4-3-16: 0.569 (0.608)\n",
      "-4: 2-2-32: 0.784 (0.824)\n",
      "-4: 2-3-32: 0.745 (0.824)\n",
      "-4: 2-4-32: 0.588 (0.686)\n",
      "-4: 3-2-32: 0.745 (0.843)\n",
      "-4: 4-2-32: 0.784 (0.843)\n",
      "-4: 3-3-32: 0.725 (0.765)\n",
      "-4: 4-3-32: 0.569 (0.686)\n",
      "-4: 2-2-50: 0.784 (0.843)\n",
      "-4: 2-3-50: 0.588 (0.784)\n",
      "-4: 2-4-50: 0.569 (0.706)\n",
      "-4: 3-2-50: 0.765 (0.863)\n",
      "-4: 4-2-50: 0.686 (0.745)\n",
      "-4: 3-3-50: 0.706 (0.765)\n",
      "-4: 4-3-50: 0.549 (0.627)\n",
      "-5: 2-2-4: 0.804 (0.843)\n",
      "-5: 2-3-4: 0.588 (0.667)\n",
      "-5: 2-4-4: 0.608 (0.647)\n",
      "-5: 3-2-4: 0.745 (0.784)\n",
      "-5: 4-2-4: 0.784 (0.804)\n",
      "-5: 3-3-4: 0.588 (0.667)\n",
      "-5: 4-3-4: 0.392 (0.490)\n",
      "-5: 2-2-8: 0.706 (0.765)\n",
      "-5: 2-3-8: 0.647 (0.725)\n",
      "-5: 2-4-8: 0.667 (0.667)\n",
      "-5: 3-2-8: 0.765 (0.804)\n",
      "-5: 4-2-8: 0.784 (0.863)\n",
      "-5: 3-3-8: 0.647 (0.686)\n",
      "-5: 4-3-8: 0.569 (0.608)\n",
      "-5: 2-2-16: 0.725 (0.824)\n",
      "-5: 2-3-16: 0.667 (0.706)\n",
      "-5: 2-4-16: 0.647 (0.686)\n",
      "-5: 3-2-16: 0.725 (0.804)\n",
      "-5: 4-2-16: 0.745 (0.824)\n",
      "-5: 3-3-16: 0.627 (0.725)\n",
      "-5: 4-3-16: 0.471 (0.608)\n",
      "-5: 2-2-32: 0.706 (0.824)\n",
      "-5: 2-3-32: 0.667 (0.725)\n",
      "-5: 2-4-32: 0.510 (0.608)\n",
      "-5: 3-2-32: 0.706 (0.804)\n",
      "-5: 4-2-32: 0.686 (0.765)\n",
      "-5: 3-3-32: 0.706 (0.725)\n",
      "-5: 4-3-32: 0.588 (0.647)\n",
      "-5: 2-2-50: 0.745 (0.824)\n",
      "-5: 2-3-50: 0.667 (0.706)\n",
      "-5: 2-4-50: 0.588 (0.667)\n",
      "-5: 3-2-50: 0.725 (0.804)\n",
      "-5: 4-2-50: 0.804 (0.804)\n",
      "-5: 3-3-50: 0.686 (0.725)\n",
      "-5: 4-3-50: 0.392 (0.549)\n",
      "-6: 2-2-4: 0.706 (0.745)\n",
      "-6: 2-3-4: 0.725 (0.765)\n",
      "-6: 2-4-4: 0.667 (0.706)\n",
      "-6: 3-2-4: 0.765 (0.765)\n",
      "-6: 4-2-4: 0.647 (0.706)\n",
      "-6: 3-3-4: 0.647 (0.725)\n",
      "-6: 4-3-4: 0.569 (0.647)\n",
      "-6: 2-2-8: 0.686 (0.765)\n",
      "-6: 2-3-8: 0.647 (0.725)\n",
      "-6: 2-4-8: 0.667 (0.725)\n",
      "-6: 3-2-8: 0.686 (0.745)\n",
      "-6: 4-2-8: 0.569 (0.667)\n",
      "-6: 3-3-8: 0.706 (0.725)\n",
      "-6: 4-3-8: 0.588 (0.725)\n",
      "-6: 2-2-16: 0.745 (0.765)\n",
      "-6: 2-3-16: 0.667 (0.725)\n",
      "-6: 2-4-16: 0.569 (0.647)\n",
      "-6: 3-2-16: 0.667 (0.765)\n",
      "-6: 4-2-16: 0.627 (0.686)\n",
      "-6: 3-3-16: 0.706 (0.725)\n",
      "-6: 4-3-16: 0.569 (0.647)\n",
      "-6: 2-2-32: 0.725 (0.765)\n",
      "-6: 2-3-32: 0.686 (0.745)\n",
      "-6: 2-4-32: 0.686 (0.725)\n",
      "-6: 3-2-32: 0.627 (0.745)\n",
      "-6: 4-2-32: 0.667 (0.784)\n",
      "-6: 3-3-32: 0.686 (0.725)\n",
      "-6: 4-3-32: 0.549 (0.608)\n",
      "-6: 2-2-50: 0.686 (0.725)\n",
      "-6: 2-3-50: 0.647 (0.745)\n",
      "-6: 2-4-50: 0.569 (0.667)\n",
      "-6: 3-2-50: 0.706 (0.765)\n",
      "-6: 4-2-50: 0.706 (0.745)\n",
      "-6: 3-3-50: 0.647 (0.725)\n",
      "-6: 4-3-50: 0.608 (0.667)\n",
      "-7: 2-2-4: 0.824 (0.843)\n",
      "-7: 2-3-4: 0.804 (0.804)\n",
      "-7: 2-4-4: 0.686 (0.745)\n",
      "-7: 3-2-4: 0.765 (0.824)\n",
      "-7: 4-2-4: 0.765 (0.804)\n",
      "-7: 3-3-4: 0.725 (0.804)\n",
      "-7: 4-3-4: 0.569 (0.627)\n",
      "-7: 2-2-8: 0.804 (0.843)\n",
      "-7: 2-3-8: 0.784 (0.824)\n",
      "-7: 2-4-8: 0.706 (0.784)\n",
      "-7: 3-2-8: 0.804 (0.824)\n",
      "-7: 4-2-8: 0.804 (0.824)\n",
      "-7: 3-3-8: 0.725 (0.745)\n",
      "-7: 4-3-8: 0.588 (0.608)\n",
      "-7: 2-2-16: 0.824 (0.863)\n",
      "-7: 2-3-16: 0.804 (0.824)\n",
      "-7: 2-4-16: 0.647 (0.686)\n",
      "-7: 3-2-16: 0.784 (0.824)\n",
      "-7: 4-2-16: 0.804 (0.843)\n",
      "-7: 3-3-16: 0.569 (0.627)\n",
      "-7: 4-3-16: 0.549 (0.647)\n",
      "-7: 2-2-32: 0.824 (0.863)\n",
      "-7: 2-3-32: 0.804 (0.824)\n",
      "-7: 2-4-32: 0.667 (0.706)\n",
      "-7: 3-2-32: 0.804 (0.824)\n",
      "-7: 4-2-32: 0.784 (0.843)\n",
      "-7: 3-3-32: 0.765 (0.824)\n",
      "-7: 4-3-32: 0.588 (0.686)\n",
      "-7: 2-2-50: 0.843 (0.863)\n",
      "-7: 2-3-50: 0.784 (0.824)\n",
      "-7: 2-4-50: 0.647 (0.706)\n",
      "-7: 3-2-50: 0.765 (0.863)\n",
      "-7: 4-2-50: 0.804 (0.863)\n",
      "-7: 3-3-50: 0.627 (0.725)\n",
      "-7: 4-3-50: 0.647 (0.686)\n",
      "-8: 2-2-4: 0.843 (0.863)\n",
      "-8: 2-3-4: 0.745 (0.804)\n",
      "-8: 2-4-4: 0.627 (0.725)\n",
      "-8: 3-2-4: 0.706 (0.745)\n",
      "-8: 4-2-4: 0.627 (0.745)\n",
      "-8: 3-3-4: 0.549 (0.569)\n",
      "-8: 4-3-4: 0.627 (0.686)\n",
      "-8: 2-2-8: 0.784 (0.863)\n",
      "-8: 2-3-8: 0.667 (0.745)\n",
      "-8: 2-4-8: 0.608 (0.667)\n",
      "-8: 3-2-8: 0.824 (0.824)\n",
      "-8: 4-2-8: 0.784 (0.843)\n",
      "-8: 3-3-8: 0.725 (0.745)\n",
      "-8: 4-3-8: 0.647 (0.647)\n",
      "-8: 2-2-16: 0.804 (0.843)\n",
      "-8: 2-3-16: 0.725 (0.804)\n",
      "-8: 2-4-16: 0.627 (0.706)\n",
      "-8: 3-2-16: 0.745 (0.824)\n",
      "-8: 4-2-16: 0.647 (0.745)\n",
      "-8: 3-3-16: 0.706 (0.784)\n",
      "-8: 4-3-16: 0.627 (0.706)\n",
      "-8: 2-2-32: 0.804 (0.843)\n",
      "-8: 2-3-32: 0.706 (0.765)\n",
      "-8: 2-4-32: 0.549 (0.686)\n",
      "-8: 3-2-32: 0.765 (0.804)\n",
      "-8: 4-2-32: 0.647 (0.765)\n",
      "-8: 3-3-32: 0.686 (0.804)\n",
      "-8: 4-3-32: 0.627 (0.686)\n",
      "-8: 2-2-50: 0.765 (0.863)\n",
      "-8: 2-3-50: 0.706 (0.784)\n",
      "-8: 2-4-50: 0.471 (0.706)\n",
      "-8: 3-2-50: 0.725 (0.843)\n",
      "-8: 4-2-50: 0.804 (0.824)\n",
      "-8: 3-3-50: 0.706 (0.745)\n",
      "-8: 4-3-50: 0.588 (0.667)\n",
      "-9: 2-2-4: 0.745 (0.804)\n",
      "-9: 2-3-4: 0.706 (0.765)\n",
      "-9: 2-4-4: 0.627 (0.647)\n",
      "-9: 3-2-4: 0.765 (0.824)\n",
      "-9: 4-2-4: 0.569 (0.647)\n",
      "-9: 3-3-4: 0.647 (0.686)\n",
      "-9: 4-3-4: 0.569 (0.608)\n",
      "-9: 2-2-8: 0.706 (0.804)\n",
      "-9: 2-3-8: 0.647 (0.725)\n",
      "-9: 2-4-8: 0.647 (0.667)\n",
      "-9: 3-2-8: 0.765 (0.784)\n",
      "-9: 4-2-8: 0.569 (0.627)\n",
      "-9: 3-3-8: 0.608 (0.667)\n",
      "-9: 4-3-8: 0.569 (0.588)\n",
      "-9: 2-2-16: 0.765 (0.804)\n",
      "-9: 2-3-16: 0.745 (0.745)\n",
      "-9: 2-4-16: 0.667 (0.667)\n",
      "-9: 3-2-16: 0.745 (0.784)\n",
      "-9: 4-2-16: 0.627 (0.686)\n",
      "-9: 3-3-16: 0.647 (0.667)\n",
      "-9: 4-3-16: 0.608 (0.608)\n",
      "-9: 2-2-32: 0.725 (0.804)\n",
      "-9: 2-3-32: 0.667 (0.725)\n",
      "-9: 2-4-32: 0.627 (0.647)\n",
      "-9: 3-2-32: 0.784 (0.824)\n",
      "-9: 4-2-32: 0.627 (0.667)\n",
      "-9: 3-3-32: 0.588 (0.588)\n",
      "-9: 4-3-32: 0.588 (0.588)\n",
      "-9: 2-2-50: 0.784 (0.824)\n",
      "-9: 2-3-50: 0.647 (0.706)\n",
      "-9: 2-4-50: 0.608 (0.627)\n",
      "-9: 3-2-50: 0.706 (0.824)\n",
      "-9: 4-2-50: 0.588 (0.667)\n",
      "-9: 3-3-50: 0.608 (0.686)\n",
      "-9: 4-3-50: 0.510 (0.588)\n",
      "-10: 2-2-4: 0.804 (0.843)\n",
      "-10: 2-3-4: 0.725 (0.784)\n",
      "-10: 2-4-4: 0.627 (0.725)\n",
      "-10: 3-2-4: 0.765 (0.843)\n",
      "-10: 4-2-4: 0.686 (0.843)\n",
      "-10: 3-3-4: 0.667 (0.706)\n",
      "-10: 4-3-4: 0.686 (0.706)\n",
      "-10: 2-2-8: 0.784 (0.863)\n",
      "-10: 2-3-8: 0.725 (0.745)\n",
      "-10: 2-4-8: 0.627 (0.725)\n",
      "-10: 3-2-8: 0.745 (0.804)\n",
      "-10: 4-2-8: 0.824 (0.843)\n",
      "-10: 3-3-8: 0.647 (0.765)\n",
      "-10: 4-3-8: 0.647 (0.667)\n",
      "-10: 2-2-16: 0.843 (0.863)\n",
      "-10: 2-3-16: 0.725 (0.843)\n",
      "-10: 2-4-16: 0.647 (0.667)\n",
      "-10: 3-2-16: 0.765 (0.824)\n",
      "-10: 4-2-16: 0.745 (0.843)\n",
      "-10: 3-3-16: 0.647 (0.725)\n",
      "-10: 4-3-16: 0.608 (0.667)\n",
      "-10: 2-2-32: 0.784 (0.882)\n",
      "-10: 2-3-32: 0.804 (0.824)\n",
      "-10: 2-4-32: 0.608 (0.667)\n",
      "-10: 3-2-32: 0.765 (0.843)\n",
      "-10: 4-2-32: 0.647 (0.706)\n",
      "-10: 3-3-32: 0.647 (0.765)\n",
      "-10: 4-3-32: 0.569 (0.627)\n",
      "-10: 2-2-50: 0.804 (0.863)\n",
      "-10: 2-3-50: 0.784 (0.784)\n",
      "-10: 2-4-50: 0.549 (0.667)\n",
      "-10: 3-2-50: 0.765 (0.824)\n",
      "-10: 4-2-50: 0.647 (0.784)\n",
      "-10: 3-3-50: 0.706 (0.784)\n",
      "-10: 4-3-50: 0.608 (0.667)\n",
      "-11: 2-2-4: 0.745 (0.804)\n",
      "-11: 2-3-4: 0.667 (0.706)\n",
      "-11: 2-4-4: 0.569 (0.706)\n",
      "-11: 3-2-4: 0.706 (0.784)\n",
      "-11: 4-2-4: 0.686 (0.765)\n",
      "-11: 3-3-4: 0.667 (0.725)\n",
      "-11: 4-3-4: 0.608 (0.725)\n",
      "-11: 2-2-8: 0.745 (0.784)\n",
      "-11: 2-3-8: 0.745 (0.745)\n",
      "-11: 2-4-8: 0.627 (0.706)\n",
      "-11: 3-2-8: 0.706 (0.784)\n",
      "-11: 4-2-8: 0.706 (0.765)\n",
      "-11: 3-3-8: 0.608 (0.725)\n",
      "-11: 4-3-8: 0.627 (0.686)\n",
      "-11: 2-2-16: 0.725 (0.784)\n",
      "-11: 2-3-16: 0.627 (0.765)\n",
      "-11: 2-4-16: 0.667 (0.725)\n",
      "-11: 3-2-16: 0.667 (0.745)\n",
      "-11: 4-2-16: 0.725 (0.824)\n",
      "-11: 3-3-16: 0.686 (0.745)\n",
      "-11: 4-3-16: 0.549 (0.569)\n",
      "-11: 2-2-32: 0.765 (0.804)\n",
      "-11: 2-3-32: 0.686 (0.745)\n",
      "-11: 2-4-32: 0.647 (0.667)\n",
      "-11: 3-2-32: 0.745 (0.784)\n",
      "-11: 4-2-32: 0.765 (0.784)\n",
      "-11: 3-3-32: 0.627 (0.706)\n",
      "-11: 4-3-32: 0.686 (0.686)\n",
      "-11: 2-2-50: 0.745 (0.784)\n",
      "-11: 2-3-50: 0.608 (0.706)\n",
      "-11: 2-4-50: 0.627 (0.725)\n",
      "-11: 3-2-50: 0.686 (0.765)\n",
      "-11: 4-2-50: 0.706 (0.784)\n",
      "-11: 3-3-50: 0.667 (0.706)\n",
      "-11: 4-3-50: 0.490 (0.608)\n",
      "-12: 2-2-4: 0.843 (0.902)\n",
      "-12: 2-3-4: 0.784 (0.804)\n",
      "-12: 2-4-4: 0.725 (0.804)\n",
      "-12: 3-2-4: 0.843 (0.902)\n",
      "-12: 4-2-4: 0.804 (0.863)\n",
      "-12: 3-3-4: 0.765 (0.804)\n",
      "-12: 4-3-4: 0.588 (0.706)\n",
      "-12: 2-2-8: 0.843 (0.902)\n",
      "-12: 2-3-8: 0.824 (0.843)\n",
      "-12: 2-4-8: 0.745 (0.784)\n",
      "-12: 3-2-8: 0.863 (0.882)\n",
      "-12: 4-2-8: 0.843 (0.882)\n",
      "-12: 3-3-8: 0.765 (0.843)\n",
      "-12: 4-3-8: 0.804 (0.843)\n",
      "-12: 2-2-16: 0.863 (0.902)\n",
      "-12: 2-3-16: 0.804 (0.843)\n",
      "-12: 2-4-16: 0.686 (0.765)\n",
      "-12: 3-2-16: 0.843 (0.882)\n",
      "-12: 4-2-16: 0.804 (0.843)\n",
      "-12: 3-3-16: 0.804 (0.843)\n",
      "-12: 4-3-16: 0.725 (0.784)\n",
      "-12: 2-2-32: 0.804 (0.902)\n",
      "-12: 2-3-32: 0.765 (0.843)\n",
      "-12: 2-4-32: 0.608 (0.784)\n",
      "-12: 3-2-32: 0.843 (0.863)\n",
      "-12: 4-2-32: 0.804 (0.863)\n",
      "-12: 3-3-32: 0.765 (0.824)\n",
      "-12: 4-3-32: 0.784 (0.804)\n",
      "-12: 2-2-50: 0.843 (0.922)\n",
      "-12: 2-3-50: 0.804 (0.843)\n",
      "-12: 2-4-50: 0.647 (0.667)\n",
      "-12: 3-2-50: 0.863 (0.902)\n",
      "-12: 4-2-50: 0.804 (0.882)\n",
      "-12: 3-3-50: 0.706 (0.804)\n",
      "-12: 4-3-50: 0.843 (0.843)\n",
      "-13: 2-2-4: 0.765 (0.784)\n",
      "-13: 2-3-4: 0.647 (0.725)\n",
      "-13: 2-4-4: 0.569 (0.627)\n",
      "-13: 3-2-4: 0.745 (0.765)\n",
      "-13: 4-2-4: 0.745 (0.745)\n",
      "-13: 3-3-4: 0.647 (0.706)\n",
      "-13: 4-3-4: 0.627 (0.686)\n",
      "-13: 2-2-8: 0.765 (0.804)\n",
      "-13: 2-3-8: 0.608 (0.706)\n",
      "-13: 2-4-8: 0.529 (0.608)\n",
      "-13: 3-2-8: 0.745 (0.784)\n",
      "-13: 4-2-8: 0.647 (0.745)\n",
      "-13: 3-3-8: 0.667 (0.765)\n",
      "-13: 4-3-8: 0.608 (0.706)\n",
      "-13: 2-2-16: 0.765 (0.804)\n",
      "-13: 2-3-16: 0.706 (0.725)\n",
      "-13: 2-4-16: 0.549 (0.647)\n",
      "-13: 3-2-16: 0.784 (0.804)\n",
      "-13: 4-2-16: 0.686 (0.706)\n",
      "-13: 3-3-16: 0.706 (0.804)\n",
      "-13: 4-3-16: 0.647 (0.686)\n",
      "-13: 2-2-32: 0.745 (0.804)\n",
      "-13: 2-3-32: 0.706 (0.725)\n",
      "-13: 2-4-32: 0.588 (0.647)\n",
      "-13: 3-2-32: 0.745 (0.804)\n",
      "-13: 4-2-32: 0.725 (0.804)\n",
      "-13: 3-3-32: 0.686 (0.745)\n",
      "-13: 4-3-32: 0.490 (0.569)\n",
      "-13: 2-2-50: 0.745 (0.824)\n",
      "-13: 2-3-50: 0.706 (0.765)\n",
      "-13: 2-4-50: 0.588 (0.608)\n",
      "-13: 3-2-50: 0.725 (0.804)\n",
      "-13: 4-2-50: 0.745 (0.804)\n",
      "-13: 3-3-50: 0.627 (0.647)\n",
      "-13: 4-3-50: 0.490 (0.588)\n",
      "-14: 2-2-4: 0.765 (0.824)\n",
      "-14: 2-3-4: 0.725 (0.784)\n",
      "-14: 2-4-4: 0.627 (0.706)\n",
      "-14: 3-2-4: 0.784 (0.824)\n",
      "-14: 4-2-4: 0.667 (0.784)\n",
      "-14: 3-3-4: 0.686 (0.745)\n",
      "-14: 4-3-4: 0.549 (0.667)\n",
      "-14: 2-2-8: 0.765 (0.804)\n",
      "-14: 2-3-8: 0.765 (0.824)\n",
      "-14: 2-4-8: 0.569 (0.686)\n",
      "-14: 3-2-8: 0.804 (0.824)\n",
      "-14: 4-2-8: 0.765 (0.824)\n",
      "-14: 3-3-8: 0.725 (0.745)\n",
      "-14: 4-3-8: 0.471 (0.510)\n",
      "-14: 2-2-16: 0.804 (0.843)\n",
      "-14: 2-3-16: 0.725 (0.804)\n",
      "-14: 2-4-16: 0.569 (0.686)\n",
      "-14: 3-2-16: 0.765 (0.824)\n",
      "-14: 4-2-16: 0.725 (0.765)\n",
      "-14: 3-3-16: 0.647 (0.765)\n",
      "-14: 4-3-16: 0.588 (0.627)\n",
      "-14: 2-2-32: 0.824 (0.863)\n",
      "-14: 2-3-32: 0.765 (0.824)\n",
      "-14: 2-4-32: 0.569 (0.686)\n",
      "-14: 3-2-32: 0.804 (0.843)\n",
      "-14: 4-2-32: 0.824 (0.843)\n",
      "-14: 3-3-32: 0.706 (0.765)\n",
      "-14: 4-3-32: 0.549 (0.647)\n",
      "-14: 2-2-50: 0.765 (0.843)\n",
      "-14: 2-3-50: 0.765 (0.824)\n",
      "-14: 2-4-50: 0.608 (0.667)\n",
      "-14: 3-2-50: 0.765 (0.824)\n",
      "-14: 4-2-50: 0.745 (0.784)\n",
      "-14: 3-3-50: 0.588 (0.706)\n",
      "-14: 4-3-50: 0.588 (0.627)\n",
      "-15: 2-2-4: 0.686 (0.765)\n",
      "-15: 2-3-4: 0.667 (0.745)\n",
      "-15: 2-4-4: 0.549 (0.627)\n",
      "-15: 3-2-4: 0.824 (0.843)\n",
      "-15: 4-2-4: 0.667 (0.725)\n",
      "-15: 3-3-4: 0.627 (0.686)\n",
      "-15: 4-3-4: 0.608 (0.706)\n",
      "-15: 2-2-8: 0.725 (0.784)\n",
      "-15: 2-3-8: 0.647 (0.745)\n",
      "-15: 2-4-8: 0.569 (0.647)\n",
      "-15: 3-2-8: 0.804 (0.824)\n",
      "-15: 4-2-8: 0.725 (0.824)\n",
      "-15: 3-3-8: 0.667 (0.706)\n",
      "-15: 4-3-8: 0.490 (0.569)\n",
      "-15: 2-2-16: 0.706 (0.765)\n",
      "-15: 2-3-16: 0.686 (0.725)\n",
      "-15: 2-4-16: 0.686 (0.725)\n",
      "-15: 3-2-16: 0.745 (0.804)\n",
      "-15: 4-2-16: 0.725 (0.765)\n",
      "-15: 3-3-16: 0.647 (0.706)\n",
      "-15: 4-3-16: 0.588 (0.627)\n",
      "-15: 2-2-32: 0.745 (0.824)\n",
      "-15: 2-3-32: 0.667 (0.745)\n",
      "-15: 2-4-32: 0.588 (0.725)\n",
      "-15: 3-2-32: 0.765 (0.784)\n",
      "-15: 4-2-32: 0.725 (0.745)\n",
      "-15: 3-3-32: 0.627 (0.745)\n",
      "-15: 4-3-32: 0.549 (0.627)\n",
      "-15: 2-2-50: 0.725 (0.784)\n",
      "-15: 2-3-50: 0.608 (0.745)\n",
      "-15: 2-4-50: 0.608 (0.647)\n",
      "-15: 3-2-50: 0.686 (0.804)\n",
      "-15: 4-2-50: 0.745 (0.804)\n",
      "-15: 3-3-50: 0.667 (0.725)\n",
      "-15: 4-3-50: 0.471 (0.549)\n",
      "-16: 2-2-4: 0.667 (0.745)\n",
      "-16: 2-3-4: 0.647 (0.686)\n",
      "-16: 2-4-4: 0.725 (0.745)\n",
      "-16: 3-2-4: 0.667 (0.706)\n",
      "-16: 4-2-4: 0.529 (0.608)\n",
      "-16: 3-3-4: 0.667 (0.745)\n",
      "-16: 4-3-4: 0.686 (0.765)\n",
      "-16: 2-2-8: 0.706 (0.745)\n",
      "-16: 2-3-8: 0.667 (0.725)\n",
      "-16: 2-4-8: 0.647 (0.725)\n",
      "-16: 3-2-8: 0.667 (0.765)\n",
      "-16: 4-2-8: 0.647 (0.784)\n",
      "-16: 3-3-8: 0.667 (0.745)\n",
      "-16: 4-3-8: 0.647 (0.686)\n",
      "-16: 2-2-16: 0.706 (0.765)\n",
      "-16: 2-3-16: 0.647 (0.745)\n",
      "-16: 2-4-16: 0.647 (0.725)\n",
      "-16: 3-2-16: 0.706 (0.745)\n",
      "-16: 4-2-16: 0.627 (0.706)\n",
      "-16: 3-3-16: 0.706 (0.745)\n",
      "-16: 4-3-16: 0.608 (0.627)\n",
      "-16: 2-2-32: 0.725 (0.784)\n",
      "-16: 2-3-32: 0.647 (0.745)\n",
      "-16: 2-4-32: 0.588 (0.706)\n",
      "-16: 3-2-32: 0.706 (0.745)\n",
      "-16: 4-2-32: 0.706 (0.765)\n",
      "-16: 3-3-32: 0.706 (0.745)\n",
      "-16: 4-3-32: 0.608 (0.647)\n",
      "-16: 2-2-50: 0.725 (0.765)\n",
      "-16: 2-3-50: 0.667 (0.745)\n",
      "-16: 2-4-50: 0.588 (0.745)\n",
      "-16: 3-2-50: 0.627 (0.745)\n",
      "-16: 4-2-50: 0.706 (0.745)\n",
      "-16: 3-3-50: 0.745 (0.784)\n",
      "-16: 4-3-50: 0.608 (0.686)\n",
      "-17: 2-2-4: 0.824 (0.863)\n",
      "-17: 2-3-4: 0.745 (0.824)\n",
      "-17: 2-4-4: 0.549 (0.686)\n",
      "-17: 3-2-4: 0.725 (0.804)\n",
      "-17: 4-2-4: 0.667 (0.725)\n",
      "-17: 3-3-4: 0.667 (0.765)\n",
      "-17: 4-3-4: 0.647 (0.686)\n",
      "-17: 2-2-8: 0.824 (0.824)\n",
      "-17: 2-3-8: 0.784 (0.804)\n",
      "-17: 2-4-8: 0.627 (0.667)\n",
      "-17: 3-2-8: 0.745 (0.843)\n",
      "-17: 4-2-8: 0.765 (0.804)\n",
      "-17: 3-3-8: 0.745 (0.765)\n",
      "-17: 4-3-8: 0.510 (0.647)\n",
      "-17: 2-2-16: 0.824 (0.843)\n",
      "-17: 2-3-16: 0.804 (0.824)\n",
      "-17: 2-4-16: 0.588 (0.686)\n",
      "-17: 3-2-16: 0.765 (0.824)\n",
      "-17: 4-2-16: 0.804 (0.843)\n",
      "-17: 3-3-16: 0.765 (0.824)\n",
      "-17: 4-3-16: 0.627 (0.627)\n",
      "-17: 2-2-32: 0.784 (0.863)\n",
      "-17: 2-3-32: 0.745 (0.804)\n",
      "-17: 2-4-32: 0.686 (0.725)\n",
      "-17: 3-2-32: 0.824 (0.843)\n",
      "-17: 4-2-32: 0.686 (0.824)\n",
      "-17: 3-3-32: 0.765 (0.824)\n",
      "-17: 4-3-32: 0.667 (0.686)\n",
      "-17: 2-2-50: 0.824 (0.863)\n",
      "-17: 2-3-50: 0.784 (0.824)\n",
      "-17: 2-4-50: 0.627 (0.686)\n",
      "-17: 3-2-50: 0.804 (0.843)\n",
      "-17: 4-2-50: 0.784 (0.843)\n",
      "-17: 3-3-50: 0.765 (0.765)\n",
      "-17: 4-3-50: 0.549 (0.647)\n",
      "-18: 2-2-4: 0.784 (0.863)\n",
      "-18: 2-3-4: 0.706 (0.804)\n",
      "-18: 2-4-4: 0.667 (0.725)\n",
      "-18: 3-2-4: 0.745 (0.824)\n",
      "-18: 4-2-4: 0.745 (0.784)\n",
      "-18: 3-3-4: 0.686 (0.706)\n",
      "-18: 4-3-4: 0.706 (0.706)\n",
      "-18: 2-2-8: 0.745 (0.863)\n",
      "-18: 2-3-8: 0.745 (0.804)\n",
      "-18: 2-4-8: 0.608 (0.608)\n",
      "-18: 3-2-8: 0.745 (0.843)\n",
      "-18: 4-2-8: 0.627 (0.725)\n",
      "-18: 3-3-8: 0.706 (0.765)\n",
      "-18: 4-3-8: 0.627 (0.667)\n",
      "-18: 2-2-16: 0.765 (0.843)\n",
      "-18: 2-3-16: 0.725 (0.765)\n",
      "-18: 2-4-16: 0.510 (0.686)\n",
      "-18: 3-2-16: 0.784 (0.843)\n",
      "-18: 4-2-16: 0.686 (0.824)\n",
      "-18: 3-3-16: 0.627 (0.725)\n",
      "-18: 4-3-16: 0.667 (0.667)\n",
      "-18: 2-2-32: 0.804 (0.863)\n",
      "-18: 2-3-32: 0.569 (0.745)\n",
      "-18: 2-4-32: 0.686 (0.706)\n",
      "-18: 3-2-32: 0.706 (0.824)\n",
      "-18: 4-2-32: 0.667 (0.765)\n",
      "-18: 3-3-32: 0.647 (0.784)\n",
      "-18: 4-3-32: 0.412 (0.569)\n",
      "-18: 2-2-50: 0.784 (0.863)\n",
      "-18: 2-3-50: 0.686 (0.784)\n",
      "-18: 2-4-50: 0.431 (0.529)\n",
      "-18: 3-2-50: 0.804 (0.843)\n",
      "-18: 4-2-50: 0.627 (0.706)\n",
      "-18: 3-3-50: 0.667 (0.686)\n",
      "-18: 4-3-50: 0.647 (0.725)\n",
      "-19: 2-2-4: 0.804 (0.843)\n",
      "-19: 2-3-4: 0.667 (0.686)\n",
      "-19: 2-4-4: 0.569 (0.627)\n",
      "-19: 3-2-4: 0.608 (0.627)\n",
      "-19: 4-2-4: 0.569 (0.647)\n",
      "-19: 3-3-4: 0.608 (0.667)\n",
      "-19: 4-3-4: 0.588 (0.647)\n",
      "-19: 2-2-8: 0.784 (0.784)\n",
      "-19: 2-3-8: 0.706 (0.725)\n",
      "-19: 2-4-8: 0.588 (0.608)\n",
      "-19: 3-2-8: 0.784 (0.804)\n",
      "-19: 4-2-8: 0.588 (0.667)\n",
      "-19: 3-3-8: 0.588 (0.686)\n",
      "-19: 4-3-8: 0.588 (0.608)\n",
      "-19: 2-2-16: 0.784 (0.784)\n",
      "-19: 2-3-16: 0.647 (0.725)\n",
      "-19: 2-4-16: 0.608 (0.647)\n",
      "-19: 3-2-16: 0.706 (0.784)\n",
      "-19: 4-2-16: 0.627 (0.647)\n",
      "-19: 3-3-16: 0.627 (0.686)\n",
      "-19: 4-3-16: 0.529 (0.588)\n",
      "-19: 2-2-32: 0.765 (0.804)\n",
      "-19: 2-3-32: 0.686 (0.706)\n",
      "-19: 2-4-32: 0.608 (0.667)\n",
      "-19: 3-2-32: 0.765 (0.804)\n",
      "-19: 4-2-32: 0.588 (0.627)\n",
      "-19: 3-3-32: 0.667 (0.725)\n",
      "-19: 4-3-32: 0.569 (0.608)\n",
      "-19: 2-2-50: 0.784 (0.804)\n",
      "-19: 2-3-50: 0.667 (0.706)\n",
      "-19: 2-4-50: 0.569 (0.627)\n",
      "-19: 3-2-50: 0.706 (0.824)\n",
      "-19: 4-2-50: 0.549 (0.627)\n",
      "-19: 3-3-50: 0.608 (0.686)\n",
      "-19: 4-3-50: 0.608 (0.608)\n",
      "-20: 2-2-4: 0.804 (0.843)\n",
      "-20: 2-3-4: 0.745 (0.765)\n",
      "-20: 2-4-4: 0.647 (0.725)\n",
      "-20: 3-2-4: 0.784 (0.843)\n",
      "-20: 4-2-4: 0.647 (0.706)\n",
      "-20: 3-3-4: 0.667 (0.706)\n",
      "-20: 4-3-4: 0.627 (0.686)\n",
      "-20: 2-2-8: 0.843 (0.863)\n",
      "-20: 2-3-8: 0.784 (0.784)\n",
      "-20: 2-4-8: 0.667 (0.745)\n",
      "-20: 3-2-8: 0.745 (0.882)\n",
      "-20: 4-2-8: 0.647 (0.765)\n",
      "-20: 3-3-8: 0.667 (0.765)\n",
      "-20: 4-3-8: 0.667 (0.667)\n",
      "-20: 2-2-16: 0.824 (0.863)\n",
      "-20: 2-3-16: 0.725 (0.804)\n",
      "-20: 2-4-16: 0.706 (0.725)\n",
      "-20: 3-2-16: 0.824 (0.843)\n",
      "-20: 4-2-16: 0.745 (0.843)\n",
      "-20: 3-3-16: 0.686 (0.784)\n",
      "-20: 4-3-16: 0.588 (0.627)\n",
      "-20: 2-2-32: 0.784 (0.843)\n",
      "-20: 2-3-32: 0.686 (0.765)\n",
      "-20: 2-4-32: 0.608 (0.686)\n",
      "-20: 3-2-32: 0.784 (0.824)\n",
      "-20: 4-2-32: 0.784 (0.863)\n",
      "-20: 3-3-32: 0.667 (0.745)\n",
      "-20: 4-3-32: 0.627 (0.647)\n",
      "-20: 2-2-50: 0.765 (0.882)\n",
      "-20: 2-3-50: 0.706 (0.784)\n",
      "-20: 2-4-50: 0.569 (0.706)\n",
      "-20: 3-2-50: 0.824 (0.863)\n",
      "-20: 4-2-50: 0.667 (0.784)\n",
      "-20: 3-3-50: 0.686 (0.745)\n",
      "-20: 4-3-50: 0.608 (0.627)\n",
      "----- 20.60 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'L': 2, 'K': 2, 'hid_dim': 4},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 4},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 4},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 4},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 4},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 4},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 4},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 8},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 8},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 8},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 8},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 8},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 8},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 8},\n",
    "    \n",
    "        {'L': 2, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 32},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 32},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 32},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 32},\n",
    "\n",
    "        {'L': 2, 'K': 2, 'hid_dim': 50},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 50},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 50},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 50},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 50},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 50},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 50},\n",
    "        ]\n",
    "\n",
    "best_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs3b = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, exp['hid_dim'], OUT_DIM, exp['L'], exp['K'], act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0, diff_layer=GFGCN_noh_Layer)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "\n",
    "        best_accs3b[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs3b[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}: {best_val_accs3[j,i]:.3f} ({best_accs3[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}' for exp in EXPS]\n",
    "table_over3b = summary_table(best_accs3b, index_name)\n",
    "table3b = summary_table(best_val_accs3b, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-4</th>\n",
       "      <td>0.743137</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.065002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-4</th>\n",
       "      <td>0.665686</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.071904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-4</th>\n",
       "      <td>0.626471</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.080243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-4</th>\n",
       "      <td>0.675490</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.063981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-4</th>\n",
       "      <td>0.675490</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.058979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-4</th>\n",
       "      <td>0.636275</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.068340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-4</th>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.598039</td>\n",
       "      <td>0.093626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-8</th>\n",
       "      <td>0.759804</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.050706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-8</th>\n",
       "      <td>0.717647</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.053483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-8</th>\n",
       "      <td>0.651961</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.063195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-8</th>\n",
       "      <td>0.743137</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.058791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-8</th>\n",
       "      <td>0.694118</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.052759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-8</th>\n",
       "      <td>0.662745</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.071669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-8</th>\n",
       "      <td>0.600980</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>0.084853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.040895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.709804</td>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.058364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-16</th>\n",
       "      <td>0.653922</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.062091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.750980</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.053375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-16</th>\n",
       "      <td>0.709804</td>\n",
       "      <td>0.725490</td>\n",
       "      <td>0.075840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-16</th>\n",
       "      <td>0.674510</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.052759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-16</th>\n",
       "      <td>0.614706</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.073699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-32</th>\n",
       "      <td>0.779412</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.047171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-32</th>\n",
       "      <td>0.707843</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.058463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-32</th>\n",
       "      <td>0.640196</td>\n",
       "      <td>0.627451</td>\n",
       "      <td>0.056583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-32</th>\n",
       "      <td>0.757843</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.051608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-32</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.696078</td>\n",
       "      <td>0.084814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-32</th>\n",
       "      <td>0.684314</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.073340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-32</th>\n",
       "      <td>0.606863</td>\n",
       "      <td>0.568627</td>\n",
       "      <td>0.090382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-2-50</th>\n",
       "      <td>0.780392</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.049059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-50</th>\n",
       "      <td>0.710784</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.056447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-50</th>\n",
       "      <td>0.632353</td>\n",
       "      <td>0.647059</td>\n",
       "      <td>0.055068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-50</th>\n",
       "      <td>0.749020</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.051729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-50</th>\n",
       "      <td>0.716667</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.067205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-50</th>\n",
       "      <td>0.673529</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>0.052348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-50</th>\n",
       "      <td>0.604902</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.101180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean accs       med       std\n",
       "2-2-4    0.743137  0.745098  0.065002\n",
       "2-3-4    0.665686  0.647059  0.071904\n",
       "2-4-4    0.626471  0.627451  0.080243\n",
       "3-2-4    0.675490  0.666667  0.063981\n",
       "4-2-4    0.675490  0.676471  0.058979\n",
       "3-3-4    0.636275  0.627451  0.068340\n",
       "4-3-4    0.588235  0.598039  0.093626\n",
       "2-2-8    0.759804  0.754902  0.050706\n",
       "2-3-8    0.717647  0.705882  0.053483\n",
       "2-4-8    0.651961  0.647059  0.063195\n",
       "3-2-8    0.743137  0.725490  0.058791\n",
       "4-2-8    0.694118  0.686275  0.052759\n",
       "3-3-8    0.662745  0.666667  0.071669\n",
       "4-3-8    0.600980  0.607843  0.084853\n",
       "2-2-16   0.774510  0.764706  0.040895\n",
       "2-3-16   0.709804  0.696078  0.058364\n",
       "2-4-16   0.653922  0.666667  0.062091\n",
       "3-2-16   0.750980  0.745098  0.053375\n",
       "4-2-16   0.709804  0.725490  0.075840\n",
       "3-3-16   0.674510  0.666667  0.052759\n",
       "4-3-16   0.614706  0.627451  0.073699\n",
       "2-2-32   0.779412  0.774510  0.047171\n",
       "2-3-32   0.707843  0.705882  0.058463\n",
       "2-4-32   0.640196  0.627451  0.056583\n",
       "3-2-32   0.757843  0.754902  0.051608\n",
       "4-2-32   0.700000  0.696078  0.084814\n",
       "3-3-32   0.684314  0.666667  0.073340\n",
       "4-3-32   0.606863  0.568627  0.090382\n",
       "2-2-50   0.780392  0.764706  0.049059\n",
       "2-3-50   0.710784  0.705882  0.056447\n",
       "2-4-50   0.632353  0.647059  0.055068\n",
       "3-2-50   0.749020  0.754902  0.051729\n",
       "4-2-50   0.716667  0.705882  0.067205\n",
       "3-3-50   0.673529  0.676471  0.052348\n",
       "4-3-50   0.604902  0.588235  0.101180"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_over3b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearities and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.745 (0.784)\n",
      "-1: ReLU()-Softmax(dim=1)-NLLLoss(): 0.745 (0.824)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.745 (0.784)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.725 (0.765)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.824)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.784 (0.804)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.784)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.745 (0.804)\n",
      "-1: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.804)\n",
      "-1: Identity()-Softmax(dim=1)-NLLLoss(): 0.765 (0.804)\n",
      "-1: Identity()-Identity()-CrossEntropyLoss(): 0.745 (0.804)\n",
      "-2: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.882 (0.882)\n",
      "-2: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.882)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.824 (0.902)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.843 (0.882)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-2: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.922)\n",
      "-2: Identity()-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-2: Identity()-Identity()-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-3: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.804)\n",
      "-3: ReLU()-Softmax(dim=1)-NLLLoss(): 0.725 (0.804)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.784 (0.824)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.784 (0.804)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.745 (0.784)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.765 (0.824)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.745 (0.824)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.784 (0.804)\n",
      "-3: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.784)\n",
      "-3: Identity()-Softmax(dim=1)-NLLLoss(): 0.745 (0.804)\n",
      "-3: Identity()-Identity()-CrossEntropyLoss(): 0.745 (0.765)\n",
      "-4: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.843)\n",
      "-4: ReLU()-Softmax(dim=1)-NLLLoss(): 0.804 (0.843)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.765 (0.824)\n",
      "-4: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.804 (0.843)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.824)\n",
      "-4: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.784 (0.843)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.863)\n",
      "-4: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.765 (0.863)\n",
      "-4: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.843)\n",
      "-4: Identity()-Softmax(dim=1)-NLLLoss(): 0.784 (0.843)\n",
      "-4: Identity()-Identity()-CrossEntropyLoss(): 0.765 (0.824)\n",
      "-5: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.725 (0.804)\n",
      "-5: ReLU()-Softmax(dim=1)-NLLLoss(): 0.706 (0.784)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.706 (0.745)\n",
      "-5: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.706 (0.804)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.745 (0.745)\n",
      "-5: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.706 (0.784)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.686 (0.784)\n",
      "-5: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.686 (0.765)\n",
      "-5: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.686 (0.765)\n",
      "-5: Identity()-Softmax(dim=1)-NLLLoss(): 0.706 (0.765)\n",
      "-5: Identity()-Identity()-CrossEntropyLoss(): 0.745 (0.765)\n",
      "-6: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.686 (0.745)\n",
      "-6: ReLU()-Softmax(dim=1)-NLLLoss(): 0.745 (0.784)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.667 (0.745)\n",
      "-6: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.686 (0.765)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.725 (0.765)\n",
      "-6: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.765 (0.765)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.667 (0.784)\n",
      "-6: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.706 (0.745)\n",
      "-6: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.784)\n",
      "-6: Identity()-Softmax(dim=1)-NLLLoss(): 0.667 (0.765)\n",
      "-6: Identity()-Identity()-CrossEntropyLoss(): 0.686 (0.765)\n",
      "-7: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.843)\n",
      "-7: ReLU()-Softmax(dim=1)-NLLLoss(): 0.824 (0.843)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.804 (0.843)\n",
      "-7: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.804 (0.843)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.843)\n",
      "-7: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.804 (0.843)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.843)\n",
      "-7: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.804 (0.843)\n",
      "-7: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.843)\n",
      "-7: Identity()-Softmax(dim=1)-NLLLoss(): 0.824 (0.863)\n",
      "-7: Identity()-Identity()-CrossEntropyLoss(): 0.804 (0.824)\n",
      "-8: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.745 (0.843)\n",
      "-8: ReLU()-Softmax(dim=1)-NLLLoss(): 0.843 (0.843)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.745 (0.824)\n",
      "-8: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.745 (0.824)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.863)\n",
      "-8: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.765 (0.863)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.824)\n",
      "-8: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.765 (0.843)\n",
      "-8: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.863)\n",
      "-8: Identity()-Softmax(dim=1)-NLLLoss(): 0.725 (0.863)\n",
      "-8: Identity()-Identity()-CrossEntropyLoss(): 0.784 (0.824)\n",
      "-9: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.745 (0.784)\n",
      "-9: ReLU()-Softmax(dim=1)-NLLLoss(): 0.725 (0.804)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.765 (0.784)\n",
      "-9: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.765 (0.804)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.784)\n",
      "-9: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.765 (0.804)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.804)\n",
      "-9: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.765 (0.804)\n",
      "-9: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.804)\n",
      "-9: Identity()-Softmax(dim=1)-NLLLoss(): 0.686 (0.784)\n",
      "-9: Identity()-Identity()-CrossEntropyLoss(): 0.784 (0.804)\n",
      "-10: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.843)\n",
      "-10: ReLU()-Softmax(dim=1)-NLLLoss(): 0.804 (0.843)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-10: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.804 (0.843)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.863)\n",
      "-10: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.765 (0.882)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.882)\n",
      "-10: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.824 (0.863)\n",
      "-10: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-10: Identity()-Softmax(dim=1)-NLLLoss(): 0.765 (0.882)\n",
      "-10: Identity()-Identity()-CrossEntropyLoss(): 0.765 (0.843)\n",
      "-11: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.725 (0.784)\n",
      "-11: ReLU()-Softmax(dim=1)-NLLLoss(): 0.765 (0.804)\n",
      "-11: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.765 (0.784)\n",
      "-11: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.686 (0.784)\n",
      "-11: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.745 (0.804)\n",
      "-11: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.725 (0.784)\n",
      "-11: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.784)\n",
      "-11: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.745 (0.784)\n",
      "-11: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.804)\n",
      "-11: Identity()-Softmax(dim=1)-NLLLoss(): 0.745 (0.804)\n",
      "-11: Identity()-Identity()-CrossEntropyLoss(): 0.784 (0.784)\n",
      "-12: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.863 (0.902)\n",
      "-12: ReLU()-Softmax(dim=1)-NLLLoss(): 0.863 (0.902)\n",
      "-12: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.902 (0.902)\n",
      "-12: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.804 (0.882)\n",
      "-12: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.902)\n",
      "-12: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-12: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.902)\n",
      "-12: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.902)\n",
      "-12: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.922)\n",
      "-12: Identity()-Softmax(dim=1)-NLLLoss(): 0.863 (0.922)\n",
      "-12: Identity()-Identity()-CrossEntropyLoss(): 0.902 (0.902)\n",
      "-13: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.804)\n",
      "-13: ReLU()-Softmax(dim=1)-NLLLoss(): 0.784 (0.824)\n",
      "-13: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.745 (0.804)\n",
      "-13: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.745 (0.784)\n",
      "-13: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.804)\n",
      "-13: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.765 (0.804)\n",
      "-13: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.745 (0.804)\n",
      "-13: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.824 (0.843)\n",
      "-13: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.745 (0.804)\n",
      "-13: Identity()-Softmax(dim=1)-NLLLoss(): 0.784 (0.824)\n",
      "-13: Identity()-Identity()-CrossEntropyLoss(): 0.784 (0.784)\n",
      "-14: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.843)\n",
      "-14: ReLU()-Softmax(dim=1)-NLLLoss(): 0.765 (0.843)\n",
      "-14: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.745 (0.824)\n",
      "-14: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.765 (0.843)\n",
      "-14: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.863)\n",
      "-14: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.765 (0.843)\n",
      "-14: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.843)\n",
      "-14: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.745 (0.843)\n",
      "-14: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.824)\n",
      "-14: Identity()-Softmax(dim=1)-NLLLoss(): 0.804 (0.882)\n",
      "-14: Identity()-Identity()-CrossEntropyLoss(): 0.804 (0.824)\n",
      "-15: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.706 (0.784)\n",
      "-15: ReLU()-Softmax(dim=1)-NLLLoss(): 0.686 (0.784)\n",
      "-15: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.725 (0.784)\n",
      "-15: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.725 (0.784)\n",
      "-15: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.706 (0.804)\n",
      "-15: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.725 (0.765)\n",
      "-15: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.706 (0.765)\n",
      "-15: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.725 (0.765)\n",
      "-15: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.784)\n",
      "-15: Identity()-Softmax(dim=1)-NLLLoss(): 0.745 (0.784)\n",
      "-15: Identity()-Identity()-CrossEntropyLoss(): 0.686 (0.745)\n",
      "-16: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.725 (0.745)\n",
      "-16: ReLU()-Softmax(dim=1)-NLLLoss(): 0.686 (0.765)\n",
      "-16: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.667 (0.725)\n",
      "-16: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.745 (0.765)\n",
      "-16: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.725 (0.765)\n",
      "-16: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.706 (0.765)\n",
      "-16: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.745 (0.765)\n",
      "-16: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.725 (0.765)\n",
      "-16: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.706 (0.745)\n",
      "-16: Identity()-Softmax(dim=1)-NLLLoss(): 0.765 (0.784)\n",
      "-16: Identity()-Identity()-CrossEntropyLoss(): 0.706 (0.784)\n",
      "-17: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.804 (0.843)\n",
      "-17: ReLU()-Softmax(dim=1)-NLLLoss(): 0.804 (0.843)\n",
      "-17: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.824 (0.843)\n",
      "-17: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.824 (0.843)\n",
      "-17: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.863)\n",
      "-17: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.824 (0.843)\n",
      "-17: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.843)\n",
      "-17: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.843 (0.843)\n",
      "-17: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.863)\n",
      "-17: Identity()-Softmax(dim=1)-NLLLoss(): 0.824 (0.843)\n",
      "-17: Identity()-Identity()-CrossEntropyLoss(): 0.804 (0.824)\n",
      "-18: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.843)\n",
      "-18: ReLU()-Softmax(dim=1)-NLLLoss(): 0.824 (0.863)\n",
      "-18: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.765 (0.824)\n",
      "-18: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.745 (0.863)\n",
      "-18: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.863)\n",
      "-18: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.784 (0.863)\n",
      "-18: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.824)\n",
      "-18: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.745 (0.843)\n",
      "-18: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.863)\n",
      "-18: Identity()-Softmax(dim=1)-NLLLoss(): 0.804 (0.863)\n",
      "-18: Identity()-Identity()-CrossEntropyLoss(): 0.765 (0.824)\n",
      "-19: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.765 (0.784)\n",
      "-19: ReLU()-Softmax(dim=1)-NLLLoss(): 0.765 (0.804)\n",
      "-19: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.725 (0.804)\n",
      "-19: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.765 (0.784)\n",
      "-19: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.745 (0.804)\n",
      "-19: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.765 (0.784)\n",
      "-19: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.804)\n",
      "-19: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.765 (0.804)\n",
      "-19: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.725 (0.804)\n",
      "-19: Identity()-Softmax(dim=1)-NLLLoss(): 0.745 (0.784)\n",
      "-19: Identity()-Identity()-CrossEntropyLoss(): 0.765 (0.765)\n",
      "-20: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.824 (0.863)\n",
      "-20: ReLU()-Softmax(dim=1)-NLLLoss(): 0.824 (0.882)\n",
      "-20: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.824 (0.863)\n",
      "-20: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.863 (0.882)\n",
      "-20: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.843 (0.882)\n",
      "-20: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.804 (0.882)\n",
      "-20: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.863)\n",
      "-20: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.765 (0.824)\n",
      "-20: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.784 (0.863)\n",
      "-20: Identity()-Softmax(dim=1)-NLLLoss(): 0.843 (0.882)\n",
      "-20: Identity()-Identity()-CrossEntropyLoss(): 0.824 (0.843)\n",
      "----- 3.90 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        # {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},]\n",
    "\n",
    "best_accs4 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs4 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=exp['act'], last_act=exp['lact'],\n",
    "                     dropout=DROPOUT, init_h0=h0, diff_layer=GFGCN_noh_Layer)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = NodeClassModel(arch, S, masks, exp['loss'], device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "\n",
    "        best_accs4[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs4[j,i] = model.test(feat, S, labels, masks['test'])\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}: {best_val_accs4[j,i]:.3f} ({best_accs4[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}' for exp in EXPS]\n",
    "table_over4 = summary_table(best_accs4, index_name)\n",
    "table4 = summary_table(best_val_accs4, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.769608</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.047979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.777451</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.053438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.056795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.047989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.777451</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.040789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.773529</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.039932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.769608</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.046349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.771569</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.046926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.779412</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.044226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.772549</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.053483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.775490</td>\n",
       "      <td>0.774510</td>\n",
       "      <td>0.051309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean accs       med  \\\n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()             0.769608  0.764706   \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                      0.777451  0.774510   \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()          0.766667  0.764706   \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                   0.766667  0.764706   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()     0.777451  0.774510   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()              0.773529  0.764706   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...   0.769608  0.764706   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...   0.771569  0.764706   \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()         0.779412  0.774510   \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                  0.772549  0.764706   \n",
       "Identity()-Identity()-CrossEntropyLoss()             0.775490  0.774510   \n",
       "\n",
       "                                                         std  \n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()            0.047979  \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                     0.053438  \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()         0.056795  \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                  0.047989  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()    0.040789  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()             0.039932  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...  0.046349  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...  0.046926  \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()        0.044226  \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                 0.053483  \n",
       "Identity()-Identity()-CrossEntropyLoss()            0.051309  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.818627</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.041537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.828431</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.036089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.815686</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.047788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.819608</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.039509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.827451</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.045395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.828431</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.046349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.824510</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>0.041351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.822549</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.043171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.828431</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.047577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.832353</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.047008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.809804</td>\n",
       "      <td>0.813725</td>\n",
       "      <td>0.041641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean accs       med  \\\n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()             0.818627  0.823529   \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                      0.828431  0.833333   \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()          0.815686  0.823529   \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                   0.819608  0.813725   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()     0.827451  0.823529   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()              0.828431  0.833333   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...   0.824510  0.823529   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...   0.822549  0.833333   \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()         0.828431  0.813725   \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                  0.832353  0.833333   \n",
       "Identity()-Identity()-CrossEntropyLoss()             0.809804  0.813725   \n",
       "\n",
       "                                                         std  \n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()            0.041537  \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                     0.036089  \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()         0.047788  \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                  0.039509  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()    0.045395  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()             0.046349  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...  0.041351  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...  0.043171  \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()        0.047577  \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                 0.047008  \n",
       "Identity()-Identity()-CrossEntropyLoss()            0.041641  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_over4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the GSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_RUNS = 3\n",
    "NORM = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training paramerters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:43: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(1/S.sum(1))\n",
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:51: RuntimeWarning: invalid value encountered in matmul\n",
      "  return D_inv_sqr @ S @ D_inv_sqr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 200-0.5-0.0005-0: 0.020 (0.020)\n",
      "-1: 200-0.1-0.0005-0: 0.020 (0.020)\n",
      "-2: 200-0.5-0.0005-0: 0.039 (0.039)\n",
      "-2: 200-0.1-0.0005-0: 0.039 (0.039)\n",
      "-3: 200-0.5-0.0005-0: 0.039 (0.039)\n",
      "-3: 200-0.1-0.0005-0: 0.039 (0.039)\n",
      "----- 0.04 mins -----\n"
     ]
    }
   ],
   "source": [
    "EXPS = [\n",
    "        {'epochs': 200, 'lr': .5, 'wd': 5e-4, 'drop': 0},\n",
    "        {'epochs': 200, 'lr': .1, 'wd': 5e-4, 'drop': 0},\n",
    "        # {'epochs': 200, 'lr': .05, 'wd': 5e-4, 'drop': 0},\n",
    "        # {'epochs': 200, 'lr': .01, 'wd': 5e-4, 'drop': 0},\n",
    "        # {'epochs': 200, 'lr': .001, 'wd': 5e-4, 'drop': 0},\n",
    "        \n",
    "        # {'epochs': 200, 'lr': .05, 'wd': 1e-3, 'drop': 0},\n",
    "        # {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': 0},\n",
    "        # {'epochs': 200, 'lr': .05, 'wd': 5e-2, 'drop': 0},\n",
    "\n",
    "        # {'epochs': 200, 'lr': .1, 'wd': 1e-3, 'drop': 0},\n",
    "        # {'epochs': 200, 'lr': .1, 'wd': 1e-2, 'drop': 0},\n",
    "        # {'epochs': 200, 'lr': .1, 'wd': 5e-2, 'drop': 0},\n",
    "        \n",
    "        # {'epochs': 500, 'lr': .1, 'wd': 1e-2, 'drop': 0},\n",
    "        # {'epochs': 500, 'lr': .05, 'wd': 1e-2, 'drop': 0},\n",
    "        # {'epochs': 500, 'lr': .01, 'wd': 1e-2, 'drop': 0},\n",
    "\n",
    "        # {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': .25},\n",
    "        # {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': .5},\n",
    "        # {'epochs': 200, 'lr': .05, 'wd': 1e-2, 'drop': .75},\n",
    "        # {'epochs': 500, 'lr': .05, 'wd': 1e-2, 'drop': .25},\n",
    "        # {'epochs': 500, 'lr': .05, 'wd': 1e-2, 'drop': .5},\n",
    "        # {'epochs': 750, 'lr': .05, 'wd': 1e-2, 'drop': .5},\n",
    "        # {'epochs': 750, 'lr': .05, 'wd': 1e-2, 'drop': .75},\n",
    "        # {'epochs': 500, 'lr': .05, 'wd': 1e-2, 'drop': .75},\n",
    "        ]\n",
    "\n",
    "best_accs5 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs5 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=exp['drop'], init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, exp['epochs'], exp['lr'], exp['wd'])\n",
    "\n",
    "        best_accs5[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs5[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}: {best_val_accs5[j,i]:.3f} ({best_accs5[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"epochs\"]}-{exp[\"lr\"]}-{exp[\"wd\"]}-{exp[\"drop\"]}' for exp in EXPS]\n",
    "table_over5 = summary_table(best_accs5, index_name)\n",
    "table5 = summary_table(best_val_accs5, index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200-0.5-0.0005-0</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200-0.1-0.0005-0</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  mean accs       med       std\n",
       "200-0.5-0.0005-0    0.03268  0.039216  0.009243\n",
       "200-0.1-0.0005-0    0.03268  0.039216  0.009243"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:43: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(1/S.sum(1))\n",
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:51: RuntimeWarning: invalid value encountered in matmul\n",
      "  return D_inv_sqr @ S @ D_inv_sqr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: 2-2-16: 0.020 (0.020)\n",
      "-1: 2-3-16: 0.020 (0.020)\n",
      "-1: 2-4-16: 0.020 (0.020)\n",
      "-1: 3-2-16: 0.020 (0.020)\n",
      "-1: 4-2-16: 0.020 (0.020)\n",
      "-1: 3-3-16: 0.020 (0.020)\n",
      "-1: 4-3-16: 0.020 (0.020)\n",
      "-2: 2-2-16: 0.039 (0.039)\n",
      "-2: 2-3-16: 0.039 (0.039)\n",
      "-2: 2-4-16: 0.039 (0.039)\n",
      "-2: 3-2-16: 0.039 (0.039)\n",
      "-2: 4-2-16: 0.039 (0.039)\n",
      "-2: 3-3-16: 0.039 (0.039)\n",
      "-2: 4-3-16: 0.039 (0.039)\n",
      "-3: 2-2-16: 0.039 (0.039)\n",
      "-3: 2-3-16: 0.039 (0.039)\n",
      "-3: 2-4-16: 0.039 (0.039)\n",
      "-3: 3-2-16: 0.039 (0.039)\n",
      "-3: 4-2-16: 0.039 (0.039)\n",
      "-3: 3-3-16: 0.039 (0.039)\n",
      "-3: 4-3-16: 0.039 (0.039)\n",
      "----- 0.18 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [\n",
    "    # {'L': 2, 'K': 2, 'hid_dim': 4},\n",
    "        # {'L': 2, 'K': 3, 'hid_dim': 4},\n",
    "        # {'L': 2, 'K': 4, 'hid_dim': 4},\n",
    "\n",
    "        # {'L': 3, 'K': 2, 'hid_dim': 4},\n",
    "        # {'L': 4, 'K': 2, 'hid_dim': 4},\n",
    "        # {'L': 3, 'K': 3, 'hid_dim': 4},\n",
    "        # {'L': 4, 'K': 3, 'hid_dim': 4},\n",
    "\n",
    "        # {'L': 2, 'K': 2, 'hid_dim': 8},\n",
    "        # {'L': 2, 'K': 3, 'hid_dim': 8},\n",
    "        # {'L': 2, 'K': 4, 'hid_dim': 8},\n",
    "\n",
    "        # {'L': 3, 'K': 2, 'hid_dim': 8},\n",
    "        # {'L': 4, 'K': 2, 'hid_dim': 8},\n",
    "        # {'L': 3, 'K': 3, 'hid_dim': 8},\n",
    "        # {'L': 4, 'K': 3, 'hid_dim': 8},\n",
    "    \n",
    "        {'L': 2, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 2, 'K': 4, 'hid_dim': 16},\n",
    "\n",
    "        {'L': 3, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 2, 'hid_dim': 16},\n",
    "        {'L': 3, 'K': 3, 'hid_dim': 16},\n",
    "        {'L': 4, 'K': 3, 'hid_dim': 16},\n",
    "\n",
    "        # {'L': 2, 'K': 2, 'hid_dim': 32},\n",
    "        # {'L': 2, 'K': 3, 'hid_dim': 32},\n",
    "        # {'L': 2, 'K': 4, 'hid_dim': 32},\n",
    "\n",
    "        # {'L': 3, 'K': 2, 'hid_dim': 32},\n",
    "        # {'L': 4, 'K': 2, 'hid_dim': 32},\n",
    "        # {'L': 3, 'K': 3, 'hid_dim': 32},\n",
    "        # {'L': 4, 'K': 3, 'hid_dim': 32},\n",
    "        ]\n",
    "\n",
    "best_accs7 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs7 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, exp['hid_dim'], OUT_DIM, exp['L'], exp['K'], act=ACT, last_act=LAST_ACT,\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "\n",
    "        best_accs7[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs7[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}: {best_val_accs7[j,i]:.3f} ({best_accs7[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"L\"]}-{exp[\"K\"]}-{exp[\"hid_dim\"]}' for exp in EXPS]\n",
    "table_over7 = summary_table(best_accs7, index_name)\n",
    "table7 = summary_table(best_val_accs7, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2-2-16</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-3-16</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2-4-16</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-2-16</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-2-16</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3-3-16</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4-3-16</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean accs       med       std\n",
       "2-2-16    0.03268  0.039216  0.009243\n",
       "2-3-16    0.03268  0.039216  0.009243\n",
       "2-4-16    0.03268  0.039216  0.009243\n",
       "3-2-16    0.03268  0.039216  0.009243\n",
       "4-2-16    0.03268  0.039216  0.009243\n",
       "3-3-16    0.03268  0.039216  0.009243\n",
       "4-3-16    0.03268  0.039216  0.009243"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinearities and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.020 (0.020)\n",
      "-1: ReLU()-Softmax(dim=1)-NLLLoss(): 0.020 (0.020)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.020 (0.020)\n",
      "-1: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.020 (0.020)\n",
      "-1: ReLU()-Identity()-CrossEntropyLoss(): 0.020 (0.020)\n",
      "-1: ReLU()-Identity()-NLLLoss(): 0.020 (0.020)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.020 (0.020)\n",
      "-1: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.020 (0.020)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.020 (0.020)\n",
      "-1: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.020 (0.020)\n",
      "-1: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.020 (0.020)\n",
      "-1: Identity()-Softmax(dim=1)-NLLLoss(): 0.020 (0.020)\n",
      "-1: Identity()-Identity()-CrossEntropyLoss(): 0.020 (0.020)\n",
      "-2: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.039 (0.039)\n",
      "-2: ReLU()-Softmax(dim=1)-NLLLoss(): 0.039 (0.039)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.039 (0.039)\n",
      "-2: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.039 (0.039)\n",
      "-2: ReLU()-Identity()-CrossEntropyLoss(): 0.039 (0.039)\n",
      "-2: ReLU()-Identity()-NLLLoss(): 0.039 (0.039)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.039 (0.039)\n",
      "-2: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.039 (0.039)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.039 (0.039)\n",
      "-2: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.039 (0.039)\n",
      "-2: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.039 (0.039)\n",
      "-2: Identity()-Softmax(dim=1)-NLLLoss(): 0.039 (0.039)\n",
      "-2: Identity()-Identity()-CrossEntropyLoss(): 0.039 (0.039)\n",
      "-3: ReLU()-Softmax(dim=1)-CrossEntropyLoss(): 0.039 (0.039)\n",
      "-3: ReLU()-Softmax(dim=1)-NLLLoss(): 0.039 (0.039)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss(): 0.039 (0.039)\n",
      "-3: ReLU()-LogSoftmax(dim=1)-NLLLoss(): 0.039 (0.039)\n",
      "-3: ReLU()-Identity()-CrossEntropyLoss(): 0.039 (0.039)\n",
      "-3: ReLU()-Identity()-NLLLoss(): 0.039 (0.039)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss(): 0.039 (0.039)\n",
      "-3: ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss(): 0.039 (0.039)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss(): 0.039 (0.039)\n",
      "-3: LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss(): 0.039 (0.039)\n",
      "-3: Identity()-Softmax(dim=1)-CrossEntropyLoss(): 0.039 (0.039)\n",
      "-3: Identity()-Softmax(dim=1)-NLLLoss(): 0.039 (0.039)\n",
      "-3: Identity()-Identity()-CrossEntropyLoss(): 0.039 (0.039)\n",
      "----- 0.23 mins -----\n"
     ]
    }
   ],
   "source": [
    "# layers, filter order, weightd\n",
    "EXPS = [{'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.LogSoftmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ReLU(), 'lact': nn.Identity(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.ELU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.LeakyReLU(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.CrossEntropyLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Softmax(dim=1), 'loss': nn.NLLLoss()},\n",
    "        {'act': nn.Identity(), 'lact': nn.Identity(dim=1), 'loss': nn.CrossEntropyLoss()},]\n",
    "\n",
    "best_accs8 = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs8 = np.zeros((len(EXPS), N_RUNS))\n",
    "t_i = time.time()\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i)\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=exp['act'], last_act=exp['lact'],\n",
    "                     dropout=DROPOUT, init_h0=h0)\n",
    "        if NORM:\n",
    "            S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "        else:\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        model = NodeClassModel(arch, S, masks, exp['loss'], device=device)\n",
    "        _, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "\n",
    "        best_accs8[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs8[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'-{i+1}: {exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}: {best_val_accs8[j,i]:.3f} ({best_accs8[j,i]:.3f})')\n",
    "\n",
    "ellapsed_t = (time.time()-t_i)/60\n",
    "print(f'----- {ellapsed_t:.2f} mins -----')\n",
    "\n",
    "# Get table with results\n",
    "index_name = [f'{exp[\"act\"]}-{exp[\"lact\"]}-{exp[\"loss\"]}' for exp in EXPS]\n",
    "table_over8 = summary_table(best_accs8, index_name)\n",
    "table8 = summary_table(best_val_accs8, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-LogSoftmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ReLU()-Identity()-NLLLoss()</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-CrossEntropyLoss()</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Softmax(dim=1)-NLLLoss()</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Identity()-Identity()-CrossEntropyLoss()</th>\n",
       "      <td>0.03268</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.009243</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    mean accs       med  \\\n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()              0.03268  0.039216   \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                       0.03268  0.039216   \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()           0.03268  0.039216   \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                    0.03268  0.039216   \n",
       "ReLU()-Identity()-CrossEntropyLoss()                  0.03268  0.039216   \n",
       "ReLU()-Identity()-NLLLoss()                           0.03268  0.039216   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()      0.03268  0.039216   \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()               0.03268  0.039216   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...    0.03268  0.039216   \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...    0.03268  0.039216   \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()          0.03268  0.039216   \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                   0.03268  0.039216   \n",
       "Identity()-Identity()-CrossEntropyLoss()              0.03268  0.039216   \n",
       "\n",
       "                                                         std  \n",
       "ReLU()-Softmax(dim=1)-CrossEntropyLoss()            0.009243  \n",
       "ReLU()-Softmax(dim=1)-NLLLoss()                     0.009243  \n",
       "ReLU()-LogSoftmax(dim=1)-CrossEntropyLoss()         0.009243  \n",
       "ReLU()-LogSoftmax(dim=1)-NLLLoss()                  0.009243  \n",
       "ReLU()-Identity()-CrossEntropyLoss()                0.009243  \n",
       "ReLU()-Identity()-NLLLoss()                         0.009243  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-CrossEntropyLoss()    0.009243  \n",
       "ELU(alpha=1.0)-Softmax(dim=1)-NLLLoss()             0.009243  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-C...  0.009243  \n",
       "LeakyReLU(negative_slope=0.01)-Softmax(dim=1)-N...  0.009243  \n",
       "Identity()-Softmax(dim=1)-CrossEntropyLoss()        0.009243  \n",
       "Identity()-Softmax(dim=1)-NLLLoss()                 0.009243  \n",
       "Identity()-Identity()-CrossEntropyLoss()            0.009243  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPS = [\n",
    "        {'name': 'Kipf', 'norm': 'none'},\n",
    "        {'name': 'Kipf', 'norm': 'both'},\n",
    "\n",
    "        {'name': 'A-GCNN', 'norm': False},\n",
    "        {'name': 'A-GCNN', 'norm': True},\n",
    "\n",
    "        {'name': 'H-GCNN', 'norm': False}, # This should be the same as A-GCNN not norm\n",
    "        {'name': 'H-GCNN', 'norm': True},\n",
    "\n",
    "        {'name': 'W-GCN-A', 'norm': False},\n",
    "        {'name': 'W-GCN-A', 'norm': True},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- RUN: 1\n",
      "\tKipf-none: acc = 0.529  -  acc (over) = 0.667\n",
      "\tKipf-both: acc = 0.588  -  acc (over) = 0.627\n",
      "\tA-GCNN-False: acc = 0.784  -  acc (over) = 0.824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:43: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  D_inv = np.diag(1/S.sum(1))\n",
      "/home/srey/Investigacion/robust_minmax_gnn/gsp_utils/data.py:51: RuntimeWarning: invalid value encountered in matmul\n",
      "  return D_inv_sqr @ S @ D_inv_sqr\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tA-GCNN-True: acc = 0.020  -  acc (over) = 0.020\n",
      "\tH-GCNN-False: acc = 0.804  -  acc (over) = 0.843\n",
      "\tH-GCNN-True: acc = 0.804  -  acc (over) = 0.843\n",
      "\tW-GCN-A-False: acc = 0.765  -  acc (over) = 0.784\n",
      "\tW-GCN-A-True: acc = 0.020  -  acc (over) = 0.020\n",
      "- RUN: 2\n",
      "\tKipf-none: acc = 0.686  -  acc (over) = 0.686\n",
      "\tKipf-both: acc = 0.608  -  acc (over) = 0.686\n",
      "\tA-GCNN-False: acc = 0.843  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.039  -  acc (over) = 0.039\n",
      "\tH-GCNN-False: acc = 0.882  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.863  -  acc (over) = 0.922\n",
      "\tW-GCN-A-False: acc = 0.843  -  acc (over) = 0.902\n",
      "\tW-GCN-A-True: acc = 0.039  -  acc (over) = 0.039\n",
      "- RUN: 3\n",
      "\tKipf-none: acc = 0.431  -  acc (over) = 0.431\n",
      "\tKipf-both: acc = 0.608  -  acc (over) = 0.627\n",
      "\tA-GCNN-False: acc = 0.863  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.039  -  acc (over) = 0.039\n",
      "\tH-GCNN-False: acc = 0.882  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.863  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.765  -  acc (over) = 0.804\n",
      "\tW-GCN-A-True: acc = 0.039  -  acc (over) = 0.039\n",
      "- RUN: 4\n",
      "\tKipf-none: acc = 0.549  -  acc (over) = 0.588\n",
      "\tKipf-both: acc = 0.549  -  acc (over) = 0.608\n",
      "\tA-GCNN-False: acc = 0.922  -  acc (over) = 0.922\n",
      "\tA-GCNN-True: acc = 0.039  -  acc (over) = 0.039\n",
      "\tH-GCNN-False: acc = 0.902  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.902  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.784  -  acc (over) = 0.863\n",
      "\tW-GCN-A-True: acc = 0.039  -  acc (over) = 0.039\n",
      "- RUN: 5\n",
      "\tKipf-none: acc = 0.392  -  acc (over) = 0.392\n",
      "\tKipf-both: acc = 0.529  -  acc (over) = 0.667\n",
      "\tA-GCNN-False: acc = 0.863  -  acc (over) = 0.863\n",
      "\tA-GCNN-True: acc = 0.020  -  acc (over) = 0.020\n",
      "\tH-GCNN-False: acc = 0.824  -  acc (over) = 0.882\n",
      "\tH-GCNN-True: acc = 0.843  -  acc (over) = 0.863\n",
      "\tW-GCN-A-False: acc = 0.706  -  acc (over) = 0.765\n",
      "\tW-GCN-A-True: acc = 0.020  -  acc (over) = 0.020\n",
      "- RUN: 6\n",
      "\tKipf-none: acc = 0.608  -  acc (over) = 0.647\n",
      "\tKipf-both: acc = 0.588  -  acc (over) = 0.627\n",
      "\tA-GCNN-False: acc = 0.843  -  acc (over) = 0.882\n",
      "\tA-GCNN-True: acc = 0.078  -  acc (over) = 0.078\n",
      "\tH-GCNN-False: acc = 0.882  -  acc (over) = 0.882\n",
      "\tH-GCNN-True: acc = 0.824  -  acc (over) = 0.843\n",
      "\tW-GCN-A-False: acc = 0.745  -  acc (over) = 0.765\n",
      "\tW-GCN-A-True: acc = 0.078  -  acc (over) = 0.078\n",
      "- RUN: 7\n",
      "\tKipf-none: acc = 0.627  -  acc (over) = 0.667\n",
      "\tKipf-both: acc = 0.608  -  acc (over) = 0.686\n",
      "\tA-GCNN-False: acc = 0.843  -  acc (over) = 0.882\n",
      "\tA-GCNN-True: acc = 0.039  -  acc (over) = 0.039\n",
      "\tH-GCNN-False: acc = 0.863  -  acc (over) = 0.882\n",
      "\tH-GCNN-True: acc = 0.804  -  acc (over) = 0.863\n",
      "\tW-GCN-A-False: acc = 0.824  -  acc (over) = 0.843\n",
      "\tW-GCN-A-True: acc = 0.039  -  acc (over) = 0.039\n",
      "- RUN: 8\n",
      "\tKipf-none: acc = 0.569  -  acc (over) = 0.569\n",
      "\tKipf-both: acc = 0.588  -  acc (over) = 0.667\n",
      "\tA-GCNN-False: acc = 0.588  -  acc (over) = 0.627\n",
      "\tA-GCNN-True: acc = 0.059  -  acc (over) = 0.059\n",
      "\tH-GCNN-False: acc = 0.843  -  acc (over) = 0.863\n",
      "\tH-GCNN-True: acc = 0.843  -  acc (over) = 0.843\n",
      "\tW-GCN-A-False: acc = 0.824  -  acc (over) = 0.882\n",
      "\tW-GCN-A-True: acc = 0.059  -  acc (over) = 0.059\n",
      "- RUN: 9\n",
      "\tKipf-none: acc = 0.451  -  acc (over) = 0.451\n",
      "\tKipf-both: acc = 0.608  -  acc (over) = 0.627\n",
      "\tA-GCNN-False: acc = 0.804  -  acc (over) = 0.824\n",
      "\tA-GCNN-True: acc = 0.020  -  acc (over) = 0.020\n",
      "\tH-GCNN-False: acc = 0.784  -  acc (over) = 0.863\n",
      "\tH-GCNN-True: acc = 0.843  -  acc (over) = 0.843\n",
      "\tW-GCN-A-False: acc = 0.765  -  acc (over) = 0.804\n",
      "\tW-GCN-A-True: acc = 0.020  -  acc (over) = 0.020\n",
      "- RUN: 10\n",
      "\tKipf-none: acc = 0.451  -  acc (over) = 0.451\n",
      "\tKipf-both: acc = 0.510  -  acc (over) = 0.569\n",
      "\tA-GCNN-False: acc = 0.902  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.020  -  acc (over) = 0.020\n",
      "\tH-GCNN-False: acc = 0.882  -  acc (over) = 0.902\n",
      "\tH-GCNN-True: acc = 0.843  -  acc (over) = 0.882\n",
      "\tW-GCN-A-False: acc = 0.824  -  acc (over) = 0.863\n",
      "\tW-GCN-A-True: acc = 0.020  -  acc (over) = 0.020\n",
      "- RUN: 11\n",
      "\tKipf-none: acc = 0.490  -  acc (over) = 0.549\n",
      "\tKipf-both: acc = 0.608  -  acc (over) = 0.667\n",
      "\tA-GCNN-False: acc = 0.824  -  acc (over) = 0.843\n",
      "\tA-GCNN-True: acc = 0.020  -  acc (over) = 0.020\n",
      "\tH-GCNN-False: acc = 0.824  -  acc (over) = 0.843\n",
      "\tH-GCNN-True: acc = 0.784  -  acc (over) = 0.863\n",
      "\tW-GCN-A-False: acc = 0.725  -  acc (over) = 0.824\n",
      "\tW-GCN-A-True: acc = 0.020  -  acc (over) = 0.020\n",
      "- RUN: 12\n",
      "\tKipf-none: acc = 0.588  -  acc (over) = 0.706\n",
      "\tKipf-both: acc = 0.667  -  acc (over) = 0.706\n",
      "\tA-GCNN-False: acc = 0.863  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.039  -  acc (over) = 0.039\n",
      "\tH-GCNN-False: acc = 0.863  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.902  -  acc (over) = 0.941\n",
      "\tW-GCN-A-False: acc = 0.882  -  acc (over) = 0.902\n",
      "\tW-GCN-A-True: acc = 0.039  -  acc (over) = 0.039\n",
      "- RUN: 13\n",
      "\tKipf-none: acc = 0.412  -  acc (over) = 0.412\n",
      "\tKipf-both: acc = 0.608  -  acc (over) = 0.647\n",
      "\tA-GCNN-False: acc = 0.882  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.039  -  acc (over) = 0.039\n",
      "\tH-GCNN-False: acc = 0.882  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.863  -  acc (over) = 0.863\n",
      "\tW-GCN-A-False: acc = 0.784  -  acc (over) = 0.784\n",
      "\tW-GCN-A-True: acc = 0.039  -  acc (over) = 0.039\n",
      "- RUN: 14\n",
      "\tKipf-none: acc = 0.431  -  acc (over) = 0.431\n",
      "\tKipf-both: acc = 0.529  -  acc (over) = 0.608\n",
      "\tA-GCNN-False: acc = 0.922  -  acc (over) = 0.941\n",
      "\tA-GCNN-True: acc = 0.039  -  acc (over) = 0.039\n",
      "\tH-GCNN-False: acc = 0.922  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.882  -  acc (over) = 0.902\n",
      "\tW-GCN-A-False: acc = 0.784  -  acc (over) = 0.843\n",
      "\tW-GCN-A-True: acc = 0.039  -  acc (over) = 0.039\n",
      "- RUN: 15\n",
      "\tKipf-none: acc = 0.549  -  acc (over) = 0.627\n",
      "\tKipf-both: acc = 0.529  -  acc (over) = 0.667\n",
      "\tA-GCNN-False: acc = 0.431  -  acc (over) = 0.529\n",
      "\tA-GCNN-True: acc = 0.020  -  acc (over) = 0.020\n",
      "\tH-GCNN-False: acc = 0.471  -  acc (over) = 0.588\n",
      "\tH-GCNN-True: acc = 0.824  -  acc (over) = 0.863\n",
      "\tW-GCN-A-False: acc = 0.725  -  acc (over) = 0.784\n",
      "\tW-GCN-A-True: acc = 0.020  -  acc (over) = 0.020\n",
      "- RUN: 16\n",
      "\tKipf-none: acc = 0.471  -  acc (over) = 0.529\n",
      "\tKipf-both: acc = 0.588  -  acc (over) = 0.667\n",
      "\tA-GCNN-False: acc = 0.863  -  acc (over) = 0.882\n",
      "\tA-GCNN-True: acc = 0.078  -  acc (over) = 0.078\n",
      "\tH-GCNN-False: acc = 0.588  -  acc (over) = 0.647\n",
      "\tH-GCNN-True: acc = 0.784  -  acc (over) = 0.824\n",
      "\tW-GCN-A-False: acc = 0.706  -  acc (over) = 0.765\n",
      "\tW-GCN-A-True: acc = 0.078  -  acc (over) = 0.078\n",
      "- RUN: 17\n",
      "\tKipf-none: acc = 0.471  -  acc (over) = 0.471\n",
      "\tKipf-both: acc = 0.608  -  acc (over) = 0.667\n",
      "\tA-GCNN-False: acc = 0.863  -  acc (over) = 0.882\n",
      "\tA-GCNN-True: acc = 0.039  -  acc (over) = 0.039\n",
      "\tH-GCNN-False: acc = 0.863  -  acc (over) = 0.882\n",
      "\tH-GCNN-True: acc = 0.824  -  acc (over) = 0.863\n",
      "\tW-GCN-A-False: acc = 0.824  -  acc (over) = 0.843\n",
      "\tW-GCN-A-True: acc = 0.039  -  acc (over) = 0.039\n",
      "- RUN: 18\n",
      "\tKipf-none: acc = 0.510  -  acc (over) = 0.510\n",
      "\tKipf-both: acc = 0.588  -  acc (over) = 0.706\n",
      "\tA-GCNN-False: acc = 0.804  -  acc (over) = 0.824\n",
      "\tA-GCNN-True: acc = 0.059  -  acc (over) = 0.059\n",
      "\tH-GCNN-False: acc = 0.824  -  acc (over) = 0.843\n",
      "\tH-GCNN-True: acc = 0.804  -  acc (over) = 0.824\n",
      "\tW-GCN-A-False: acc = 0.804  -  acc (over) = 0.882\n",
      "\tW-GCN-A-True: acc = 0.059  -  acc (over) = 0.059\n",
      "- RUN: 19\n",
      "\tKipf-none: acc = 0.431  -  acc (over) = 0.451\n",
      "\tKipf-both: acc = 0.588  -  acc (over) = 0.627\n",
      "\tA-GCNN-False: acc = 0.804  -  acc (over) = 0.843\n",
      "\tA-GCNN-True: acc = 0.020  -  acc (over) = 0.020\n",
      "\tH-GCNN-False: acc = 0.824  -  acc (over) = 0.863\n",
      "\tH-GCNN-True: acc = 0.804  -  acc (over) = 0.863\n",
      "\tW-GCN-A-False: acc = 0.765  -  acc (over) = 0.804\n",
      "\tW-GCN-A-True: acc = 0.020  -  acc (over) = 0.020\n",
      "- RUN: 20\n",
      "\tKipf-none: acc = 0.471  -  acc (over) = 0.569\n",
      "\tKipf-both: acc = 0.529  -  acc (over) = 0.569\n",
      "\tA-GCNN-False: acc = 0.902  -  acc (over) = 0.902\n",
      "\tA-GCNN-True: acc = 0.020  -  acc (over) = 0.020\n",
      "\tH-GCNN-False: acc = 0.922  -  acc (over) = 0.922\n",
      "\tH-GCNN-True: acc = 0.843  -  acc (over) = 0.882\n",
      "\tW-GCN-A-False: acc = 0.804  -  acc (over) = 0.863\n",
      "\tW-GCN-A-True: acc = 0.020  -  acc (over) = 0.020\n"
     ]
    }
   ],
   "source": [
    "N_RUNS = 20\n",
    "\n",
    "best_accs = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs = np.zeros((len(EXPS), N_RUNS))\n",
    "best_val_accs2 = np.zeros((len(EXPS), N_RUNS))\n",
    "for i in range(N_RUNS):\n",
    "    A, feat, labels, n_class, masks = utils.get_data_dgl(dataset_name, dev=device, idx=i%10)\n",
    "    print(f'- RUN: {i+1}')\n",
    "    for j, exp in enumerate(EXPS):\n",
    "        # t_i = time.time()\n",
    "        if exp['name'] == 'Kipf':\n",
    "            arch = GCNN_2L(IN_DIM, HID_DIM, OUT_DIM, act=ACT, last_act=LAST_ACT,\n",
    "                           dropout=DROPOUT, norm=exp['norm'])\n",
    "            S = dgl.from_networkx(nx.from_numpy_array(A)).add_self_loop().to(device)\n",
    "            \n",
    "        elif exp['name'] == 'A-GCNN':\n",
    "            arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "            dropout=DROPOUT, diff_layer=GFGCNLayer, init_h0=h0)\n",
    "            if exp['norm']:\n",
    "                S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "            else:\n",
    "                S = torch.Tensor(A).to(device)\n",
    "\n",
    "        elif exp['name'] == 'H-GCNN':\n",
    "            arch = GFGCN_Spows(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                               dropout=DROPOUT, norm=exp['norm'], dev=device)\n",
    "            S = torch.Tensor(A).to(device)\n",
    "\n",
    "        elif exp['name'] == 'W-GCN-A':\n",
    "            arch = GFGCN(IN_DIM, HID_DIM, OUT_DIM, N_LAYERS, K, act=ACT, last_act=LAST_ACT,\n",
    "                         dropout=DROPOUT, diff_layer=GFGCN_noh_Layer)\n",
    "            if exp['norm']:\n",
    "                S = torch.Tensor(normalize_gso(A, 'both')).to(device)\n",
    "            else:\n",
    "                S = torch.Tensor(A).to(device)  \n",
    "            \n",
    "        else:\n",
    "            raise Exception(f'ERROR: Unknown architecture: {exp[\"name\"]}')\n",
    "\n",
    "        if exp['name'] in ['Kipf', 'W-GCN-A']:\n",
    "            model = NodeClassModel(arch, S, masks, LOSS_FN, device=device)\n",
    "        else:\n",
    "            model = GF_NodeClassModel(arch, S, K, masks, LOSS_FN, device=device)\n",
    "\n",
    "        loss, acc = model.train(feat, labels, N_EPOCHS, LR, WD)\n",
    "\n",
    "        best_accs[j,i] = np.max(acc[\"test\"])\n",
    "        best_val_accs[j,i] = model.test(feat, model.S, labels, masks['test'])\n",
    "        best_val_accs2[j,i] = acc[\"test\"][np.argmax(acc[\"val\"])]\n",
    "\n",
    "        print(f'\\t{exp[\"name\"]}-{exp[\"norm\"]}: acc = {best_val_accs[j,i]:.3f}  -  acc (over) = {best_accs[j,i]:.3f}')\n",
    "\n",
    "\n",
    "# Print results\n",
    "index_name = [f'{exp[\"name\"]}-{exp[\"norm\"]}' for exp in EXPS]\n",
    "table_comp_over = summary_table(best_accs, index_name)\n",
    "table_comp = summary_table(best_val_accs, index_name)\n",
    "table_comp2 = summary_table(best_val_accs2, index_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kipf-none</th>\n",
       "      <td>0.505882</td>\n",
       "      <td>0.480392</td>\n",
       "      <td>0.077841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kipf-both</th>\n",
       "      <td>0.581373</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.038361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-False</th>\n",
       "      <td>0.820588</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.113026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-True</th>\n",
       "      <td>0.037255</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.018498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-False</th>\n",
       "      <td>0.826471</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.107087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-True</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>0.034018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-False</th>\n",
       "      <td>0.782353</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.045943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-True</th>\n",
       "      <td>0.037255</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.018498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean accs       med       std\n",
       "Kipf-none       0.505882  0.480392  0.077841\n",
       "Kipf-both       0.581373  0.588235  0.038361\n",
       "A-GCNN-False    0.820588  0.852941  0.113026\n",
       "A-GCNN-True     0.037255  0.039216  0.018498\n",
       "H-GCNN-False    0.826471  0.862745  0.107087\n",
       "H-GCNN-True     0.837255  0.843137  0.034018\n",
       "W-GCN-A-False   0.782353  0.784314  0.045943\n",
       "W-GCN-A-True    0.037255  0.039216  0.018498"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kipf-none</th>\n",
       "      <td>0.506863</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.083021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kipf-both</th>\n",
       "      <td>0.585294</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.046514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-False</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.125183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-True</th>\n",
       "      <td>0.037255</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.018498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-False</th>\n",
       "      <td>0.818627</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.104286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-True</th>\n",
       "      <td>0.837255</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.039265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-False</th>\n",
       "      <td>0.772549</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.050144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-True</th>\n",
       "      <td>0.037255</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.018498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean accs       med       std\n",
       "Kipf-none       0.506863  0.490196  0.083021\n",
       "Kipf-both       0.585294  0.588235  0.046514\n",
       "A-GCNN-False    0.800000  0.833333  0.125183\n",
       "A-GCNN-True     0.037255  0.039216  0.018498\n",
       "H-GCNN-False    0.818627  0.852941  0.104286\n",
       "H-GCNN-True     0.837255  0.833333  0.039265\n",
       "W-GCN-A-False   0.772549  0.784314  0.050144\n",
       "W-GCN-A-True    0.037255  0.039216  0.018498"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_comp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean accs</th>\n",
       "      <th>med</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Kipf-none</th>\n",
       "      <td>0.540196</td>\n",
       "      <td>0.539216</td>\n",
       "      <td>0.098913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kipf-both</th>\n",
       "      <td>0.646078</td>\n",
       "      <td>0.656863</td>\n",
       "      <td>0.038461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-False</th>\n",
       "      <td>0.849020</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.097074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A-GCNN-True</th>\n",
       "      <td>0.037255</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.018498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-False</th>\n",
       "      <td>0.860784</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.086118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>H-GCNN-True</th>\n",
       "      <td>0.869608</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.030550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-False</th>\n",
       "      <td>0.828431</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.045087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W-GCN-A-True</th>\n",
       "      <td>0.037255</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.018498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean accs       med       std\n",
       "Kipf-none       0.540196  0.539216  0.098913\n",
       "Kipf-both       0.646078  0.656863  0.038461\n",
       "A-GCNN-False    0.849020  0.882353  0.097074\n",
       "A-GCNN-True     0.037255  0.039216  0.018498\n",
       "H-GCNN-False    0.860784  0.882353  0.086118\n",
       "H-GCNN-True     0.869608  0.862745  0.030550\n",
       "W-GCN-A-False   0.828431  0.833333  0.045087\n",
       "W-GCN-A-True    0.037255  0.039216  0.018498"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_comp_over"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
